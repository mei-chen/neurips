





<!DOCTYPE html>
<html lang="en" style="scroll-padding-top: 70px;"> 

<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="/static/virtual/js/virtual.js"></script>
    <meta name="google-site-verification" content="0jwPnVXIAk4FvFdT37dwMmd-kjHF86e5DKwvqlStUW0">


    
    <link rel="stylesheet" href="/static/core/css/core.css" type="text/css">
    <link rel="stylesheet" href="/static/virtual/css/virtual.css" type="text/css">
     <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">

    <link rel="stylesheet" href="/static/core/css/custom.css" type="text/css">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.14.0-beta2/dist/css/bootstrap-select.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Exo:wght@400;700&family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
      "tex2jax": {
        "inlineMath": [["$","$"], ["\\(","\\)"]],
        "displayMath": [["\\[","\\]"]],
        "processEscapes": true
      }
    }
    );
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <!--This script keeps local links inside the web app rather than opening them
in Safari, and has nothing to do with editing or Aloha.-->

<script >
	(function(document,navigator,standalone) {
		// prevents links from apps from opening in mobile safari
		// this javascript must be the first script in your <head>
		if ((standalone in navigator) && navigator[standalone]) {
			var curnode, location=document.location, stop=/^(a|html)$/i;
			document.addEventListener('click', function(e) {
				curnode=e.target;
				while (!(stop).test(curnode.nodeName)) {
					curnode=curnode.parentNode;
				}
				// Conditions to do this only on links to your own app
				// if you want all links, use if('href' in curnode) instead.
				if(
					'href' in curnode && // is a link
					(chref=curnode.href).replace(location.href,'').indexOf('#') && // is not an anchor
					(	!(/^[a-z\+\.\-]+:/i).test(chref) ||                       // either does not have a proper scheme (relative links)
						chref.indexOf(location.protocol+'//'+location.host)===0 ) // or is in the same protocol and domain
				) {
					e.preventDefault();
					location.href = curnode.href;
				}
			},false);
		}
	})(document,window.navigator,'standalone');
</script>        

<!-- This style sets the minimum size of a blurb to 260 px unless there is a
template context variable blurb_min_height that sets it otherwise. If blurbs
aren't all about the same size, they don't flow well when the window is
resized.-->


<style>
/*This is here rather that in a .css file for a reason.*/
    @media screen and (min-width: 767px) {
        .blurb {
            min-height:260px;
        }
    }
</style>
    

<script src="https://code.jquery.com/jquery-3.6.1.min.js"
        integrity="sha256-o88AwQnZB+VDvE9tvIXrMQaPlFFSUTR+nldQm1LuPXQ=" crossorigin="anonymous">
</script>

<script>
    if (typeof jQuery === 'undefined') {
        var script = document.createElement('script');
        script.type = 'text/javascript';
        script.src = "/static/core/js/jquery-3.6.1.min.js";
        document.head.appendChild(script);
    }
</script>


    <script>
        var $ = jQuery;
        /*Store a pointer to jquery2, so I can reference it later.  Aloha loads jquery 1.7 and much
        of bootstrap 3 is not compatible. This comment is deprecated. */
    </script>

    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4" crossorigin="anonymous"></script>

    <script src="/static/core/js/ajax-csrf-snippet.js"></script>
    <script src="https://kit.fontawesome.com/be44b7e05d.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.14.0-beta3/dist/js/bootstrap-select.min.js"></script>


    <style>
        body {
            font-family: Exo;}
    </style>








        


    <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">
    <link href="https://fonts.googleapis.com/css2?family=Exo:wght@400;700&family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/static/virtual/css/virtual.css">
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/corejs-typeahead/1.3.1/typeahead.bundle.min.js" integrity="sha512-lEb9Vp/rkl9g2E/LdHIMFTqz21+LA79f84gqP75fbimHqVTu6483JG1AwJlWLLQ8ezTehty78fObKupq3HSHPQ==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script>
    <script src="/static/core/js/ajax-csrf-snippet.js" ></script>
    <script src="/static/virtual/js/virtual.js"></script>
    


    
    <title>Track: Poster Session 1 East</title>
    <script src='https://slideslive.com/embed_presentation.js'></script>

    <title>NeurIPS 2024</title>
</head>

<body>




<div class="noprint">
    
        <!--Navbar start-->
<header>
    <a href="#child-menu" class="off-screen">Skip to yearly menu bar</a>
    <a href="#main" class="off-screen">Skip to main content</a>
    <div id="id_navbar" class="navbar navbar-expand-sm navbar-dark" aria-label="Main Navigation"
         style="background-color:#212529">
        <h2 class="off-screen">Main Navigation</h2>
        <div class="container-fluid">
            <div><a class="navbar-brand" href="/">

                <img src="/static/core/img/neurips-navbar-logo.svg" alt="conference_logo" height="40"></a></div>


            <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                    data-bs-target="#navbarToggler1"
                    aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarToggler1">
                <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                    
    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" 
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            NeurIPS
        </a>
        <ul class="dropdown-menu dropdown-menu-dark">
            
    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/FAQ">
                    <span >
                        Help/FAQ
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Help/Contact">
                    <span >
                        Contact NeurIPS
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Conferences/2023/EthicsGuidelines">
                    <span >
                        Code of Ethics
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/public/CodeOfConduct">
                    <span >
                        Code of Conduct
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Profile/create">
                    <span >
                        Create Profile
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/public/JournalToConference">
                    <span >
                        Journal To Conference Track
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/public/DiversityInclusion">
                    <span >
                        Diversity &amp; Inclusion
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="https://proceedings.neurips.cc/">
                    <span >
                        Proceedings
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Conferences/FutureMeetings">
                    <span >
                        Future Meetings
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Conferences/2024/Press">
                    <span >
                        Press
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Exhibitors/exhibitorinfo">
                    <span >
                        Exhibitor Information
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/public/PrivacyPolicy">
                    <span >
                        Privacy Policy
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Downloads">
                    <span >
                        Downloads
                    </span>
                </a>
                
            </li>

        

    



        </ul>
    </li>
    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/MyStuff">
                    <span >
                        My Stuff
                    </span>
                </a>
                
            </li>

        

    



                </ul>

                <form class="d-flex mx-2" aria-label="Search" role="search" action="/search">
                    <div class="input-group" role="search" style="outline-color:green;">
                        <input type="text" class="form-control" placeholder="Search" name="q"
                               value=""
                               aria-label="Search" aria-describedby="btnGroupAddon"
                                id="navbar-search">
                        <div class="input-group-text btn-primary" id="btnGroupAddon">
                            <button style="border: none; background-color: transparent; padding: 0;" type="submit">
                                <i class="fa-solid fa-magnifying-glass"></i>
                            </button>
                        </div>
                    </div>
                </form>

                
                    
                    <div class="btn-group d-none d-sm-block nav-item" role="group"
                         aria-label="Button group with nested dropdown">
                        <div class="btn-group" role="group">
                            <button type="button" class="btn btn-light dropdown-toggle" data-bs-toggle="dropdown"
                               id="id_navbar_username" aria-expanded="false">
                                Mei
                            </button>
                            <ul class="dropdown-menu dropdown-menu-end dropdown-menu-dark">
                                
                                <li>
                                    <a class="dropdown-item" href="/MyStuff"> <i class="fa-regular fa-user"></i> &nbsp;My
                                        Stuff</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li>
                                    <a class="dropdown-item" href="/virtual/2024/mycalendar"> <i class="fa-regular fa-user"></i> &nbsp;My
                                        Bookmarks</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li>
                                    <a class="dropdown-item" href="/Profile/change-password"> <i
                                            class="fa-solid fa-lock"></i> &nbsp;Change Password</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li>
                                    <a class="dropdown-item" href="/resetpassword"> <i class="fa-solid fa-unlock"></i>
                                        &nbsp;Reset Password</a>
                                </li>

                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/EditProfile"> <i
                                        class="fa-regular fa-address-card"></i> &nbsp;Edit Profile </a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/MergeAccounts"> <i
                                        class="fa-solid fa-code-merge"></i> &nbsp;Merge Profiles </a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item"
                                       href="/set_timezone?nextp=/virtual/2024/session/108362"> <i
                                        class="fa-solid fa-earth-americas"></i>
                                    &nbsp;TZ: America/Chicago</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/logout"> <i
                                        class="fa-solid fa-right-from-bracket"></i> &nbsp;Log Out</a></li>
                            </ul>
                        </div>
                    </div>
                    <br>
                    
                    <div class="btn-group d-block d-sm-none " role="group"
                         aria-label="Button group with nested dropdown">

                        <div class="btn-group" role="group">
                            <button class="btn btn-light dropdown-toggle" data-bs-toggle="dropdown"
                                    aria-expanded="false">
                                Mei
                            </button>
                            <ul class="dropdown-menu dropdown-menu-dark">

                                <li>
                                    <a class="dropdown-item" href="/MyStuff"> <span
                                            class="glyphicon glyphicon-cog"></span> My Stuff</a>
                                </li>
                                <li>
                                    <a class="dropdown-item" href="/virtual/2024/mycalendar"> <span
                                            class="glyphicon glyphicon-cog"></span> My Bookmarks</a>
                                </li>
                                <li>
                                    <a class="dropdown-item" href="/Profile/change-password"> <span
                                            class="glyphicon glyphicon-cog"></span> Change Password</a>
                                </li>

                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/EditProfile"><span
                                        class="fa-solid fa-pen-to-square"></span> Edit Profile </a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item"
                                       href="/set_timezone?nextp=/virtual/2024/session/108362">TZ: America/Chicago</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/logout"><span
                                        class="fa-solid fa-right-from-bracket"></span> Log Out</a></li>
                            </ul>
                        </div>
                    </div>
                

            </div>
        </div>
    </div>
</header>
<!--Navbar end-->
    
</div><!--noprint div-->

<!--This holds the whole page including the navbar-->

<main id="main">
    
        <div class="container-fluid">
            <!--Navbar start-->

<div class="dropdown" id="child-menu">
    <nav class="align-middle navbar navbar-expand-md  rounded-bottom"
         style="min-height: 57px; background-image: url(/static/virtual/img/navbackground.png); background-repeat: repeat-x;">
        <div class="container-fluid">

            <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                    data-bs-target="#navbarToggler987"
                    aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarToggler987">
                <ul class="navbar-nav me-auto mb-lg-0">
                    


    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle border-3  btn btn-primary text-white p-1" style= "background-color: #070bff; font-size: 1.2 em;"
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Select Year: (2024)
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2024">2024
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2023">2023
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2022">2022
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2021">2021
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2020">2020
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2019">2019
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2018">2018
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2017">2017
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2016">2016
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2015">2015
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2014">2014
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2013">2013
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2012">2012
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2011">2011
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2010">2010
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2009">2009
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2008">2008
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2007">2007
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2006">2006
                </a>
                
            </li>
        

    



        </ul>
    </li>
    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/index.html">
                    <span >
                        Start Here
                    </span>
                </a>
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/calendar">
                    <span >
                        Schedule
                    </span>
                </a>
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/tutorial">
                    <span >
                        Tutorials
                    </span>
                </a>
                
            </li>

        

    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" 
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Main Conference
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/eventlistwithbios/Invited%20Talk">
                    <span >
                        Invited Talks
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/oral">
                    <span >
                        Orals
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/spotlight-posters-2024">
                    <span >
                        Spotlights
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/papers.html">
                    <span >
                        Papers
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="https://neurips2024.vizhub.ai">
                    <span >
                        Paper Visualization
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/Competition">
                    <span >
                        Competitions
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/datasets-benchmarks-2024">
                    <span >
                        Datasets &amp; Benchmarks
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/journal_track_2024">
                    <span >
                        Journal Track
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/creative-ai-2024">
                    <span >
                        Creative AI Track
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/awards_detail">
                    <span >
                        Outstanding Paper Awards
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/affinity%20workshop">
                    <span >
                        Affinity Workshops
                    </span>
                </a>
                
            </li>

        

    



        </ul>
    </li>
    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" 
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Community
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/affinity_events">
                    <span >
                        Affinity Events
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/community?show=mentorship">
                    <span >
                        Mentorship Event
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/session/101095">
                    <span >
                        Bridging the Future
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/social">
                    <span >
                        Socials
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/careers">
                    <span >
                        Careers
                    </span>
                </a>
                
            </li>

        

    



        </ul>
    </li>
    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/workshop">
                    <span >
                        Workshops
                    </span>
                </a>
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/sponsor_list">
                    <span >
                        Exhibitors
                    </span>
                </a>
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/search">
                    <span >
                        <i class="fas fa-search"></i>
                    </span>
                </a>
                
            </li>

        

    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" 
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Help
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/FAQ">
                    <span >
                        FAQ
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="https://chat.neurips.cc/channel/HelpDesk">
                    <span >
                        Helpdesk in RocketChat
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/organizers">
                    <span >
                        Organizers
                    </span>
                </a>
                
            </li>

        

    



        </ul>
    </li>
    



                </ul>
            </div>
        </div>
    </nav>
</div>
    <!--Navbar end-->
        </div>
        <br><br>
    
    
        
        <div class="container">
    
    

    

    

        <div class="container">
            
                

    <span id="the-bookmark-1" class="bump20 bookmark-right fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='the-bookmark-1', event_id='108362', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>



            
            <!-- Title -->
            <div class="" style="">
                <div class="card-header">
                    <h3 class="text-center ">Poster Session</h3>
                    <h2 class="card-title main-title text-center" style="">
                        Poster Session 1 East
                    </h2>
                    
                        <h5 class="text-center text-muted">East Exhibit Hall A-C</h5>
                    
                    


                    
                    

                    


                    <div class="text-center">
                        

                        <div>
                            
                        </div>
                        <div>
                            
                        </div>
                        <div>
                            
                        </div>

                        
                        
                        

                        

                        

                        

                            
                                <div>Wed 11 Dec 1 p.m. CST 
                                    &mdash; 4 p.m. CST  

    <span id="bookmark-number-0" class="bump20 " title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-0', event_id='108362', bookmark_event_number='1',
                  alt_bookmark_element_id='the-bookmark-1');">
        
            <span class="green">(Bookmark)</span>
        
    </span>



                                    

                                </div>
                                <div class="schedule-html-detail"></div>

                            

                            

                        
                        

                        
                    </div>
                    <div class=" text-center text-muted text-monospace ">
                        <div> 
                        </div>
                    </div>
                </div>
            </div>
            <div id="details" class="pp-card m-3 collapse">
                <div class="card-body p-3">

                    <div id="abstractExample">
                        <span class="font-weight-bold">Abstract:</span> 
                    </div>


                </div>
            </div>
        </div>
        <!-- SlidesLive -->

        
            <div class="container">
                <div class="col-xs-12 my-auto p-2 admin centered">
                    
                </div>
            </div>

            

            
         

        <!--Children in this session -->
        <p></p>

        <div class="container" style="padding-bottom: 30px; padding-top:30px">
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-1" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-1', event_id='93753', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1000</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93753">SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering</a></strong></h5>


                        <p class="text-muted">
                            John Yang &middot; Carlos Jimenez &middot; Alexander Wettig &middot; Kilian Lieret &middot; Shunyu Yao &middot; Karthik Narasimhan &middot; Ofir Press
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Language model agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that language model agents represent a new category of end users with their own needs and abilities, and would benefit from specially built interfaces to the software they use. We investigate how the role of interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates language model agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive language models. Finally, we provide insight on how the design of the agent-computer interface can impact agents' behavior and performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-2" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-2', event_id='94171', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1001</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94171">GFlowNet Assisted Biological Sequence Editing</a></strong></h5>


                        <p class="text-muted">
                            Pouya M. Ghari &middot; Alex Tseng &middot; Gokcen Eraslan &middot; Romain Lopez &middot; Tommaso Biancalani &middot; Gabriele Scalia &middot; Ehsan Hajiramezanali
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Editing biological sequences has extensive applications in synthetic biology and medicine, such as designing regulatory elements for nucleic-acid therapeutics and treating genetic disorders. The primary objective in biological-sequence editing is to determine the optimal modifications to a sequence which augment certain biological properties while adhering to a minimal number of alterations to ensure predictability and potentially support safety. In this paper, we propose GFNSeqEditor, a novel biological-sequence editing algorithm which builds on the recently proposed area of generative flow networks (GFlowNets). Our proposed GFNSeqEditor identifies elements within a starting seed sequence that may compromise a desired biological property. Then, using a learned stochastic policy, the algorithm makes edits at these identified locations, offering diverse modifications for each sequence to enhance the desired property. The number of edits can be regulated through specific hyperparameters. We conducted extensive experiments on a range of real-world datasets and biological applications, and our results underscore the superior performance of our proposed algorithm compared to existing state-of-the-art sequence editing methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-3" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-3', event_id='94325', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1002</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94325">Implicitly Guided Design with PropEn: Match your Data to Follow the Gradient</a></strong></h5>


                        <p class="text-muted">
                            Nata≈°a Tagasovska &middot; Vladimir Gligorijevic &middot; Kyunghyun Cho &middot; Andreas Loukas
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Across scientific domains, generating new models or optimizing existing ones while meeting specific criteria is crucial. Traditional machine learning frameworks for guided design use a generative model and a surrogate model (discriminator), requiring large datasets. However, real-world scientific applications often have limited data and complex landscapes, making data-hungry models inefficient or impractical. We propose a new framework, PropEn, inspired by ``matching'', which enables implicit guidance without training a discriminator. By matching each sample with a similar one that has a better property value, we create a larger training dataset that inherently indicates the direction of improvement. Matching, combined with an encoder-decoder architecture, forms a domain-agnostic generative framework for property enhancement. We show that training with a matched dataset approximates the gradient of the property of interest while remaining within the data distribution, allowing efficient design optimization. Extensive evaluations in toy problems and scientific applications, such as therapeutic protein design and airfoil optimization, demonstrate PropEn's advantages over common baselines. Notably, the protein design results are validated with wet lab experiments, confirming the competitiveness and effectiveness of our approach. Our code is available at https://github.com/prescient-design/propen.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-4" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-4', event_id='94445', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1003</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94445">How Does Message Passing Improve Collaborative Filtering?</a></strong></h5>


                        <p class="text-muted">
                            Mingxuan Ju &middot; William Shiao &middot; Zhichun Guo &middot; Yanfang Ye &middot; Yozen Liu &middot; Neil Shah &middot; Tong Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Collaborative filtering (CF) has exhibited prominent results for recommender systems and been broadly utilized for real-world applications.A branch of research enhances CF methods by message passing (MP) used in graph neural networks, due to its strong capabilities of extracting knowledge from graph-structured data, like user-item bipartite graphs that naturally exist in CF. They assume that MP helps CF methods in a manner akin to its benefits for graph-based learning tasks in general (e.g., node classification). However, even though MP empirically improves CF, whether or not this assumption is correct still needs verification. To address this gap, we formally investigate why MP helps CF from multiple perspectives and show that many assumptions made by previous works are not entirely accurate. With our curated ablation studies and theoretical analyses, we discover that (i) MP improves the CF performance primarily by additional representations passed from neighbors during the forward pass instead of additional gradient updates to neighbor representations during the model back-propagation and (ii) MP usually helps low-degree nodes more than high-degree nodes.}Utilizing these novel findings, we present Test-time Aggregation for Collaborative Filtering, namely TAG-CF, a test-time augmentation framework that only conducts MP once at inference time. The key novelty of TAG-CF is that it effectively utilizes graph knowledge while circumventing most of notorious computational overheads of MP. Besides, TAG-CF is extremely versatile can be used as a plug-and-play module to enhance representations trained by different CF supervision signals. Evaluated on six datasets (i.e., five academic benchmarks and one real-world industrial dataset), TAG-CF consistently improves the recommendation performance of CF methods without graph by up to 39.2% on cold users and 31.7% on all users, with little to no extra computational overheads. Furthermore, compared with trending graph-enhanced CF methods, TAG-CF delivers comparable or even better performance with less than 1% of their total training times. Our code is publicly available at https://github.com/snap-research/Test-time-Aggregation-for-CF.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-5" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-5', event_id='94551', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1004</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94551">MotionTTT: 2D Test-Time-Training Motion Estimation for 3D Motion Corrected MRI</a></strong></h5>


                        <p class="text-muted">
                            Tobit Klug &middot; Kun Wang &middot; Stefan Ruschke &middot; Reinhard Heckel
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>A major challenge of the long measurement times in magnetic resonance imaging (MRI), an important medical imaging technology, is that patients may move during data acquisition. This leads to severe motion artifacts in the reconstructed images and volumes. In this paper, we propose MotionTTT a deep learning-based test-time-training (TTT) method for accurate motion estimation. The key idea is that a neural network trained for motion-free reconstruction has a small loss if there is no motion, thus optimizing over motion parameters passed through the reconstruction network enables accurate estimation of motion. The estimated motion parameters enable to correct for the motion and to reconstruct accurate motion-corrected images. Our method uses 2D reconstruction networks to estimate rigid motion in 3D, and constitutes the first deep learning based method for 3D rigid motion estimation towards 3D-motion-corrected MRI. We show that our method can provably reconstruct motion parameters for a simple signal and neural network model. We demonstrate the effectiveness of our method for both retrospectively simulated motion and prospectively collected real motion-corrupted data. Code is available at \url{https://github.com/MLI-lab/MRI_MotionTTT}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-6" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-6', event_id='94962', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1005</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94962">Coupled Mamba: Enhanced Multimodal Fusion with Coupled State Space Model</a></strong></h5>


                        <p class="text-muted">
                            Wenbing Li &middot; Hang Zhou &middot; Junqing Yu &middot; Zikai Song &middot; Wei Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The essence of multi-modal fusion lies in exploiting the complementary information inherent in diverse modalities.However, most prevalent fusion methods rely on traditional neural architectures and are inadequately equipped to capture the dynamics of interactions across modalities, particularly in presence of complex intra- and inter-modality correlations.Recent advancements in State Space Models (SSMs), notably exemplified by the Mamba model, have emerged as promising contenders. Particularly, its state evolving process implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an appealing direction. However, fusing multiple modalities is challenging for SSMs due to its hardware-aware parallelism designs. To this end, this paper proposes the Coupled SSM model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes. Specifically, in our coupled scheme, we devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step. To fully comply with the hardware-aware parallelism, we obtain the global convolution kernel by deriving the state equation while introducing the historical state.Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2 through multi-domain input verify the effectiveness of our model compared to current state-of-the-art methods, improved F1-Score by 0.4%, 0.9%, and 2.3% on the three datasets respectively, 49% faster inference and 83.7% GPU memory save. The results demonstrate that Coupled Mamba model is capable of enhanced multi-modal fusion.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-7" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-7', event_id='95091', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1006</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95091">Information Re-Organization Improves Reasoning in Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Xiaoxia Cheng &middot; Zeqi Tan &middot; Wei Xue &middot; Weiming Lu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Improving the reasoning capabilities of large language models (LLMs) has attracted considerable interest. Recent approaches primarily focus on improving the reasoning process to yield a more precise final answer. However, in scenarios involving contextually aware reasoning, these methods neglect the importance of first identifying logical relationships from the context before proceeding with the reasoning. This oversight could lead to a superficial understanding and interaction with the context, potentially undermining the quality and reliability of the reasoning outcomes. In this paper, we propose an information re-organization (\textbf{InfoRE}) method before proceeding with the reasoning to enhance the reasoning ability of LLMs. Our re-organization method involves initially extracting logical relationships from the contextual content, such as documents or paragraphs, and subsequently pruning redundant content to minimize noise. Then, we utilize the re-organized information in the reasoning process. This enables LLMs to deeply understand the contextual content by clearly perceiving these logical relationships, while also ensuring high-quality responses by eliminating potential noise. To demonstrate the effectiveness of our approach in improving the reasoning ability, we conduct experiments using Llama2-70B, GPT-3.5, and GPT-4 on various contextually aware multi-hop reasoning tasks. Using only a zero-shot setting, our method achieves an average absolute improvement of 4\% across all tasks, highlighting its potential to improve the reasoning performance of LLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-8" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-8', event_id='95237', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1007</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95237">Learning Commonality, Divergence and Variety for Unsupervised Visible-Infrared Person Re-identification</a></strong></h5>


                        <p class="text-muted">
                            Jiangming Shi &middot; Xiangbo Yin &middot; Yachao Zhang &middot; zhizhong zhang &middot; Yuan Xie &middot; Yanyun Qu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match specified persons in infrared images to visible images without annotations, and vice versa. USVI-ReID is a challenging yet underexplored task. Most existing methods address the USVI-ReID through cluster-based contrastive learning, which simply employs the cluster center to represent an individual. However, the cluster center primarily focuses on commonality, overlooking divergence and variety. To address the problem, we propose a Progressive Contrastive Learning with Hard and Dynamic Prototypes for USVI-ReID. In brief, we generate the hard prototype by selecting the sample with the maximum distance from the cluster center. We reveal that the inclusion of the hard prototype in contrastive loss helps to emphasize divergence. Additionally, instead of rigidly aligning query images to a specific prototype, we generate the dynamic prototype by randomly picking samples within a cluster. The dynamic prototype is used to encourage variety. Finally, we introduce a progressive learning strategy to gradually shift the model's attention towards divergence and variety, avoiding cluster deterioration. Extensive experiments conducted on the publicly available SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-9" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-9', event_id='95937', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1008</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95937">Learning Plaintext-Ciphertext Cryptographic Problems via ANF-based SAT Instance Representation</a></strong></h5>


                        <p class="text-muted">
                            Xinhao Zheng &middot; Yang Li &middot; Cunxin Fan &middot; Huaijin Wu &middot; Xinhao Song &middot; Junchi Yan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Cryptographic problems, operating within binary variable spaces, can be routinely transformed into Boolean Satisfiability (SAT) problems regarding specific cryptographic conditions like plaintext-ciphertext matching. With the fast development of learning for discrete data, this SAT representation also facilitates the utilization of machine-learning approaches with the hope of automatically capturing patterns and strategies inherent in cryptographic structures in a data-driven manner. Existing neural SAT solvers consistently adopt conjunctive normal form (CNF) for instance representation, which in the cryptographic context can lead to scale explosion and a loss of high-level semantics. In particular, extensively used XOR operations in cryptographic problems can incur an exponential number of clauses. In this paper, we propose a graph structure based on Arithmetic Normal Form (ANF) to efficiently handle the XOR operation bottleneck. Additionally, we design an encoding method for AND operations in these ANF-based graphs, demonstrating improved efficiency over alternative general graph forms for SAT. We then propose CryptoANFNet, a graph learning approach that trains a classifier based on a message-passing scheme to predict plaintext-ciphertext satisfiability. Using ANF-based SAT instances, CryptoANFNet demonstrates superior scalability and can naturally capture higher-order operational information. Empirically, CryptoANFNet achieves a 50x speedup over heuristic solvers and outperforms SOTA learning-based SAT solver NeuroSAT, with 96\% vs. 91\% accuracy on small-scale and 72\% vs. 55\% on large-scale datasets from real encryption algorithms. We also introduce a key-solving algorithm that simplifies ANF-based SAT instances from plaintext and ciphertext, enhancing key decryption accuracy from 76.5\% to 82\% and from 72\% to 75\% for datasets generated from two real encryption algorithms.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-10" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-10', event_id='96856', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1009</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96856">Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf</a></strong></h5>


                        <p class="text-muted">
                            Xuanfa Jin &middot; Ziyan Wang &middot; Yali Du &middot; Meng Fang &middot; Haifeng Zhang &middot; Jun Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Communication is a fundamental aspect of human society, facilitating the exchange of information and beliefs among people. Despite the advancements in large language models (LLMs), recent agents built with these often neglect the control over discussion tactics, which are essential in communication scenarios and games. As a variant of the famous communication game Werewolf, <em>One Night Ultimate Werewolf</em> (ONUW) requires players to develop strategic discussion policies due to the potential role changes that increase the uncertainty and complexity of the game. In this work, we first present the existence of the Perfect Bayesian Equilibria (PBEs) in two scenarios of the ONUW game: one with discussion and one without. The results showcase that the discussion greatly changes players' utilities by affecting their beliefs, emphasizing the significance of discussion tactics. Based on the insights obtained from the analyses, we propose an RL-instructed language agent framework, where a discussion policy trained by reinforcement learning (RL) is employed to determine appropriate discussion tactics to adopt. Our experimental results on several ONUW game settings demonstrate the effectiveness and generalizability of our proposed framework.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-11" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-11', event_id='97528', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1010</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97528">M3LEO: A Multi-Modal, Multi-Label Earth Observation Dataset Integrating Interferometric SAR and RGB Data</a></strong></h5>


                        <p class="text-muted">
                            Matthew Allen &middot; Francisco Dorr &middot; Joseph Alejandro Gallego Mejia &middot; Laura Mart√≠nez-Ferrer &middot; Anna Jungbluth &middot; Freddie Kalaitzis &middot; Raul Ramos-Poll√°n
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Satellite-based remote sensing has revolutionised the way we address global chal-lenges in a rapidly evolving world. Huge quantities of Earth Observation (EO) dataare generated by satellite sensors daily, but processing these large datasets for use inML pipelines is technically and computationally challenging. Specifically, differenttypes of EO data are often hosted on a variety of platforms, with differing degreesof availability for Python preprocessing tools. In addition, spatial alignment acrossdata sources and data tiling for easier handling can present significant technicalhurdles for novice users. While some preprocessed Earth observation datasets exist,their content is often limited to optical or near-optical wavelength data, which isineffective at night or in adverse weather conditions. Synthetic Aperture Radar(SAR), an active sensing technique based on microwave length radiation, offersa viable alternative. However, the application of machine learning to SAR hasbeen limited due to a lack of ML-ready data and pipelines, particularly for the fulldiversity of SAR data, including polarimetry, coherence and interferometry. In thiswork, we introduce M3LEO, a multi-modal, multi-label Earth observation datasetthat includes polarimetric, interferometric, and coherence SAR data derived fromSentinel-1, alongside Sentinel-2 RGB imagery and a suite of labelled tasks formodel evaluation. M3LEO spans 17.5TB and contains approximately 10M datachips, each measuring 4x4 km, across six diverse geographic regions. The datasetis complemented by a flexible PyTorch Lightning framework, with configurationmanagement using Hydra, to accommodate its use across diverse ML applicationsin Earth observation. Additionally, we provide tools to process any dataset availableon popular platforms such as Google Earth Engine for seamless integration withour framework. Initial experiments validate the utility of our data and framework,and show that SAR imagery contains information additional to that extractablefrom RGB data alone. Data is available at huggingface.co/M3LEO, and code atgithub.com/spaceml-org/M3LEO.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-12" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-12', event_id='97784', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1011</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97784">Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Jiahao Ying &middot; Yixin Cao &middot; Yushi Bai &middot; QIANRU SUN &middot; Bo Wang &middot; Wei Tang &middot; Zhaojun Ding &middot; Yizhe Yang &middot; Xuanjing Huang &middot; Shuicheng Yan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) have achieved impressive performance across various natural language benchmarks, prompting a continual need to curate more difficult datasets for larger LLMs, which is costly and time-consuming. In this paper, we propose to automate dataset updating and provide systematical analysis regarding its effectiveness in dealing with benchmark leakage issue, difficulty control, and stability. Thus, once current benchmark has been mastered or leaked, we can update it for timely and reliable evaluation. There are two updating strategies: 1) mimicking strategy to generate similar samples based on original data, preserving stylistic and contextual essence, and 2) extending strategy that further expands existing samples at varying cognitive levels by adapting Bloom‚Äôs taxonomy of educational objectives. Extensive experiments on updated MMLU and BIG-Bench demonstrate the stability of the proposed strategies and find that the mimicking strategy can effectively alleviate issues of overestimation from benchmark leakage. In cases where the efficient mimicking strategy fails, our extending strategy still shows promising results. Additionally, by controlling the difficulty, we can better discern the models‚Äô performance and enable fine-grained analysis ‚Äî neither too difficult nor too easy an exam can fairly judge students‚Äô learning status. To the best of our knowledge, we are the first to automate updating benchmarks for reliable and timely evaluation. Our demo leaderboard can be found at https://yingjiahao14.github.io/Automating-DatasetUpdates/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-13" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-13', event_id='93636', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1100</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93636">NeuralSteiner: Learning Steiner Tree for Overflow-avoiding Global Routing in Chip Design</a></strong></h5>


                        <p class="text-muted">
                            RUIZHI LIU &middot; ZhishengZeng &middot; Shizhe Ding &middot; Jingyan Sui &middot; Xingquan Li &middot; Dongbo Bu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Global routing plays a critical role in modern chip design. The routing paths generated by global routers often form a rectilinear Steiner tree (RST). Recent advances from the machine learning community have shown the power of learning-based route generation; however, the yielded routing paths by the existing approaches often suffer from considerable overflow, thus greatly hindering their application in practice.We propose NeuralSteiner, an accurate approach to overflow-avoiding global routing in chip design. The key idea of NeuralSteiner approach is to learn Steiner trees: we first predict the locations of highly likely Steiner points by adopting a neural network considering full-net spatial and overflow information, then select appropriate points by running a graph-based post-processing algorithm, and finally connect these points with the input pins to yield overflow-avoiding RSTs. NeuralSteiner offers two advantages over previous learning-based models. First, by using the learning scheme, NeuralSteiner ensures the connectivity of generated routes while significantly reducing congestion. Second, NeuralSteiner can effectively scale to large nets and transfer to unseen chip designs without any modifications or fine-tuning.  Extensive experiments over public large-scale benchmarks reveal that, compared with the state-of-the-art deep generative methods, NeuralSteiner achieves up to a 99.8\% reduction in overflow while speeding up the generation and maintaining a slight wirelength loss within only 1.8\%.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-14" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-14', event_id='93190', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1101</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93190">PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition</a></strong></h5>


                        <p class="text-muted">
                            Jinghui Lu &middot; Yanjie Wang &middot; Ziwei Yang &middot; Xuejing Liu &middot; Brian Mac Namee &middot; Can Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets. All resources are available at https://github.com/GeorgeLuImmortal/PaDeLLM_NER.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-15" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-15', event_id='93061', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1102</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93061">LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential Recommendation</a></strong></h5>


                        <p class="text-muted">
                            Qidong Liu &middot; Xian Wu &middot; Yejing Wang &middot; Zijian Zhang &middot; Feng Tian &middot; Yefeng Zheng &middot; Xiangyu Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Sequential recommender systems (SRS) aim to predict users' subsequent choices based on their historical interactions and have found applications in diverse fields such as e-commerce and social media. However, in real-world systems, most users interact with only a handful of items, while the majority of items are seldom consumed. These two issues, known as the long-tail user and long-tail item challenges, often pose difficulties for existing SRS. These challenges can adversely affect user experience and seller benefits, making them crucial to address. Though a few works have addressed the challenges, they still struggle with the seesaw or noisy issues due to the intrinsic scarcity of interactions. The advancements in large language models (LLMs) present a promising solution to these problems from a semantic perspective. As one of the pioneers in this field, we propose the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR). This framework utilizes semantic embeddings derived from LLMs to enhance SRS without adding extra inference load. To address the long-tail item challenge, we design a dual-view modeling framework that combines semantics from LLMs and collaborative signals from conventional SRS. For the long-tail user challenge, we propose a retrieval augmented self-distillation method to enhance user preference representation using more informative interactions from similar users. To verify the effectiveness and versatility of our proposed enhancement framework, we conduct extensive experiments on three real-world datasets using three popular SRS models. The results consistently show that our method surpasses existing baselines. The implementation code is available in Supplementary Material.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-16" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-16', event_id='97813', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1103</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97813">GV-Rep: A Large-Scale Dataset for Genetic Variant Representation Learning</a></strong></h5>


                        <p class="text-muted">
                            Zehui Li &middot; Vallijah Subasri &middot; Guy-Bart Stan &middot; Yiren Zhao &middot; Bo Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Genetic variants (GVs) are defined as differences in the DNA sequences among individuals and play a crucial role in diagnosing and treating genetic diseases. The rapid decrease in next generation sequencing cost, analogous to Moore‚Äôs Law, has led to an exponential increase in the availability of patient-level GV data. This growth poses a challenge for clinicians who must efficiently prioritize patient-specific GVs and integrate them with existing genomic databases to inform patient management. To addressing the interpretation of GVs, genomic foundation models (GFMs) have emerged. However, these models lack standardized performance assessments, leading to considerable variability in model evaluations. This poses the question: *How effectively do deep learning methods classify unknown GVs and align them with clinically-verified GVs?* We argue that representation learning, which transforms raw data into meaningful feature spaces, is an effective approach for addressing both indexing and classification challenges. We introduce a large-scale Genetic Variant dataset, named $\textsf{GV-Rep}$, featuring variable-length contexts and detailed annotations, designed for deep learning models to learn GV representations across various traits, diseases, tissue types, and experimental contexts. Our contributions are three-fold: (i) $\textbf{Construction}$ of a comprehensive dataset with 7 million records, each labeled with characteristics of the corresponding variants, alongside additional data from 17,548 gene knockout tests across 1,107 cell types, 1,808 variant combinations, and 156 unique clinically-verified GVs from real-world patients. (ii) $\textbf{Analysis}$ of the structure and properties of the dataset. (iii) $\textbf{Experimentation}$ of the dataset with pre-trained genomic foundation models (GFMs). The results highlight a significant disparity between the current capabilities of GFMs and the accurate representation of GVs. We hope this dataset will advance genomic deep learning to bridge this gap.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-17" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-17', event_id='97514', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1104</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97514">HEST-1k: A Dataset For Spatial Transcriptomics and Histology Image Analysis</a></strong></h5>


                        <p class="text-muted">
                            Guillaume Jaume &middot; Paul Doucet &middot; Andrew Song &middot; Ming Y. Lu &middot; Cristina Almagro P√©rez &middot; Sophia Wagner &middot; Anurag Vaidya &middot; Richard Chen &middot; Drew Williamson &middot; Ahrong Kim &middot; Faisal Mahmood
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Spatial transcriptomics (ST) enables interrogating the molecular composition of tissue with ever-increasing resolution, depth, and sensitivity. However, costs, rapidly evolving technology, and lack of standards have constrained computational methods in ST to narrow tasks and small cohorts. In addition, the underlying tissue morphology as reflected by H&amp;E-stained whole slide images (WSIs) encodes rich information often overlooked in ST studies. Here, we introduce HEST-1k, a collection of 1,108 spatial transcriptomic profiles, each linked to a WSI and metadata. HEST-1k was assembled from 131 public and internal cohorts encompassing 25 organs, two species (Homo Sapiens and Mus Musculus), and 320 cancer samples from 25 cancer types. HEST-1k processing enabled the identification of 1.5 million expression--morphology pairs and 60 million nuclei. HEST-1k is tested on three use cases: (1) benchmarking foundation models for histopathology, (2) biomarker identification, and (3) multimodal representation learning. We provide access to HEST website, library, and metadata in Supplemental.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-18" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-18', event_id='96613', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1105</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96613">Molecule Generation with Fragment Retrieval Augmentation</a></strong></h5>


                        <p class="text-muted">
                            Seul Lee &middot; Karsten Kreis &middot; Srimukh Veccham &middot; Meng Liu &middot; Danny Reidenbach &middot; Saee Paliwal &middot; Arash Vahdat &middot; Weili Nie
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Fragment-based drug discovery, in which molecular fragments are assembled into new molecules with desirable biochemical properties, has achieved great success. However, many fragment-based molecule generation methods show limited exploration beyond the existing fragments in the database as they only reassemble or slightly modify the given ones. To tackle this problem, we propose a new fragment-based molecule generation framework with retrieval augmentation, namely <em>Fragment Retrieval-Augmented Generation</em> (<em>f</em>-RAG). <em>f</em>-RAG is based on a pre-trained molecular generative model that proposes additional fragments from input fragments to complete and generate a new molecule. Given a fragment vocabulary, <em>f</em>-RAG retrieves two types of fragments: (1) <em>hard fragments</em>, which serve as building blocks that will be explicitly included in the newly generated molecule, and (2) <em>soft fragments</em>, which serve as reference to guide the generation of new fragments through a trainable <em>fragment injection module</em>. To extrapolate beyond the existing fragments, <em>f</em>-RAG updates the fragment vocabulary with generated fragments via an iterative refinement process which is further enhanced with post-hoc genetic fragment modification. <em>f</em>-RAG can achieve an improved exploration-exploitation trade-off by maintaining a pool of fragments and expanding it with novel and high-quality fragments through a strong generative prior.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-19" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-19', event_id='96270', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1106</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96270">Disentangling Interpretable Factors with Supervised Independent Subspace Principal Component Analysis</a></strong></h5>


                        <p class="text-muted">
                            Jiayu Su &middot; David A Knowles &middot; Ra√∫l Rabad√°n
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The success of machine learning models relies heavily on effectively representing high-dimensional data. However, ensuring data representations capture human-understandable concepts remains difficult, often requiring the incorporation of prior knowledge and decomposition of data into multiple subspaces. Traditional linear methods fall short in modeling more than one space, while more expressive deep learning approaches lack interpretability. Here, we introduce Supervised Independent Subspace Principal Component Analysis ($\texttt{sisPCA}$), a PCA extension designed for multi-subspace learning. Leveraging the Hilbert-Schmidt Independence Criterion (HSIC), $\texttt{sisPCA}$ incorporates supervision and simultaneously ensures subspace disentanglement. We demonstrate $\texttt{sisPCA}$'s connections with autoencoders and regularized linear regression and showcase its ability to identify and separate hidden data structures through extensive applications, including breast cancer diagnosis from image features, learning aging-associated DNA methylation changes, and single-cell analysis of malaria infection. Our results reveal distinct functional pathways associated with malaria colonization, underscoring the essentiality of explainable representation in high-dimensional data analysis.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-20" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-20', event_id='95842', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1107</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95842">Approximating mutual information of high-dimensional variables using learned representations</a></strong></h5>


                        <p class="text-muted">
                            Gokul Gowri &middot; Xiaokang Lun &middot; Allon Klein &middot; Peng Yin
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Mutual information (MI) is a general measure of statistical dependence with widespread application across the sciences. However, estimating MI between multi-dimensional variables is challenging because the number of samples necessary to converge to an accurate estimate scales unfavorably with dimensionality. In practice, existing techniques can reliably estimate MI in up to tens of dimensions, but fail in higher dimensions, where sufficient sample sizes are infeasible. Here, we explore the idea that underlying low-dimensional structure in high-dimensional data can be exploited to faithfully approximate MI in high-dimensional settings with realistic sample sizes. We develop a method that we call latent MI (LMI) approximation, which applies a nonparametric MI estimator to low-dimensional representations learned by a simple, theoretically-motivated model architecture. Using several benchmarks, we show that unlike existing techniques, LMI can approximate MI well for variables with $> 10^3$ dimensions if their dependence structure is captured by low-dimensional representations. Finally, we showcase LMI on two open problems in biology. First, we approximate MI between protein language model (pLM) representations of interacting proteins, and find that pLMs encode non-trivial information about protein-protein interactions. Second, we quantify cell fate information contained in single-cell RNA-seq (scRNA-seq) measurements of hematopoietic stem cells, and find a sharp transition during neutrophil differentiation when fate information captured by scRNA-seq increases dramatically. An implementation of LMI is available at *latentmi.readthedocs.io.*</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-21" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-21', event_id='95825', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1108</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95825">A probability contrastive learning framework for 3D molecular representation learning</a></strong></h5>


                        <p class="text-muted">
                            Jiayu Qin &middot; Jian Chen &middot; Rohan Sharma &middot; Jingchen Sun &middot; Changyou Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Contrastive Learning (CL) plays a crucial role in molecular representation learning, enabling unsupervised learning from large scale unlabeled molecule datasets. It has inspired various applications in molecular property prediction and drug design.However, existing molecular representation learning methods often introduce potential false positive and false negative pairs through conventional graph augmentations like node masking and subgraph removal. The issue can lead to suboptimal performance when applying standard contrastive learning techniques to molecular datasets. To address the issue of false positive and negative pairs in molecular representation learning, we propose a novel probability-based contrastive learning (CL) framework. Unlike conventional methods, our approach introduces a learnable weight distribution via Bayesian modeling to automatically identify and mitigate false positive and negative pairs. This method is particularly effective because it dynamically adjusts to the data, improving the accuracy of the learned representations. Our model is learned by a stochastic expectation-maximization process, which optimizes the model by iteratively refining the probability estimates of sample weights and updating the model parameters.Experimental results indicate that our method outperforms existing approaches in 13 out of 15 molecular property prediction benchmarks in MoleculeNet dataset and 8 out of 12 benchmarks in the QM9 benchmark, achieving new state-of-the-art results on average.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-22" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-22', event_id='95784', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1109</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95784">Full-Atom Peptide Design with Geometric Latent Diffusion</a></strong></h5>


                        <p class="text-muted">
                            Xiangzhe Kong &middot; Yinjun Jia &middot; Wenbing Huang &middot; Yang Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Peptide design plays a pivotal role in therapeutics, allowing brand new possibility to leverage target binding sites that are previously undruggable. Most existing methods are either inefficient or only concerned with the target-agnostic design of 1D sequences. In this paper, we propose a generative model for full-atom Peptide design with Geometric LAtent Diffusion (PepGLAD) given the binding site. We first establish a benchmark consisting of both 1D sequences and 3D structures from Protein Data Bank (PDB) and literature for systematic evaluation. We then identify two major challenges of leveraging current diffusion-based models for peptide design: the full-atom geometry and the variable binding geometry. To tackle the first challenge, PepGLAD derives a variational autoencoder that first encodes full-atom residues of variable size into fixed-dimensional latent representations, and then decodes back to the residue space after conducting the diffusion process in the latent space. For the second issue, PepGLAD explores a receptor-specific affine transformation to convert the 3D coordinates into a shared standard space, enabling better generalization ability across different binding shapes. Experimental Results show that our method not only improves diversity and binding affinity significantly in the task of sequence-structure co-design, but also excels at recovering reference structures for binding conformation generation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-23" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-23', event_id='95593', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1110</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95593">How Molecules Impact Cells: Unlocking Contrastive PhenoMolecular Retrieval</a></strong></h5>


                        <p class="text-muted">
                            Philip Fradkin &middot; Puria Azadi Moghadam &middot; Karush Suri &middot; Frederik Wenkel &middot; Ali Bashashati &middot; Maciej Sypetkowski &middot; Dominique Beaini
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Predicting molecular impact on cellular function is a core challenge in therapeutic design. Phenomic experiments, designed to capture cellular morphology, utilize microscopy based techniques and demonstrate a high throughput solution for uncovering molecular impact on the cell. In this work, we learn a joint latent space between molecular structures and microscopy phenomic experiments, aligning paired samples with contrastive learning. Specifically, we study the problem of Contrastive PhenoMolecular Retrieval, which consists of zero-shot molecular structure identification conditioned on phenomic experiments. We assess challenges in multi-modal learning of phenomics and molecular modalities such as experimental batch effect, inactive molecule perturbations, and encoding perturbation concentration. We demonstrate improved multi-modal learner retrieval through (1) a uni-modal pre-trained phenomics model, (2) a novel inter sample similarity aware loss, and (3) models conditioned on a representation of molecular concentration. Following this recipe, we propose MolPhenix, a molecular phenomics model. MolPhenix leverages a pre-trained phenomics model to demonstrate significant performance gains across perturbation concentrations, molecular scaffolds, and activity thresholds. In particular, we demonstrate an 8.1 times improvement in zero shot molecular retrieval of active molecules over the previous state-of-the-art, reaching 77.33% in top-1% accuracy. These results open the door for machine learning to be applied in virtual phenomics screening, which can significantly benefit drug discovery applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-24" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-24', event_id='96721', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1111</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96721">Cluster-wise Graph Transformer with Dual-granularity Kernelized Attention</a></strong></h5>


                        <p class="text-muted">
                            Siyuan Huang &middot; Yunchong Song &middot; Jiayue Zhou &middot; Zhouhan Lin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In the realm of graph learning, there is a category of methods that conceptualize graphs as hierarchical structures, utilizing node clustering to capture broader structural information. While generally effective, these methods often rely on a fixed graph coarsening routine, leading to overly homogeneous cluster representations and loss of node-level information. In this paper, we envision the graph as a network of interconnected node sets without compressing each cluster into a single embedding. To enable effective information transfer among these node sets, we propose the Node-to-Cluster Attention (N2C-Attn) mechanism. N2C-Attn incorporates techniques from Multiple Kernel Learning into the kernelized attention framework, effectively capturing information at both node and cluster levels. We then devise an efficient form for N2C-Attn using the cluster-wise message-passing framework, achieving linear time complexity. We further analyze how N2C-Attn combines bi-level feature maps of queries and keys, demonstrating its capability to merge dual-granularity information. The resulting architecture, Cluster-wise Graph Transformer (Cluster-GT), which uses node clusters as tokens and employs our proposed N2C-Attn module, shows superior performance on various graph-level tasks. Code is available at https://github.com/LUMIA-Group/Cluster-wise-Graph-Transformer.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-25" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-25', event_id='96501', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1200</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96501">Adaptive Visual Scene Understanding: Incremental Scene Graph Generation</a></strong></h5>


                        <p class="text-muted">
                            Naitik Khandelwal &middot; Xiao Liu &middot; Mengmi Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Scene graph generation (SGG) analyzes images to extract meaningful information about objects and their relationships. In the dynamic visual world, it is crucial for AI systems to continuously detect new objects and establish their relationships with existing ones. Recently, numerous studies have focused on continual learning within the domains of object detection and image recognition. However, a limited amount of research focuses on a more challenging continual learning problem in SGG. This increased difficulty arises from the intricate interactions and dynamic relationships among objects, and their associated contexts. Thus, in continual learning, SGG models are often required to expand, modify, retain, and reason scene graphs within the process of adaptive visual scene understanding. To systematically explore Continual Scene Graph Generation (CSEGG), we present a comprehensive benchmark comprising three learning regimes: relationship incremental, scene incremental, and relationship generalization. Moreover, we introduce a ``Replays via Analysis by Synthesis" method named RAS. This approach leverages the scene graphs, decomposes and re-composes them to represent different scenes, and replays the synthesized scenes based on these compositional scene graphs. The replayed synthesized scenes act as a means to practice and refine proficiency in SGG in known and unknown environments. Our experimental results not only highlight the challenges of directly combining existing continual learning methods with SGG backbones but also demonstrate the effectiveness of our proposed approach, enhancing CSEGG efficiency while simultaneously preserving privacy and memory usage. All data and source code will be made public.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-26" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-26', event_id='96515', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1201</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96515">DreamMesh4D: Video-to-4D Generation with Sparse-Controlled Gaussian-Mesh Hybrid Representation</a></strong></h5>


                        <p class="text-muted">
                            Zhiqi Li &middot; Yiming Chen &middot; Peidong Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advancements in 2D/3D generative techniques have facilitated the generation of dynamic 3D objects from monocular videos. Previous methods mainly rely on the implicit neural radiance fields (NeRF) or explicit Gaussian Splatting as the underlying representation, and struggle to achieve satisfactory spatial-temporal consistency and surface appearance. Drawing inspiration from modern 3D animation pipelines, we introduce DreamMesh4D, a novel framework combining mesh representation with geometric skinning technique to generate high-quality 4D object from a monocular video. Instead of utilizing classical texture map for appearance, we bind Gaussian splats to triangle face of mesh for differentiable optimization of both the texture and mesh vertices. In particular, DreamMesh4D begins with a coarse mesh obtained through an image-to-3D generation procedure. Sparse points are then uniformly sampled across the mesh surface, and are used to build a deformation graph to drive the motion of the 3D object for the sake of computational efficiency and providing additional constraint. For each step, transformations of sparse control points are predicted using a deformation network, and the mesh vertices as well as the surface Gaussians are deformed via a novel geometric skinning algorithm. The skinning algorithm is a hybrid approach combining LBS (linear blending skinning) and DQS (dual-quaternion skinning), mitigating drawbacks associated with both approaches. The static surface Gaussians and mesh vertices as well as the dynamic deformation network are learned via reference view photometric loss, score distillation loss as well as other regularization losses in a two-stage manner. Extensive experiments demonstrate superior performance of our method in terms of both rendering quality and spatial-temporal consistency. Furthermore, our method is compatible with modern graphic pipelines, showcasing its potential in the 3D gaming and film industry.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-27" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-27', event_id='96520', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1202</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96520">Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis</a></strong></h5>


                        <p class="text-muted">
                            Xin Jin &middot; Pengyi Jiao &middot; Zheng-Peng Duan &middot; Xingchao Yang &middot; Chongyi Li &middot; Chun-Le Guo &middot; Bo Ren
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Volumetric rendering-based methods, like NeRF, excel in HDR view synthesis from RAW images, especially for nighttime scenes. They suffer from long training times and cannot perform real-time rendering due to dense sampling requirements. The advent of 3D Gaussian Splatting (3DGS) enables real-time rendering and faster training. However, implementing RAW image-based view synthesis directly using 3DGS is challenging due to its inherent drawbacks: 1) in nighttime scenes, extremely low SNR leads to poor structure-from-motion (SfM) estimation in dis- tant views; 2) the limited representation capacity of the spherical harmonics (SH) function is unsuitable for RAW linear color space; and 3) inaccurate scene structure hampers downstream tasks such as refocusing. To address these issues, we propose LE3D (Lighting Every darkness with 3DGS). Our method proposes Cone Scatter Initialization to enrich the estimation of SfM and replaces SH with a Color MLP to represent the RAW linear color space. Additionally, we introduce depth distortion and near-far regularizations to improve the accuracy of scene structure for down- stream tasks. These designs enable LE3D to perform real-time novel view synthesis, HDR rendering, refocusing, and tone-mapping changes. Compared to previous vol- umetric rendering-based methods, LE3D reduces training time to 1% and improves rendering speed by up to 4,000 times for 2K resolution images in terms of FPS. Code and viewer can be found in https://srameo.github.io/projects/le3d.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-28" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-28', event_id='96538', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1203</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96538">MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting</a></strong></h5>


                        <p class="text-muted">
                            Ruijie Zhu &middot; Yanzhe Liang &middot; Hanzhi Chang &middot; Jiacheng Deng &middot; Jiahao Lu &middot; Wenfei Yang &middot; Tianzhu Zhang &middot; Yongdong Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Dynamic scene reconstruction is a long-term challenge in the field of 3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem. Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on object motion, leading to optimization difficulties and performance degradation. To address the above issues, we propose a novel deformable 3D Gaussian splatting framework called MotionGS, which explores explicit motion priors to guide the deformation of 3D Gaussians. Specifically, we first introduce an optical flow decoupling module that decouples optical flow into camera flow and motion flow, corresponding to camera movement and object motion respectively. Then the motion flow can effectively constrain the deformation of 3D Gaussians, thus simulating the motion of dynamic objects. Additionally, a camera pose refinement module is proposed to alternately optimize 3D Gaussians and camera poses, mitigating the impact of inaccurate camera poses. Extensive experiments in the monocular dynamic scenes validate that MotionGS surpasses state-of-the-art methods and exhibits significant superiority in both qualitative and quantitative results. Project page: https://ruijiezhu94.github.io/MotionGS_page.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-29" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-29', event_id='96562', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1204</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96562">LION: Linear Group RNN for 3D Object Detection in Point Clouds</a></strong></h5>


                        <p class="text-muted">
                            Zhe Liu &middot; Jinghua Hou &middot; Xinyu Wang &middot; Xiaoqing Ye &middot; Jingdong Wang &middot; Hengshuang Zhao &middot; Xiang Bai
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The benefit of transformers in large-scale 3D point cloud perception tasks, such as 3D object detection, is limited by their quadratic computation cost when modeling long-range relationships. In contrast, linear RNNs have low computational complexity and are suitable for long-range modeling. Toward this goal, we propose a simple and effective window-based framework built on Linear group RNN (i.e., perform linear RNN for grouped features) for accurate 3D object detection, called LION. The key property is to allow sufficient feature interaction in a much larger group than transformer-based methods. However, effectively applying linear group RNN to 3D object detection in highly sparse point clouds is not trivial due to its limitation in handling spatial modeling. To tackle this problem, we simply introduce a 3D spatial feature descriptor and integrate it into the linear group RNN operators to enhance their spatial features rather than blindly increasing the number of scanning orders for voxel features. To further address the challenge in highly sparse point clouds, we propose a 3D voxel generation strategy to densify foreground features thanks to linear group RNN as a natural property of auto-regressive models. Extensive experiments verify the effectiveness of the proposed components and the generalization of our LION on different linear group RNN operators including Mamba, RWKV, and RetNet. Furthermore, it is worth mentioning that our LION-Mamba achieves state-of-the-art on Waymo, nuScenes, Argoverse V2, and ONCE datasets. Last but not least, our method supports kinds of advanced linear RNN operators (e.g., RetNet, RWKV, Mamba, xLSTM and TTT) on small but popular KITTI dataset for a quick experience with our linear RNN-based framework.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-30" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-30', event_id='96723', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1205</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96723">LaSe-E2V: Towards Language-guided Semantic-aware Event-to-Video Reconstruction</a></strong></h5>


                        <p class="text-muted">
                            Kanghao Chen &middot; Hangyu Li &middot; Jiazhou Zhou &middot; Zeyu Wang &middot; Lin Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Event cameras harness advantages such as low latency, high temporal resolution, and high dynamic range (HDR), compared to standard cameras. Due to the distinct imaging paradigm shift, a dominant line of research focuses on event-to-video (E2V) reconstruction to bridge event-based and standard computer vision. However, this task remains challenging due to its inherently ill-posed nature: event cameras only detect the edge and motion information locally. Consequently, the reconstructed videos are often plagued by artifacts and regional blur, primarily caused by the ambiguous semantics of event data. In this paper, we find language naturally conveys abundant semantic information, rendering it stunningly superior in ensuring semantic consistency for E2V reconstruction. Accordingly, we propose a novel framework, called LaSe-E2V,  that can achieve semantic-aware high-quality E2V reconstruction from a language-guided perspective, buttressed by the text-conditional diffusion models. However, due to diffusion models' inherent diversity and randomness, it is hardly possible to directly apply them to achieve spatial and temporal consistency for E2V reconstruction. Thus, we first propose an Event-guided Spatiotemporal Attention (ESA) module to condition the event data to the denoising pipeline effectively. We then introduce an event-aware mask loss to ensure temporal coherence and a noise initialization strategy to enhance spatial consistency. Given the absence of event-text-video paired data, we aggregate existing E2V datasets and generate textual descriptions using the tagging models for training and evaluation. Extensive experiments on three datasets covering diverse challenging scenarios (e.g., fast motion, low light) demonstrate the superiority of our method. Demo videos for the results are attached to the project page.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-31" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-31', event_id='96900', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1206</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96900">3D Focusing-and-Matching Network for Multi-Instance Point Cloud Registration</a></strong></h5>


                        <p class="text-muted">
                            Liyuan Zhang &middot; Le Hui &middot; qi liu &middot; Bo Li &middot; Yuchao Dai
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multi-instance point cloud registration aims to estimate the pose of all instances of a model point cloud in the whole scene. Existing methods all adopt the strategy of first obtaining the global correspondence and then clustering to obtain the pose of each instance. However, due to the cluttered and occluded objects in the scene, it is difficult to obtain an accurate correspondence between the model point cloud and all instances in the scene. To this end, we propose a simple yet powerful 3D focusing-and-matching network for multi-instance point cloud registration by learning the multiple pair-wise point cloud registration. Specifically, we first present a 3D multi-object focusing module to locate the center of each object and generate object proposals. By using self-attention and cross-attention to associate the model point cloud with structurally similar objects, we can locate potential matching instances by regressing object centers. Then, we propose a 3D dual-masking instance matching module to estimate the pose between the model point cloud and each object proposal. It performs instance mask and overlap mask masks to accurately predict the pair-wise correspondence. Extensive experiments on two public benchmarks, Scan2CAD and ROBI, show that our method achieves a new state-of-the-art performance on the multi-instance point cloud registration task.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-32" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-32', event_id='92987', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1207</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92987">MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering</a></strong></h5>


                        <p class="text-muted">
                            Yizhen Luo &middot; Zikun Nie &middot; Massimo Hong &middot; Suyuan Zhao &middot; Hao Zhou &middot; Zaiqing Nie
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Studying protein mutations within amino acid sequences holds tremendous significance in life sciences. Protein language models (PLMs) have demonstrated strong capabilities in broad biological applications. However, due to architectural design and lack of supervision, PLMs model mutations implicitly with evolutionary plausibility, which is not satisfactory to serve as explainable and engineerable tools in real-world studies. To address these issues, we present MutaPLM, a unified framework for interpreting and navigating protein mutations with protein language models. MutaPLM introduces a protein <em>delta</em> network that captures explicit protein mutation representations within a unified feature space, and a transfer learning pipeline with a chain-of-thought (CoT) strategy to harvest protein mutation knowledge from biomedical texts. We also construct MutaDescribe, the first large-scale protein mutation dataset with rich textual annotations, which provides cross-modal supervision signals. Through comprehensive experiments, we demonstrate that MutaPLM excels at providing human-understandable explanations for mutational effects and prioritizing novel mutations with desirable properties. Our code, model, and data are open-sourced at https://github.com/PharMolix/MutaPLM.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-33" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-33', event_id='93022', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1208</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93022">Generative Modeling of Molecular Dynamics Trajectories</a></strong></h5>


                        <p class="text-muted">
                            Bowen Jing &middot; Hannes St√§rk &middot; Tommi Jaakkola &middot; Bonnie Berger
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show preliminary results on scaling to protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. Code is available at https://github.com/bjing2016/mdgen.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-34" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-34', event_id='93299', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1209</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93299">DRACO: A Denoising-Reconstruction Autoencoder for Cryo-EM</a></strong></h5>


                        <p class="text-muted">
                            YingJun Shen &middot; Haizhao Dai &middot; Qihe Chen &middot; Yan Zeng &middot; Jiakai Zhang &middot; Yuan Pei &middot; Jingyi Yu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Foundation models in computer vision have demonstrated exceptional performance in zero-shot and few-shot tasks by extracting multi-purpose features from large-scale datasets through self-supervised pre-training methods. However, these models often overlook the severe corruption in cryogenic electron microscopy (cryo-EM) images by high-level noises. We introduce DRACO, a Denoising-Reconstruction Autoencoder for CryO-EM, inspired by the Noise2Noise (N2N) approach. By processing cryo-EM movies into odd and even images and treating them as independent noisy observations, we apply a denoising-reconstruction hybrid training scheme. We mask both images to create denoising and reconstruction tasks. For DRACO's pre-training, the quality of the dataset is essential, we hence build a high-quality, diverse dataset from an uncurated public database, including over 270,000 movies or micrographs. After pre-training, DRACO naturally serves as a generalizable cryo-EM image denoiser and a foundation model for various cryo-EM downstream tasks. DRACO demonstrates the best performance in denoising, micrograph curation, and particle picking tasks compared to state-of-the-art baselines.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-35" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-35', event_id='93963', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1210</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93963">Kermut: Composite kernel regression for protein variant effects</a></strong></h5>


                        <p class="text-muted">
                            Peter M√∏rch Groth &middot; Mads Kerrn &middot; Lars Olsen &middot; Jesper Salomon &middot; Wouter Boomsma
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Reliable prediction of protein variant effects is crucial for both protein optimization and for advancing biological understanding. For practical use in protein engineering, it is important that we can also provide reliable uncertainty estimates for our predictions, and while prediction accuracy has seen much progress in recent years, uncertainty metrics are rarely reported. We here provide a Gaussian process regression model, Kermut, with a novel composite kernel for modeling mutation similarity, which obtains state-of-the-art performance for supervised protein variant effect prediction while also offering estimates of uncertainty through its posterior. An analysis of the quality of the uncertainty estimates demonstrates that our model provides meaningful levels of overall calibration, but that instance-specific uncertainty calibration remains more challenging.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-36" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-36', event_id='94717', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1211</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94717">Reprogramming Pretrained Target-Specific Diffusion Models for Dual-Target Drug Design</a></strong></h5>


                        <p class="text-muted">
                            Xiangxin Zhou &middot; Jiaqi Guan &middot; Yijia Zhang &middot; Xingang Peng &middot; Liang Wang &middot; Jianzhu Ma
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Dual-target therapeutic strategies have become a compelling approach and attracted significant attention due to various benefits, such as their potential in overcoming drug resistance in cancer therapy. Considering the tremendous success that deep generative models have achieved in structure-based drug design in recent years, we formulate dual-target drug design as a generative task and curate a novel dataset of potential target pairs based on synergistic drug combinations. We propose to design dual-target drugs with diffusion models that are trained on single-target protein-ligand complex pairs. Specifically, we align two pockets in 3D space with protein-ligand binding priors and build two complex graphs with shared ligand nodes for SE(3)-equivariant composed message passing, based on which we derive a composed drift in both 3D and categorical probability space in the generative process. Our algorithm can well transfer the knowledge gained in single-target pretraining to dual-target scenarios in a zero-shot manner. We also repurpose linker design methods as strong baselines for this task. Extensive experiments demonstrate the effectiveness of our method compared with various baselines.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-37" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-37', event_id='96496', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1300</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96496">Memorize What Matters: Emergent Scene Decomposition from Multitraverse</a></strong></h5>


                        <p class="text-muted">
                            Yiming Li &middot; Zehong Wang &middot; Yue Wang &middot; Zhiding Yu &middot; Zan Gojcic &middot; Marco Pavone &middot; Chen Feng &middot; Jose M. Alvarez
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Humans naturally retain memories of permanent elements, while ephemeral moments often slip through the cracks of memory. This selective retention is crucial for robotic perception, localization, and mapping. To endow robots with this capability, we introduce 3D Gaussian Mapping (3DGM), a self-supervised, camera-only offline mapping framework grounded in 3D Gaussian Splatting. 3DGM converts multitraverse RGB videos from the same region into a Gaussian-based environmental map while concurrently performing 2D ephemeral object segmentation. Our key observation is that the environment remains consistent across traversals, while objects frequently change. This allows us to exploit self-supervision from repeated traversals to achieve environment-object decomposition. More specifically, 3DGM formulates multitraverse environmental mapping as a robust 3D representation learning problem, treating pixels of the environment and objects as inliers and outliers, respectively. Using robust feature distillation, feature residual mining, and robust optimization, 3DGM simultaneously performs 2D segmentation and 3D mapping without human intervention. We build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets, to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and neural rendering. Extensive results verify the effectiveness and potential of our method for self-driving and robotics.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-38" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-38', event_id='96495', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1301</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96495">Consensus Learning with Deep Sets for Essential Matrix Estimation</a></strong></h5>


                        <p class="text-muted">
                            Dror Moran &middot; Yuval Margalit &middot; Guy Trostianetsky &middot; Fadi Khatib &middot; Meirav Galun &middot; Ronen Basri
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Robust estimation of the essential matrix, which encodes the relative position and orientation of two cameras, is a fundamental step in structure from motion pipelines. Recent deep-based methods achieved accurate estimation by using complex network architectures that involve graphs, attention layers, and hard pruning steps. Here, we propose a simpler network architecture based on Deep Sets. Given a collection of point matches extracted from two images, our method identifies outlier point matches and models the displacement noise in inlier matches. A weighted DLT module uses these predictions to regress the essential matrix. Our network achieves accurate recovery that is superior to existing networks with significantly more complex architectures.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-39" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-39', event_id='96331', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1302</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96331">UNION: Unsupervised 3D Object Detection using Object Appearance-based Pseudo-Classes</a></strong></h5>


                        <p class="text-muted">
                            Ted Lentsch &middot; Holger Caesar &middot; Dariu Gavrila
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Unsupervised 3D object detection methods have emerged to leverage vast amounts of data without requiring manual labels for training. Recent approaches rely on dynamic objects for learning to detect mobile objects but penalize the detections of static instances during training. Multiple rounds of (self) training are used to add detected static instances to the set of training targets; this procedure to improve performance is computationally expensive. To address this, we propose the method UNION. We use spatial clustering and self-supervised scene flow to obtain a set of static and dynamic object proposals from LiDAR. Subsequently, object proposals' visual appearances are encoded to distinguish static objects in the foreground and background by selecting static instances that are visually similar to dynamic objects. As a result, static and dynamic mobile objects are obtained together, and existing detectors can be trained with a single training. In addition, we extend 3D object discovery to detection by using object appearance-based cluster labels as pseudo-class labels for training object classification. We conduct extensive experiments on the nuScenes dataset and increase the state-of-the-art performance for unsupervised 3D object discovery, i.e. UNION more than doubles the average precision to 38.4. The code is available at github.com/TedLentsch/UNION.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-40" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-40', event_id='92943', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1303</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92943">Exploring Context Window of Large Language Models via Decomposed Positional Vectors</a></strong></h5>


                        <p class="text-muted">
                            Zican Dong &middot; Junyi Li &middot; Xin Men &middot; Xin Zhao &middot; Bingning Wang &middot; Zhen Tian &middot; weipeng chen &middot; Ji-Rong Wen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Transformer-based large language models (LLMs) typically have a limited context window, resulting in significant performance degradation when processing text beyond the length of the context window. Extensive studies have been proposed to extend the context window and achieve length extrapolation of LLMs, but there is still a lack of in-depth interpretation of these approaches. In this study, we explore the positional information within and beyond the context window for deciphering the underlying mechanism of LLMs. By using a mean-based decomposition method, we disentangle positional vectors from hidden states of LLMs and analyze their formation and effect on attention. Furthermore, when texts exceed the context window, we analyze the change of positional vectors in two settings, i.e., direct extrapolation and context window extension. Based on our findings, we design two training-free context window extension methods, positional vector replacement and attention window extension. Experimental results show that our methods can effectively extend the context window length.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-41" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-41', event_id='96273', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1304</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96273">2DQuant: Low-bit Post-Training Quantization for Image Super-Resolution</a></strong></h5>


                        <p class="text-muted">
                            Kai Liu &middot; Haotong Qin &middot; Yong Guo &middot; Xin Yuan &middot; Linghe Kong &middot; Guihai Chen &middot; Yulun Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Low-bit quantization has become widespread for compressing image super-resolution (SR) models for edge deployment, which allows advanced SR models to enjoy compact low-bit parameters and efficient integer/bitwise constructions for storage compression and inference acceleration, respectively. However, it is notorious that low-bit quantization degrades the accuracy of SR models compared to their full-precision (FP) counterparts. Despite several efforts to alleviate the degradation, the transformer-based SR model still suffers severe degradation due to its distinctive activation distribution. In this work, we present a dual-stage low-bit post-training quantization (PTQ) method for image super-resolution, namely 2DQuant, which achieves efficient and accurate SR under low-bit quantization. The proposed method first investigates the weight and activation and finds that the distribution is characterized by coexisting symmetry and asymmetry, long tails. Specifically, we propose Distribution-Oriented Bound Initialization (DOBI), using different searching strategies to search a coarse bound for quantizers. To obtain refined quantizer parameters, we further propose Distillation Quantization Calibration (DQC), which employs a distillation approach to make the quantized model learn from its FP counterpart. Through extensive experiments on different bits and scaling factors, the performance of DOBI can reach the state-of-the-art (SOTA) while after stage two, our method surpasses existing PTQ in both metrics and visual effects. 2DQuant gains an increase in PSNR as high as 4.52dB on Set5 (x2) compared with SOTA when quantized to 2-bit and enjoys a 3.60x compression ratio and 5.08x speedup ratio. The code and models are available at https://github.com/Kai-Liu001/2DQuant.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-42" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-42', event_id='96153', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1305</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96153">Self-Supervised Adversarial Training via Diverse Augmented Queries and Self-Supervised Double Perturbation</a></strong></h5>


                        <p class="text-muted">
                            Ruize Zhang &middot; Sheng Tang &middot; Juan Cao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recently, there have been some works studying self-supervised adversarial training, a learning paradigm that learns robust features without labels. While those works have narrowed the performance gap between self-supervised adversarial training (SAT) and supervised adversarial training (supervised AT), a well-established formulation of SAT and its connections with supervised AT are under-explored. Based on a simple SAT benchmark, we find that SAT still faces the problem of large robust generalization gap and degradation on natural samples. We hypothesize this is due to the lack of data complexity and model regularization and propose a method named as DAQ-SDP (Diverse Augmented Queries Self-supervised Double Perturbation). We first challenge the previous conclusion that complex data augmentations degrade robustness in SAT by using diversely augmented samples as queries to guide adversarial training. Inspired by previous works in supervised AT, we then incorporate a self-supervised double perturbation scheme to self-supervised learning (SSL), which promotes robustness transferable to downstream classification. Our work can be seamlessly combined with models pretrained by different SSL frameworks without revising the learning objectives and helps to bridge the gap between SAT and AT. Our method also improves both robust and natural accuracies across different SSL frameworks. Our code is available at https://github.com/rzzhang222/DAQ-SDP.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-43" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-43', event_id='96009', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1306</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96009">Effective Rank Analysis and Regularization for Enhanced 3D Gaussian Splatting</a></strong></h5>


                        <p class="text-muted">
                            Junha Hyung &middot; Susung Hong &middot; Sungwon Hwang &middot; Jaeseong Lee &middot; Jaegul Choo &middot; Jin-Hwa Kim
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>3D reconstruction from multi-view images is one of the fundamental challenges in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a promising technique capable of real-time rendering with high-quality 3D reconstruction. This method utilizes 3D Gaussian representation and tile-based splatting techniques, bypassing the expensive neural field querying. Despite its potential, 3DGS encounters challenges, including needle-like artifacts, suboptimal geometries, and inaccurate normals, due to the Gaussians converging into anisotropic Gaussians with one dominant variance.We propose using effective rank analysis to examine the shape statistics of 3D Gaussian primitives, and identify the Gaussians indeed converge into needle-like shapes with the effective rank 1. To address this, we introduce effective rank as a regularization, which constrains the structure of the Gaussians. Our new regularization method enhances normal and geometry reconstruction while reducing needle-like artifacts. The approach can be integrated as an add-on module to other 3DGS variants, improving their quality without compromising visual fidelity.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-44" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-44', event_id='95793', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1307</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95793">NVRC: Neural Video Representation Compression</a></strong></h5>


                        <p class="text-muted">
                            Ho Man Kwan &middot; Ge Gao &middot; Fan Zhang &middot; Andrew Gower &middot; David Bull
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advances in implicit neural representation (INR)-based video coding havedemonstrated its potential to compete with both conventional and other learning-based approaches. With INR methods, a neural network is trained to overfit avideo sequence, with its parameters compressed to obtain a compact representationof the video content. However, although promising results have been achieved,the best INR-based methods are still out-performed by the latest standard codecs,such as VVC VTM, partially due to the simple model compression techniquesemployed. In this paper, rather than focusing on representation architectures, whichis a common focus in many existing works, we propose a novel INR-based videocompression framework, Neural Video Representation Compression (NVRC),targeting compression of the representation. Based on its novel quantization andentropy coding approaches, NVRC is the first framework capable of optimizing anINR-based video representation in a fully end-to-end manner for the rate-distortiontrade-off. To further minimize the additional bitrate overhead introduced by theentropy models, NVRC also compresses all the network, quantization and entropymodel parameters hierarchically. Our experiments show that NVRC outperformsmany conventional and learning-based benchmark codecs, with a 23% averagecoding gain over VVC VTM (Random Access) on the UVG dataset, measuredin PSNR. As far as we are aware, this is the first time an INR-based video codecachieving such performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-45" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-45', event_id='95790', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1308</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95790">Real-world Image Dehazing with Coherence-based Pseudo Labeling and Cooperative Unfolding Network</a></strong></h5>


                        <p class="text-muted">
                            Chengyu Fang &middot; Chunming He &middot; Fengyang Xiao &middot; Yulun Zhang &middot; Longxiang Tang &middot; Yuelin Zhang &middot; Kai Li &middot; Xiu Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Real-world Image Dehazing (RID) aims to alleviate haze-induced degradation in real-world settings. This task remains challenging due to the complexities in accurately modeling real haze distributions and the scarcity of paired real-world data. To address these challenges, we first introduce a cooperative unfolding network that jointly models atmospheric scattering and image scenes, effectively integrating physical knowledge into deep networks to restore haze-contaminated details. Additionally, we propose the first RID-oriented iterative mean-teacher framework, termed the Coherence-based Label Generator, to generate high-quality pseudo labels for network training. Specifically, we provide an optimal label pool to store the best pseudo-labels during network training, leveraging both global and local coherence to select high-quality candidates and assign weights to prioritize haze-free regions. We verify the effectiveness of our method, with experiments demonstrating that it achieves state-of-the-art performance on RID tasks. Code will be available at https://github.com/cnyvfang/CORUN-Colabator.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-46" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-46', event_id='95753', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1310</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95753">DMesh: A Differentiable Mesh Representation</a></strong></h5>


                        <p class="text-muted">
                            Sanghyun Son &middot; Matheus Gadelha &middot; Yang Zhou &middot; Zexiang Xu &middot; Ming Lin &middot; Yi Zhou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present a differentiable representation, DMesh, for general 3D triangular meshes. DMesh considers both the geometry and connectivity information of a mesh. In our design, we first get a set of convex tetrahedra that compactly tessellates the domain based on Weighted Delaunay Triangulation (WDT), and select triangular faces on the tetrahedra to define the final mesh. We formulate probability of faces to exist on the actual surface in a differentiable manner based on the WDT. This enables DMesh to represent meshes of various topology in a differentiable way, and allows us to reconstruct the mesh under various observations, such as point clouds and multi-view images using gradient-based optimization. We publicize the source code and supplementary material at our project page (https://sonsang.github.io/dmesh-project).</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-47" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-47', event_id='95609', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1311</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95609">Where&#x27;s Waldo: Diffusion Features For Personalized Segmentation and Retrieval</a></strong></h5>


                        <p class="text-muted">
                            Dvir Samuel &middot; Rami Ben-Ari &middot; Matan Levy &middot; Nir Darshan &middot; Gal Chechik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Personalized retrieval and segmentation aim to locate specific instances within a dataset based on an input image and a short description of the reference instance. While supervised methods are effective, they require extensive labeled data for training. Recently, self-supervised foundation models have been introduced to these tasks showing comparable results to supervised methods. However, a significant flaw in these models is evident: they struggle to locate a desired instance when other instances within the same class are presented. In this paper, we explore text-to-image diffusion models for these tasks. Specifically, we propose a novel approach called PDM for Personalized Diffusion Features Matching, that leverages intermediate features of pre-trained text-to-image models for personalization tasks without any additional training. PDM demonstrates superior performance on popular retrieval and segmentation benchmarks, outperforming even supervised methods. We also highlight notable shortcomings in current instance and segmentation datasets and propose new benchmarks for these tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-48" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-48', event_id='94194', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1400</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94194">Exploiting Descriptive Completeness Prior for Cross Modal Hashing with Incomplete Labels</a></strong></h5>


                        <p class="text-muted">
                            Haoyang Luo &middot; Zheng Zhang &middot; Yadan Luo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we tackle the challenge of generating high-quality hash codes for cross-modal retrieval in the presence of incomplete labels, which creates uncertainty in distinguishing between positive and negative pairs. Vision-language models such as CLIP offer a potential solution by providing generic knowledge for missing label recovery, yet their zero-shot performance remains insufficient. To address this, we propose a novel Prompt Contrastive Recovery approach, \textbf{PCRIL}, which progressively identifies promising positive classes from unknown label sets and recursively searches for other relevant labels. Identifying unknowns is nontrivial due to the fixed and long-tailed patterns of positive label sets in training data, which hampers the discovery of new label combinations. Therefore, we consider each subset of positive labels and construct three types of negative prompts through deletion, addition, and replacement for prompt learning. The augmented supervision guides the model to measure the completeness of label sets, thus facilitating the subsequent greedy tree search for label completion. We also address extreme cases of significant unknown labels and lack of negative pairwise supervision by deriving two augmentation strategies: seeking unknown-complementary samples for mixup and random flipping for negative labels. Extensive experiments reveal the vulnerability of current methods and demonstrate the effectiveness of PCRIL, achieving an average 12\% mAP improvement to the current SOTA across all datasets. Our code is available at https://github.com/E-Galois/PCRIL.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-49" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-49', event_id='96030', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1401</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96030">Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in Federated Learning</a></strong></h5>


                        <p class="text-muted">
                            Runhua Xu &middot; Shiqi Gao &middot; Chao Li &middot; James Joshi &middot; Jianxin Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Federated learning (FL) is inherently susceptible to privacy breaches and poisoning attacks. To tackle these challenges, researchers have separately devised secure aggregation mechanisms to protect data privacy and robust aggregation methods that withstand poisoning attacks. However, simultaneously addressing both concerns is challenging; secure aggregation facilitates poisoning attacks as most anomaly detection techniques require access to unencrypted local model updates, which are obscured by secure aggregation. Few recent efforts to simultaneously tackle both challenges offen depend on impractical assumption of non-colluding two-server setups that disrupt FL's topology, or three-party computation which introduces scalability issues, complicating deployment and application. To overcome this dilemma, this paper introduce a Dual Defense Federated learning (DDFed) framework. DDFed simultaneously boosts privacy protection and mitigates poisoning attacks, without introducing new participant roles or disrupting the existing FL topology. DDFed initially leverages cutting-edge fully homomorphic encryption (FHE) to securely aggregate model updates, without the impractical requirement for non-colluding two-server setups and ensures strong privacy protection. Additionally, we proposes a unique two-phase anomaly detection mechanism for encrypted model updates, featuring secure similarity computation and feedback-driven collaborative selection, with additional measures to prevent potential privacy breaches from Byzantine clients incorporated into the detection process. We conducted extensive experiments on various model poisoning attacks and FL scenarios, including both cross-device and cross-silo FL. Experiments on publicly available datasets demonstrate that DDFed successfully protects model privacy and effectively defends against model poisoning threats.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-50" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-50', event_id='94398', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1402</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94398">Transformer Doctor: Diagnosing and Treating Vision Transformers</a></strong></h5>


                        <p class="text-muted">
                            Jiacong Hu &middot; Hao Chen &middot; Kejia Chen &middot; Yang Gao &middot; Jingwen Ye &middot; Xingen Wang &middot; Mingli Song &middot; Zunlei Feng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Due to its powerful representational capabilities, Transformers have gradually become the mainstream model in the field of machine vision. However, the vast and complex parameters of Transformers impede researchers from gaining a deep understanding of their internal mechanisms, especially error mechanisms. Existing methods for interpreting Transformers mainly focus on understanding them from the perspectives of the importance of input tokens or internal modules, as well as the formation and meaning of features. In contrast, inspired by research on information integration mechanisms and conjunctive errors in the biological visual system, this paper conducts an in-depth exploration of the internal error mechanisms of Transformers. We first propose an information integration hypothesis for Transformers in the machine vision domain and provide substantial experimental evidence to support this hypothesis. This includes the dynamic integration of information among tokens and the static integration of information within tokens in Transformers, as well as the presence of conjunctive errors therein. Addressing these errors, we further propose heuristic dynamic integration constraint methods and rule-based static integration constraint methods to rectify errors and ultimately improve model performance. The entire methodology framework is termed as Transformer Doctor, designed for diagnosing and treating internal errors within transformers. Through a plethora of quantitative and qualitative experiments, it has been demonstrated that Transformer Doctor can effectively address internal errors in transformers, thereby enhancing model performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-51" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-51', event_id='94453', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1403</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94453">Soft Superpixel Neighborhood Attention</a></strong></h5>


                        <p class="text-muted">
                            Kent W Gauen &middot; Stanley Chan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Images contain objects with deformable boundaries, such as the contours of a human face, yet attention operators act on square windows. This mixes features from perceptually unrelated regions, which can degrade the quality of a denoiser. One can exclude pixels using an estimate of perceptual groupings, such as superpixels, but the naive use of superpixels can be theoretically and empirically worse than standard attention. Using superpixel probabilities rather than superpixel assignments, this paper proposes soft superpixel neighborhood attention (SNA), which interpolates between the existing neighborhood attention and the naive superpixel neighborhood attention. This paper presents theoretical results showing SNA is the optimal denoiser under a latent superpixel model. SNA outperforms alternative local attention modules on image denoising, and we compare the superpixels learned from denoising with those learned with supervision.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-52" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-52', event_id='94493', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1404</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94493">VQ-Map: Bird&#x27;s-Eye-View Map Layout Estimation in Tokenized Discrete Space via Vector Quantization</a></strong></h5>


                        <p class="text-muted">
                            Yiwei Zhang &middot; Jin Gao &middot; Fudong Ge &middot; Guan Luo &middot; Bing Li &middot; ZHAO-XIANG ZHANG &middot; Haibin Ling &middot; Weiming Hu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Bird's-eye-view (BEV) map layout estimation requires an accurate and full understanding of the semantics for the environmental elements around the ego car to make the results coherent and realistic. Due to the challenges posed by occlusion, unfavourable imaging conditions and low resolution, \emph{generating} the BEV semantic maps corresponding to corrupted or invalid areas in the perspective view (PV) is appealing very recently. \emph{The question is how to align the PV features with the generative models to facilitate the map estimation}. In this paper, we propose to utilize a generative model similar to the Vector Quantized-Variational AutoEncoder (VQ-VAE) to acquire prior knowledge for the high-level BEV semantics in the tokenized discrete space. Thanks to the obtained BEV tokens accompanied with a codebook embedding encapsulating the semantics for different BEV elements in the groundtruth maps, we are able to directly align the sparse backbone image features with the obtained BEV tokens from the discrete representation learning based on a specialized token decoder module, and finally generate high-quality BEV maps with the BEV codebook embedding serving as a bridge between PV and BEV. We evaluate the BEV map layout estimation performance of our model, termed VQ-Map, on both the nuScenes and Argoverse benchmarks, achieving 62.2/47.6 mean IoU for surround-view/monocular evaluation on nuScenes, as well as 73.4 IoU for monocular evaluation on Argoverse, which all set a new record for this map layout estimation task. The code and models are available on \url{https://github.com/Z1zyw/VQ-Map}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-53" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-53', event_id='94957', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1406</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94957">AverNet: All-in-one Video Restoration for Time-varying Unknown Degradations</a></strong></h5>


                        <p class="text-muted">
                            Haiyu Zhao &middot; Lei Tian &middot; Xinyan Xiao &middot; Peng Hu &middot; Yuanbiao Gou &middot; Xi Peng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Traditional video restoration approaches were designed to recover clean videos from a specific type of degradation, making them ineffective in handling multiple unknown types of degradation. To address this issue, several studies have been conducted and have shown promising results. However, these studies overlook that the degradations in video usually change over time, dubbed time-varying unknown degradations (TUD). To tackle such a less-touched challenge, we propose an innovative method, termed as All-in-one VidEo Restoration Network (AverNet), which comprises two core modules, i.e., Prompt-Guided Alignment (PGA) module and Prompt-Conditioned Enhancement (PCE) module. Specifically, PGA addresses the issue of pixel shifts caused by time-varying degradations by learning and utilizing prompts to align video frames at the pixel level. To handle multiple unknown degradations, PCE recasts it into a conditional restoration problem by implicitly establishing a conditional map between degradations and ground truths. Thanks to the collaboration between PGA and PCE modules, AverNet empirically demonstrates its effectiveness in recovering videos from TUD. Extensive experiments are carried out on two synthesized datasets featuring seven types of degradations with random corruption levels. The code is available at https://github.com/XLearning-SCU/2024-NeurIPS-AverNet.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-54" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-54', event_id='94984', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1407</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94984">3D Gaussian Splatting as Markov Chain Monte Carlo</a></strong></h5>


                        <p class="text-muted">
                            Shakiba Kheradmand &middot; Daniel Rebain &middot; Gopal Sharma &middot; Weiwei Sun &middot; Yang-Che Tseng &middot; Hossam Isack &middot; Abhishek Kar &middot; Andrea Tagliasacchi &middot; Kwang Moo Yi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While 3D Gaussian Splatting has recently become popular for neural rendering, current methods rely on carefully engineered cloning and splitting strategies for placing Gaussians, which does not always generalize and may lead to poor-quality renderings. For many real-world scenes this leads to their heavy dependence on good initializations. In this work, we rethink the set of 3D Gaussians as a random sample drawn from an underlying probability distribution describing the physical representation of the scene‚Äîin other words, Markov Chain Monte Carlo (MCMC) samples. Under this view, we show that the 3D Gaussian updates can be converted as Stochastic Gradient Langevin Dynamics (SGLD) update by simply introducing noise. We then rewrite the densification and pruning strategies in 3D Gaussian Splatting as simply a deterministic state transition of MCMC samples, removing these heuristics from the framework. To do so, we revise the ‚Äòcloning‚Äô of Gaussians into a relocalization scheme that approximately preserves sample probability. To encourage efficient use of Gaussians, we introduce an L1-regularizer on the Gaussians. On various standard evaluation scenes, we show that our method provides improved rendering quality, easy control over the number of Gaussians, and robustness to initialization. The project website is available at https://3dgs-mcmc.github.io/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-55" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-55', event_id='95130', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1408</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95130">VeXKD: The Versatile Integration of Cross-Modal Fusion and Knowledge Distillation for 3D Perception</a></strong></h5>


                        <p class="text-muted">
                            JI Yuzhe &middot; Yijie CHEN &middot; Liuqing Yang &middot; Ding Rui &middot; Meng Yang &middot; Xinhu Zheng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advancements in 3D perception have led to a proliferation of network architectures, particularly those involving multi-modal fusion algorithms. While these fusion algorithms improve accuracy, their complexity often impedes real-time performance. This paper introduces VeXKD, an effective and Versatile framework that integrates Cross-Modal Fusion with Knowledge Distillation. VeXKD applies knowledge distillation exclusively to the Bird's Eye View (BEV) feature maps, enabling the transfer of cross-modal insights to single-modal students without additional inference time overhead. It avoids volatile components that can vary across various 3D perception tasks and student modalities, thus improving versatility. The framework adopts a modality-general cross-modal fusion module to bridge the modality gap between the multi-modal teachers and single-modal students. Furthermore, leveraging byproducts generated during fusion, our BEV query guided mask generation network identifies crucial spatial locations across different BEV feature maps in a data-driven manner, significantly enhancing the effectiveness of knowledge distillation. Extensive experiments on the nuScenes dataset demonstrate notable improvements, with up to 6.9\%/4.2\% increase in mAP and NDS for 3D detection tasks and up to 4.3\% rise in mIoU for BEV map segmentation tasks, narrowing the performance gap with multi-modal models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-56" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-56', event_id='95203', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1409</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95203">EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI Detection</a></strong></h5>


                        <p class="text-muted">
                            Qinqian Lei &middot; Bo Wang &middot; Robby Tan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Detecting Human-Object Interactions (HOI) in zero-shot settings, where models must handle unseen classes, poses significant challenges. Existing methods that rely on aligning visual encoders with large Vision-Language Models (VLMs) to tap into the extensive knowledge of VLMs, require large, computationally expensive models and encounter training difficulties. Adapting VLMs with prompt learning offers an alternative to direct alignment. However, fine-tuning on task-specific datasets often leads to overfitting to seen classes and suboptimal performance on unseen classes, due to the absence of unseen class labels. To address these challenges, we introduce a novel prompt learning-based framework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce Large Language Model (LLM) and VLM guidance for learnable prompts, integrating detailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks. However, because training datasets contain seen-class labels alone, fine-tuning VLMs on such datasets tends to optimize learnable prompts for seen classes instead of unseen ones. Therefore, we design prompt learning for unseen classes using information from related seen classes, with LLMs utilized to highlight the differences between unseen and related seen classes. Quantitative evaluations on benchmark datasets demonstrate that our EZ-HOI achieves state-of-the-art performance across various zero-shot settings with only 10.35\% to 33.95\% of the trainable parameters compared to existing methods. Code is available at https://github.com/ChelsieLei/EZ-HOI.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-57" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-57', event_id='95476', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1410</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95476">Equivariant spatio-hemispherical networks for diffusion MRI deconvolution</a></strong></h5>


                        <p class="text-muted">
                            Axel Elaldi &middot; Guido Gerig &middot; Neel Dey
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Each voxel in a diffusion MRI (dMRI) image contains a spherical signal corresponding to the direction and strength of water diffusion in the brain. This paper advances the analysis of such spatio-spherical data by developing convolutional network layers that are equivariant to the $\mathbf{E(3) \times SO(3)}$ group and account for the physical symmetries of dMRI including rotations, translations, and reflections of space alongside voxel-wise rotations. Further, neuronal fibers are typically antipodally symmetric, a fact we leverage to construct highly efficient spatio-*hemispherical* graph convolutions to accelerate the analysis of high-dimensional dMRI data. In the context of sparse spherical fiber deconvolution to recover white matter microstructure, our proposed equivariant network layers yield substantial performance and efficiency gains, leading to better and more practical resolution of crossing neuronal fibers and fiber tractography. These gains are experimentally consistent across both simulation and in vivo human datasets.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-58" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-58', event_id='95535', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1411</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95535">Estimating Ego-Body Pose from Doubly Sparse Egocentric Video Data</a></strong></h5>


                        <p class="text-muted">
                            Seunggeun Chi &middot; Pin-Hao Huang &middot; Enna Sachdeva &middot; Hengbo Ma &middot; Karthik Ramani &middot; Kwonjoon Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We study the problem of estimating the body movements of a camera wearer from egocentric videos. Current methods for ego-body pose estimation rely on temporally dense sensor data, such as IMU measurements from spatially sparse body parts like the head and hands. However, we propose that even temporally sparse observations, such as hand poses captured intermittently from egocentric videos during natural or periodic hand movements, can effectively constrain overall body motion. Naively applying diffusion models to generate full-body pose from head pose and sparse hand pose leads to suboptimal results. To overcome this, we develop a two-stage approach that decomposes the problem into temporal completion and spatial completion. First, our method employs masked autoencoders to impute hand trajectories by leveraging the spatiotemporal correlations between the head pose sequence and intermittent hand poses, providing uncertainty estimates. Subsequently, we employ conditional diffusion models to generate plausible full-body motions based on these temporally dense trajectories of the head and hands, guided by the uncertainty estimates from the imputation. The effectiveness of our methods was rigorously tested and validated through comprehensive experiments conducted on various HMD setup with AMASS and Ego-Exo4D datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-59" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-59', event_id='94155', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1500</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94155">Voxel Mamba: Group-Free State Space Models for Point Cloud based 3D Object Detection</a></strong></h5>


                        <p class="text-muted">
                            Guowen Zhang &middot; Lue Fan &middot; Chenhang HE &middot; Zhen Lei &middot; ZHAO-XIANG ZHANG &middot; Lei Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Serialization-based methods, which serialize the 3D voxels and group them into multiple sequences before inputting to Transformers, have demonstrated their effectiveness in 3D object detection. However, serializing 3D voxels into 1D sequences will inevitably sacrifice the voxel spatial proximity. Such an issue is hard to be addressed by enlarging the group size with existing serialization-based methods due to the quadratic complexity of Transformers with feature sizes. Inspired by the recent advances of state space models (SSMs), we present a Voxel SSM, termed as Voxel Mamba, which employs a group-free strategy to serialize the whole space of voxels into a single sequence. The linear complexity of SSMs encourages our group-free design, alleviating the loss of spatial proximity of voxels. To further enhance the spatial proximity, we propose a Dual-scale SSM Block to establish a hierarchical structure, enabling a larger receptive field in the 1D serialization curve, as well as more complete local regions in 3D space. Moreover, we implicitly apply window partition under the group-free framework by positional encoding, which further enhances spatial proximity by encoding voxel positional information. Our experiments on Waymo Open Dataset and nuScenes dataset show that Voxel Mamba not only achieves higher accuracy than state-of-the-art methods, but also demonstrates significant advantages in computational efficiency. The source code is available at https://github.com/gwenzhang/Voxel-Mamba.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-60" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-60', event_id='93971', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1501</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93971">Multi-Object 3D Grounding with Dynamic Modules and Language-Informed Spatial Attention</a></strong></h5>


                        <p class="text-muted">
                            Haomeng Zhang &middot; Chiao-An Yang &middot; Raymond A. Yeh
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multi-object 3D Grounding involves locating 3D boxes based on a given query phrase from a point cloud. It is a challenging and significant task that has numerous applications in visual understanding, human-computer interaction, and robotics. To tackle this challenge, we introduce D-LISA, a two-stage approach that incorporates three innovations. First, a dynamic vision module that enables a variable and learnable number of box proposals. Second, a dynamic camera positioning that extracts features for each proposal. Third, a language-informed spatial attention module that better reasons over the proposals to output the final prediction. Empirically, experiments show that our method outperforms the state-of-the-art methods on multi-object 3D grounding by 12.8% (absolute) and is competitive in single-object 3D grounding.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-61" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-61', event_id='93932', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1502</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93932">Towards Croppable Implicit Neural Representations</a></strong></h5>


                        <p class="text-muted">
                            Maor Ashkenazi &middot; Eran Treister
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Implicit Neural Representations (INRs) have peaked interest in recent years due to their ability to encode natural signals using neural networks. While INRs allow for useful applications such as interpolating new coordinates and signal compression, their black-box nature makes it difficult to modify them post-training. In this paper we explore the idea of editable INRs, and specifically focus on the widely used cropping operation. To this end, we present Local-Global SIRENs - a novel INR architecture that supports cropping by design. Local-Global SIRENs are based on combining local and global feature extraction for signal encoding. What makes their design unique is the ability to effortlessly remove specific portions of an encoded signal, with a proportional weight decrease. This is achieved by eliminating the corresponding weights from the network, without the need for retraining. We further show how this architecture can be used to support the straightforward extension of previously encoded signals. Beyond signal editing, we examine how the Local-Global approach can accelerate training, enhance encoding of various signals, improve downstream performance, and be applied to modern INRs such as INCODE, highlighting its potential and flexibility. Code is available at https://github.com/maorash/Local-Global-INRs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-62" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-62', event_id='93734', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1503</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93734">FreeSplat: Generalizable 3D Gaussian Splatting Towards Free View Synthesis of Indoor Scenes</a></strong></h5>


                        <p class="text-muted">
                            Yunsong Wang &middot; Tianxin Huang &middot; Hanlin Chen &middot; Gim Hee Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Empowering 3D Gaussian Splatting with generalization ability is appealing. However, existing generalizable 3D Gaussian Splatting methods are largely confined to narrow-range interpolation between stereo images due to their heavy backbones, thus lacking the ability to accurately localize 3D Gaussian and support free-view synthesis across wide view range. In this paper, we present a novel framework FreeSplat that is capable of reconstructing geometrically consistent 3D scenes from long sequence input towards free-view synthesis.Specifically, we firstly introduce Low-cost Cross-View Aggregation achieved by constructing adaptive cost volumes among nearby views and aggregating features using a multi-scale structure. Subsequently, we present the Pixel-wise Triplet Fusion to eliminate redundancy of 3D Gaussians in overlapping view regions and to aggregate features observed across multiple views. Additionally, we propose a simple but effective free-view training strategy that ensures robust view synthesis across broader view range regardless of the number of views. Our empirical results demonstrate state-of-the-art novel view synthesis peformances in both novel view rendered color maps quality and depth maps accuracy across different numbers of input views. We also show that FreeSplat performs inference more efficiently and can effectively reduce redundant Gaussians, offering the possibility of feed-forward large scene reconstruction without depth priors. Our code will be made open-source upon paper acceptance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-63" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-63', event_id='94771', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1504</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94771">Linear Uncertainty Quantification of Graphical Model Inference</a></strong></h5>


                        <p class="text-muted">
                            Chenghua Guo &middot; Han Yu &middot; Jiaxin Liu &middot; Chao Chen &middot; Qi Li &middot; Sihong Xie &middot; Xi Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Uncertainty Quantification (UQ) is vital for decision makers as it offers insights into the potential reliability of data and model, enabling more informed and risk-aware decision-making. Graphical models, capable of representing data with complex dependencies, are widely used across domains.Existing sampling-based UQ methods are unbiased but cannot guarantee convergence and are time-consuming on large-scale graphs. There are fast UQ methods for graphical models with closed-form solutions and convergence guarantee but with uncertainty underestimation.We propose <em>LinUProp</em>, a UQ method that utilizes a novel linear propagation of uncertainty to model uncertainty among related nodes additively instead of multiplicatively, to offer linear scalability, guaranteed convergence, and closed-form solutions without underestimating uncertainty.Theoretically, we decompose the expected prediction error of the graphical model and prove that the uncertainty computed by <em>LinUProp</em> is the <em>generalized variance component</em> of the decomposition.Experimentally, we demonstrate that <em>LinUProp</em> is consistent with the sampling-based method but with linear scalability and fast convergence.Moreover, <em>LinUProp</em> outperforms competitors in uncertainty-based active learning on four real-world graph datasets, achieving higher accuracy with a lower labeling budget.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-64" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-64', event_id='93481', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1505</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93481">MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution</a></strong></h5>


                        <p class="text-muted">
                            Wei Tao &middot; Yucheng Zhou &middot; Yanlin Wang &middot; Wenqiang Zhang &middot; Hongyu Zhang &middot; Yu Cheng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In software development, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing code.Large Language Models (LLMs) have shown promise in code generation but face difficulties in resolving Github issues, particularly at the repository level. To overcome this challenge, we empirically study the reason why LLMs fail to resolve GitHub issues and analyze the major factors. Motivated by the empirical findings, we propose a novel LLM-based <strong>M</strong>ulti-<strong>A</strong>gent framework for <strong>G</strong>itHub <strong>I</strong>ssue re<strong>S</strong>olution, <strong>MAGIS</strong>, consisting of four agents customized for software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve <strong>13.94%</strong> GitHub issues, significantly outperforming the baselines.Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the advanced LLM.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-65" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-65', event_id='93457', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1506</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93457">GSDF: 3DGS Meets SDF for Improved Neural Rendering and Reconstruction</a></strong></h5>


                        <p class="text-muted">
                            Mulin Yu &middot; Tao Lu &middot; Linning Xu &middot; Lihan Jiang &middot; Yuanbo Xiangli &middot; Bo Dai
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Representing 3D scenes from multiview images remains a core challenge in computer vision and graphics, requiring both reliable rendering and reconstruction, which often conflicts due to the mismatched prioritization of image quality over precise underlying scene geometry. Although both neural implicit surfaces and explicit Gaussian primitives have advanced with neural rendering techniques, current methods impose strict constraints on density fields or primitive shapes, which enhances the affinity for geometric reconstruction at the sacrifice of rendering quality. To address this dilemma, we introduce GSDF, a dual-branch architecture combining 3D Gaussian Splatting (3DGS) and neural Signed Distance Fields (SDF). Our approach leverages mutual guidance and joint supervision during the training process to mutually enhance reconstruction and rendering. Specifically, our method guides the Gaussian primitives to locate near potential surfaces and accelerates the SDF convergence. This implicit mutual guidance ensures robustness and accuracy in both synthetic and real-world scenarios. Experimental results demonstrate that our method boosts the SDF optimization process to reconstruct more detailed geometry, while reducing floaters and blurry edge artifacts in rendering by aligning Gaussian primitives with the underlying geometry.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-66" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-66', event_id='93386', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1507</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93386">Beware of Road Markings: A New Adversarial Patch Attack to Monocular Depth Estimation</a></strong></h5>


                        <p class="text-muted">
                            Hangcheng Liu &middot; Zhenhu Wu &middot; Hao Wang &middot; Xingshuo Han &middot; Shangwei Guo &middot; Tao Xiang &middot; Tianwei Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Monocular Depth Estimation (MDE) enables the prediction of scene depths from a single RGB image, having been widely integrated into production-grade autonomous driving systems, e.g., Tesla Autopilot. Current adversarial attacks to MDE models focus on attaching an optimized adversarial patch to a designated obstacle. Although effective, this approach presents two inherent limitations: its reliance on specific obstacles and its limited malicious impact. In contrast, we propose a pioneering attack to MDE models that \textit{decouples obstacles from patches physically and deploys optimized patches on roads}, thereby extending the attack scope to arbitrary traffic participants. This approach is inspired by our groundbreaking discovery: \textit{various MDE models with different architectures, trained for autonomous driving, heavily rely on road regions} when predicting depths for different obstacles. Based on this discovery, we design the Adversarial Road Marking (AdvRM) attack, which camouflages patches as ordinary road markings and deploys them on roads, thereby posing a continuous threat within the environment. Experimental results from both dataset simulations and real-world scenarios demonstrate that AdvRM is effective, stealthy, and robust against various MDE models, achieving about 1.507 of Mean Relative Shift Ratio (MRSR) over 8 MDE models. The code is available at \url{https://github.com/a-c-a-c/AdvRM.git}</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-67" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-67', event_id='93140', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1508</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93140">LiT: Unifying LiDAR &quot;Languages&quot; with LiDAR Translator</a></strong></h5>


                        <p class="text-muted">
                            Yixing Lao &middot; Tao Tang &middot; Xiaoyang Wu &middot; Peng Chen &middot; Kaicheng Yu &middot; Hengshuang Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>LiDAR data exhibits significant domain gaps due to variations in sensors, vehicles, and driving environments, creating ‚Äúlanguage barriers‚Äù that limit the effective use of data across domains and the scalability of LiDAR perception models. To address these challenges, we introduce the LiDAR Translator (LiT), a framework that directly translates LiDAR data across domains, enabling both cross-domain adaptation and multi-domain joint learning. LiT integrates three key components: a scene modeling module for precise foreground and background reconstruction, a LiDAR modeling module that models LiDAR rays statistically and simulates ray-drop, and a fast, hardware-accelerated ray casting engine. LiT enables state-of-the-art zero-shot and unified domain detection across diverse LiDAR datasets, marking a step toward data-driven domain unification for autonomous driving systems. Source code and demos are available at: https://yxlao.github.io/lit.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-68" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-68', event_id='93134', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1509</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93134">Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis</a></strong></h5>


                        <p class="text-muted">
                            Qitao Zhao &middot; Shubham Tulsiani
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Inferring the 3D structure underlying a set of multi-view images typically requires solving two co-dependent tasks -- accurate 3D reconstruction requires precise camera poses, and predicting camera poses relies on (implicitly or explicitly) modeling the underlying 3D. The classical framework of analysis by synthesis casts this inference as a joint optimization seeking to explain the observed pixels, and recent instantiations learn expressive 3D representations (e.g., Neural Fields) with gradient-descent-based pose refinement of initial pose estimates. However, given a sparse set of observed views, the observations may not provide sufficient direct evidence to obtain complete and accurate 3D. Moreover, large errors in pose estimation may not be easily corrected and can further degrade the inferred 3D. To allow robust 3D reconstruction and pose estimation in this challenging setup, we propose SparseAGS, a method that adapts this analysis-by-synthesis approach by: a) including novel-view-synthesis-based generative priors in conjunction with photometric objectives to improve the quality of the inferred 3D, and b) explicitly reasoning about outliers and using a discrete search with a continuous optimization-based strategy to correct them. We validate our framework across real-world and synthetic datasets in combination with several off-the-shelf pose estimation systems as initialization. We find that it significantly improves the base systems' pose accuracy while yielding high-quality 3D reconstructions that outperform the results from current multi-view reconstruction baselines.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-69" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-69', event_id='92993', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1510</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92993">ReGS: Reference-based Controllable Scene Stylization with Gaussian Splatting</a></strong></h5>


                        <p class="text-muted">
                            Yiqun Mei &middot; Jiacong Xu &middot; Vishal Patel
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Referenced-based scene stylization that edits the appearance based on a content-aligned reference image is an emerging research area. Starting with a pretrained neural radiance field (NeRF), existing methods typically learn a novel appearance that matches the given style. Despite their effectiveness, they inherently suffer from time-consuming volume rendering, and thus are impractical for many real-time applications. In this work, we propose ReGS, which adapts 3D Gaussian Splatting (3DGS) for reference-based stylization to enable real-time stylized view synthesis. Editing the appearance of a pretrained 3DGS is challenging as it uses discrete Gaussians as 3D representation, which tightly bind appearance with geometry. Simply optimizing the appearance as prior methods do is often insufficient for modeling continuous textures in the given reference image. To address this challenge, we propose a novel texture-guided control mechanism that adaptively adjusts local responsible Gaussians to a new geometric arrangement, serving for desired texture details. The proposed process is guided by texture clues for effective appearance editing, and regularized by scene depth for preserving original geometric structure. With these novel designs, we show ReGs can produce state-of-the-art stylization results that respect the reference texture while embracing real-time rendering speed for free-view navigation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-70" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-70', event_id='92975', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1511</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92975">Unveiling the Hidden: Online Vectorized HD Map Construction with Clip-Level Token Interaction and Propagation</a></strong></h5>


                        <p class="text-muted">
                            Nayeon Kim &middot; Hongje Seong &middot; Daehyun Ji &middot; Sujin Jang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Predicting and constructing road geometric information (e.g., lane lines, road markers) is a crucial task for safe autonomous driving, while such static map elements can be repeatedly occluded by various dynamic objects on the road. Recent studies have shown significantly improved vectorized high-definition (HD) map construction performance, but there has been insufficient investigation of temporal information across adjacent input frames (i.e., clips), which may lead to inconsistent and suboptimal prediction results. To tackle this, we introduce a novel paradigm of clip-level vectorized HD map construction, MapUnveiler, which explicitly unveils the occluded map elements within a clip input by relating dense image representations with efficient clip tokens. Additionally, MapUnveiler associates inter-clip information through clip token propagation, effectively utilizing long- term temporal map information. MapUnveiler runs efficiently with the proposed clip-level pipeline by avoiding redundant computation with temporal stride while building a global map relationship. Our extensive experiments demonstrate that MapUnveiler achieves state-of-the-art performance on both the nuScenes and Argoverse2 benchmark datasets. We also showcase that MapUnveiler significantly outperforms state-of-the-art approaches in a challenging setting, achieving +10.7% mAP improvement in heavily occluded driving road scenes. The project page can be found at https://mapunveiler.github.io.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-71" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-71', event_id='95583', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1600</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95583">Zero-shot Image Editing with Reference Imitation</a></strong></h5>


                        <p class="text-muted">
                            Xi Chen &middot; Yutong Feng &middot; Mengting Chen &middot; Yiyang Wang &middot; Shilong Zhang &middot; Yu Liu &middot; Yujun Shen &middot; Hengshuang Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Image editing serves as a practical yet challenging task considering the diverse demands from users, where one of the hardest parts is to precisely describe how the edited image should look like. In this work, we present a new form of editing, termed imitative editing, to help users exercise their creativity more conveniently. Concretely, to edit an image region of interest, users are free to directly draw inspiration from some in-the-wild references (e.g., some relative pictures come across online), without having to cope with the fit between the reference and the source. Such a design requires the system to automatically figure out what to expect from the reference to perform the editing. For this purpose, we propose a generative training framework, dubbed MimicBrush, which randomly selects two frames from a video clip, masks some regions of one frame, and learns to recover the masked regions using the information from the other frame. That way, our model, developed from a diffusion prior, is able to capture the semantic correspondence between separate images in a self-supervised manner. We experimentally show the effectiveness of our method under various test cases as well as its superiority over existing alternatives. We also construct a benchmark to facilitate further research.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-72" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-72', event_id='95642', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1601</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95642">DiMSUM: Diffusion Mamba - A Scalable and Unified Spatial-Frequency Method for Image Generation</a></strong></h5>


                        <p class="text-muted">
                            Hao Phung &middot; Quan Dao &middot; Trung Dao &middot; Viet Hoang Phan &middot; Dimitris Metaxas &middot; Anh Tran
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce a novel state-space architecture for diffusion models, effectively harnessing spatial and frequency information to enhance the inductive bias towards local features in input images for image generation tasks. While state-space networks, including Mamba, a revolutionary advancement in recurrent neural networks, typically scan input sequences from left to right, they face difficulties in designing effective scanning strategies, especially in the processing of image data. Our method demonstrates that integrating wavelet transformation into Mamba enhances the local structure awareness of visual inputs and better captures long-range relations of frequencies by disentangling them into wavelet subbands, representing both low- and high-frequency components. These wavelet-based outputs are then processed and seamlessly fused with the original Mamba outputs through a cross-attention fusion layer, combining both spatial and frequency information to optimize the order awareness of state-space models which is essential for the details and overall quality of image generation. Besides, we introduce a globally-shared transformer to supercharge the performance of Mamba, harnessing its exceptional power to capture global relationships. Through extensive experiments on standard benchmarks, our method demonstrates superior results compared to DiT and DIFFUSSM, achieving faster training convergence and delivering high-quality outputs. The codes and pretrained models are released at https://github.com/VinAIResearch/DiMSUM.git.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-73" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-73', event_id='96021', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1602</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96021">SHMT: Self-supervised Hierarchical Makeup Transfer via Latent Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Zhaoyang Sun &middot; Shengwu Xiong &middot; Yaxiong Chen &middot; Fei Du &middot; Weihua Chen &middot; Fan Wang &middot; Yi Rong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper studies the challenging task of makeup transfer, which aims to apply diverse makeup styles precisely and naturally to a given facial image.  Due to the absence of paired data, current methods typically synthesize sub-optimal pseudo ground truths to guide the model training, resulting in low makeup fidelity. Additionally, different makeup styles generally have varying effects on the person face, but existing methods struggle to deal with this diversity. To address these issues, we propose a novel Self-supervised Hierarchical Makeup Transfer (SHMT) method via latent diffusion models. Following a "decoupling-and-reconstruction" paradigm, SHMT works in a self-supervised manner, freeing itself from the misguidance of imprecise pseudo-paired data. Furthermore, to accommodate a variety of makeup styles, hierarchical texture details are decomposed via a Laplacian pyramid and selectively introduced to the content representation. Finally, we design a novel Iterative Dual Alignment (IDA) module that dynamically adjusts the injection condition of the diffusion model, allowing the alignment errors caused by the domain gap between content and makeup representations to be corrected. Extensive quantitative and qualitative analyses demonstrate the effectiveness of our method. Our code is available at https://github.com/Snowfallingplum/SHMT.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-74" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-74', event_id='96290', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1603</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96290">UDPM: Upsampling Diffusion Probabilistic Models</a></strong></h5>


                        <p class="text-muted">
                            Shady Abu-Hussein &middot; Raja Giryes
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Denoising Diffusion Probabilistic Models (DDPM) have recently gained significant attention. DDPMs compose a Markovian process that begins in the data domain and gradually adds noise until reaching pure white noise. DDPMs generate high-quality samples from complex data distributions by defining an inverse process and training a deep neural network to learn this mapping. However, these models are inefficient because they require many diffusion steps to produce aesthetically pleasing samples. Additionally, unlike generative adversarial networks (GANs), the latent space of diffusion models is less interpretable. In this work, we propose to generalize the denoising diffusion process into an Upsampling Diffusion Probabilistic Model (UDPM). In the forward process, we reduce the latent variable dimension through downsampling, followed by the traditional noise perturbation. As a result, the reverse process gradually denoises and upsamples the latent variable to produce a sample from the data distribution. We formalize the Markovian diffusion processes of UDPM and demonstrate its generation capabilities on the popular FFHQ, AFHQv2, and CIFAR10 datasets. UDPM generates images with as few as three network evaluations, whose overall computational cost is less than a single DDPM or EDM step while achieving an FID score of 6.86. This surpasses current state-of-the-art efficient diffusion models that use a single denoising step for sampling. Additionally, UDPM offers an interpretable and interpolable latent space, which gives it an advantage over traditional DDPMs. Our code is available online: \url{https://github.com/shadyabh/UDPM/}</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-75" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-75', event_id='96399', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1604</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96399">Rethinking The Training And Evaluation of Rich-Context Layout-to-Image Generation</a></strong></h5>


                        <p class="text-muted">
                            Jiaxin Cheng &middot; ZIXU ZHAO &middot; Tong He &middot; Tianjun Xiao &middot; Yicong Zhou &middot; Zheng Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advancements in generative models have significantly enhanced their capacity for image generation, enabling a wide range of applications such as image editing, completion and video editing. A specialized area within generative modeling is layout-to-image (L2I) generation, where predefined layouts of objects guide the generative process. In this study, we introduce a novel regional cross-attention module tailored to enrich layout-to-image generation. This module notably improves the representation of layout regions, particularly in scenarios where existing methods struggle with highly complex and detailed textual descriptions. Moreover, while current open-vocabulary L2I methods are trained in an open-set setting, their evaluations often occur in closed-set environments. To bridge this gap, we propose two metrics to assess L2I performance in open-vocabulary scenarios. Additionally, we conduct a comprehensive user study to validate the consistency of these metrics with human preferences.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-76" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-76', event_id='96852', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1605</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96852">Unlocking the Capabilities of Masked Generative Models for Image Synthesis via Self-Guidance</a></strong></h5>


                        <p class="text-muted">
                            Jiwan Hur &middot; DongJae Lee &middot; Gyojin Han &middot; Jaehyun Choi &middot; Yunho Jeon &middot; Junmo Kim
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Masked generative models (MGMs) have shown impressive generative ability while providing an order of magnitude efficient sampling steps compared to continuous diffusion models. However, MGMs still underperform in image synthesis compared to recent well-developed continuous diffusion models with similar size in terms of quality and diversity of generated samples. A key factor in the performance of continuous diffusion models stems from the guidance methods, which enhance the sample quality at the expense of diversity. In this paper, we extend these guidance methods to generalized guidance formulation for MGMs and propose a self-guidance sampling method, which leads to better generation quality. The proposed approach leverages an auxiliary task for semantic smoothing in vector-quantized token space, analogous to the Gaussian blur in continuous pixel space. Equipped with the parameter-efficient fine-tuning method and high-temperature sampling, MGMs with the proposed self-guidance achieve a superior quality-diversity trade-off, outperforming existing sampling methods in MGMs with more efficient training and sampling costs. Extensive experiments with the various sampling hyperparameters confirm the effectiveness of the proposed self-guidance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-77" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-77', event_id='97473', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1606</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97473">Learning Action and Reasoning-Centric Image Editing from Videos and Simulation</a></strong></h5>


                        <p class="text-muted">
                            Benno Krojer &middot; Dheeraj Vattikonda &middot; Luis Lara &middot; Varun Jampani &middot; Eva Portelance &middot; Chris Pal &middot; Siva Reddy
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>An image editing model should be able to perform diverse edits, ranging from object replacement, changing attributes or style, to performing actions or movement, which require many forms of reasoning. Current <em>general</em> instruction-guided editing models have significant shortcomings with action and reasoning-centric edits.Object, attribute or stylistic changes can be learned from visually static datasets. On the other hand, high-quality data for action and reasoning-centric edits is scarce and has to come from entirely different sources that cover e.g. physical dynamics, temporality and spatial reasoning.To this end, we meticulously curate the <strong>A</strong>U<strong>RO</strong>R<strong>A</strong> Dataset (<strong>A</strong>ction-<strong>R</strong>easoning-<strong>O</strong>bject-<strong>A</strong>ttribute), a collection of high-quality training data, human-annotated and curated from videos and simulation engines.We focus on a key aspect of quality training data: triplets (source image, prompt, target image) contain a single meaningful visual change described by the prompt, i.e., <em>truly minimal</em> changes between source and target images.To demonstrate the value of our dataset, we evaluate an <strong>A</strong>U<strong>RO</strong>R<strong>A</strong>-finetuned model on a new expert-curated benchmark (<strong>A</strong>U<strong>RO</strong>R<strong>A-Bench</strong>) covering 8 diverse editing tasks.Our model significantly outperforms previous editing models as judged by human raters.For automatic evaluations, we find important flaws in previous metrics and caution their use for semantically hard editing tasks.Instead, we propose a new automatic metric that focuses on discriminative understanding.We hope that our efforts : (1) curating a quality training dataset and an evaluation benchmark, (2) developing critical evaluations, and (3) releasing a state-of-the-art model, will fuel further progress on general image editing.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-78" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-78', event_id='93769', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1607</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93769">START: A Generalized State Space Model with Saliency-Driven Token-Aware Transformation</a></strong></h5>


                        <p class="text-muted">
                            Jintao Guo &middot; Lei Qi &middot; Yinghuan Shi &middot; Yang Gao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Domain Generalization (DG) aims to enable models to generalize to unseen target domains by learning from multiple source domains. Existing DG methods primarily rely on convolutional neural networks (CNNs), which inherently learn texture biases due to their limited receptive fields, making them prone to overfitting source domains. While some works have introduced transformer-based methods (ViTs) for DG to leverage the global receptive field, these methods incur high computational costs due to the quadratic complexity of self-attention. Recently, advanced state space models (SSMs), represented by Mamba, have shown promising results in supervised learning tasks by achieving linear complexity in sequence length during training and fast RNN-like computation during inference. Inspired by this, we investigate the generalization ability of the Mamba model under domain shifts and find that input-dependent matrices within SSMs could accumulate and amplify domain-specific features, thus hindering model generalization. To address this issue, we propose a novel SSM-based architecture with saliency-based token-aware transformation (namely START), which achieves state-of-the-art (SOTA) performances and offers a competitive alternative to CNNs and ViTs. Our START can selectively perturb and suppress domain-specific features in salient tokens within the input-dependent matrices of SSMs, thus effectively reducing the discrepancy between different domains. Extensive experiments on five benchmarks demonstrate that START outperforms existing SOTA DG methods with efficient linear complexity. Our code is available at https://github.com/lingeringlight/START.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-79" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-79', event_id='94677', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1608</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94677">AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation</a></strong></h5>


                        <p class="text-muted">
                            Yuhan Zhu &middot; Yuyang Ji &middot; Zhiyu Zhao &middot; Gangshan Wu &middot; Limin Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Pre-trained vision-language models (VLMs) have shown impressive results in various visual classification tasks.However, we often fail to fully unleash their potential when adapting them for new concept understanding due to limited information on new classes.To address this limitation, we introduce a novel adaptation framework, AWT (Augment, Weight, then Transport). AWT comprises three key components: augmenting inputs with diverse visual perspectives and enriched class descriptions through image transformations and language models; dynamically weighting inputs based on the prediction entropy; and employing optimal transport to mine semantic correlations in the vision-language space.AWT can be seamlessly integrated into various VLMs, enhancing their zero-shot capabilities without additional training and facilitating few-shot learning through an integrated multimodal adapter module.We verify AWT in multiple challenging scenarios, including zero-shot and few-shot image classification, zero-shot video action recognition, and out-of-distribution generalization. AWT consistently outperforms the state-of-the-art methods in each setting. In addition, our extensive studies further demonstrate AWT's effectiveness and adaptability across different VLMs, architectures, and scales.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-80" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-80', event_id='96426', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1609</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96426">Improving Visual Prompt Tuning by Gaussian Neighborhood Minimization for Long-Tailed Visual Recognition</a></strong></h5>


                        <p class="text-muted">
                            Mengke Li &middot; Ye Liu &middot; Yang Lu &middot; Yiqun Zhang &middot; Yiu-ming Cheung &middot; Hui Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Long-tailed visual recognition has received increasing attention recently. Despite fine-tuning techniques represented by visual prompt tuning (VPT) achieving substantial performance improvement by leveraging pre-trained knowledge, models still exhibit unsatisfactory generalization performance on tail classes. To address this issue, we propose a novel optimization strategy called Gaussian neighborhood minimization prompt tuning (GNM-PT), for VPT to address the long-tail learning problem. We introduce a novel Gaussian neighborhood loss, which provides a tight upper bound on the loss function of data distribution, facilitating a flattened loss landscape correlated to improved model generalization. Specifically, GNM-PT seeks the gradient descent direction within a random parameter neighborhood, independent of input samples, during each gradient update. Ultimately, GNM-PT enhances generalization across all classes while simultaneously reducing computational overhead. The proposed GNM-PT achieves state-of-the-art classification accuracies of 90.3%, 76.5%, and 50.1% on benchmark datasets CIFAR100-LT (IR 100), iNaturalist 2018, and Places-LT, respectively. The source code is available at https://github.com/Keke921/GNM-PT.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-81" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-81', event_id='99344', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1610</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/99344">Reproducibility study of ‚ÄúLICO: Explainable Models with Language-Image Consistency&quot;</a></strong></h5>


                        <p class="text-muted">
                            Luan Fletcher &middot; Robert van der Klis &middot; Martin Sedlacek &middot; Stefan Vasilev &middot; Christos Athanasiadis
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The growing reproducibility crisis in machine learning has brought forward a need for careful examination of research findings. This paper investigates the claims made by Lei et al. (2023) regarding their proposed method, LICO, for enhancing post-hoc interpretability techniques and improving image classification performance. LICO leverages natural language supervision from a vision-language model to enrich feature representations and guide the learning process. We conduct a comprehensive reproducibility study, employing (Wide) ResNets and established interpretability methods like Grad-CAM and RISE. We were mostly unable to reproduce the authors' results. In particular, we did not find that LICO consistently led to improved classification performance or improvements in quantitative and qualitative measures of interpretability. Thus, our findings highlight the importance of rigorous evaluation and transparent reporting in interpretability research.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-82" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-82', event_id='92942', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1611</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92942">SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution</a></strong></h5>


                        <p class="text-muted">
                            Qi Tang &middot; Yao Zhao &middot; Meiqin Liu &middot; Chao Yao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion-based Video Super-Resolution (VSR) is renowned for generating perceptually realistic videos, yet it grapples with maintaining detail consistency across frames due to stochastic fluctuations. The traditional approach of pixel-level alignment is ineffective for diffusion-processed frames because of iterative disruptions. To overcome this, we introduce SeeClear--a novel VSR framework leveraging conditional video generation, orchestrated by instance-centric and channel-wise semantic controls. This framework integrates a Semantic Distiller and a Pixel Condenser, which synergize to extract and upscale semantic details from low-resolution frames. The Instance-Centric Alignment Module (InCAM) utilizes video-clip-wise tokens to dynamically relate pixels within and across frames, enhancing coherency. Additionally, the Channel-wise Texture Aggregation Memory (CaTeGory) infuses extrinsic knowledge, capitalizing on long-standing semantic textures. Our method also innovates the blurring diffusion process with the ResShift mechanism, finely balancing between sharpness and diffusion effects. Comprehensive experiments confirm our framework's advantage over state-of-the-art diffusion-based VSR techniques.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-83" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-83', event_id='95260', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1700</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95260">FashionR2R: Texture-preserving Rendered-to-Real Image Translation with Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Rui Hu &middot; Qian He &middot; Gaofeng He &middot; Jiedong Zhuang &middot; Huang Chen &middot; Huafeng Liu &middot; Huamin Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Modeling and producing lifelike clothed human images has attracted researchers' attention from different areas for decades, with the complexity from highly articulated and structured content. Rendering algorithms decompose and simulate the imaging process of a camera, while are limited by the accuracy of modeled variables and the efficiency of computation. Generative models can produce impressively vivid human images, however still lacking in controllability and editability. This paper studies photorealism enhancement of rendered images, leveraging generative power from diffusion models on the controlled basis of rendering. We introduce a novel framework to translate rendered images into their realistic counterparts, which consists of two stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG). In DKI, we adopt positive (real) domain finetuning and negative (rendered) domain embedding to inject knowledge into a pretrained Text-to-image (T2I) diffusion model. In RIG, we generate the realistic image corresponding to the input rendered image, with a Texture-preserving Attention Control (TAC) to preserve fine-grained clothing textures, exploiting the decoupled features encoded in the UNet structure. Additionally, we introduce SynFashion dataset, featuring high-quality digital clothing images with diverse textures. Extensive experimental results demonstrate the superiority and effectiveness of our method in rendered-to-real image translation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-84" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-84', event_id='94657', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1701</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94657">pcaGAN: Improving Posterior-Sampling cGANs via Principal Component Regularization</a></strong></h5>


                        <p class="text-muted">
                            Matthew Bendel &middot; Rizwan Ahmad &middot; Philip Schniter
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In ill-posed imaging inverse problems, there can exist many hypotheses that fit both the observed measurements and prior knowledge of the true image. Rather than returning just one hypothesis of that image, posterior samplers aim to explore the full solution space by generating many probable hypotheses, which can later be used to quantify uncertainty or construct recoveries that appropriately navigate the perception/distortion trade-off. In this work, we propose a fast and accurate posterior-sampling conditional generative adversarial network (cGAN) that, through a novel form of regularization, aims for correctness in the posterior mean as well as the trace and K principal components of the posterior covariance matrix. Numerical experiments demonstrate that our method outperforms competitors in a wide range of ill-posed imaging inverse problems.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-85" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-85', event_id='94397', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1702</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94397">IF-Font: Ideographic Description Sequence-Following Font Generation</a></strong></h5>


                        <p class="text-muted">
                            Xinping Chen &middot; Xiao Ke &middot; Wenzhong Guo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Few-shot font generation (FFG) aims to learn the target style from a limited number of reference glyphs and generate the remaining glyphs in the target font. Previous works focus on disentangling the content and style features of glyphs, combining the content features of the source glyph with the style features of the reference glyph to generate new glyphs. However, the disentanglement is challenging due to the complexity of glyphs, often resulting in glyphs that are influenced by the style of the source glyph and prone to artifacts. We propose IF-Font, a novel paradigm which incorporates Ideographic Description Sequence (IDS) instead of the source glyph to control the semantics of generated glyphs. To achieve this, we quantize the reference glyphs into tokens, and model the token distribution of target glyphs using corresponding IDS and reference tokens. The proposed method excels in synthesizing glyphs with neat and correct strokes, and enables the creation of new glyphs based on provided IDS. Extensive experiments demonstrate that our method greatly outperforms state-of-the-art methods in both one-shot and few-shot settings, particularly when the target styles differ significantly from the training font styles. The code is available at <a href="https://github.com/Stareven233/IF-Font">https://github.com/Stareven233/IF-Font</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-86" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-86', event_id='94016', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1704</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94016">ReFIR: Grounding Large Restoration Models with Retrieval Augmentation</a></strong></h5>


                        <p class="text-muted">
                            Hang Guo &middot; Tao Dai &middot; Zhihao Ouyang &middot; Taolin Zhang &middot; Yaohua Zha &middot; Bin Chen &middot; Shu-Tao Xia
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advances in diffusion-based Large Restoration Models (LRMs) have significantly improved photo-realistic image restoration by leveraging the internal knowledge embedded within model weights. However, existing LRMs often suffer from the hallucination dilemma, i.e., producing incorrect contents or textures when dealing with severe degradations, due to their heavy reliance on limited internal knowledge. In this paper, we propose an orthogonal solution called the Retrieval-augmented Framework for Image Restoration (ReFIR), which incorporates retrieved images as external knowledge to extend the knowledge boundary of existing LRMs in generating details faithful to the original scene.  Specifically, we first introduce the nearest neighbor lookup to retrieve content-relevant high-quality images as reference, after which we propose the cross-image injection to modify existing LRMs to utilize high-quality textures from retrieved images. Thanks to the additional external knowledge, our ReFIR can well handle the hallucination challenge and facilitate faithfully results. Extensive experiments demonstrate that ReFIR can achieve not only high-fidelity but also realistic restoration results. Importantly, our ReFIR requires no training and is adaptable to various LRMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-87" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-87', event_id='93397', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1705</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93397">HairFastGAN: Realistic and Robust Hair Transfer with a Fast Encoder-Based Approach</a></strong></h5>


                        <p class="text-muted">
                            Maxim Nikolaev &middot; Mikhail Kuznetsov &middot; Dmitry Vetrov &middot; Aibek Alanov
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Our paper addresses the complex task of transferring a hairstyle from a reference image to an input photo for virtual hair try-on. This task is challenging due to the need to adapt to various photo poses, the sensitivity of hairstyles, and the lack of objective metrics. The current state of the art hairstyle transfer methods use an optimization process for different parts of the approach, making them inexcusably slow. At the same time, faster encoder-based models are of very low quality because they either operate in StyleGAN's W+ space or use other low-dimensional image generators. Additionally, both approaches have a problem with hairstyle transfer when the source pose is very different from the target pose, because they either don't consider the pose at all or deal with it inefficiently. In our paper, we present the HairFast model, which uniquely solves these problems and achieves high resolution, near real-time performance, and superior reconstruction compared to optimization problem-based methods. Our solution includes a new architecture operating in the FS latent space of StyleGAN, an enhanced inpainting approach, and improved encoders for better alignment, color transfer, and a new encoder for post-processing. The effectiveness of our approach is demonstrated on realism metrics after random hairstyle transfer and reconstruction when the original hairstyle is transferred. In the most difficult scenario of transferring both shape and color of a hairstyle from different images, our method performs in less than a second on the Nvidia V100.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-88" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-88', event_id='96318', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1706</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96318">Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation</a></strong></h5>


                        <p class="text-muted">
                            Jian Hu &middot; Jiayi Lin &middot; Junchi Yan &middot; Shaogang Gong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Promptable segmentation typically requires instance-specific manual prompts to guide the segmentation of each desired object. To minimize such a need, task-generic promptable segmentation has been introduced, which employs a single task-generic prompt to segment various images of different objects in the same task. Current methods use Multimodal Large Language Models (MLLMs) to reason detailed instance-specific prompts from a task-generic prompt for improving segmentation accuracy. The effectiveness of this segmentation heavily depends on the precision of these derived prompts. However, MLLMs often suffer hallucinations during reasoning, resulting in inaccurate prompting.  While existing methods focus on eliminating hallucinations to improve a model, we argue that MLLM hallucinations can reveal valuable contextual insights when leveraged correctly, as they represent pre-trained large-scale knowledge beyond individual images. In this paper, we first utilize hallucinations to mine task-related information from images and verify its accuracy to enhance precision of the generated prompts.  Specifically, we introduce an iterative \textbf{Pro}mpt-\textbf{Ma}sk \textbf{C}ycle generation framework (ProMaC) with a prompt generator and a mask generator.  The prompt generator uses a multi-scale chain of thought prompting, initially leveraging hallucinations to extract extended contextual prompts on a test image. These hallucinations are then minimized to formulate precise instance-specific prompts, directing the mask generator to produce masks that are consistent with task semantics by mask semantic alignment. Iteratively the generated masks induce the prompt generator to focus more on task-relevant image areas and reduce irrelevant hallucinations, resulting jointly in better prompts and masks. Experiments on 5 benchmarks demonstrate the effectiveness of ProMaC. Code is in https://lwpyh.github.io/ProMaC/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-89" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-89', event_id='94820', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1707</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94820">OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding</a></strong></h5>


                        <p class="text-muted">
                            Tao Zhang &middot; Xiangtai Li &middot; Hao Fei &middot; Haobo Yuan &middot; Shengqiong Wu &middot; Shunping Ji &middot; Chen Change Loy &middot; Shuicheng Yan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Current universal segmentation methods demonstrate strong capabilities in pixel-level image and video understanding. However, they lack reasoning abilities and cannot be controlled via text instructions. In contrast, large vision-language multimodal models exhibit powerful vision-based conversation and reasoning capabilities but lack pixel-level understanding and have difficulty accepting visual prompts for flexible user interaction. This paper proposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level vision understanding with reasoning abilities. It can accept various visual and text prompts for flexible user interaction. Specifically, we use a universal segmentation method as the visual encoder, integrating image information, perception priors, and visual prompts into visual tokens provided to the LLM. The LLM is responsible for understanding the user's text instructions and providing text responses and pixel-level segmentation results based on the visual information. We propose perception prior embedding to better integrate perception priors with image features. OMG-LLaVA achieves image-level, object-level, and pixel-level reasoning and understanding in a single model, matching or surpassing the performance of specialized methods on multiple benchmarks. Rather than using LLM to connect each specialist, our work aims at end-to-end training on one encoder, one decoder, and one LLM. The code and model have been released for further research.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-90" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-90', event_id='94482', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1708</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94482">One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos</a></strong></h5>


                        <p class="text-muted">
                            Zechen Bai &middot; Tong He &middot; Haiyang Mei &middot; Pichao WANG &middot; Ziteng Gao &middot; Joya Chen &middot; liulei &middot; Zheng Zhang &middot; Mike Zheng Shou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-91" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-91', event_id='94140', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1709</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94140">Automated Label Unification for Multi-Dataset Semantic Segmentation with GNNs</a></strong></h5>


                        <p class="text-muted">
                            Ma Rong &middot; Jie Chen &middot; Xiangyang Xue &middot; Jian Pu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deep supervised models possess significant capability to assimilate extensive training data, thereby presenting an opportunity to enhance model performance through training on multiple datasets. However, conflicts arising from different label spaces among datasets may adversely affect model performance. In this paper, we propose a novel approach to automatically construct a unified label space across multiple datasets using graph neural networks. This enables semantic segmentation models to be trained simultaneously on multiple datasets, resulting in performance improvements. Unlike existing methods, our approach facilitates seamless training without the need for additional manual reannotation or taxonomy reconciliation. This significantly enhances the efficiency and effectiveness of multi-dataset segmentation model training. The results demonstrate that our method significantly outperforms other multi-dataset training methods when trained on seven datasets simultaneously, and achieves state-of-the-art performance on the WildDash 2 benchmark. Our code can be found in https://github.com/Mrhonor/AutoUniSeg.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-92" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-92', event_id='94069', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1710</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94069">LESS: Label-Efficient and Single-Stage Referring 3D Segmentation</a></strong></h5>


                        <p class="text-muted">
                            Xuexun Liu &middot; Xiaoxu Xu &middot; Jinlong Li &middot; Qiudan Zhang &middot; Xu Wang &middot; Nicu Sebe &middot; Lin Ma
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Referring 3D Segmentation is a visual-language task that segments all points of the specified object from a 3D point cloud described by a sentence of query. Previous works perform a two-stage paradigm, first conducting language-agnostic instance segmentation then matching with given text query. However, the semantic concepts from text query and visual cues are separately interacted during the training, and both instance and semantic labels for each object are required, which is time consuming and human-labor intensive. To mitigate these issues, we propose a novel Referring 3D Segmentation pipeline, Label-Efficient and Single-Stage, dubbed LESS, which is only under the supervision of efficient binary mask. Specifically, we design a Point-Word Cross-Modal Alignment module for aligning the fine-grained features of points and textual embedding. Query Mask Predictor module and Query-Sentence Alignment module are introduced for coarse-grained alignment between masks and query. Furthermore, we propose an area regularization loss, which coarsely reduces irrelevant background predictions on a large scale. Besides, a point-to-point contrastive loss is proposed concentrating on distinguishing points with subtly similar features. Through extensive experiments, we achieve state-of-the-art performance on ScanRefer dataset by surpassing the previous methods about 3.7% mIoU using only binary labels. Code is available at https://github.com/mellody11/LESS.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-93" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-93', event_id='93908', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1711</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93908">Towards Flexible Visual Relationship Segmentation</a></strong></h5>


                        <p class="text-muted">
                            Fangrui Zhu &middot; Jianwei Yang &middot; Huaizu Jiang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Visual relationship understanding has been studied separately in human-object interaction(HOI) detection, scene graph generation(SGG),  and referring relationships(RR) tasks. Given the complexity and interconnectedness of these tasks, it is crucial to have a flexible framework that can effectively address these tasks in a cohesive manner.In this work, we propose FleVRS, a single model that seamlessly integrates the above three aspects in standard and promptable visual relationship segmentation, and further possesses the capability for open-vocabulary segmentation to adapt to novel scenarios. FleVRS leverages the synergy between text and image modalities, to ground various types of relationships from images and use textual features from vision-language models to visual conceptual understanding.Empirical validation across various datasets demonstrates that our framework outperforms existing models in standard, promptable, and open-vocabulary tasks, e.g., +1.9 $mAP$ on HICO-DET, +11.4 $Acc$ on VRD,  +4.7 $mAP$ on unseen HICO-DET.Our FleVRS represents a significant step towards a more intuitive, comprehensive, and scalable understanding of visual relationships.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-94" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-94', event_id='97789', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1800</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97789">ShareGPT4Video: Improving Video Understanding and Generation with Better Captions</a></strong></h5>


                        <p class="text-muted">
                            Lin Chen &middot; Xilin Wei &middot; Jinsong Li &middot; Xiaoyi Dong &middot; Pan Zhang &middot; Yuhang Zang &middot; Zehui Chen &middot; Haodong Duan &middot; lin bin &middot; Zhenyu Tang &middot; Li Yuan &middot; Yu Qiao &middot; Dahua Lin &middot; Feng Zhao &middot; Jiaqi Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions. The series comprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and annotating strategy. 2) ShareCaptioner-Video, an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it. 3) ShareGPT4Video-8B, a simple yet superb LVLM that reached SOTA performance on three advancing video benchmarks. To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 1) Inter-frame precise temporal change understanding. 2) Intra-frame detailed content description. 3) Frame-number scalability for arbitrary-length videos. To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length. Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos. We annotated 4.8M aesthetically appealing videos by it and verified their effectiveness on a 10-second text2video generation task. For video understanding, we verified the effectiveness of ShareGPT4Video on several current LVLM architectures and presented our superb new LVLM ShareGPT4Video-8B. All the models, strategies, and annotations will be open-sourced and we hope this project can serve as a pivotal resource for advancing both the LVLMs and T2VMs community.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-95" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-95', event_id='93659', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1801</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93659">ActAnywhere: Subject-Aware Video Background Generation</a></strong></h5>


                        <p class="text-muted">
                            Boxiao Pan &middot; Zhan Xu &middot; Chun-Hao Huang &middot; Krishna Kumar Singh &middot; Yang Zhou &middot; Leonidas Guibas &middot; Jimei Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We study a novel problem to automatically generate video background that tailors to foreground subject motion. It is an important problem for the movie industry and visual effects community, which traditionally requires tedious manual efforts to solve. To this end, we propose ActAnywhere, a video diffusion model that takes as input a sequence of foreground subject segmentation and an image of a novel background and generates a video of the subject interacting in this background. We train our model on a large-scale dataset of 2.4M videos of human-scene interactions. Through extensive evaluation, we show that our model produces videos with realistic foreground-background interaction while strictly following the guidance of the condition image. Our model generalizes to diverse scenarios including non-human subjects, gaming and animation clips, as well as videos with multiple moving subjects. Both quantitative and qualitative comparisons demonstrate that our model significantly outperforms existing methods, which fail to accomplish the studied task. Please visit our project webpage at https://actanywhere.github.io.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-96" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-96', event_id='94507', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1802</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94507">NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing</a></strong></h5>


                        <p class="text-muted">
                            Ting-Hsuan Chen &middot; Jie Wen Chan &middot; Hau-Shiang Shiu &middot; Shih-Han Yen &middot; Changhan Yeh &middot; Yu-Lun Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose a video editing framework, NaRCan, which integrates a hybrid deformation field and diffusion prior to generate high-quality natural canonical images to represent the input video. Our approach utilizes homography to model global motion and employs multi-layer perceptrons (MLPs) to capture local residual deformations, enhancing the model‚Äôs ability to handle complex video dynamics. By introducing a diffusion prior from the early stages of training, our model ensures that the generated images retain a high-quality natural appearance, making the produced canonical images suitable for various downstream tasks in video editing, a capability not achieved by current canonical-based methods. Furthermore, we incorporate low-rank adaptation (LoRA) fine-tuning and introduce a noise and diffusion prior update scheduling technique that accelerates the training process by 14 times. Extensive experimental results show that our method outperforms existing approaches in various video editing tasks and produces coherent and high-quality edited video sequences. See our project page for video results: <a href="https://koi953215.github.io/NaRCan_page/">koi953215.github.io/NaRCan_page</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-97" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-97', event_id='94527', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1803</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94527">Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control</a></strong></h5>


                        <p class="text-muted">
                            Zhengfei Kuang &middot; Shengqu Cai &middot; Hao He &middot; Yinghao Xu &middot; Hongsheng Li &middot; Leonidas Guibas &middot; Gordon Wetzstein
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Research on video generation has recently made tremendous progress, enabling high-quality videos to be generated from text prompts or images. Adding control to the video generation process is an important goal moving forward and recent approaches that condition video generation models on camera trajectories take an important step towards this goal. Yet, it remains challenging to generate a video of the same scene from multiple different camera trajectories. Solutions to this multi-video generation problem could enable large-scale 3D scene generation with editable camera trajectories, among other applications. We introduce collaborative video diffusion (CVD) as an important step towards this vision. The CVD framework includes a novel cross-video synchronization module that promotes consistency between corresponding frames of the same video rendered from different camera poses using an epipolar attention mechanism. Trained on top of a state-of-the-art camera-control module for video generation, CVD generates multiple videos rendered from different camera trajectories with significantly better consistency than baselines, as shown in extensive experiments.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-98" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-98', event_id='95105', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1804</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95105">4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Heng Yu &middot; Chaoyang Wang &middot; Peiye Zhuang &middot; Willi Menapace &middot; Aliaksandr Siarohin &middot; Junli Cao &middot; L√°szl√≥ Jeni &middot; Sergey Tulyakov &middot; Hsin-Ying Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Existing dynamic scene generation methods mostly rely on distilling knowledge from pre-trained 3D generative models, which are typically fine-tuned on synthetic object datasets.As a result, the generated scenes are often object-centric and lack photorealism. To address these limitations, we introduce a novel pipeline designed for photorealistic text-to-4D scene generation, discarding the dependency on multi-view generative models and instead fully utilizing video generative models trained on diverse real-world datasets. Our method begins by generating a reference video using the video generation model.We then learn the canonical 3D representation of the video using a freeze-time video, delicately generated from the reference video.To handle inconsistencies in the freeze-time video, we jointly learn a per-frame deformation to model these imperfections.We then learn the temporal deformation based on the canonical representation to capture dynamic interactions in the reference video. The pipeline facilitates the generation of dynamic scenes with enhanced photorealism and structural integrity, viewable from multiple perspectives, thereby setting a new standard in 4D scene generation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-99" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-99', event_id='96694', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1805</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96694">COVE: Unleashing the Diffusion Feature Correspondence for Consistent Video Editing</a></strong></h5>


                        <p class="text-muted">
                            Jiangshan Wang &middot; Yue Ma &middot; Jiayi Guo &middot; Yicheng Xiao &middot; Gao Huang &middot; Xiu Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Video editing is an emerging task, in which most current methods adopt the pre-trained text-to-image (T2I) diffusion model to edit the source video in a zero-shot manner. Despite extensive efforts, maintaining the temporal consistency of edited videos remains challenging due to the lack of temporal constraints in the regular T2I diffusion model. To address this issue, we propose COrrespondence-guided Video Editing (COVE), leveraging the inherent diffusion feature correspondence to achieve high-quality and consistent video editing. Specifically, we propose an efficient sliding-window-based strategy to calculate the similarity among tokens in the diffusion features of source videos, identifying the tokens with high correspondence across frames. During the inversion and denoising process, we sample the tokens in noisy latent based on the correspondence and then perform self-attention within them. To save the usage of GPU memory and accelerate the editing process, we further introduce the temporal-dimensional token merging strategy, which can effectively reduce the redundancy. COVE can be seamlessly integrated into the pre-trained T2I diffusion model without the need for extra training or optimization. Extensive experiment results demonstrate that COVE achieves the start-of-the-art performance in various video editing scenarios, outperforming existing methods both quantitatively and qualitatively. The source code will be released.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-100" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-100', event_id='96771', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1806</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96771">X-Ray: A Sequential 3D Representation For Generation</a></strong></h5>


                        <p class="text-muted">
                            Tao Hu &middot; Wenhang Ge &middot; Yuyang Zhao &middot; Gim Hee Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce X-Ray, a novel 3D sequential representation inspired by the penetrability of x-ray scans. X-Ray transforms a 3D object into a series of surface frames at different layers, making it suitable for generating 3D models from images. Our method utilizes ray casting from the camera center to capture geometric and textured details, including depth, normal, and color, across all intersected surfaces. This process efficiently condenses the whole 3D object into a multi-frame video format, motivating the utilize of a network architecture similar to those in video diffusion models. This design ensures an efficient 3D representation by focusing solely on surface information. Also, we propose a two-stage pipeline to generate 3D objects from X-Ray Diffusion Model and Upsampler. We demonstrate the practicality and adaptability of our X-Ray representation by synthesizing the complete visible and hidden surfaces of a 3D object from a single input image. Experimental results reveal the state-of-the-art superiority of our representation in enhancing the accuracy of 3D generation, paving the way for new 3D representation research and practical applications. Our project page is in \url{https://tau-yihouxiang.github.io/projects/X-Ray/X-Ray.html}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-101" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-101', event_id='96841', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1807</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96841">MotionBooth: Motion-Aware Customized Text-to-Video Generation</a></strong></h5>


                        <p class="text-muted">
                            Jianzong Wu &middot; Xiangtai Li &middot; Yanhong Zeng &middot; Jiangning Zhang &middot; Qianyu Zhou &middot; Yining Li &middot; Yunhai Tong &middot; Kai Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this work, we present MotionBooth, an innovative framework designed for animating customized subjects with precise control over both object and camera movements. By leveraging a few images of a specific object, we efficiently fine-tune a text-to-video model to capture the object's shape and attributes accurately. Our approach presents subject region loss and video preservation loss to enhance the subject's learning performance, along with a subject token cross-attention loss to integrate the customized subject with motion control signals. Additionally, we propose training-free techniques for managing subject and camera motions during inference. In particular, we utilize cross-attention map manipulation to govern subject motion and introduce a novel latent shift module for camera movement control as well. MotionBooth excels in preserving the appearance of subjects while simultaneously controlling the motions in generated videos. Extensive quantitative and qualitative evaluations demonstrate the superiority and effectiveness of our method. Models and codes will be made publicly available.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-102" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-102', event_id='95485', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1808</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95485">LRM-Zero: Training Large Reconstruction Models with Synthesized Data</a></strong></h5>


                        <p class="text-muted">
                            Desai Xie &middot; Sai Bi &middot; Zhixin Shu &middot; Kai Zhang &middot; Zexiang Xu &middot; Yi Zhou &middot; Soeren Pirk &middot; Arie Kaufman &middot; Xin Sun &middot; Hao Tan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on synthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The core of LRM-Zero is our procedural 3D dataset, Zeroverse, which is automatically synthesized from simple primitive shapes with random texturing and augmentations (e.g., height fields, boolean differences, and wireframes). Unlike previous 3D datasets (e.g., Objaverse) which are often captured or crafted by humans to approximate real 3D data, Zeroverse completely ignores realistic global semantics but is rich in complex geometric and texture details that are locally similar to or even more intricate than real objects. We demonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse, can achieve high visual quality in the reconstruction of real-world objects, competitive with models trained on Objaverse. We also analyze several critical design choices of Zeroverse that contribute to LRM-Zero's capability and training stability. Our work demonstrates that 3D reconstruction, one of the core tasks in 3D vision, can potentially be addressed without the semantics of real-world objects. The Zeroverse's procedural synthesis code and interactive visualization are available at: https://desaixie.github.io/lrm-zero/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-103" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-103', event_id='92979', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1809</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92979">XMask3D: Cross-modal Mask Reasoning for Open Vocabulary 3D Semantic Segmentation</a></strong></h5>


                        <p class="text-muted">
                            Ziyi Wang &middot; Yanbo Wang &middot; Xumin Yu &middot; Jie Zhou &middot; Jiwen Lu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Existing methodologies in open vocabulary 3D semantic segmentation primarily concentrate on establishing a unified feature space encompassing 3D, 2D, and textual modalities. Nevertheless, traditional techniques such as global feature alignment or vision-language model distillation tend to impose only approximate correspondence, struggling notably with delineating fine-grained segmentation boundaries. To address this gap, we propose a more meticulous mask-level alignment between 3D features and the 2D-text embedding space through a cross-modal mask reasoning framework, XMask3D. In our approach, we developed a mask generator based on the denoising UNet from a pre-trained diffusion model, leveraging its capability for precise textual control over dense pixel representations and enhancing the open-world adaptability of the generated masks. We further integrate 3D global features as implicit conditions into the pre-trained 2D denoising UNet, enabling the generation of segmentation masks with additional 3D geometry awareness. Subsequently, the generated 2D masks are employed to align mask-level 3D representations with the vision-language feature space, thereby augmenting the open vocabulary capability of 3D geometry embeddings. Finally, we fuse complementary 2D and 3D mask features, resulting in competitive performance across multiple benchmarks for 3D open vocabulary semantic segmentation. Code is available at https://github.com/wangzy22/XMask3D.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-104" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-104', event_id='93582', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1810</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93582">A Surprisingly Simple Approach to Generalized Few-Shot Semantic Segmentation</a></strong></h5>


                        <p class="text-muted">
                            Tomoya Sakai &middot; Haoxiang Qiu &middot; Takayuki Katsuki &middot; Daiki Kimura &middot; Takayuki Osogami &middot; Tadanobu Inoue
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The goal of *generalized* few-shot semantic segmentation (GFSS) is to recognize *novel-class* objects through training with a few annotated examples and the *base-class* model that learned the knowledge about the base classes.Unlike the classic few-shot semantic segmentation, GFSS aims to classify pixels into both base and novel classes, meaning it is a more practical setting.Current GFSS methods rely on several techniques such as using combinations of customized modules, carefully designed loss functions, meta-learning, and transductive learning.However, we found that a simple rule and standard supervised learning substantially improve the GFSS performance.In this paper, we propose a simple yet effective method for GFSS that does not use the techniques mentioned above.Also, we theoretically show that our method perfectly maintains the segmentation performance of the base-class model over most of the base classes.Through numerical experiments, we demonstrated the effectiveness of our method.It improved in novel-class segmentation performance in the $1$-shot scenario by $6.1$% on the PASCAL-$5^i$ dataset, $4.7$% on the PASCAL-$10^i$ dataset, and $1.0$% on the COCO-$20^i$ dataset.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-105" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-105', event_id='93633', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1811</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93633">AdaPKC: PeakConv with Adaptive Peak Receptive Field for Radar Semantic Segmentation</a></strong></h5>


                        <p class="text-muted">
                            Teng Li &middot; Liwen Zhang &middot; Youcheng Zhang &middot; ZijunHu &middot; Pengcheng Pi &middot; Zongqing Lu &middot; Qingmin Liao &middot; Zhe Ma
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deep learning-based radar detection technology is receiving increasing attention in areas such as autonomous driving, UAV surveillance, and marine monitoring. Among recent efforts, PeakConv (PKC) provides a solution that can retain the peak response characteristics of radar signals and play the characteristics of deep convolution, thereby improving the effect of radar semantic segmentation (RSS). However, due to the use of a pre-set fixed peak receptive field sampling rule, PKC still has limitations in dealing with problems such as inconsistency of target frequency domain response broadening, non-homogeneous and time-varying characteristic of noise/clutter distribution. Therefore, this paper proposes an idea of adaptive peak receptive field, and upgrades PKC to AdaPKC based on this idea. Beyond that, a novel fine-tuning technology to further boost the performance of AdaPKC-based RSS networks is presented. Through experimental verification using various real-measured radar data (including publicly available low-cost millimeter-wave radar dataset for autonomous driving and self-collected Ku-band surveillance radar dataset), we found that the performance of AdaPKC-based models surpasses other SoTA methods in RSS tasks. The code is available at https://github.com/lihua199710/AdaPKC.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-106" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-106', event_id='97748', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1900</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97748">E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding</a></strong></h5>


                        <p class="text-muted">
                            Ye Liu &middot; Zongyang Ma &middot; Zhongang Qi &middot; Yang Wu &middot; Ying Shan &middot; Chang Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advances in Video Large Language Models (Video-LLMs) have demonstrated their great potential in general-purpose video understanding. To verify the significance of these models, a number of benchmarks have been proposed to diagnose their capabilities in different scenarios. However, existing benchmarks merely evaluate models through video-level question-answering, lacking fine-grained event-level assessment and task diversity. To fill this gap, we introduce E.T. Bench (Event-Level &amp; Time-Sensitive Video Understanding Benchmark), a large-scale and high-quality benchmark for open-ended event-level video understanding. Categorized within a 3-level task taxonomy, E.T. Bench encompasses 7.8K samples under 12 tasks with 7.7K videos (266.3h total length) under 8 domains, providing comprehensive evaluations. We extensively evaluated 9 Image-LLMs and 10 Video-LLMs on our benchmark, and the results reveal that state-of-the-art models for coarse-level (video-level) understanding struggle to solve our fine-grained tasks, e.g., grounding event-of-interests within videos, largely due to the short video context length, improper time representations, and lack of multi-event training data. Focusing on these issues, we further propose a strong baseline model, E.T. Chat, together with an instruction-tuning dataset E.T. 164K tailored for fine-grained event-level understanding. Our simple but effective solution demonstrates superior performance in multiple scenarios. This project will be publicly available.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-107" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-107', event_id='94895', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1901</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94895">A Motion-aware Spatio-temporal Graph for Video Salient Object Ranking</a></strong></h5>


                        <p class="text-muted">
                            Hao Chen &middot; Zhu Yufei &middot; Yongjian Deng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Video salient object ranking aims to simulate the human attention mechanism by dynamically prioritizing the visual attraction of objects in a scene over time. Despite its numerous practical applications, this area remains underexplored. In this work, we propose a graph model for video salient object ranking. This graph simultaneously explores multi-scale spatial contrasts and intra-/inter-instance temporal correlations across frames to extract diverse spatio-temporal saliency cues. It has two advantages: 1. Unlike previous methods that only perform global inter-frame contrast or compare all proposals across frames globally, we explicitly model the motion of each instance by comparing its features with those in the same spatial region in adjacent frames, thus obtaining more accurate motion saliency cues. 2. We synchronize the spatio-temporal saliency cues in a single graph for joint optimization, which exhibits better dynamics compared to the previous stage-wise methods that prioritize spatial cues followed by temporal cues. Additionally, we propose a simple yet effective video retargeting method based on video saliency ranking. Extensive experiments demonstrate the superiority of our model in video salient object ranking and the effectiveness of the video retargeting method. Our codes/models are released at <a href="https://github.com/zyf-815/VSOR/tree/main">https://github.com/zyf-815/VSOR/tree/main</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-108" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-108', event_id='92985', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1902</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92985">Towards Multi-Domain Learning for Generalizable Video Anomaly Detection</a></strong></h5>


                        <p class="text-muted">
                            MyeongAh Cho &middot; Taeoh Kim &middot; Minho Shim &middot; Dongyoon Wee &middot; Sangyoun Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Most of the existing Video Anomaly Detection (VAD) studies have been conducted within single-domain learning, where training and evaluation are performed on a single dataset. However, the criteria for abnormal events differ across VAD datasets, making it problematic to apply a single-domain model to other domains. In this paper, we propose a new task called Multi-Domain learning forVAD (MDVAD) to explore various real-world abnormal events using multiple datasets for a general model. MDVAD involves training on datasets from multiple domains simultaneously, and we experimentally observe that Abnormal Conflicts between domains hinder learning and generalization. The task aims to address two key objectives: (i) better distinguishing between general normal and abnormal events across multiple domains, and (ii) being aware of ambiguous abnormal conflicts. This paper is the first to tackle abnormal conflict issue and introduces a new benchmark, baselines, and evaluation protocols for MDVAD. As baselines, we propose a framework with Null(Angular)-Multiple Instance Learning and an Abnormal Conflict classifier. Through experiments on a MDVAD benchmark composed of six VAD datasets and using four different evaluation protocols, we reveal abnormal conflicts and demonstrate that the proposed baseline effectively handles these conflicts, showing robustness and adaptability across multiple domains.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-109" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-109', event_id='96307', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1903</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96307">No &quot;Zero-Shot&quot; Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</a></strong></h5>


                        <p class="text-muted">
                            Vishaal Udandarao &middot; Ameya Prabhu &middot; Adhiraj Ghosh &middot; Yash Sharma &middot; Philip Torr &middot; Adel Bibi &middot; Samuel Albanie &middot; Matthias Bethge
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Web-crawled pretraining datasets underlie the impressive "zero-shot" evaluation performance of multimodal models, such as CLIP for classification and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of "zero-shot" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during "zero-shot" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets?We comprehensively investigate this question across 34 models and 5 standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting "zero-shot" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream "zero-shot" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the Let it Wag! benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to "zero-shot" generalization capabilities under large-scale training data and compute paradigms remains to be found.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-110" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-110', event_id='95687', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1904</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95687">REDUCR: Robust Data Downsampling using Class Priority Reweighting</a></strong></h5>


                        <p class="text-muted">
                            William Bankes &middot; George Hughes &middot; Ilija Bogunovic &middot; Zi Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Modern machine learning models are becoming increasingly expensive to train for real-world image and text classification tasks, where massive web-scale data is collected in a streaming fashion. To reduce the training cost, online batch selection techniques have been developed to choose the most informative datapoints. However, many existing techniques are not robust to class imbalance and distributional shifts, and can suffer from poor worst-class generalization performance. This work introduces REDUCR, a robust and efficient data downsampling method that uses class priority reweighting. REDUCR reduces the training data while preserving worst-class generalization performance. REDUCR assigns priority weights to datapoints in a class-aware manner using an online learning algorithm. We demonstrate the data efficiency and robust performance of REDUCR on vision and text classification tasks. On web-scraped datasets with imbalanced class distributions, REDUCR significantly improves worst-class test accuracy (and average accuracy), surpassing state-of-the-art methods by around 15\%.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-111" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-111', event_id='94889', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1905</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94889">Data Acquisition via Experimental Design for Data Markets</a></strong></h5>


                        <p class="text-muted">
                            Charles Lu &middot; Baihe Huang &middot; Sai Praneeth Karimireddy &middot; Praneeth Vepakomma &middot; Michael Jordan &middot; Ramesh Raskar
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The acquisition of training data is crucial for machine learning applications. Data markets can increase the supply of data, particularly in data-scarce domains such as healthcare, by incentivizing potential data providers to join the market. A major challenge for a data buyer in such a market is choosing the most valuable data points from a data seller.  Unlike prior work in data valuation, which assumes centralized data access, we propose a federated approach to the data acquisition problem that is inspired by linear experimental design. Our proposed data acquisition method achieves lower prediction error without requiring labeled validation data and can be optimized in a fast and federated procedure. The key insight of our work is that a method that directly estimates the benefit of acquiring data for test set prediction is particularly compatible with a decentralized market setting.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-112" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-112', event_id='95993', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1906</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95993">Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts</a></strong></h5>


                        <p class="text-muted">
                            Mikayel Samvelyan &middot; Sharath Chandra Raparthy &middot; Andrei Lupu &middot; Eric Hambro &middot; Aram Markosyan &middot; Manish Bhatt &middot; Yuning Mao &middot; Minqi Jiang &middot; Jack Parker-Holder &middot; Jakob Foerster &middot; Tim Rockt√§schel &middot; Roberta Raileanu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90% across all tested models. Furthermore, we demonstrate that prompts generated by Rainbow Teaming are highly transferable and that fine-tuning models with synthetic data generated by our method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-113" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-113', event_id='95948', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1907</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95948">TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models</a></strong></h5>


                        <p class="text-muted">
                            Andrei Margeloiu &middot; Xiangjian Jiang &middot; Nikola Simidjievski &middot; Mateja Jamnik
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Data collection is often difficult in critical fields such as medicine, physics, and chemistry, yielding typically only small tabular datasets. However, classification methods tend to struggle with these small datasets, leading to poor predictive performance. Increasing the training set with additional synthetic data, similar to data augmentation in images, is commonly believed to improve downstream tabular classification performance. However, current tabular generative methods that learn either the joint distribution $ p(\mathbf{x}, y) $ or the class-conditional distribution $ p(\mathbf{x} \mid y) $ often overfit on small datasets, resulting in poor-quality synthetic data, usually worsening classification performance compared to using real data alone. To solve these challenges, we introduce TabEBM, a novel class-conditional generative method using Energy-Based Models (EBMs). Unlike existing tabular methods that use a shared model to approximate all class-conditional densities, our key innovation is to create distinct EBM generative models for each class, each modelling its class-specific data distribution individually. This approach creates robust energy landscapes, even in ambiguous class distributions. Our experiments show that TabEBM generates synthetic data with higher quality and better statistical fidelity than existing methods. When used for data augmentation, our synthetic data consistently leads to improved classification performance across diverse datasets of various sizes, especially small ones. Code is available at https://github.com/andreimargeloiu/TabEBM.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-114" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-114', event_id='96565', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1908</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96565">Algorithmic progress in language models</a></strong></h5>


                        <p class="text-muted">
                            Wing Hin Anson Ho &middot; Tamay Besiroglu &middot; Ege Erdil &middot; Zifan Guo &middot; David Owen &middot; Robi Rahman &middot; David Atkinson &middot; Neil Thompson &middot; Jaime Sevilla
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We investigate the rate at which algorithms for pre-training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that the compute required to reach a set performance threshold has halved approximately every 8 months, with a 90\% confidence interval of around 2 to 22 months, substantially faster than hardware gains per Moore's Law. We estimate augmented scaling laws, which enable us to quantify algorithmic progress and determine the relative contributions of scaling models versus innovations in training algorithms. Despite the rapid pace of algorithmic progress and the development of new architectures such as the transformer, our analysis reveals that the increase in compute made an even larger contribution to overall performance improvements over this time period. Though limited by noisy benchmark data, our analysis quantifies the rapid progress in language modeling, shedding light on the relative contributions from compute and algorithms.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-115" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-115', event_id='96366', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1909</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96366">Optimal and Approximate Adaptive Stochastic Quantization</a></strong></h5>


                        <p class="text-muted">
                            Ran Ben-Basat &middot; Yaniv Ben-Itzhak &middot; Michael Mitzenmacher &middot; Shay Vargaftik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Quantization is a fundamental optimization for many machine learning (ML) use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is adaptive, where the error is minimized with respect to a given input rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.We revisit the Adaptive Stochastic Quantization (ASQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexities. Our experiments indicate that our algorithms may open the door to using ASQ more extensively in a variety of ML applications. We also present an even faster approximation algorithm for quantizing large inputs on the fly.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-116" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-116', event_id='95542', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1910</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95542">Improving robustness to corruptions with multiplicative weight perturbations</a></strong></h5>


                        <p class="text-muted">
                            Quoc Trung Trinh &middot; Markus Heinonen &middot; Luigi Acerbi &middot; Samuel Kaski
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deep neural networks (DNNs) excel on clean images but struggle with corrupted ones. Incorporating specific corruptions into the data augmentation pipeline can improve robustness to those corruptions but may harm performance on clean images and other types of distortion. In this paper, we introduce an alternative approach that improves the robustness of DNNs to a wide range of corruptions without compromising accuracy on clean images. We first demonstrate that input perturbations can be mimicked by multiplicative perturbations in the weight space. Leveraging this, we propose Data Augmentation via Multiplicative Perturbation (DAMP), a training method that optimizes DNNs under random multiplicative weight perturbations. We also examine the recently proposed Adaptive Sharpness-Aware Minimization (ASAM) and show that it optimizes DNNs under adversarial multiplicative weight perturbations. Experiments on image classification datasets (CIFAR-10/100, TinyImageNet and ImageNet) and neural network architectures (ResNet50, ViT-S/16, ViT-B/16) show that DAMP enhances model generalization performance in the presence of corruptions across different settings. Notably, DAMP is able to train a ViT-S/16 on ImageNet from scratch, reaching the top-1 error of 23.7% which is comparable to ResNet50 without extensive data augmentations.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-117" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-117', event_id='95324', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1911</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95324">Credal Deep Ensembles for Uncertainty Quantification</a></strong></h5>


                        <p class="text-muted">
                            Kaizheng Wang &middot; Fabio Cuzzolin &middot; Shireen Kudukkil Manchingal - &middot; Keivan Shariatmadar &middot; David Moens &middot; Hans Hallez
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper introduces an innovative approach to classification called Credal Deep Ensembles (CreDEs), namely, ensembles of novel Credal-Set Neural Networks (CreNets). CreNets are trained to predict a lower and an upper probability bound for each class, which, in turn, determine a convex set of probabilities (credal set) on the class set. The training employs a loss inspired by distributionally robust optimization which simulates the potential divergence of the test distribution from the training distribution, in such a way that the width of the predicted probability interval reflects the epistemic uncertainty about the future data distribution. Ensembles can be constructed by training multiple CreNets, each associated with a different random seed, and averaging the outputted intervals. Extensive experiments are conducted on various out-of-distributions (OOD) detection benchmarks (CIFAR10/100 vs SVHN/Tiny-ImageNet, CIFAR10 vs CIFAR10-C, ImageNet vs ImageNet-O) and using different network architectures (ResNet50, VGG16, and ViT Base). Compared to Deep Ensemble baselines, CreDEs demonstrate higher test accuracy, lower expected calibration error, and significantly improved epistemic uncertainty estimation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-118" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-118', event_id='94017', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2000</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94017">Understanding the Differences in Foundation Models: Attention, State  Space Models, and Recurrent Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Jerome Sieber &middot; Carmen Amo Alonso &middot; Alexandre Didier &middot; Melanie Zeilinger &middot; Antonio Orvieto
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-119" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-119', event_id='94839', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2001</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94839">Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers</a></strong></h5>


                        <p class="text-muted">
                            Yibo Jiang &middot; Goutham Rajendran &middot; Pradeep Ravikumar &middot; Bryon Aragam
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large Language Models (LLMs) have the capacity to store and recall facts. Through experimentation with open-source models, we observe that this ability to retrieve facts can be easily manipulated by changing contexts, even without altering their factual meanings. These findings highlight that LLMs might behave like an associative memory model where certain tokens in the contexts serve as clues to retrieving facts. We mathematically explore this property by studying how transformers, the building blocks of LLMs, can complete such memory tasks. We study a simple latent concept association problem with a one-layer transformer and we show theoretically and empirically that the transformer gathers information using self-attention and uses the value matrix for associative memory.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-120" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-120', event_id='95086', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2002</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95086">Single Image Reflection Separation via Dual-Stream Interactive Transformers</a></strong></h5>


                        <p class="text-muted">
                            Qiming Hu &middot; Hainuo Wang &middot; Xiaojie Guo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite satisfactory results on ``easy'' cases of single image reflection separation, prior dual-stream methods still suffer from considerable performance degradation when facing complex ones, i.e, the transmission layer is densely entangled with the reflection having a wide distribution of spatial intensity. The main reasons come from the lack of concern on the feature correlation during interaction, and the limited receptive field. To remedy these deficiencies, this paper presents a Dual-Stream Interactive Transformer (DSIT) design. Specifically, we devise a dual-attention interactive structure that embraces a dual-stream self-attention and a layer-aware dual-stream cross-attention mechanism to simultaneously capture intra-layer and inter-layer feature correlations. Meanwhile, the introduction of attention mechanisms can also mitigate the receptive field limitation. We modulate single-stream pre-trained Transformer embeddings with dual-stream convolutional features through cross-architecture interactions to provide richer semantic priors, thereby further relieving the ill-posedness of the problem. Extensive experimental results reveal the merits of the proposed DSIT over other state-of-the-art alternatives. Our code is publicly available at https://github.com/mingcv/DSIT.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-121" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-121', event_id='93089', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2003</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93089">Infinite-Dimensional Feature Interaction</a></strong></h5>


                        <p class="text-muted">
                            Chenhui Xu &middot; FUXUN YU &middot; Maoliang Li &middot; Zihao Zheng &middot; Zirui Xu &middot; Jinjun Xiong &middot; Xiang Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The past neural network design has largely focused on feature \textit{representation space} dimension and its capacity scaling (e.g., width, depth), but overlooked the feature \textit{interaction space} scaling.  Recent advancements have shown shifted focus towards element-wise multiplication to facilitate higher-dimensional feature interaction space for better information transformation. Despite this progress, multiplications predominantly capture low-order interactions, thus remaining confined to a finite-dimensional interaction space. To transcend this limitation, classic kernel methods emerge as a promising solution to engage features in an infinite-dimensional space. We introduce InfiNet, a model architecture that enables feature interaction within an infinite-dimensional space created by RBF kernel. Our experiments reveal that InfiNet achieves new state-of-the-art, owing to its capability to leverage infinite-dimensional interactions, significantly enhancing model performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-122" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-122', event_id='93106', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2004</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93106">DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation</a></strong></h5>


                        <p class="text-muted">
                            Sunghyeon Woo &middot; Baeseong Park &middot; Byeongwook Kim &middot; Minjung Jo &middot; Se Jung Kwon &middot; Dongsuk Jeon &middot; Dongsoo Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Large language models (LLMs) have achieved significant success across various domains. However, training these LLMs typically involves substantial memory and computational costs during both forward and backward propagation. While parameter-efficient fine-tuning (PEFT) considerably reduces the training memory associated with parameters, it does not address the significant computational costs and activation memory. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs and activation memory while maintaining accuracy. DropBP randomly drops layers during backward propagation, which is essentially equivalent to training shallow submodules generated by undropped layers and residual connections. Additionally, DropBP calculates the sensitivity of each layer to assign an appropriate drop rate, thereby stabilizing the training process. DropBP is not only applicable to full fine-tuning but can also be orthogonally integrated with all types of PEFT by dropping layers during backward propagation. Specifically, DropBP can reduce training time by 44% with comparable accuracy to the baseline, accelerate convergence to the same perplexity by 1.5$\times$, and enable training with a sequence length 6.2$\times$ larger on a single NVIDIA-A100 GPU. Furthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100 GPU and 117% on an Intel Gaudi2 HPU. The code is available at [https://github.com/WooSunghyeon/dropbp](https://github.com/WooSunghyeon/dropbp).</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-123" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-123', event_id='93432', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2005</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93432">3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability</a></strong></h5>


                        <p class="text-muted">
                            Baohao Liao &middot; Christof Monz
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Parameter-efficient finetuning (PEFT) methods effectively adapt large language models (LLMs) to diverse downstream tasks, reducing storage and GPU memory demands. Despite these advantages, several applications pose new challenges to PEFT beyond mere parameter efficiency. One notable challenge involves the efficient deployment of LLMs equipped with multiple task- or user-specific adapters, particularly when different adapters are needed for distinct requests within the same batch. Another challenge is the interpretability of LLMs, which is crucial for understanding how LLMs function. Previous studies introduced various approaches to address different challenges. In this paper, we introduce a novel method, RoAd, which employs a straightforward 2D rotation to adapt LLMs and addresses all the above challenges: (1) RoAd is remarkably parameter-efficient, delivering optimal performance on GLUE, eight commonsense reasoning tasks and four arithmetic reasoning tasks with &lt;0.1% trainable parameters; (2) RoAd facilitates the efficient serving of requests requiring different adapters within a batch, with an overhead comparable to element-wise multiplication instead of batch matrix multiplication; (3) RoAd enhances LLM's interpretability through integration within a framework of distributed interchange intervention, demonstrated via composition experiments.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-124" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-124', event_id='93672', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2006</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93672">A Layer-Wise Natural Gradient Optimizer for Training Deep Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Xiaolei Liu &middot; Shaoshuai Li &middot; Kaixin Gao &middot; Binfeng Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Second-order optimization algorithms, such as the Newton method and the natural gradient descent (NGD) method exhibit excellent convergence properties for training deep neural networks, but the high computational cost limits its practical application. In this paper, we focus on the NGD method and propose a novel layer-wise natural gradient descent (LNGD) method to further reduce computational costs and accelerate the training process. Specifically, based on the block diagonal approximation of the Fisher information matrix, we first propose the layer-wise sample method to compute each block matrix without performing a complete back-propagation. Then, each block matrix is approximated as a Kronecker product of two smaller matrices, one of which is a diagonal matrix, while keeping the traces equal before and after approximation. By these two steps, we provide a new approximation for the Fisher information matrix, which can effectively reduce the computational cost while preserving the main information of each block matrix. Moreover, we propose a new adaptive layer-wise learning rate to further accelerate training. Based on these new approaches, we propose the LNGD optimizer. The global convergence analysis of LNGD is established under some assumptions. Experiments on image classification and machine translation tasks show that our method is quite competitive compared to the state-of-the-art methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-125" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-125', event_id='93774', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2007</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93774">Neglected Hessian component explains mysteries in sharpness regularization</a></strong></h5>


                        <p class="text-muted">
                            Yann Dauphin &middot; Atish Agarwala &middot; Hossein Mobahi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent work has shown that methods that regularize second order information like SAM can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We investigate this inconsistency and reveal its connection to the the structure of the Hessian of the loss. Specifically, its decomposition into the positive semi-definite Gauss-Newton matrix and an indefinite matrix, which we call the Nonlinear Modeling Error (NME) matrix. Previous studies have largely overlooked the significance of the NME in their analysis for various reasons. However, we provide empirical and theoretical evidence that the NME is important to the performance of gradient penalties and explains their sensitivity to activation functions. We also provide evidence that the difference in regularization performance between gradient penalties and weight noise can be explained by the NME. Our findings emphasize the necessity of considering the NME in both experimental design and theoretical analysis for sharpness regularization.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-126" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-126', event_id='94544', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2008</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94544">Learning 3D Equivariant Implicit Function with Patch-Level Pose-Invariant Representation</a></strong></h5>


                        <p class="text-muted">
                            Xin Hu &middot; Xiaole Tang &middot; Ruixuan Yu &middot; Jian Sun
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Implicit neural representation gains popularity in modeling the continuous 3D surface for 3D representation and reconstruction. In this work, we are motivated by the fact that the local 3D patches repeatedly appear on 3D shapes/surfaces if the factor of poses is removed. Based on this observation, we propose the 3D patch-level equivariant implicit function (PEIF) based on the 3D patch-level pose-invariant representation, allowing us to reconstruct 3D surfaces by estimating equivariant displacement vector fields for query points. Specifically, our model is based on the pose-normalized query/patch pairs and enhanced by the proposed intrinsic patch geometry representation, modeling the intrinsic 3D patch geometry feature by learnable multi-head memory banks. Extensive experiments show that our model achieves state-of-the-art performance on multiple surface reconstruction datasets, and also exhibits better generalization to crossdataset shapes and robustness to arbitrary rotations. Our code will be available at https://github.com/mathXin112/PEIF.git.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-127" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-127', event_id='94731', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2009</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94731">Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations</a></strong></h5>


                        <p class="text-muted">
                            Alex H√§gele &middot; Elie Bakouch &middot; Atli Kosson &middot; Loubna Ben allal &middot; Leandro Von Werra &middot; Martin Jaggi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Scale has become a main ingredient in obtaining strong machine learning models. As a result, understanding a model's scaling properties is key to effectively designing both the right training setup as well as future generations of architectures. In this work, we argue that scale and training research has been needlessly complex due to reliance on the cosine schedule, which prevents training across different lengths for the same model size. We investigate the training behavior of a direct alternative --- constant learning rate and cooldowns --- and find that it scales predictably and reliably similar to cosine. Additionally, we show that stochastic weight averaging yields improved performance along the training trajectory, without additional training costs, across different scales. Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs. Our code is available at https://github.com/epfml/schedules-and-scaling/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-128" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-128', event_id='94805', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2010</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94805">Building on Efficient Foundations: Effective Training of LLMs with Structured Feedforward Layers</a></strong></h5>


                        <p class="text-muted">
                            Xiuying Wei &middot; Skander Moalla &middot; Razvan Pascanu &middot; Caglar Gulcehre
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>State-of-the-art results in large language models (LLMs) often rely on scale, whichbecomes computationally expensive. This has sparked a research agenda to reducethese models‚Äô parameter counts and computational costs without significantlyimpacting their performance. Our study focuses on transformer-based LLMs,specifically targeting the computationally intensive feedforward networks (FFNs),which are less studied than attention blocks. We consider three structured linearparameterizations of the FFN using efficient low-rank and block-diagonal matrices.In contrast to many previous works that examined these approximations, our studyi) explores these structures from a training-from-scratch perspective, ii) scales upto 1.3B parameters, and iii) is conducted within recent Transformer-based LLMsrather than convolutional architectures. We demonstrate that these structures canlead to actual computational gains in various scenarios, including online decodingwhen using a pre-merge technique. Additionally, we propose a novel trainingregime, called self-guided training, aimed at improving the poor training dynamicsthat these approximations exhibit when used from initialization. Interestingly,the scaling performance of structured matrices is explored, revealing steepercurves in scaling training FLOPs, along with a favorable scaling trend in theovertraining regime. Specifically, we show that wide and structured networkscan utilize training FLOPs more efficiently, with fewer parameters and lowerloss than dense models at their optimal trade-off. Our code is available athttps://github.com/CLAIRE-Labo/StructuredFFN/tree/main.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-129" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-129', event_id='94943', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2011</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94943">Spectral Adapter: Fine-Tuning in Spectral Space</a></strong></h5>


                        <p class="text-muted">
                            Fangzhao Zhang &middot; Mert Pilanci
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pretrained deep neural networks have captured widespread interest. In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure. We investigate two spectral adaptation mechanisms, namely additive tuning and orthogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space. We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget. We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tuning performance as well as benefits multi-adapter fusion.  The source code will be open-sourced for reproducibility.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-130" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-130', event_id='93587', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2100</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93587">Linear Transformers are Versatile In-Context Learners</a></strong></h5>


                        <p class="text-muted">
                            Max Vladymyrov &middot; Johannes von Oswald &middot; Mark Sandler &middot; Rong Ge
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that each layer of a linear transformer maintains a weight vector for an implicit linear regression problem and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We analyze this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our findings show that even linear transformers possess the surprising ability to discover sophisticated optimization strategies.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-131" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-131', event_id='93558', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2101</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93558">KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization</a></strong></h5>


                        <p class="text-muted">
                            Tianyi Zhang &middot; Jonah Yi &middot; Zhaozhuo Xu &middot; Anshumali Shrivastava
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As batch size, context length, or model size increases, the size of key and value (KV) cache quickly becomes the main contributor to GPU memory usage and the bottleneck of inference latency and throughput. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. Currently, KV cache quantization is performed per-channel or per-token independently. Our analysis shows that distinct channels of a key/value activation embedding are highly interdependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropy, which implies that per-channel independent quantization is sub-optimal. To mitigate this sub-optimality, we propose Coupled Quantization (CQ), which couples multiple key/value channels together for quantization to exploit their interdependence and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ compares favorably with existing baselines in preserving model quality, and improves inference throughput by 1.4‚Äì3.5$\times$ relative to the uncompressed baseline. Furthermore, we demonstrate that CQ can preserve model quality reasonably with KV cache quantized down to 1 bit.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-132" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-132', event_id='93328', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2102</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93328">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</a></strong></h5>


                        <p class="text-muted">
                            Jay Shah &middot; Ganesh Bikshandi &middot; Ying Zhang &middot; Vijay Thakkar &middot; Pradeep Ramani &middot; Tri Dao
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU.We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0$\times$ with BF16 reaching up to 840 TFLOPs/s (85\% utilization), and with FP8 reaching 1.3 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6$\times$ lower numerical error than a baseline FP8 attention.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-133" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-133', event_id='96914', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2103</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96914">Mobility-LLM: Learning Visiting Intentions and Travel Preference from Human Mobility Data with Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Letian Gong &middot; Yan Lin &middot; Xinyue Zhang &middot; Yiwen Lu &middot; Xuedi Han &middot; Yichen Liu &middot; Shengnan Guo &middot; Youfang Lin &middot; Huaiyu Wan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Location-based services (LBS) have accumulated extensive human mobility data on diverse behaviors through check-in sequences. These sequences offer valuable insights into users‚Äô intentions and preferences. Yet, existing models analyzing check-in sequences fail to consider the semantics contained in these sequences, which closely reflect human visiting intentions and travel preferences, leading to an incomplete comprehension. Drawing inspiration from the exceptional semantic understanding and contextual information processing capabilities of large language models (LLMs) across various domains, we present Mobility-LLM, a novel framework that leverages LLMs to analyze check-in sequences for multiple tasks. Since LLMs cannot directly interpret check-ins, we reprogram these sequences to help LLMs comprehensively understand the semantics of human visiting intentions and travel preferences. Specifically, we introduce a visiting intention memory network (VIMN) to capture the visiting intentions at each record, along with a shared pool of human travel preference prompts (HTPP) to guide the LLM in understanding users‚Äô travel preferences. These components enhance the model‚Äôs ability to extract and leverage semantic information from human mobility data effectively. Extensive experiments on four benchmark datasets and three downstream tasks demonstrate that our approach significantly outperforms existing models, underscoring the effectiveness of Mobility-LLM in advancing our understanding of human mobility data within LBS contexts.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-134" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-134', event_id='96699', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2104</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96699">Learning symmetries via weight-sharing with doubly stochastic tensors</a></strong></h5>


                        <p class="text-muted">
                            Putri van der Linden &middot; Alejandro Garc√≠a-Castellanos &middot; Sharvaree Vadgama &middot; Thijs Kuipers &middot; Erik Bekkers
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Group equivariance has emerged as a valuable inductive bias in deep learning, enhancing generalization, data efficiency, and robustness. Classically, group equivariant methods require the groups of interest to be known beforehand, which may not be realistic for real-world data. Additionally, baking in fixed group equivariance may impose overly restrictive constraints on model architecture. This highlights the need for methods that can dynamically discover and apply symmetries as soft constraints. For neural network architectures, equivariance is commonly achieved through group transformations of a canonical weight tensor, resulting in weight sharing over a given group $G$. In this work, we propose to *learn* such a weight-sharing scheme by defining a collection of learnable doubly stochastic matrices that act as soft permutation matrices on canonical weight tensors, which can take regular group representations as a special case. This yields learnable kernel transformations that are jointly optimized with downstream tasks. We show that when the dataset exhibits strong symmetries, the permutation matrices will converge to regular group representations and our weight-sharing networks effectively become regular group convolutions. Additionally, the flexibility of the method enables it to effectively pick up on partial symmetries.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-135" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-135', event_id='96618', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2105</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96618">Bootstrapping Top-down Information for Self-modulating Slot Attention</a></strong></h5>


                        <p class="text-muted">
                            Dongwon Kim &middot; Seoyeon Kim &middot; Suha Kwak
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Object-centric learning (OCL) aims to learn representations of individual objects within visual scenes without manual supervision, facilitating efficient and effective visual reasoning. Traditional OCL methods primarily employ bottom-up approaches that aggregate homogeneous visual features to represent objects. However, in complex visual environments, these methods often fall short due to the heterogeneous nature of visual features within an object. To address this, we propose a novel OCL framework incorporating a top-down pathway. This pathway first bootstraps the semantics of individual objects and then modulates the model to prioritize features relevant to these semantics. By dynamically modulating the model based on its own output, our top-down pathway enhances the representational quality of objects. Our framework achieves state-of-the-art performance across multiple synthetic and real-world object-discovery benchmarks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-136" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-136', event_id='96533', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2106</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96533">Compositional Automata Embeddings for Goal-Conditioned Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Beyazit Yalcinkaya &middot; Niklas Lauffer &middot; Marcell Vazquez-Chanlatte &middot; Sanjit Seshia
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Goal-conditioned reinforcement learning is a powerful way to control an AI agent's behavior at runtime. That said, popular goal representations, e.g., target states or natural language, are either limited to Markovian tasks or rely on ambiguous task semantics. We propose representing temporal goals using compositions of deterministic finite automata (cDFAs) and use cDFAs to guide RL agents. cDFAs balance the need for formal temporal semantics with ease of interpretation: if one can understand a flow chart, one can understand a cDFA. On the other hand, cDFAs form a countably infinite concept class with Boolean semantics, and subtle changes to the automaton can result in very different tasks, making them difficult to condition agent behavior on. To address this, we observe that all paths through a DFA correspond to a series of reach-avoid tasks and propose pre-training graph neural network embeddings on "reach-avoid derived" DFAs. Through empirical evaluation, we demonstrate that the proposed pre-training method enables zero-shot generalization to various cDFA task classes and accelerated policy specialization without the myopic suboptimality of hierarchical methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-137" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-137', event_id='95833', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2107</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95833">Preventing Model Collapse in Deep Canonical Correlation Analysis by Noise Regularization</a></strong></h5>


                        <p class="text-muted">
                            Junlin He &middot; Jinxiao Du &middot; Susu Xu &middot; Wei Ma
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multi-View Representation Learning (MVRL) aims to learn a unified representation of an object from multi-view data.Deep Canonical Correlation Analysis (DCCA) and its variants share simple formulations and demonstrate state-of-the-art performance. However, with extensive experiments, we observe the issue of model collapse, i.e., the performance of DCCA-based methods will drop drastically when training proceeds. The model collapse issue could significantly hinder the wide adoption of DCCA-based methods because it is challenging to decide when to early stop. To this end, we develop NR-DCCA, which is equipped with a novel noise regularization approach to prevent model collapse. Theoretical analysis shows that the Correlation Invariant Property is the key to preventing model collapse, and our noise regularization forces the neural network to possess such a property. A framework to construct synthetic data with different common and complementary information is also developed to compare MVRL methods comprehensively. The developed NR-DCCA outperforms baselines stably and consistently in both synthetic and real-world datasets, and the proposed noise regularization approach can also be generalized to other DCCA-based methods such as DGCCA.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-138" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-138', event_id='95293', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2108</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95293">Algebraic Positional Encodings</a></strong></h5>


                        <p class="text-muted">
                            Konstantinos Kogkalidis &middot; Jean-Philippe Bernardy &middot; Vikas Garg
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce a novel positional encoding strategy for Transformer-style models, addressing the shortcomings of existing, often ad hoc, approaches. Our framework provides a flexible mapping from the algebraic specification of a domain to an interpretation as orthogonal operators. This design preserves the algebraic characteristics of the source domain, ensuring that the model upholds the desired structural properties. Our scheme can accommodate various structures, including sequences, grids and trees, as well as their compositions. We conduct a series of experiments to demonstrate the practical applicability of our approach. Results suggest performance on par with or surpassing the current state-of-the-art, without hyperparameter optimizations or ``task search'' of any kind. Code is available through https://aalto-quml.github.io/ape/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-139" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-139', event_id='94621', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2109</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94621">TripletCLIP:  Improving Compositional Reasoning of CLIP via Synthetic Vision-Language Negatives</a></strong></h5>


                        <p class="text-muted">
                            Maitreya Patel &middot; Naga Sai Abhiram Kusumba &middot; Sheng Cheng &middot; Changhoon Kim &middot; Tejas Gokhale &middot; Chitta Baral &middot; &#x27;YZ&#x27; Yezhou Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Contrastive Language-Image Pretraining (CLIP) models maximize the mutual information between text and visual modalities to learn representations. This makes the nature of the training data a significant factor in the efficacy of CLIP for downstream tasks. However, the lack of compositional diversity in contemporary image-text datasets limits the compositional reasoning ability of CLIP. We show that generating ``hard'' negative captions via in-context learning and synthesizing corresponding negative images with text-to-image generators offers a solution. We introduce a novel contrastive pre-training strategy that leverages these hard negative captions and images in an alternating fashion to train CLIP. We demonstrate that our method, named TripletCLIP, when applied to existing datasets such as CC3M and CC12M, enhances the compositional capabilities of CLIP, resulting in an absolute improvement of over 9% on the SugarCrepe benchmark on an equal computational budget, as well as improvements in zero-shot image classification and image retrieval. Our code, models, and data are available at: tripletclip.github.io.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-140" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-140', event_id='94617', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2110</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94617">VMamba: Visual State Space Model</a></strong></h5>


                        <p class="text-muted">
                            Yue Liu &middot; Yunjie Tian &middot; Yuzhong Zhao &middot; Hongtian Yu &middot; Lingxi Xie &middot; Yaowei Wang &middot; Qixiang Ye &middot; Jianbin Jiao &middot; Yunfan Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Designing computationally efficient network architectures remains an ongoing necessity in computer vision. In this paper, we adapt Mamba, a state-space language model, into VMamba, a vision backbone with linear time complexity. At the core of VMamba is a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D bridges the gap between the ordered nature of 1D selective scan and the non-sequential structure of 2D vision data, which facilitates the collection of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments demonstrate VMamba‚Äôs promising performance across diverse visual perception tasks, highlighting its superior input scaling efficiency compared to existing benchmark models. Source code is available at https://github.com/MzeroMiko/VMamba</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-141" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-141', event_id='94462', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2111</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94462">Decoupled Kullback-Leibler Divergence Loss</a></strong></h5>


                        <p class="text-muted">
                            Jiequan Cui &middot; Zhuotao Tian &middot; Zhisheng Zhong &middot; Xiaojuan Qi &middot; Bei Yu &middot; Hanwang Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In this paper, we delve deeper into the Kullback‚ÄìLeibler (KL) Divergence loss and mathematically prove that it is equivalent to the Decoupled Kullback-Leibler (DKL) Divergence loss that consists of 1) a weighted Mean Square Error ($\mathbf{w}$MSE) loss and 2) a Cross-Entropy loss incorporating soft labels. Thanks to the decomposed formulation of DKL loss, we have identified two areas for improvement. Firstly, we address the limitation of KL/DKL in scenarios like knowledge distillation by breaking its asymmetric optimization property. This modification ensures that the $\mathbf{w}$MSE component is always effective during training, providing extra constructive cues.Secondly, we introduce class-wise global information into KL/DKL to mitigate bias from individual samples.With these two enhancements, we derive the Improved Kullback‚ÄìLeibler (IKL) Divergence loss and evaluate its effectiveness by conducting experiments on CIFAR-10/100 and ImageNet datasets, focusing on adversarial training, and knowledge distillation tasks. The proposed approach achieves new state-of-the-art adversarial robustness on the public leaderboard --- \textit{RobustBench} and competitive performance on knowledge distillation, demonstrating the substantial practical merits. Our code is available at https://github.com/jiequancui/DKL.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-142" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-142', event_id='95998', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2200</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95998">SAM-Guided Masked Token Prediction for 3D Scene Understanding</a></strong></h5>


                        <p class="text-muted">
                            Zhimin Chen &middot; Liang Yang &middot; Yingwei Li &middot; Longlong Jing &middot; Bing Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Foundation models have significantly enhanced 2D task performance, and recent works like Bridge3D have successfully applied these models to improve 3D scene understanding through knowledge distillation, marking considerable advancements. Nonetheless, challenges such as the misalignment between 2D and 3D representations and the persistent long-tail distribution in 3D datasets still restrict the effectiveness of knowledge distillation from 2D to 3D using foundation models. To tackle these issues, we introduce a novel SAM-guided tokenization method that seamlessly aligns 3D transformer structures with region-level knowledge distillation, replacing the traditional KNN-based tokenization techniques. Additionally, we implement a group-balanced re-weighting strategy to effectively address the long-tail problem in knowledge distillation. Furthermore, inspired by the recent success of masked feature prediction, our framework incorporates a two-stage masked token prediction process in which the student model predicts both the global embeddings and token-wise local embeddings derived from the teacher models trained in the first stage. Our methodology has been validated across multiple datasets, including SUN RGB-D, ScanNet, and S3DIS, for tasks like 3D object detection and semantic segmentation. The results demonstrate significant improvements over current state-of-the-art self-supervised methods, establishing new benchmarks in this field.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-143" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-143', event_id='93030', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2201</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93030">Transferable Adversarial Attacks on SAM and Its Downstream Models</a></strong></h5>


                        <p class="text-muted">
                            Song Xia &middot; Wenhan Yang &middot; Yi Yu &middot; Xun Lin &middot; Henghui Ding &middot; LINGYU DUAN &middot; Xudong Jiang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The utilization of large foundational models has a dilemma: while fine-tuning downstream tasks from them holds promise for making use of the well-generalized knowledge in practical applications, their open accessibility also poses threats of adverse usage.This paper, for the first time, explores the feasibility of adversarial attacking various downstream models fine-tuned from the segment anything model (SAM), by solely utilizing the information from the open-sourced SAM. In contrast to prevailing transfer-based adversarial attacks, we demonstrate the existence of adversarial dangers even without accessing the downstream task and dataset to train a similar surrogate model.To enhance the effectiveness of the adversarial attack towards models fine-tuned on unknown datasets, we propose a universal meta-initialization (UMI) algorithm to extract the intrinsic vulnerability inherent in the foundation model, which is then utilized as the prior knowledge to guide the generation of adversarial perturbations.Moreover, by formulating the gradient difference in the attacking process between the open-sourced SAM and its fine-tuned downstream models, we theoretically demonstrate that a deviation occurs in the adversarial update direction by directly maximizing the distance of encoded feature embeddings in the open-sourced SAM.Consequently, we propose a gradient robust loss that simulates the associated uncertainty with gradient-based noise augmentation to enhance the robustness of generated adversarial examples (AEs) towards this deviation, thus improving the transferability.Extensive experiments demonstrate the effectiveness of the proposed universal meta-initialized and gradient robust adversarial attack (UMI-GRAT) toward SAMs and their downstream models.Code is available at https://github.com/xiasong0501/GRAT.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-144" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-144', event_id='93142', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2202</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93142">Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective</a></strong></h5>


                        <p class="text-muted">
                            Andi Zhang &middot; Mingtian Zhang &middot; Damon Wischik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose a probabilistic perspective on adversarial examples, allowing us to embed subjective understanding of semantics as a distribution into the process of generating adversarial examples, in a principled manner. Despite significant pixel-level modifications compared to traditional adversarial attacks, our method preserves the overall semantics of the image, making the changes difficult for humans to detect. This extensive pixel-level modification enhances our method's ability to deceive classifiers designed to defend against adversarial attacks. Our empirical findings indicate that the proposed methods achieve higher success rates in circumventing adversarial defense mechanisms, while remaining difficult for human observers to detect.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-145" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-145', event_id='93578', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2203</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93578">Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning Zero-Shot Models</a></strong></h5>


                        <p class="text-muted">
                            Kaican Li &middot; Weiyan XIE &middot; Yongxiang Huang &middot; Didan Deng &middot; Lanqing Hong &middot; Zhenguo Li &middot; Ricardo Silva &middot; Nevin L. Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Fine-tuning foundation models often compromises their robustness to distribution shifts. To remedy this, most robust fine-tuning methods aim to preserve the pre-trained features. However, not all pre-trained features are robust and those methods are largely indifferent to which ones to preserve. We propose dual risk minimization (DRM), which combines empirical risk minimization with worst-case risk minimization, to better preserve the core features of downstream tasks. In particular, we utilize core-feature descriptions generated by LLMs to induce core-based zero-shot predictions which then serve as proxies to estimate the worst-case risk. DRM balances two crucial aspects of model robustness: expected performance and worst-case performance, establishing a new state of the art on various real-world benchmarks. DRM significantly improves the out-of-distribution performance of CLIP ViT-L/14@336 on ImageNet (75.9$\to$77.1), WILDS-iWildCam (47.1$\to$51.8), and WILDS-FMoW (50.7$\to$53.1); opening up new avenues for robust fine-tuning. Our code is available at https://github.com/vaynexie/DRM.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-146" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-146', event_id='94791', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2204</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94791">Certified Robustness for Deep Equilibrium Models via Serialized Random Smoothing</a></strong></h5>


                        <p class="text-muted">
                            Weizhi Gao &middot; Zhichao Hou &middot; Han Xu &middot; Xiaorui Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Implicit models such as Deep Equilibrium Models (DEQs) have emerged as promising alternative approaches for building deep neural networks. Their certified robustness has gained increasing research attention due to security concerns. Existing certified defenses for DEQs employing interval bound propagation and Lipschitz-bounds not only offer conservative certification bounds but also are restricted to specific forms of DEQs. In this paper, we provide the first randomized smoothing certified defense for DEQs to solve these limitations. Our study reveals that simply applying randomized smoothing to certify DEQs provides certified robustness generalized to large-scale datasets but incurs extremely expensive computation costs. To reduce computational redundancy, we propose a novel Serialized Randomized Smoothing (SRS) approach that leverages historical information. Additionally, we derive a new certified radius estimation for SRS to theoretically ensure the correctness of our algorithm. Extensive experiments and ablation studies on image recognition demonstrate that our algorithm can significantly accelerate the certification of DEQs by up to 7x almost without sacrificing the certified accuracy. The implementation will be publicly available upon the acceptance of this work. Our code is available at https://github.com/WeizhiGao/Serialized-Randomized-Smoothing.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-147" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-147', event_id='95529', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2205</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95529">Adaptive Randomized Smoothing: Certified Adversarial Robustness for Multi-Step Defences</a></strong></h5>


                        <p class="text-muted">
                            Saiyue Lyu &middot; Shadab Shaikh &middot; Frederick Shpilevskiy &middot; Evan Shelhamer &middot; Mathias L√©cuyer
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We propose Adaptive Randomized Smoothing (ARS) to certify the predictions of our test-time adaptive models against adversarial examples.ARS extends the analysis of randomized smoothing using $f$-Differential Privacy to certify the adaptive composition of multiple steps.For the first time, our theory covers the sound adaptive composition of general and high-dimensional functions of noisy inputs.We instantiate ARS on deep image classification to certify predictions against adversarial examples of bounded $L_{\infty}$ norm.In the $L_{\infty}$ threat model, ARS enables flexible adaptation through high-dimensional input-dependent masking.We design adaptivity benchmarks, based on CIFAR-10 and CelebA, and show that ARS improves standard test accuracy by  1 to 15\% points.On ImageNet, ARS improves certified test accuracy by up to 1.6% points over standard RS without adaptivity. Our code is available at [https://github.com/ubc-systopia/adaptive-randomized-smoothing](https://github.com/ubc-systopia/adaptive-randomized-smoothing).</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-148" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-148', event_id='95939', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2206</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95939">Scalable Neural Network Verification with Branch-and-bound Inferred Cutting Planes</a></strong></h5>


                        <p class="text-muted">
                            Duo Zhou &middot; Christopher Brix &middot; Grani A. Hanasusanto &middot; Huan Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Recently, cutting-plane methods such as GCP-CROWN have been explored to enhance neural network verifiers and made significant advancements. However, GCP-CROWN currently relies on ${\it generic}$ cutting planes ("cuts") generated from external mixed integer programming (MIP) solvers. Due to the poor scalability of MIP solvers, large neural networks cannot benefit from these cutting planes. In this paper, we exploit the structure of the neural network verification problem to generate efficient and scalable cutting planes ${\it specific}$ to this problem setting. We propose a novel approach, Branch-and-bound Inferred Cuts with COnstraint Strengthening (BICCOS), that leverages the logical relationships of neurons within verified subproblems in the branch-and-bound search tree, and we introduce cuts that preclude these relationships in other subproblems. We develop a mechanism that assigns influence scores to neurons in each path to allow the strengthening of these cuts. Furthermore, we design a multi-tree search technique to identify more cuts, effectively narrowing the search space and accelerating the BaB algorithm. Our results demonstrate that BICCOS can generate hundreds of useful cuts during the branch-and-bound process and consistently increase the number of verifiable instances compared to other state-of-the-art neural network verifiers on a wide range of benchmarks, including large networks that previous cutting plane methods could not scale to.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-149" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-149', event_id='95956', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2207</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95956">Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?</a></strong></h5>


                        <p class="text-muted">
                            Zhanke Zhou &middot; Rong Tao &middot; Jianing Zhu &middot; Yiwen Luo &middot; Zengmao Wang &middot; Bo Han
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We construct NoRa dataset that is tailored to evaluate the robustness of reasoning in the presence of noisy rationales. Our findings on NoRa dataset reveal a prevalent vulnerability to such noise among current LLMs, with existing robust methods like self-correction and self-consistency showing limited efficacy. Notably, compared to prompting with clean rationales, base LLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more drastically by 2.2%-40.4% with inaccurate thoughts.Addressing this challenge necessitates external supervision that should be accessible in practice. Here, we propose the method of contrastive denoising with noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning capabilities by contrasting noisy rationales with only one clean rationale, which can be the minimal requirement for denoising-purpose prompting. This method follows a principle of exploration and exploitation: (1) rephrasing and selecting rationales in the input space to achieve explicit denoising and (2) exploring diverse reasoning paths and voting on answers in the output space. Empirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy over the base model and shows significantly stronger denoising capabilities than baseline methods. The source code is publicly available at: https://github.com/tmlr-group/NoisyRationales.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-150" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-150', event_id='96305', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2208</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96305">Coded Computing for Resilient Distributed Computing: A Learning-Theoretic Framework</a></strong></h5>


                        <p class="text-muted">
                            Parsa Moradi &middot; Behrooz Tahmasebi &middot; Mohammad Maddah-Ali
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Coded computing has emerged as a promising framework for tackling significant challenges in large-scale distributed computing, including the presence of slow, faulty, or compromised servers. In this approach, each worker node processes a combination of the data, rather than the raw data itself. The final result then is decoded from the collective outputs of the worker nodes.  However, there is a significant gap between current coded computing approaches and the broader landscape of general distributed computing, particularly when it comes to machine learning workloads. To bridge this gap, we propose a novel foundation for coded computing, integrating the principles of learning theory, and developing a framework that seamlessly adapts with machine learning applications. In this framework, the objective is to find the encoder and decoder functions that minimize the loss function, defined as the mean squared error between the estimated and true values. Facilitating the search for the optimum decoding and functions, we show that the loss function can be upper-bounded by the summation of two terms: the generalization error of the decoding function and the training error of the encoding function. Focusing on the second-order Sobolev space, we then derive the optimal encoder and decoder. We show that in the proposed solution, the mean squared error of the estimation decays with the rate of $\mathcal{O}(S^3 N^{-3})$ and $\mathcal{O}(S^{\frac{8}{5}}N^{\frac{-3}{5}})$ in noiseless and noisy computation settings, respectively, where $N$ is the number of worker nodes with at most $S$ slow servers (stragglers). Finally, we evaluate the proposed scheme on inference tasks for various machine learning models and demonstrate that the proposed framework outperforms the state-of-the-art in terms of accuracy and rate of convergence.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-151" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-151', event_id='92966', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2209</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92966">A teacher-teacher framework for clinical language representation learning</a></strong></h5>


                        <p class="text-muted">
                            Feiqing Huang &middot; Shenghan Zhang &middot; Sara Sweet &middot; Tianxi Cai
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In recent years, there has been a proliferation of ready-to-use large language models (LLMs) designed for various applications, both general-purpose and domain-specific. Instead of advocating for the development of a new model or continuous pretraining of an existing one, this paper introduces a pragmatic teacher-teacher framework to facilitate mutual learning between two pre-existing models.By leveraging two teacher models possessing complementary knowledge, we introduce a LIghtweight kNowledge alignmEnt (LINE) module aimed at harmonizing their knowledge within a unified representation space. This framework is particularly valuable in clinical settings, where stringent regulations and privacy considerations dictate the handling of detailed clinical notes. Our trained LINE module excels in capturing critical information from clinical notes, leveraging highly de-identified data. Validation and downstream tasks further demonstrate the effectiveness of the proposed framework.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-152" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-152', event_id='92973', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2210</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92973">Learning diverse causally emergent representations from time series data</a></strong></h5>


                        <p class="text-muted">
                            David McSharry &middot; Christos Kaplanis &middot; Fernando Rosas &middot; Pedro A.M Mediano
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Cognitive processes usually take place at a macroscopic scale in systems characterised by emergent properties, which make the whole ‚Äòmore than the sum of its parts.‚Äô While recent proposals have provided quantitative, information-theoretic metrics to detect emergence in time series data, it is often highly non-trivial to identify the relevant macroscopic variables a priori. In this paper we leverage recent advances in representation learning and differentiable information estimators to put forward a data-driven method to find emergent variables. The proposed method successfully detects emergent variables and recovers the ground-truth emergence values in a synthetic dataset. Furthermore, we show the method can be extended to learn multiple independent features, extracting a diverse set of emergent quantities. We finally show that a modified method scales to real experimental data from primate brain activity, paving the ground for future analyses uncovering the emergent structure of cognitive representations in biological and artificial intelligence systems.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-153" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-153', event_id='92974', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2211</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92974">Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning</a></strong></h5>


                        <p class="text-muted">
                            CHENYU YANG &middot; Xizhou Zhu &middot; Jinguo Zhu &middot; Weijie Su &middot; Junjie Wang &middot; Xuan Dong &middot; Wenhai Wang &middot; Bin Li &middot; Jie Zhou &middot; Yu Qiao &middot; Jifeng Dai
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recently, vision model pre-training has evolved from relying on manually annotated datasets to leveraging large-scale, web-crawled image-text data. Despite these advances, there is no pre-training method that effectively exploits the interleaved image-text data, which is very prevalent on the Internet. Inspired by the recent success of compression learning in natural language processing, we propose a novel vision model pre-training method called Latent Compression Learning (LCL) for interleaved image-text data. This method performs latent compression learning by maximizing the mutual information between the inputs and outputs of a causal attention model. The training objective can be decomposed into two basic tasks: 1) contrastive learning between visual representation and preceding context, and 2) generating subsequent text based on visual representation. Our experiments demonstrate that our method not only matches the performance of CLIP on paired pre-training datasets (e.g., LAION), but can also leverage interleaved pre-training data (e.g., MMC4) to learn robust visual representations from scratch, showcasing the potential of vision model pre-training with interleaved image-text data.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-154" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-154', event_id='96239', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2300</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96239">Understanding Transformer Reasoning Capabilities via Graph Algorithms</a></strong></h5>


                        <p class="text-muted">
                            Clayton Sanford &middot; Bahare Fatemi &middot; Ethan Hall &middot; Anton Tsitsulin &middot; Mehran Kazemi &middot; Jonathan Halcrow &middot; Bryan Perozzi &middot; Vahab Mirrokni
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Which transformer scaling regimes are able to perfectly solve different classes of algorithmic problems?   While tremendous empirical advances have been attained by transformer-based neural networks, a theoretical understanding of their algorithmic reasoning capabilities in realistic parameter regimes is lacking.  We investigate this question in terms of the network‚Äôs depth, width, and number of extra tokens for algorithm execution.  Our novel representational hierarchy separates 9 algorithmic reasoning problems into classes solvable by transformers in different realistic parameter scaling regimes.  We prove that logarithmic depth is necessary and sufficient for tasks like graph connectivity, while single-layer transformers with small embedding dimensions can solve contextual retrieval tasks. We also support our theoretical analysis with ample empirical evidence using the GraphQA benchmark.  These results show that transformers excel at many graph reasoning tasks, even outperforming specialized graph neural networks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-155" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-155', event_id='95016', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2301</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95016">Task Confusion and Catastrophic Forgetting in Class-Incremental Learning: A Mathematical Framework for Discriminative and Generative Modelings</a></strong></h5>


                        <p class="text-muted">
                            Milad Khademi Nori &middot; Il-Min Kim
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In class-incremental learning (class-IL), models must classify all previously seen classes at test time without task-IDs, leading to task confusion. Despite being a key challenge, task confusion lacks a theoretical understanding. We present a novel mathematical framework for class-IL and prove the Infeasibility Theorem, showing optimal class-IL is impossible with discriminative modeling due to task confusion. However, we establish the Feasibility Theorem, demonstrating that generative modeling can achieve optimal class-IL by overcoming task confusion. We then assess popular class-IL strategies, including regularization, bias-correction, replay, and generative classifier, using our framework. Our analysis suggests that adopting generative modeling, either for generative replay or direct classification (generative classifier), is essential for optimal class-IL.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-156" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-156', event_id='94759', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2302</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94759">How does Gradient Descent Learn Features --- A Local Analysis for Regularized Two-Layer Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Mo Zhou &middot; Rong Ge
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The ability of learning useful features is one of the major advantages of neural networks. Although recent works show that neural network can operate in a neural tangent kernel (NTK) regime that does not allow feature learning, many works also demonstrate the potential for neural networks to go beyond NTK regime and perform feature learning. Recently, a line of work highlighted the feature learning capabilities of the early stages of gradient-based training. In this paper we consider another mechanism for feature learning via gradient descent through a local convergence analysis. We show that once the loss is below a certain threshold, gradient descent with a carefully regularized objective will capture ground-truth directions. We further strengthen this local convergence analysis by incorporating early-stage feature learning analysis. Our results demonstrate that feature learning not only happens at the initial gradient steps, but can also occur towards the end of training.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-157" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-157', event_id='94399', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2303</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94399">Large Stepsize Gradient Descent for Non-Homogeneous Two-Layer Networks: Margin Improvement and Fast Optimization</a></strong></h5>


                        <p class="text-muted">
                            Yuhang Cai &middot; Jingfeng Wu &middot; Song Mei &middot; Michael Lindsey &middot; Peter Bartlett
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The typical training of neural networks using large stepsize gradient descent (GD) under the logistic loss often involves two distinct phases, where the empirical risk oscillates in the first phase but decreases monotonically in the second phase. We investigate this phenomenon in two-layer networks that satisfy a near-homogeneity condition. We show that the second phase begins once the empirical risk falls below a certain threshold, dependent on the stepsize. Additionally, we show that the normalized margin grows nearly monotonically in the second phase, demonstrating an implicit bias of GD in training non-homogeneous predictors. If the dataset is linearly separable and the derivative of the activation function is bounded away from zero, we show that the average empirical risk decreases, implying that the first phase must stop in finite steps. Finally, we demonstrate that by choosing a suitably large stepsize, GD that undergoes this phase transition is more efficient than GD that monotonically decreases the risk. Our analysis applies to networks of any width, beyond the well-known neural tangent kernel and mean-field regimes.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-158" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-158', event_id='95330', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2304</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95330">GACL: Exemplar-Free Generalized Analytic Continual Learning</a></strong></h5>


                        <p class="text-muted">
                            HUIPING ZHUANG &middot; Yizhu Chen &middot; Di Fang &middot; Run He &middot; Kai Tong &middot; Hongxin Wei &middot; Ziqian Zeng &middot; Cen Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Class incremental learning (CIL) trains a network on sequential tasks with separated categories in each task but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more real-world scenario, where incoming data have mixed data categories and unknown sample size distribution. Existing attempts for the GCIL either have poor performance or invade data privacy by saving exemplars. In this paper, we propose a new exemplar-free GCIL technique named generalized analytic continual learning (GACL). The GACL adopts analytic learning (a gradient-free training technique) and delivers an analytical  (i.e., closed-form) solution to the GCIL scenario. This solution is derived via decomposing the incoming data into exposed and unexposed classes, thereby attaining a weight-invariant property, a rare yet valuable property supporting an equivalence between incremental learning and its joint training. Such an equivalence is crucial in GCIL settings as data distributions among different tasks no longer pose challenges to adopting our GACL. Theoretically, this equivalence property is validated through matrix analysis tools. Empirically, we conduct extensive experiments where, compared with existing GCIL methods, our GACL exhibits a consistently leading performance across various datasets and GCIL settings. Source code is available at https://github.com/CHEN-YIZHU/GACL.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-159" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-159', event_id='94083', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2305</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94083">Transformers are Minimax Optimal Nonparametric In-Context Learners</a></strong></h5>


                        <p class="text-muted">
                            Juno Kim &middot; Tai Nakamaki &middot; Taiji Suzuki
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In-context learning (ICL) of large language models has proven to be a surprisingly effective method of learning a new task from only a few demonstrative examples. In this paper, we shed light on the efficacy of ICL from the viewpoint of statistical learning theory. We develop approximation and generalization error analyses for a transformer model composed of a deep neural network and one linear attention layer, pretrained on nonparametric regression tasks sampled from general function spaces including the Besov space and piecewise $\gamma$-smooth class. In particular, we show that sufficiently trained transformers can achieve -- and even improve upon -- the minimax optimal estimation risk in context by encoding the most relevant basis representations during pretraining. Our analysis extends to high-dimensional or sequential data and distinguishes the \emph{pretraining} and \emph{in-context} generalization gaps, establishing upper and lower bounds w.r.t. both the number of tasks and in-context examples. These findings shed light on the effectiveness of few-shot prompting and the roles of task diversity and representation learning for ICL.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-160" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-160', event_id='93907', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2306</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93907">On the Saturation Effects of Spectral Algorithms in Large Dimensions</a></strong></h5>


                        <p class="text-muted">
                            Weihao Lu &middot; haobo Zhang &middot; Yicheng Li &middot; Qian Lin
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The saturation effects, which originally refer to the fact that kernel ridge regression (KRR) fails to achieve the information-theoretical lower bound when the regression function is over-smooth, have been observed for almost 20 years and were rigorously proved recently for kernel ridge regression and some other spectral algorithms over a fixed dimensional domain. The main focus of this paper is to explore the saturation effects for a large class of spectral algorithms (including the KRR, gradient descent, etc.) in large dimensional settings where $n \asymp d^{\gamma}$. More precisely, we first propose an improved minimax lower bound for the kernel regression problem in large dimensional settings and show that the gradient flow with early stopping strategy will result in an estimator achieving this lower bound (up to a logarithmic factor). Similar to the results in KRR, we can further determine the exact convergence rates (both upper and lower bounds) of a large class of (optimal tuned) spectral algorithms with different qualification $\tau$'s. In particular, we find that these exact rate curves (varying along $\gamma$) exhibit the periodic plateau behavior and the polynomial approximation barrier. Consequently, we can fully depict the saturation effects of the spectral algorithms and reveal a new phenomenon in large dimensional settings (i.e.,  the saturation effect occurs in large dimensional setting as long as the source condition $s>\tau$ while it occurs in fixed dimensional setting as long as $s>2\tau$).</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-161" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-161', event_id='93656', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2307</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93656">Precise asymptotics of reweighted least-squares algorithms for linear diagonal networks</a></strong></h5>


                        <p class="text-muted">
                            Chiraag Kaushik &middot; Justin Romberg &middot; Vidya Muthukumar
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The classical iteratively reweighted least-squares (IRLS) algorithm aims to recover an unknown signal from linear measurements by performing a sequence of weighted least squares problems, where the weights are recursively updated at each step. Varieties of this algorithm have been shown to achieve favorable empirical performance and theoretical guarantees for sparse recovery and $\ell_p$-norm minimization. Recently, some preliminary connections have also been made between IRLS and certain types of non-convex linear neural network architectures that are observed to exploit low-dimensional structure in high-dimensional linear models. In this work, we provide a unified asymptotic analysis for a family of algorithms that encompasses IRLS, the recently proposed lin-RFM algorithm (which was motivated by feature learning in neural networks), and the alternating minimization algorithm on linear diagonal neural networks. Our analysis operates in a "batched" setting with i.i.d. Gaussian covariates and shows that, with appropriately chosen reweighting policy, the algorithm can achieve favorable performance in only a handful of iterations. We also extend our results to the case of group-sparse recovery and show that leveraging this structure in the reweighting scheme provably improves test error compared to coordinate-wise reweighting.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-162" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-162', event_id='95425', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2308</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95425">Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents</a></strong></h5>


                        <p class="text-muted">
                            Wenkai Yang &middot; Xiaohan Bi &middot; Yankai Lin &middot; Sishuo Chen &middot; Jie Zhou &middot; Xu Sun
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis of different forms of agent backdoor attacks. Specifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct. (2) Furthermore, the former category can be divided into two subcategories based on trigger locations, in which the backdoor trigger can either be hidden in the user query or appear in an intermediate observation returned by the external environment. We implement the above variations of agent backdoor attacks on two typical agent tasks including web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks and such backdoor vulnerability cannot be easily mitigated by current textual backdoor defense algorithms. This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-163" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-163', event_id='92977', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2309</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92977">Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame</a></strong></h5>


                        <p class="text-muted">
                            Evan Markou &middot; Thalaiyasingam Ajanthan &middot; Stephen Gould
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Neural Collapse (NC) is a recently observed phenomenon in neural networks that characterises the solution space of the final classifier layer when trained until zero training loss. Specifically, NC suggests that the final classifier layer converges to a Simplex Equiangular Tight Frame (ETF), which maximally separates the weights corresponding to each class. By duality, the penultimate layer feature means also converge to the same simplex ETF. Since this simple symmetric structure is optimal, our idea is to utilise this property to improve convergence speed. Specifically, we introduce the notion of \textit{nearest simplex ETF geometry} for the penultimate layer features at any given training iteration, by formulating it as a Riemannian optimisation. Then, at each iteration, the classifier weights are implicitly set to the nearest simplex ETF by solving this inner-optimisation, which is encapsulated within a declarative node to allow backpropagation. Our experiments on synthetic and real-world architectures on classification tasks demonstrate that our approach accelerates convergence and enhances training stability.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-164" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-164', event_id='95259', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2310</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95259">What makes unlearning hard and what to do about it</a></strong></h5>


                        <p class="text-muted">
                            KAIRAN ZHAO &middot; Meghdad Kurmanji &middot; George-Octavian BƒÉrbulescu &middot; Eleni Triantafillou &middot; Peter Triantafillou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Machine unlearning is the problem of removing the effect of a subset of training data (the ``forget set'') from a trained model without damaging the model's utility e.g. to comply with users' requests to delete their data, or remove mislabeled, poisoned or otherwise problematic data.With unlearning research still being at its infancy, many fundamental open questions exist: Are there interpretable characteristics of forget sets that substantially affect the difficulty of the problem? How do these characteristics affect different state-of-the-art algorithms?With this paper, we present the first investigation aiming to answer these questions. We identify two key factors affecting unlearning difficulty and the performance of unlearning algorithms. Evaluation on forget sets that isolate these identified factors reveals previously-unknown behaviours of state-of-the-art algorithms that don't materialize on random forget sets.Based on our insights, we develop a framework coined Refined-Unlearning Meta-algorithm (RUM) that encompasses: (i) refining the forget set into homogenized subsets, according to different characteristics; and (ii) a meta-algorithm that employs existing algorithms to unlearn each subset and finally delivers a model that has unlearned the overall forget set. We find that RUM substantially improves top-performing unlearning algorithms. Overall, we view our work as an important step in (i) deepening our scientific understanding of unlearning and (ii) revealing new pathways to improving the state-of-the-art.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-165" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-165', event_id='94964', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2311</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94964">Toward a Well-Calibrated Discrimination via Survival Outcome-Aware Contrastive Learning</a></strong></h5>


                        <p class="text-muted">
                            Dongjoon Lee &middot; Hyeryn Park &middot; Changhee Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Previous deep learning approaches for survival analysis have primarily relied on ranking losses to improve discrimination performance, which often comes at the expense of calibration performance. To address such an issue, we propose a novel contrastive learning approach specifically designed to enhance discrimination without sacrificing calibration. Our method employs weighted sampling within a contrastive learning framework, assigning lower penalties to samples with similar survival outcomes. This aligns well with the assumption that patients with similar event times share similar clinical statuses. Consequently, when augmented with the commonly used negative log-likelihood loss, our approach significantly improves discrimination performance without directly manipulating the model outputs, thereby achieving better calibration.Experiments on multiple real-world clinical datasets demonstrate that our method outperforms state-of-the-art deep survival models in both discrimination and calibration. Through comprehensive ablation studies, we further validate the effectiveness of our approach through quantitative and qualitative analyses.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-166" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-166', event_id='96563', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2400</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96563">ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification</a></strong></h5>


                        <p class="text-muted">
                            Yefei He &middot; Luoming Zhang &middot; Weijia Wu &middot; Jing Liu &middot; Hong Zhou &middot; Bohan Zhuang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>KV cache stores key and value states from previous tokens to avoid re-computation, yet it demands substantial storage space, especially for long sequences.   Adaptive KV cache compression seeks to discern the saliency of tokens, preserving vital information while aggressively compressing those of less importance.  However, previous methods of this approach exhibit significant performance degradation at high compression ratios due to inaccuracies in identifying salient tokens.   Additionally, the compression process introduces excessive overhead, substantially increasing memory burdens and the generation latency.  In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for large language models (LLMs).   First, we construct a strong baseline for quantizing KV cache. Through the proposed channel-separable tokenwise quantization scheme, the memory overhead of quantization parameters are substantially reduced compared to fine-grained groupwise quantization.  To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix. The quantization bit-width for each token is then adaptively assigned based on their saliency.  Moreover, we develop an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention.  Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and minimal performance losses compared with previous KV cache compression methods. For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by $4.98\times$, with only a 0.38% drop in accuracy. In terms of efficiency, ZipCache also showcases a 37.3% reduction in prefill-phase latency, a 56.9% reduction in decoding-phase latency, and a 19.8% reduction in GPU memory usage when evaluating LLaMA3-8B model with a input length of 4096. Code is available at https://github.com/ThisisBillhe/ZipCache/.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-167" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-167', event_id='96577', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2401</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96577">Categorical Flow Matching on Statistical Manifolds</a></strong></h5>


                        <p class="text-muted">
                            Chaoran Cheng &middot; Jiahan Li &middot; Jian Peng &middot; Ge Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce Statistical Flow Matching (SFM), a novel and mathematically rigorous flow-matching framework on the manifold of parameterized probability measures inspired by the results from information geometry. We demonstrate the effectiveness of our method on the discrete generation problem by instantiating SFM on the manifold of categorical distributions whose geometric properties remain unexplored in previous discrete generative models. Utilizing the Fisher information metric, we equip the manifold with a Riemannian structure whose intrinsic geometries are effectively leveraged by following the shortest paths of geodesics. We develop an efficient training and sampling algorithm that overcomes numerical stability issues with a diffeomorphism between manifolds. Our distinctive geometric perspective of statistical manifolds allows us to apply optimal transport during training and interpret SFM as following the steepest direction of the natural gradient. Unlike previous models that rely on variational bounds for likelihood estimation, SFM enjoys the exact likelihood calculation for arbitrary probability measures. We manifest that SFM can learn more complex patterns on the statistical manifold where existing models often fail due to strong prior assumptions. Comprehensive experiments on real-world generative tasks ranging from image, text to biological domains further demonstrate that SFM achieves higher sampling quality and likelihood than other discrete diffusion or flow-based models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-168" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-168', event_id='96883', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2402</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96883">Invariant Tokenization of Crystalline Materials for Language Model Enabled Generation</a></strong></h5>


                        <p class="text-muted">
                            Keqiang Yan &middot; Xiner Li &middot; Hongyi Ling &middot; Kenna Ashen &middot; Carl Edwards &middot; Raymundo Arroyave &middot; Marinka Zitnik &middot; Heng Ji &middot; Xiaofeng Qian &middot; Xiaoning Qian &middot; Shuiwang Ji
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We consider the problem of crystal materials generation using language models (LMs). A key step is to convert 3D crystal structures into 1D sequences to be processed by LMs. Prior studies used the crystallographic information framework (CIF) file stream, which fails to ensure SE(3) and periodic invariance and may not lead to unique sequence representations for a given crystal structure. Here, we propose a novel method, known as Mat2Seq, to tackle this challenge. Mat2Seq converts 3D crystal structures into 1D sequences and ensures that different mathematical descriptions of the same crystal are represented in a single unique sequence, thereby provably achieving SE(3) and periodic invariance. Experimental results show that, with language models, Mat2Seq achieves promising performance in crystal structure generation as compared with prior methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-169" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-169', event_id='92923', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2403</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92923">Stress-Testing Capability Elicitation With Password-Locked Models</a></strong></h5>


                        <p class="text-muted">
                            Ryan Greenblatt &middot; Fabien Roger &middot; Dmitrii Krasheninnikov &middot; David Krueger
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>To determine the safety of large language models (LLMs), AI developers must be able to assess their dangerous capabilities. But simple prompting strategies often fail to elicit an LLM‚Äôs full capabilities. One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task. In this paper, we investigate the conditions under which fine-tuning-based elicitation suffices to elicit capabilities. To do this, we introduce password-locked models, LLMs fine-tuned such that some of their capabilities are deliberately hidden. Specifically, these LLMs are trained to exhibit these capabilities only when a password is present in the prompt, and to imitate a much weaker LLM otherwise. Password-locked models enable a novel method of evaluating capabilities elicitation methods, by testing whether these password-locked capabilities can be elicited without using the password. We find that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. More surprisingly, fine-tuning can elicit other capabilities that have been locked using the same password, or even different passwords. Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities. Overall, our findings suggest that fine-tuning is an effective method of eliciting hidden capabilities of current models but may be unreliable when high-quality demonstrations are not available, e.g., as may be the case when models‚Äô (hidden) capabilities exceed those of human demonstrators.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-170" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-170', event_id='93110', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2404</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93110">Hierarchical Selective Classification</a></strong></h5>


                        <p class="text-muted">
                            Shani Goren &middot; Ido Galil &middot; Ran El-Yaniv
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deploying deep neural networks for risk-sensitive tasks necessitates an uncertainty estimation mechanism. This paper introduces <em>hierarchical selective classification</em>, extending selective classification to a hierarchical setting. Our approach leverages the inherent structure of class relationships, enabling models to reduce the specificity of their predictions when faced with uncertainty. In this paper, we first formalize hierarchical risk and coverage, and introduce hierarchical risk-coverage curves. Next, we develop algorithms for hierarchical selective classification (which we refer to as "inference rules"), and propose an efficient algorithm that guarantees a target accuracy constraint with high probability. Lastly, we conduct extensive empirical studies on over a thousand ImageNet classifiers, revealing that training regimes such as CLIP, pretraining on ImageNet21k and knowledge distillation boost hierarchical selective performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-171" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-171', event_id='93128', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2405</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93128">Learning Infinitesimal Generators of Continuous Symmetries from Data</a></strong></h5>


                        <p class="text-muted">
                            Gyeonghoon Ko &middot; Hyunsu Kim &middot; Juho Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Exploiting symmetry inherent in data can significantly improve the sample efficiency of a learning procedure and the generalization of learned models. When data clearly reveals underlying symmetry, leveraging this symmetry can naturally inform the design of model architectures or learning strategies. Yet, in numerous real-world scenarios, identifying the specific symmetry within a given data distribution often proves ambiguous. To tackle this, some existing works learn symmetry in a data-driven manner, parameterizing and learning expected symmetry through data. However, these methods often rely on explicit knowledge, such as pre-defined Lie groups, which are typically restricted to linear or affine transformations. In this paper, we propose a novel symmetry learning algorithm based on transformations defined with one-parameter groups, continuously parameterized transformations flowing along the directions of vector fields called infinitesimal generators. Our method is built upon minimal inductive biases, encompassing not only commonly utilized symmetries rooted in Lie groups but also extending to symmetries derived from nonlinear generators. To learn these symmetries, we introduce a notion of a validity score that examine whether the transformed data is still valid for the given task. The validity score is designed to be fully differentiable and easily computable, enabling effective searches for transformations that achieve symmetries innate to the data. We apply our method mainly in two domains: image data and partial differential equations, and demonstrate its advantages. Our codes are available at \url{https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-172" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-172', event_id='93401', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2406</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93401">AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks</a></strong></h5>


                        <p class="text-muted">
                            Jin Li &middot; Ziqiang He &middot; Anwei Luo &middot; Jian-Fang Hu &middot; Z. Jane Wang &middot; Xiangui Kang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Imperceptible adversarial attacks aim to fool DNNs by adding imperceptible perturbation to the input data. Previous methods typically improve the imperceptibility of attacks by integrating common attack paradigms with specifically designed perception-based losses or the capabilities of generative models. In this paper, we propose Adversarial Attacks in Diffusion (AdvAD), a novel modeling framework distinct from existing attack paradigms. AdvAD innovatively conceptualizes attacking as a non-parametric diffusion process by theoretically exploring basic modeling approach rather than using the denoising or generation abilities of regular diffusion models requiring neural networks. At each step, much subtler yet effective adversarial guidance is crafted using only the attacked model without any additional network, which gradually leads the end of diffusion process from the original image to a desired imperceptible adversarial example. Grounded in a solid theoretical foundation of the proposed non-parametric diffusion process, AdvAD achieves high attack efficacy and imperceptibility with intrinsically lower overall perturbation strength. Additionally, an enhanced version AdvAD-X is proposed to evaluate the extreme of our novel framework under an ideal scenario. Extensive experiments demonstrate the effectiveness of the proposed AdvAD and AdvAD-X. Compared with state-of-the-art imperceptible attacks, AdvAD achieves an average of 99.9% (+17.3%) ASR with 1.34 (-0.97) $l_2$ distance, 49.74 (+4.76) PSNR and 0.9971 (+0.0043) SSIM against four prevalent DNNs with three different architectures on the ImageNet-compatible dataset. Code is available at https://github.com/XianguiKang/AdvAD.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-173" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-173', event_id='93573', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2407</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93573">The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof</a></strong></h5>


                        <p class="text-muted">
                            Derek Lim &middot; Theo Putterman &middot; Robin Walters &middot; Haggai Maron &middot; Stefanie Jegelka
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Many algorithms and observed phenomena in deep learning appear to be affected by parameter symmetries --- transformations of neural network parameters that do not change the underlying neural network function. These include linear mode connectivity, model merging, Bayesian neural network inference, metanetworks, and several other characteristics of optimization or loss-landscapes. However, theoretical analysis of the relationship between parameter space symmetries and these phenonmena is difficult. In this work, we empirically investigate the impact of neural parameter symmetries by introducing new neural network architectures that have reduced parameter space symmetries. We develop two methods, with some provable guarantees, of modifying standard neural networks to reduce parameter space symmetries.  With these new methods, we conduct a comprehensive experimental study consisting of multiple tasks aimed at assessing the effect of removing parameter symmetries. Our experiments reveal several interesting observations on the empirical impact of parameter symmetries; for instance, we observe linear mode connectivity between our networks without alignment of weight spaces, and we find that our networks allow for faster and more effective Bayesian neural network training.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-174" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-174', event_id='93662', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2408</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93662">SS1: Accelerating Inference with Fast and Expressive Sketch Structured Transform</a></strong></h5>


                        <p class="text-muted">
                            Aditya Desai &middot; Kimia Saedi &middot; Apoorv Walia &middot; Jihyeong Lee &middot; Keren Zhou &middot; Anshumali Shrivastava
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Tensor multiplication with learned weight matrices is the fundamental building block in deep learning models. These matrices can often be sparsified, decomposed, quantized, or subjected to random parameter sharing without losing accuracy, suggesting the possibility of more efficient transforms. Although many variants of weight matrices exist, unstructured ones are incompatible with modern hardware, slowing inference and training. On the other hand, structured variants often limit expressivity or fail to deliver the promised latency benefits. We present Sketch Structured Transform (SS1), an expressive and GPU-friendly operator that accelerates inference. SS1 leverages parameter sharing in a random yet structured manner to reduce computation while retraining the rich expressive nature of parameter sharing. We confirm empirically that SS1 offers better quality-efficiency tradeoffs than competing variants. Interestingly SS1 can be combined with Quantization to achieve gains unattainable by either method alone, a finding we justify via theoretical analysis. The analysis may be of independent interest.Moreover, existing pre-trained models can be projected onto SS1 and finetuned for efficient deployment. Surprisingly, these projected models can perform reasonably well even without finetuning. Our experiments highlight various applications of the SS1:(a) Training GPT2 and DLRM models from scratch for faster inference. (b) Finetuning projected BERT models for 1.31√ó faster inference while maintaining GLUE scores. (c) Proof of concept with Llama-3-8b, showing 1.11√ó faster wall clock inference using projected SS1 layers without finetuning. We open source our code :https://github.com/apd10/Sketch-Structured-Linear/</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-175" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-175', event_id='93797', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2409</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93797">CODA: A Correlation-Oriented Disentanglement and Augmentation Modeling Scheme for Better Resisting Subpopulation Shifts</a></strong></h5>


                        <p class="text-muted">
                            Ziquan OU &middot; Zijun Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Data-driven models learned often struggle to generalize due to widespread subpopulation shifts, especially the presence of both spurious correlations and group imbalance (SC-GI). To learn models more powerful for defending against SC-GI, we propose a {\bf Correlation-Oriented Disentanglement and Augmentation (CODA)} modeling scheme, which includes two unique developments: (1) correlation-oriented disentanglement and (2) strategic sample augmentation with reweighted consistency (RWC) loss. In (1), a bi-branch encoding process is developed to enable the disentangling of variant and invariant correlations by coordinating with a decoy classifier and the decoder reconstruction. In (2), a strategic sample augmentation based on disentangled latent features with RWC loss is designed to reinforce the training of a more generalizable model. The effectiveness of CODA is verified by benchmarking against a set of SOTA models in terms of worst-group accuracy and maximum group accuracy gap based on two famous datasets, ColoredMNIST and CelebA.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-176" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-176', event_id='93999', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2410</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93999">Spiking Token Mixer:  A event-driven friendly Former structure for spiking neural networks</a></strong></h5>


                        <p class="text-muted">
                            Shikuang Deng &middot; Yuhang Wu &middot; KANGRUI DU &middot; Shi Gu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Spiking neural networks (SNNs), inspired by biological processes, use spike signals for inter-layer communication, presenting an energy-efficient alternative to traditional neural networks. To realize the theoretical advantages of SNNs in energy efficiency, it is essential to deploy them onto neuromorphic chips. On clock-driven synchronous chips, employing shorter time steps can enhance energy efficiency but reduce SNN performance. Compared to the clock-driven synchronous chip, the event-driven asynchronous chip achieves much lower energy consumption but only supports some specific network operations. Recently, a series of SNN projects have achieved tremendous success, significantly improving the SNN's performance. However, event-driven asynchronous chips do not support some of the proposed structures, making it impossible to integrate these SNNs into asynchronous hardware. In response to these problems, we propose the Spiking Token Mixer (STMixer) architecture, which consists exclusively of operations supported by asynchronous scenarios including convolutional, fully connected layers, and residual paths. Our series of experiments also demonstrate that STMixer achieves performance on par with spiking transformers in synchronous scenarios with very low timesteps. This indicates its ability to achieve the same level of performance with lower power consumption in synchronous scenarios. Codes are available at \url{https://github.com/brain-intelligence-lab/STMixer_demo}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-177" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-177', event_id='94712', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2411</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94712">Acoustic Volume Rendering for Neural Impulse Response Fields</a></strong></h5>


                        <p class="text-muted">
                            Zitong Lan &middot; Chenhao Zheng &middot; Zhiwei Zheng &middot; Mingmin Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Realistic audio synthesis that captures accurate acoustic phenomena is essential for creating immersive experiences in virtual and augmented reality. Synthesizing the sound received at any position relies on the estimation of impulse response (IR), which characterizes how sound propagates in one scene along different paths before arriving at the listener position. In this paper, we present Acoustic Volume Rendering (AVR), a novel approach that adapts volume rendering techniques to model acoustic impulse responses. While volume rendering has been successful in modeling radiance fields for images and neural scene representations, IRs present unique challenges as time-series signals. To address these challenges, we introduce frequency-domain volume rendering and use spherical integration to fit the IR measurements. Our method constructs an impulse response field that inherently encodes wave propagation principles and achieves state of-the-art performance in synthesizing impulse responses for novel poses. Experiments show that AVR surpasses current leading methods by a substantial margin. Additionally, we develop an acoustic simulation platform, AcoustiX, which provides more accurate and realistic IR simulations than existing simulators. Code for AVR and AcoustiX are available at https://zitonglan.github.io/avr.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-178" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-178', event_id='96143', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2500</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96143">SceneCraft: Layout-Guided 3D Scene Generation</a></strong></h5>


                        <p class="text-muted">
                            Xiuyu Yang &middot; Yunze Man &middot; Junkun Chen &middot; Yu-Xiong Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The creation of complex 3D scenes tailored to user specifications has been a tedious and challenging task with traditional 3D modeling tools. Although some pioneering methods have achieved automatic text-to-3D generation, they are generally limited to small-scale scenes with restricted control over the shape and texture. We introduce SceneCraft, a novel method for generating detailed indoor scenes that adhere to textual descriptions and spatial layout preferences provided by users. Central to our method is a rendering-based technique, which converts 3D semantic layouts into multi-view 2D proxy maps. Furthermore, we design a semantic and depth conditioned diffusion model to generate multi-view images, which are used to learn a neural radiance field (NeRF) as the final scene representation. Without the constraints of panorama image generation, we surpass previous methods in supporting complicated indoor space generation beyond a single room, even as complicated as a whole multi-bedroom apartment with irregular shapes and layouts. Through experimental analysis, we demonstrate that our method significantly outperforms existing approaches in complex indoor scene generation with diverse textures, consistent geometry, and realistic visual quality.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-179" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-179', event_id='96008', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2501</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96008">Towards a Scalable Reference-Free Evaluation of Generative Models</a></strong></h5>


                        <p class="text-muted">
                            Azim Ospanov &middot; Jingwei Zhang &middot; Mohammad Jalali &middot; Xuenan Cao &middot; Andrej Bogdanov &middot; Farzan Farnia
                        </p>

                    </div>
                    <div class="abstract">
                        <p>While standard evaluation scores for generative models are mostly reference-based, a reference-dependent assessment of generative models could be generally difficult due to the unavailability of applicable reference datasets. Recently, the reference-free entropy scores, VENDI and RKE, have been proposed to evaluate the diversity of generated data. However, estimating these scores from data leads to significant computational costs for large-scale generative models. In this work, we leverage the random Fourier features framework to reduce the metrics' complexity and propose the *Fourier-based Kernel Entropy Approximation (FKEA)* method. We utilize FKEA's approximated eigenspectrum of the kernel matrix to efficiently estimate the mentioned entropy scores. Furthermore, we show the application of FKEA's proxy eigenvectors to reveal the method's identified modes in evaluating the diversity of produced samples. We provide a stochastic implementation of the FKEA assessment algorithm with a complexity $O(n)$ linearly growing with sample size $n$. We extensively evaluate FKEA's numerical performance in application to standard image, text, and video datasets. Our empirical results indicate the method's scalability and interpretability applied to large-scale generative models.  The codebase is available at [https://github.com/aziksh-ospanov/FKEA](https://github.com/aziksh-ospanov/FKEA).</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-180" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-180', event_id='95683', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2502</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95683">Image Reconstruction Via Autoencoding Sequential Deep Image Prior</a></strong></h5>


                        <p class="text-muted">
                            Ismail Alkhouri &middot; Shijun Liang &middot; Evan Bell &middot; Qing Qu &middot; Rongrong Wang &middot; Saiprasad Ravishankar
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recently, Deep Image Prior (DIP) has emerged as an effective unsupervised one-shot learner, delivering competitive results across various image recovery problems. This method only requires the noisy measurements and a forward operator, relying solely on deep networks initialized with random noise to learn and restore the structure of the data. However, DIP is notorious for its vulnerability to overfitting due to the overparameterization of the network. Building upon insights into the impact of the DIP input and drawing inspiration from the gradual denoising process in cutting-edge diffusion models, we introduce Autoencoding Sequential DIP (aSeqDIP) for image reconstruction. This method progressively denoises and reconstructs the image through a sequential optimization of network weights. This is achieved using an input-adaptive DIP objective, combined with an autoencoding regularization term. Compared to diffusion models, our method does not require training data and outperforms other DIP-based methods in mitigating noise overfitting while maintaining a similar number of parameter updates as Vanilla DIP. Through extensive experiments, we validate the effectiveness of our method in various image reconstruction tasks, such as MRI and CT reconstruction, as well as in image restoration tasks like image denoising, inpainting, and non-linear deblurring.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-181" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-181', event_id='95153', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2503</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95153">B&#x27;MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory</a></strong></h5>


                        <p class="text-muted">
                            Luca Zancato &middot; Arjun Seshadri &middot; Yonatan Dukler &middot; Aditya Sharad Golatkar &middot; Yantao Shen &middot; Benjamin Bowman &middot; Matthew Trager &middot; Alessandro Achille &middot; Stefano Soatto
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span ('context' in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory 'in-context,' permanent structural memory 'in-weights,' fading memory 'in-state,' and long-term eidetic memory 'in-storage' by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced,  that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we test whether models trained inductively on a-priori bounded sequences (up to 8K tokens) can still perform transductive inference on sequences many-fold longer. B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-182" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-182', event_id='95021', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2504</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95021">Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models</a></strong></h5>


                        <p class="text-muted">
                            Alliot Nagle &middot; Adway Girish &middot; Marco Bondaschi &middot; Michael Gastpar &middot; Ashok Vardhan Makkuva &middot; Hyeji Kim
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We formalize the problem of prompt compression for large language models (LLMs) and present a framework to unify token-level prompt compression methods which create hard prompts for black-box models. We derive the distortion-rate function for this setup as a linear program, and provide an efficient algorithm to compute this fundamental limit via the dual of the linear program. Using the distortion-rate function as the baseline, we study the performance of existing compression schemes on a synthetic dataset consisting of prompts generated from a Markov chain, natural language queries, and their respective answers. Our empirical analysis demonstrates the criticality of query-aware prompt compression, where the compressor has knowledge of the downstream task/query for the black-box LLM. We show that there is a large gap between the performance of current prompt compression methods and the optimal strategy, and propose Adaptive QuerySelect, a query-aware, variable-rate adaptation of a prior work to close the gap. We extend our experiments to a small natural language dataset to further confirm our findings on our synthetic dataset.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-183" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-183', event_id='94950', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2505</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94950">A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit Tasks in Public Health</a></strong></h5>


                        <p class="text-muted">
                            Nikhil Behari &middot; Edwin Zhang &middot; YUNFAN ZHAO &middot; Aparna Taneja &middot; Dheeraj Nagaraj &middot; Milind Tambe
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Restless multi-armed bandits (RMAB) have demonstrated success in optimizing resource allocation for large beneficiary populations in public health settings. Unfortunately, RMAB models lack flexibility to adapt to evolving public health policy priorities. Concurrently, Large Language Models (LLMs) have emerged as adept automated planners across domains of robotic control and navigation. In this paper, we propose a Decision Language Model (DLM) for RMABs, enabling dynamic fine-tuning of RMAB policies in public health settings using human-language commands. We propose using LLMs as automated planners to (1) interpret human policy preference prompts, (2) propose reward functions as code for a multi-agent RMAB environment, and (3) iterate on the generated reward functions using feedback from grounded RMAB simulations. We illustrate the application of DLM in collaboration with ARMMAN, an India-based non-profit promoting preventative care for pregnant mothers, that currently relies on RMAB policies to optimally allocate health worker calls to low-resource populations.  We conduct a technology demonstration in simulation using the Gemini Pro model, showing DLM can dynamically shape policy outcomes using only human prompts as input.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-184" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-184', event_id='94896', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2506</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94896">Stability and Generalizability in SDE Diffusion Models with Measure-Preserving Dynamics</a></strong></h5>


                        <p class="text-muted">
                            Weitong Zhang &middot; Chengqi Zang &middot; Liu Li &middot; Sarah Cechnicka &middot; Cheng Ouyang &middot; Bernhard Kainz
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Inverse problems describe the process of estimating the causal factors from a set of measurements or data. Mapping of often incomplete or degraded data to parameters is ill-posed, thus data-driven iterative solutions are required, for example when reconstructing clean images from poor signals. Diffusion models have shown promise as potent generative tools for solving inverse problems due to their superior reconstruction quality and their compatibility with iterative solvers. However, most existing approaches are limited to linear inverse problems represented as Stochastic Differential Equations (SDEs). This simplification falls short of addressing the challenging nature of real-world problems, leading to amplified cumulative errors and biases. We provide an explanation for this gap through the lens of measure-preserving dynamics of Random Dynamical Systems (RDS) with which we analyse Temporal Distribution Discrepancy and thus introduce a theoretical framework based on RDS for SDE diffusion models. We uncover several strategies that inherently enhance the stability and generalizability of diffusion models for inverse problems and introduce a novel score-based diffusion framework, the Dynamics-aware SDE Diffusion Generative Model (D^3GM). The Measure-preserving property can return the degraded measurement to the original state despite complex degradation with the RDS concept of stability.Our extensive experimental results corroborate the effectiveness of D^3GM across multiple benchmarks including a prominent application for inverse problems, magnetic resonance imaging.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-185" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-185', event_id='94782', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2507</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94782">Absorb &amp; Escape: Overcoming Single Model Limitations in Generating Heterogeneous Genomic Sequences</a></strong></h5>


                        <p class="text-muted">
                            Zehui Li &middot; Yuhao Ni &middot; Guoxuan Xia &middot; William Beardall &middot; Akashaditya Das &middot; Guy-Bart Stan &middot; Yiren Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advances in immunology and synthetic biology have accelerated the development of deep generative methods for DNA sequence design. Two dominant approaches in this field are AutoRegressive (AR) models and Diffusion Models (DMs). However, genomic sequences are functionally heterogeneous, consisting of multiple connected regions (e.g., Promoter Regions, Exons, and Introns) where elements within each region come from the same probability distribution, but the overall sequence is non-homogeneous. This heterogeneous nature presents challenges for a single model to accurately generate genomic sequences. In this paper, we analyze the properties of AR models and DMs in heterogeneous genomic sequence generation, pointing out crucial limitations in both methods: (i) AR models capture the underlying distribution of data by factorizing and learning the transition probability but fail to capture the global property of DNA sequences. (ii) DMs learn to recover the global distribution but tend to produce errors at the base pair level. To overcome the limitations of both approaches, we propose a post-training sampling method, termed Absorb &amp; Escape (A&amp;E) to perform compositional generation from AR models and DMs. This approach starts with samples generated by DMs and refines the sample quality using an AR model through the alternation of the Absorb and Escape steps.  To assess the quality of generated sequences, we conduct extensive experiments on 15 species for conditional and unconditional DNA generation. The experiment results from motif distribution, diversity checks, and genome integration tests unequivocally show that A&amp;E outperforms state-of-the-art AR models and DMs in genomic sequence generation. A&amp;E does not suffer from the slowness of traditional MCMC to sample from composed distributions with Energy-Based Models whilst it obtains higher quality samples than single models. Our research sheds light on the limitations of current single-model approaches in DNA generation and provides a simple but effective solution for heterogeneous sequence generation. Code is available at the <a href="https://github.com/Zehui127/Absorb-Escape">Github Repo</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-186" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-186', event_id='94701', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2508</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94701">Synergistic Dual Spatial-aware Generation of Image-to-text and Text-to-image</a></strong></h5>


                        <p class="text-muted">
                            Yu Zhao &middot; Hao Fei &middot; Xiangtai Li &middot; Libo Qin &middot; Jiayi Ji &middot; Hongyuan Zhu &middot; Meishan Zhang &middot; Min Zhang &middot; Jianguo Wei
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In the visual spatial understanding (VSU) field, spatial image-to-text (SI2T) and spatial text-to-image (ST2I) are two fundamental tasks that appear in dual form. Existing methods for standalone SI2T or ST2I perform imperfectly in spatial understanding, due to the difficulty of 3D-wise spatial feature modeling. In this work, we consider modeling the SI2T and ST2I together under a dual learning framework. During the dual framework, we then propose to represent the 3D spatial scene features with a novel 3D scene graph (3DSG) representation that can be shared and beneficial to both tasks. Further, inspired by the intuition that the easier 3D$\to$image and 3D$\to$text processes also exist symmetrically in the ST2I and SI2T, respectively, we propose the Spatial Dual Discrete Diffusion (SD$^3$) framework, which utilizes the intermediate features of the 3D$\to$X processes to guide the hard X$\to$3D processes, such that the overall ST2I and SI2T will benefit each other. On the visual spatial understanding dataset VSD, our system outperforms the mainstream T2I and I2T methods significantly.Further in-depth analysis reveals how our dual learning strategy advances.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-187" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-187', event_id='94522', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2509</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94522">ET-Flow: Equivariant Flow-Matching for Molecular Conformer Generation</a></strong></h5>


                        <p class="text-muted">
                            Majdi Hassan &middot; Nikhil Shenoy &middot; Jungyoon Lee &middot; Hannes St√§rk &middot; Stephan Thaler &middot; Dominique Beaini
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Predicting low-energy molecular conformations given a molecular graph is an important but challenging task in computational drug discovery. Existing state-of-the-art approaches either resort to large scale transformer-based models thatdiffuse over conformer fields, or use computationally expensive methods to gen-erate initial structures and diffuse over torsion angles. In this work, we introduceEquivariant Transformer Flow (ET-Flow). We showcase that a well-designedflow matching approach with equivariance and harmonic prior alleviates the needfor complex internal geometry calculations and large architectures, contrary tothe prevailing methods in the field. Our approach results in a straightforwardand scalable method that directly operates on all-atom coordinates with minimalassumptions. With the advantages of equivariance and flow matching, ET-Flowsignificantly increases the precision and physical validity of the generated con-formers, while being a lighter model and faster at inference. Code is availablehttps://github.com/shenoynikhil/ETFlow.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-188" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-188', event_id='94368', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2510</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94368">Self-Consuming Generative Models with Curated Data Provably Optimize Human Preferences</a></strong></h5>


                        <p class="text-muted">
                            Damien Ferbach &middot; Quentin Bertrand &middot; Joey Bose &middot; Gauthier Gidel
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The rapid progress in generative models has resulted in impressive leaps in generation quality, blurring the lines between synthetic and real data. Web-scale datasets are now prone to the inevitable contamination by synthetic data, directly impacting the training of future generated models.    Already, some theoretical results on self-consuming generative models (a.k.a., iterative retraining) have emerged in the literature, showcasing that either model collapse or stability could be possible depending on the fraction of generated data used at each retraining step.    However, in practice, synthetic data is often subject to human feedback and curated by users before being used and uploaded online. For instance, many interfaces of popular text-to-image generative models, such as Stable Diffusion or Midjourney, produce several variations of an image for a given query which can eventually be curated by the users.    In this paper, we theoretically study the impact of data curation on iterated retraining of generative models and show that it can be seen as an implicit preference optimization mechanism. However, unlike standard preference optimization, the generative model does not have access to the reward function or negative samples needed for pairwise comparisons. Moreover, our study doesn't require access to the density function, only to samples. We prove that, if the data is curated according to a reward model, then the expected reward of the iterative retraining procedure is maximized. We further provide theoretical results on the stability of the retraining loop when using a positive fraction of real data at each step. Finally, we conduct illustrative experiments on both synthetic datasets and on CIFAR10 showing that such a procedure amplifies biases of the reward model.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-189" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-189', event_id='94362', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2511</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94362">Classification Diffusion Models: Revitalizing Density Ratio Estimation</a></strong></h5>


                        <p class="text-muted">
                            Shahar Yadin &middot; Noam Elata &middot; Tomer Michaeli
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to <em>classify</em> between data samples and samples from some reference distribution. DRE-based models can directly output the likelihood for any given input, a highly desired property that is lacking in most generative techniques. Nevertheless, to date, DRE methods have failed in accurately capturing the distributions of complex high-dimensional data, like images, and have thus been drawing reduced research attention in recent years.  In this work we present <em>classification diffusion models</em> (CDMs), a DRE-based generative method that adopts the formalism of denoising diffusion models (DDMs) while making use of a classifier that predicts the level of noise added to a clean signal. Our method is based on an analytical connection that we derive between the MSE-optimal denoiser for removing white Gaussian noise and the cross-entropy-optimal classifier for predicting the noise level. Our method is the first DRE-based technique that can successfully generate images beyond the MNIST dataset. Furthermore, it can output the likelihood of any input in a single forward pass, achieving state-of-the-art negative log likelihood (NLL) among methods with this property.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-190" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-190', event_id='96234', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2600</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96234">Equivariant Blurring Diffusion for Hierarchical Molecular Conformer Generation</a></strong></h5>


                        <p class="text-muted">
                            Jiwoong Park &middot; Yang Shen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>How can diffusion models process 3D geometries in a coarse-to-fine manner, akin to our multiscale view of the world?In this paper, we address the question by focusing on a fundamental biochemical problem of generating 3D molecular conformers conditioned on molecular graphs in a multiscale manner. Our approach consists of two hierarchical stages: i) generation of coarse-grained fragment-level 3D structure from the molecular graph, and ii) generation of fine atomic details from the coarse-grained approximated structure while allowing the latter to be adjusted simultaneously.For the challenging second stage, which demands preserving coarse-grained information while ensuring SE(3) equivariance, we introduce a novel generative model termed Equivariant Blurring Diffusion (EBD), which defines a forward process that moves towards the fragment-level coarse-grained structure by blurring the fine atomic details of conformers, and a reverse process that performs the opposite operation using equivariant networks.We demonstrate the effectiveness of EBD by geometric and chemical comparison to state-of-the-art denoising diffusion models on a benchmark of drug-like molecules.Ablation studies draw insights on the design of EBD by thoroughly analyzing its architecture, which includes the design of the loss function and the data corruption process.Codes are released at https://github.com/Shen-Lab/EBD.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-191" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-191', event_id='96320', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2601</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96320">Evaluating the design space of diffusion-based generative models</a></strong></h5>


                        <p class="text-muted">
                            Yuqing Wang &middot; Ye He &middot; Molei Tao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Most existing theoretical investigations of the accuracy of diffusion models, albeit significant, assume the score function has been approximated to a certain accuracy, and then use this a priori bound to control the error of generation. This article instead provides a first quantitative understanding of the whole generation process, i.e., both training and sampling. More precisely, it conducts a non-asymptotic convergence analysis of denoising score matching under gradient descent. In addition, a refined sampling error analysis for variance exploding models is also provided. The combination of these two results yields a full error analysis, which elucidates (again, but this time theoretically) how to design the training and sampling processes for effective generation. For instance, our theory implies a preference toward noise distribution and loss weighting in training that qualitatively agree with the ones used in [Karras et al., 2022]. It also provides perspectives on the choices of time and variance schedules in sampling: when the score is well trained, the design in [Song et al., 2021] is more preferable, but when it is less trained, the design in [Karras et al., 2022] becomes more preferable.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-192" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-192', event_id='96409', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2602</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96409">Learning Diffusion Priors from Observations by Expectation Maximization</a></strong></h5>


                        <p class="text-muted">
                            Fran√ßois Rozet &middot; Gerome Andry &middot; Francois Lanusse &middot; Gilles Louppe
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion models recently proved to be remarkable priors for Bayesian inverse problems. However, training these models typically requires access to large amounts of clean data, which could prove difficult in some settings. In this work, we present a novel method based on the expectation-maximization algorithm for training diffusion models from incomplete and noisy observations only. Unlike previous works, our method leads to proper diffusion models, which is crucial for downstream tasks. As part of our method, we propose and motivate an improved posterior sampling scheme for unconditional diffusion models. We present empirical evidence supporting the effectiveness of our method.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-193" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-193', event_id='96489', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2603</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96489">DiffGS: Functional Gaussian Splatting Diffusion</a></strong></h5>


                        <p class="text-muted">
                            Junsheng Zhou &middot; Weiqi Zhang &middot; Yu-Shen Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>3D Gaussian Splatting (3DGS) has shown convincing performance in rendering speed and fidelity, yet the generation of Gaussian Splatting remains a challenge due to its discreteness and unstructured nature. In this work, we propose DiffGS, a general Gaussian generator based on latent diffusion models. DiffGS is a powerful and efficient 3D generative model which is capable of generating Gaussian primitives at arbitrary numbers for high-fidelity rendering with rasterization. The key insight is to represent Gaussian Splatting in a disentangled manner via three novel functions to model Gaussian probabilities, colors and transforms. Through the novel disentanglement of 3DGS, we represent the discrete and unstructured 3DGS with continuous Gaussian Splatting functions, where we then train a latent diffusion model with the target of generating these Gaussian Splatting functions both unconditionally and conditionally. Meanwhile, we introduce a discretization algorithm to extract Gaussians at arbitrary numbers from the generated functions via octree-guided sampling and optimization. We explore DiffGS for various tasks, including unconditional generation, conditional generation from text, image, and partial 3DGS, as well as Point-to-Gaussian generation. We believe that DiffGS provides a new direction for flexibly modeling and generating Gaussian Splatting. Project page: https://junshengzhou.github.io/DiffGS.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-194" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-194', event_id='94058', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2604</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94058">A2PO: Towards Effective Offline Reinforcement Learning from an Advantage-aware Perspective</a></strong></h5>


                        <p class="text-muted">
                            Yunpeng Qing &middot; Shunyu Liu &middot; Jingyuan Cong &middot; Kaixuan Chen &middot; Yihe Zhou &middot; Mingli Song
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Offline reinforcement learning endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the out-of-distribution problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent advantage-weighted methods prioritize samples with high advantage values for agent training while inevitably ignoring the diversity of behavior policy. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a conditional variational auto-encoder to disentangle the action distributions of intertwined behavior policies by modeling the advantage values of all training data as conditional variables. Then the agent can follow such disentangled action distribution constraints to optimize the advantage-aware policy towards high advantage values. Extensive experiments conducted on both the single-quality and mixed-quality datasets of the D4RL benchmark demonstrate that A2PO yields results superior to the counterparts. Our code is available at https://github.com/Plankson/A2PO.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-195" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-195', event_id='96953', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2605</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96953">SyncTweedies: A General Generative Framework Based on Synchronized Diffusions</a></strong></h5>


                        <p class="text-muted">
                            Jaihoon Kim &middot; Juil Koo &middot; Kyeongmin Yeo &middot; Minhyuk Sung
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce a general diffusion synchronization framework for generating diverse visual content, including ambiguous images, panorama images, 3D mesh textures, and 3D Gaussian splats textures, using a pretrained image diffusion model. We first present an analysis of various scenarios for synchronizing multiple diffusion processes through a canonical space. Based on the analysis, we introduce a synchronized diffusion method, SyncTweedies, which averages the outputs of Tweedie‚Äôs formula while conducting denoising in multiple instance spaces. Compared to previous work that achieves synchronization through finetuning, SyncTweedies is a zero-shot method that does not require any finetuning, preserving the rich prior of diffusion models trained on Internet-scale image datasets without overfitting to specific domains. We verify that SyncTweedies offers the broadest applicability to diverse applications and superior performance compared to the previous state-of-the-art for each application. Our project page is at https://synctweedies.github.io.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-196" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-196', event_id='97505', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2606</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97505">VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Wenhao Wang &middot; Yi Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, along with other text-to-video diffusion models, is highly reliant on prompts, and there is no publicly available dataset that features a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 Million unique text-to-Video Prompts from real users. Additionally, this dataset includes 6.69 million videos generated by four state-of-the-art diffusion models, alongside some related data. We initially discuss the curation of this large-scale dataset, a process that is both time-consuming and costly. Subsequently, we underscore the need for a new prompt dataset specifically designed for text-to-video generation by illustrating how VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Our extensive and diverse dataset also opens up many exciting new research areas. For instance, we suggest exploring text-to-video prompt engineering, efficient video generation, and video copy detection for diffusion models to develop better, more efficient, and safer models. The project (including the collected dataset VidProM and related code) is publicly available at https://vidprom.github.io under the CC-BY-NC 4.0 License.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-197" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-197', event_id='93024', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2607</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93024">Local Curvature Smoothing with Stein&#x27;s Identity for Efficient Score Matching</a></strong></h5>


                        <p class="text-muted">
                            GENKI OSADA &middot; Makoto Shing &middot; Takashi Nishide
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The training of score-based diffusion models (SDMs) is based on score matching. The challenge of score matching is that it includes a computationally expensive Jacobian trace. While several methods have been proposed to avoid this computation, each has drawbacks, such as instability during training and approximating the learning as learning a denoising vector field rather than a true score.We propose a novel score matching variant, local curvature smoothing with Stein's identity (LCSS). The LCSS bypasses the Jacobian trace by applying Stein's identity, enabling regularization effectiveness and efficient computation. We show that LCSS surpasses existing methods in sample generation performance and matches the performance of denoising score matching, widely adopted by most SDMs, in evaluations such as FID, Inception score, and bits per dimension. Furthermore, we show that LCSS enables realistic image generation even at a high resolution of $1024 \times 1024$.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-198" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-198', event_id='93171', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2608</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93171">Energy-Based Modelling for Discrete and Mixed Data via Heat Equations on Structured Spaces</a></strong></h5>


                        <p class="text-muted">
                            Tobias Schr√∂der &middot; Zijing Ou &middot; Yingzhen Li &middot; Andrew Duncan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Energy-based models (EBMs) offer a flexible framework for probabilistic modelling across various data domains. However, training EBMs on data in discrete or mixed state spaces poses significant challenges due to the lack of robust and fast sampling methods. In this work, we propose to train discrete EBMs with Energy Discrepancy, a loss function which only requires the evaluation of the energy function at data points and their perturbed counterparts, thus eliminating the need for Markov chain Monte Carlo. We introduce perturbations of the data distribution by simulating a diffusion process on the discrete state space endowed with a graph structure. This allows us to inform the choice of perturbation from the structure of the modelled discrete variable, while the continuous time parameter enables fine-grained control of the perturbation. Empirically, we demonstrate the efficacy of the proposed approaches in a wide range of applications, including the estimation of discrete densities with non-binary vocabulary and binary image modelling. We also introduce the first application of EBMs to tabular data sets with applications in synthetic data generation and calibrated classification.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-199" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-199', event_id='93326', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2609</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93326">Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference</a></strong></h5>


                        <p class="text-muted">
                            Jiabao Ji &middot; Yujian Liu &middot; Yang Zhang &middot; Gaowen Liu &middot; Ramana Kompella &middot; Sijia Liu &middot; Shiyu Chang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>As Large Language Models (LLMs) demonstrate extensive capability in learning from documents, LLM unlearning becomes an increasingly important research area to address concerns of LLMs in terms of privacy, copyright, etc. A conventional LLM unlearning task typically involves two goals: (1) The target LLM should forget the knowledge in the specified forget documents; and (2) it should retain the other knowledge that the LLM possesses, for which we assume access to a small number of retain documents. To achieve both goals, a mainstream class of LLM unlearning methods introduces an optimization framework with a combination of two objectives ‚Äì maximizing the prediction loss on the forget documents while minimizing that on the retain documents, which suffers from two challenges, degenerated output and catastrophic forgetting. In this paper, we propose a novel unlearning framework called Unlearning from Logit Difference (ULD), which introduces an assistant LLM that aims to achieve the opposite of the unlearning goals: remembering the forget documents and forgetting the retain knowledge. ULD then derives the unlearned LLM by computing the logit difference between the target and the assistant LLMs. We show that such reversed objectives would naturally resolve both aforementioned challenges while significantly improving the training efficiency. Extensive experiments demonstrate that our method efficiently achieves the intended forgetting while preserving the LLM‚Äôs overall capabilities, reducing training time by more than threefold. Notably, our method loses 0% of model utility on the ToFU benchmark, whereas baseline methods may sacrifice 17% of utility on average to achieve comparable forget quality.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-200" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-200', event_id='93995', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2610</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93995">DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection</a></strong></h5>


                        <p class="text-muted">
                            Hongyu Shen &middot; Yici Yan &middot; Zhizhen Jane Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Model-X knockoff has garnered significant attention among various feature selection methods due to its guarantees for controlling the false discovery rate (FDR). Since its introduction in parametric design, knockoff techniques have evolved to handle arbitrary data distributions using deep learning-based generative models. However, we have observed limitations in the current implementations of the deep Model-X knockoff framework. Notably, the "swap property" that knockoffs require often faces challenges at the sample level, resulting in diminished selection power. To address these issues, we develop "Deep Dependency Regularized Knockoff (DeepDRK)," a distribution-free deep learning method that effectively balances FDR and power. In DeepDRK, we introduce a novel formulation of the knockoff model as a learning problem under multi-source adversarial attacks. By employing an innovative perturbation technique, we achieve lower FDR and higher power. Our model outperforms existing benchmarks across synthetic, semi-synthetic, and real-world datasets, particularly when sample sizes are small and data distributions are non-Gaussian.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-201" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-201', event_id='93996', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2611</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93996">Theoretical guarantees in KL for Diffusion Flow Matching</a></strong></h5>


                        <p class="text-muted">
                            Marta Gentiloni Silveri &middot; Alain Durmus &middot; Giovanni Conforti
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Flow Matching (FM) (also referred to as stochastic interpolants or rectified flows)  stands out as a class of generative models that aims to bridge in finite time the target distribution $\nu^\star$ with an auxiliary distribution $\mu$ leveraging a fixed coupling $\pi$ and a bridge which can either be deterministic or stochastic. These two ingredients define a path measure which can then be approximated by learning the drift of its Markovian projection. The main contribution of this paper is to provide relatively mild assumption on $\nu^\star$, $\mu$ and $\pi$ to obtain non-asymptotics guarantees for Diffusion Flow Matching (DFM) models using as bridge the conditional distribution associated with the Brownian motion. More precisely, it establishes bounds on the Kullback-Leibler divergence between the target distribution and the one generated by such DFM models under moment conditions on the score of $\nu^\star$, $\mu$ and $\pi$, and a standard $\mathrm{L}^2$-drift-approximation error assumption.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-202" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-202', event_id='96210', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2700</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96210">Generative Fractional Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Gabriel Nobis &middot; Maximilian Springenberg &middot; Marco Aversa &middot; Michael Detzel &middot; Rembert Daems &middot; Roderick Murray-Smith &middot; Shinichi Nakajima &middot; Sebastian Lapuschkin &middot; Stefano Ermon &middot; Tolga Birdal &middot; Manfred Opper &middot; Christoph Knochenhauer &middot; Luis Oala &middot; Wojciech Samek
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We introduce the first continuous-time score-based generative model that leverages fractional diffusion processes for its underlying dynamics. Although diffusion models have excelled at capturing data distributions, they still suffer from various limitations such as slow convergence, mode-collapse on imbalanced data, and lack of diversity. These issues are partially linked to the use of light-tailed Brownian motion (BM) with independent increments. In this paper, we replace BM with an approximation of its non-Markovian counterpart, fractional Brownian motion (fBM), characterized by correlated increments and Hurst index $H \in (0,1)$, where $H=0.5$ recovers the classical BM. To ensure tractable inference and learning, we employ a recently popularized Markov approximation of fBM (MA-fBM) and derive its reverse-time model, resulting in *generative fractional diffusion models* (GFDM). We characterize the forward dynamics using a continuous reparameterization trick and propose *augmented score matching* to efficiently learn the score function, which is partly known in closed form, at minimal added cost. The ability to drive our diffusion model via MA-fBM offers flexibility and control. $H \leq 0.5$ enters the regime of *rough paths* whereas $H>0.5$ regularizes diffusion paths and invokes long-term memory. The Markov approximation allows added control by varying the number of Markov processes linearly combined to approximate fBM. Our evaluations on real image datasets demonstrate that GFDM achieves greater pixel-wise diversity and enhanced image quality, as indicated by a lower FID, offering a promising alternative to traditional diffusion models</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-203" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-203', event_id='96200', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2701</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96200">Model-based Diffusion for Trajectory Optimization</a></strong></h5>


                        <p class="text-muted">
                            Chaoyi Pan &middot; Zeji Yi &middot; Guanya Shi &middot; Guannan Qu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advances in diffusion models have demonstrated their strong capabilities in generating high-fidelity samples from complex distributions through an iterative refinement process. Despite the empirical success of diffusion models in motion planning and control, the model-free nature of these methods does not leverage readily available model information and limits their generalization to new scenarios beyond the training data (e.g., new robots with different dynamics). In this work, we introduce Model-Based Diffusion (MBD), an optimization approach using the diffusion process to solve trajectory optimization (TO) problems without data. The key idea is to explicitly compute the score function by leveraging the model information in TO problems, which is why we refer to our approach as model-based diffusion. Moreover, although MBD does not require external data, it can be naturally integrated with data of diverse qualities to steer the diffusion process. We also reveal that MBD has interesting connections to sampling-based optimization. Empirical evaluations show that MBD outperforms state-of-the-art reinforcement learning and sampling-based TO methods in challenging contact-rich tasks. Additionally, MBD‚Äôs ability to integrate with data enhances its versatility and practical applicability, even with imperfect and infeasible data (e.g., partial-state demonstrations for high-dimensional humanoids), beyond the scope of standard diffusion models. Videos and codes are available in the supplementary materials.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-204" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-204', event_id='96012', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2702</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96012">Constrained Diffusion Models via Dual Training</a></strong></h5>


                        <p class="text-muted">
                            Shervin Khalafi &middot; Dongsheng Ding &middot; Alejandro Ribeiro
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion models have attained prominence for their ability to synthesize a probability distribution for a given dataset via a diffusion process,  enabling the generation of new data points with high fidelity. However, diffusion processes are prone to generating samples that reflect biases in a training dataset. To address this issue, we develop constrained diffusion models by imposing diffusion constraints based on desired distributions that are informed by requirements. Specifically, we cast the training of diffusion models under requirements as a constrained distribution optimization problem that aims to reduce the distribution difference between original and generated data while obeying constraints on the distribution of generated data. We show that our constrained diffusion models generate new data from a mixture data distribution that achieves the optimal trade-off among objective and constraints. To train constrained diffusion models, we develop a dual training algorithm and characterize the optimality of the trained constrained diffusion model. We empirically demonstrate the effectiveness of our constrained models in two constrained generation tasks: (i) we consider a dataset with one or more underrepresented classes where we train the model with constraints to ensure fairly sampling from all classes during inference; (ii) we fine-tune a pre-trained diffusion model to sample from a new dataset while avoiding overfitting.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-205" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-205', event_id='95935', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2703</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95935">Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models</a></strong></h5>


                        <p class="text-muted">
                            Jiacheng Ye &middot; Shansan Gong &middot; Liheng Chen &middot; Lin Zheng &middot; Jiahui Gao &middot; Han Shi &middot; Chuan Wu &middot; Xin Jiang &middot; Zhenguo Li &middot; Wei Bi &middot; Lingpeng Kong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recently, diffusion models have garnered significant interest in the field of text processing due to their many potential advantages compared to conventional autoregressive models.In this work, we propose Diffusion-of-Thought (DoT),  a novel approach that integrates diffusion models with Chain-of-Thought, a well-established technique for improving the reasoning ability of autoregressive language models. In contrast to autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT allows reasoning steps to diffuse over time through a diffusion language model and offers greater flexibility in trading-off computation for reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication, boolean logic, and grade school math problems. In addition to that, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning with diffusion language models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-206" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-206', event_id='95843', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2704</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95843">Learning Image Priors Through Patch-Based Diffusion Models for Solving Inverse Problems</a></strong></h5>


                        <p class="text-muted">
                            Jason Hu &middot; Bowen Song &middot; Xiaojian Xu &middot; Liyue Shen &middot; Jeffrey Fessler
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion models can learn strong image priors from underlying data distribution and use them to solve inverse problems,but the training process is computationally expensive and requires lots of data.Such bottlenecks prevent most existing works from being feasible for high-dimensional and high-resolution data such as 3D images.This paper proposes a method to learn an efficient data prior for the entire image by training diffusion models only on patches of images.Specifically, we propose a patch-based position-aware diffusion inverse solver, called PaDIS, where we obtain the score function of the whole image through scores of patches and their positional encoding and utilize this as the prior for solving inverse problems.First of all, we show that this diffusion model achieves an improved memory efficiency and data efficiencywhile still maintaining the  capability to generate entire images via positional encoding.Additionally, the proposed PaDIS model is highly flexible and can be plugged in with different diffusion inverse solvers (DIS).We demonstrate that the proposed PaDIS approach enables solving various inverse problems in both natural and medical image domains, including CT reconstruction, deblurring, and superresolution, given only patch-based priors.Notably, PaDIS outperforms previous DIS methods trained on entire image priors in the case of limited training data, demonstrating the data efficiency of our proposed approach by learning patch-based prior.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-207" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-207', event_id='95696', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2705</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95696">Resfusion: Denoising Diffusion Probabilistic Models for Image Restoration Based on Prior Residual Noise</a></strong></h5>


                        <p class="text-muted">
                            Zhenning Shi &middot; haoshuai zheng &middot; Chen Xu &middot; Changsheng Dong &middot; Bin Pan &middot; Xie xueshuo &middot; Along He &middot; Tao Li &middot; Huazhu Fu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recently, research on denoising diffusion models has expanded its application to the field of image restoration. Traditional diffusion-based image restoration methods utilize degraded images as conditional input to effectively guide the reverse generation process, without modifying the original denoising diffusion process. However, since the degraded images already include low-frequency information, starting from Gaussian white noise will result in increased sampling steps. We propose Resfusion, a general framework that incorporates the residual term into the diffusion forward process, starting the reverse process directly from the noisy degraded images. The form of our inference process is consistent with the DDPM. We introduced a weighted residual noise, named resnoise, as the prediction target and explicitly provide the quantitative relationship between the residual term and the noise term in resnoise. By leveraging a smooth equivalence transformation, Resfusion determine the optimal acceleration step and maintains the integrity of existing noise schedules, unifying the training and inference processes. The experimental results demonstrate that Resfusion exhibits competitive performance on ISTD dataset, LOL dataset and Raindrop dataset with only five sampling steps. Furthermore, Resfusion can be easily applied to image generation and emerges with strong versatility. Our code and model are available at https://github.com/nkicsl/Resfusion.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-208" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-208', event_id='95657', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2706</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95657">DiffuPac: Contextual Mimicry in Adversarial Packets Generation via Diffusion Model</a></strong></h5>


                        <p class="text-muted">
                            Abdullah Bin Jasni &middot; Akiko Manada &middot; Kohei Watabe
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In domains of cybersecurity, recent advancements in Machine Learning (ML) and Deep Learning (DL) have significantly enhanced Network Intrusion Detection Systems (NIDS), improving the effectiveness of cybersecurity operations. However, attackers have also leveraged ML/DL to develop sophisticated models that generate adversarial packets capable of evading NIDS detection. Consequently, defenders must study and analyze these models to prepare for the evasion attacks that exploit NIDS detection mechanisms. Unfortunately, conventional generation models often rely on unrealistic assumptions about attackers' knowledge of NIDS components, making them impractical for real-world scenarios. To address this issue, we present DiffuPac, a first-of-its-kind generation model designed to generate adversarial packets that evade detection without relying on specific NIDS components. DiffuPac integrates a pre-trained Bidirectional Encoder Representations from Transformers (BERT) with diffusion model, which, through its capability for conditional denoising and classifier-free guidance, effectively addresses the real-world constraint of limited attacker knowledge. By concatenating malicious packets with contextually relevant normal packets and applying targeted noising only to the malicious packets, DiffuPac seamlessly blends adversarial packets into genuine network traffic. Through evaluations on real-world datasets, we demonstrate that DiffuPac achieves strong evasion capabilities against sophisticated NIDS, outperforming conventional methods by an average of 6.69 percentage points, while preserving the functionality and practicality of the generated adversarial packets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-209" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-209', event_id='95607', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2707</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95607">Warped Diffusion: Solving Video Inverse Problems with Image Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Giannis Daras &middot; Weili Nie &middot; Karsten Kreis &middot; Alex Dimakis &middot; Morteza Mardani &middot; Nikola Kovachki &middot; Arash Vahdat
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Using image models naively for solving inverse video problems often suffers from flickering, texture-sticking, and temporal inconsistency in generated videos. To tackle these problems, in this paper, we view frames as continuous functions in the 2D space, and videos as a sequence of continuous warping transformations between different frames. This perspective allows us to train function space diffusion models only on **images** and utilize them to solve temporally correlated inverse problems. The function space diffusion models need to be equivariant with respect to the underlying spatial transformations. To ensure temporal consistency, we introduce a simple post-hoc test-time guidance towards (self)-equivariant solutions. Our method allows us to deploy state-of-the-art latent diffusion models such as Stable Diffusion XL to solve video inverse problems. We demonstrate the effectiveness of our method for video inpainting and $8\times$ video super-resolution, outperforming existing techniques based on noise transformations. We provide generated video results in the following URL: https://giannisdaras.github.io/warped_diffusion.github.io/.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-210" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-210', event_id='95457', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2708</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95457">Neural Residual Diffusion Models for Deep Scalable Vision Generation</a></strong></h5>


                        <p class="text-muted">
                            Zhiyuan Ma &middot; Liangliang Zhao &middot; Biqing Qi &middot; Bowen Zhou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The most advanced diffusion models have recently adopted increasingly deep stacked networks (e.g., U-Net or Transformer) to promote the generative emergence capabilities of vision generation models similar to large language models (LLMs). However, progressively deeper stacked networks will intuitively cause numerical propagation errors and reduce noisy prediction capabilities on generative data, which hinders massively deep scalable training of vision generation models. In this paper, we first uncover the nature that neural networks being able to effectively perform generative denoising lies in the fact that the intrinsic residual unit has consistent dynamic property with the input signal's reverse diffusion process, thus supporting excellent generative abilities.Afterwards, we stand on the shoulders of two common types of deep stacked networks to propose a unified and massively scalable Neural Residual Diffusion Models framework (Neural-RDM for short), which is a simple yet meaningful change to the common architecture of deep generative networks by introducing a series of learnable gated residual parameters that conform to the generative dynamics. Experimental results on various generative tasks show that the proposed neural residual models obtain state-of-the-art scores on image's and video's generative benchmarks. Rigorous theoretical proofs and extensive experiments also demonstrate the advantages of this simple gated residual mechanism consistent with dynamic modeling in improving the  fidelity and consistency of generated content and supporting large-scale scalable training.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-211" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-211', event_id='95426', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2709</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95426">Taming Generative Diffusion Prior for Universal Blind Image Restoration</a></strong></h5>


                        <p class="text-muted">
                            Siwei Tu &middot; Weidong Yang &middot; Ben Fei
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion models have been widely utilized for image restoration. However, previous blind image restoration methods still need to assume the type of degradation model while leaving the parameters to be optimized, limiting their real-world applications. Therefore, we aim to tame generative diffusion prior for universal blind image restoration dubbed BIR-D, which utilizes an optimizable convolutional kernel to simulate the degradation model and dynamically update the parameters of the kernel in the diffusion steps, enabling it to achieve blind image restoration results even in various complex situations. Besides, based on mathematical reasoning, we have provided an empirical formula for the chosen of adaptive guidance scale, eliminating the need for a grid search for the optimal parameter. Experimentally, Our BIR-D has demonstrated superior practicality and versatility than off-the-shelf unsupervised methods across various tasks both on real-world and synthetic datasets, qualitatively and quantitatively. BIR-D is able to fulfill multi-guidance blind image restoration. Moreover, BIR-D can also restore images that undergo multiple and complicated degradations, demonstrating the practical applications. The code is available at https://github.com/Tusiwei/BIR-D.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-212" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-212', event_id='95300', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2710</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95300">Hollowed Net for On-Device Personalization of Text-to-Image Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Wonguk Cho &middot; Seokeon Choi &middot; Debasmit Das &middot; Matthias Reisser &middot; Taesup Kim &middot; Sungrack Yun &middot; Fatih Porikli
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advancements in text-to-image diffusion models have enabled the personalization of these models to generate custom images from textual prompts. This paper presents an efficient LoRA-based personalization approach for on-device subject-driven generation, where pre-trained diffusion models are fine-tuned with user-specific data on resource-constrained devices. Our method, termed Hollowed Net, enhances memory efficiency during fine-tuning by modifying the architecture of a diffusion U-Net to temporarily remove a fraction of its deep layers, creating a hollowed structure. This approach directly addresses on-device memory constraints and substantially reduces GPU memory requirements for training, in contrast to previous methods that primarily focus on minimizing training steps and reducing the number of parameters to update. Additionally, the personalized Hollowed Net can be transferred back into the original U-Net, enabling inference without additional memory overhead. Quantitative and qualitative analyses demonstrate that our approach not only reduces training memory to levels as low as those required for inference but also maintains or improves personalization performance compared to existing methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-213" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-213', event_id='95188', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2711</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95188">Your Diffusion Model is Secretly a Noise Classifier and Benefits from Contrastive Training</a></strong></h5>


                        <p class="text-muted">
                            Yunshu Wu &middot; Yingtao Luo &middot; Xianghao Kong &middot; Vagelis Papalexakis &middot; Greg Ver Steeg
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion models learn to denoise data and the trained denoiser is then used to generate new samples from the data distribution. In this paper, we revisit the diffusion sampling process and identify a fundamental cause of sample quality degradation: the denoiser is poorly estimated in regions that are far Outside Of the training Distribution (OOD), and the sampling process inevitably evaluates in these OOD regions.This can become problematic for all sampling methods, especially when we move to parallel sampling which requires us to initialize and update the entire sample trajectory of dynamics in parallel, leading to many OOD evaluations. To address this problem, we introduce a new self-supervised training objective that differentiates the levels of noise added to a sample, leading to improved OOD denoising performance. The approach is based on our observation that diffusion models implicitly define a log-likelihood ratio that distinguishes distributions with different amounts of noise, and this expression depends on denoiser performance outside the standard training distribution.We show by diverse experiments that the proposed contrastive diffusion training is effective for both sequential and parallel settings, and it improves the performance and speed of parallel samplers significantly. Code for our paper can be found at https://github.com/yunshuwu/ContrastiveDiffusionLoss</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-214" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-214', event_id='94359', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2800</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94359">Towards Understanding How Transformers Learn In-context Through a Representation Learning Lens</a></strong></h5>


                        <p class="text-muted">
                            Ruifeng Ren &middot; Yong Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Pre-trained large language models based on Transformers have demonstrated remarkable in-context learning (ICL) abilities. With just a few demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we attempt to explore the ICL process in Transformers through a lens of representation learning. Initially, leveraging kernel methods, we figure out a dual model for one softmax attention layer. The ICL inference process of the attention layer aligns with the training procedure of its  dual model, generating token representation predictions that are equivalent to the dual model's test outputs. We delve into the training process of this  dual model from a representation learning standpoint and further derive a generalization error bound related to the quantity of demonstration tokens. Subsequently, we extend our theoretical conclusions to more complicated scenarios, including one Transformer layer and multiple attention layers. Furthermore, drawing inspiration from existing representation learning methods especially contrastive learning, we propose potential modifications for the attention layer. Finally, experiments are designed to support our findings.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-215" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-215', event_id='95599', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2801</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95599">Where does In-context  Learning \\ Happen in Large Language Models?</a></strong></h5>


                        <p class="text-muted">
                            Suzanna Sia &middot; David Mueller &middot; Kevin Duh
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Self-supervised large language models have demonstrated the ability to perform various tasks via in-context learning, but little is known about where the model locates the task with respect to prompt instructions and demonstration examples.In this work, we attempt to characterize the region where large language models transition from recognizing the task to performing the task.Through a series of layer-wise context-masking experiments on \textsc{GPTNeo2.7B}, \textsc{Bloom3B}, \textsc{Llama2-7b}, \textsc{Llama2-7b-chat}, \textsc{Starcoder2-3B} and \textsc{Starcoder2-7B} on Machine Translation and Code generation, we demonstrate evidence of a "task recognition" point where the task is encoded into the input representations and attention to context is no longer necessary. We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers. Taking advantage of this redundancy results in 45\% computational savings when prompting with 5 examples, and task recognition achieved at layer 14 / 32 using an example with Machine Translation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-216" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-216', event_id='97545', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2802</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97545">Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack</a></strong></h5>


                        <p class="text-muted">
                            Xiaoyue Xu &middot; Qinyuan Ye &middot; Xiang Ren
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose Lifelong ICL, a problem setting that challenges long-context language models (LMs) to learn various language tasks sequentially through in-context learning (ICL).We further introduce Task Haystack, an evaluation suite designed for assessing and diagnosing how long-context LMs use long contexts.When given a task instruction and test inputs, long-context LMs are expected to leverage the same-task demonstrations in the Lifelong ICL prompt, avoid distraction from other tasks, and achieve a test accuracy no worse than its single-task ICL baseline.Task Haystack draws inspiration from the widely-adopted ``needle-in-a-haystack'' (NIAH) evaluation, but presents new and unique challenges. It demands that models (1) utilize context in a genuinely contextualized manner, rather than resorting to simple copying and pasting; (2) navigate through long streams of evolving topics and tasks, which closely approximates the complexities of real-world scenarios faced by long-context LMs.Additionally, Task Haystack inherits the controllability aspect of NIAH, providing model developers with tools and visualizations to identify model vulnerabilities effectively.We benchmark ten long-context LMs using Task Haystack. We find that state-of-the-art closed models such as GPT-4o still struggle in this setting, failing 12.5\% of the cases on average, while all open models we evaluate further lack behind by a large margin.In our controlled analysis, we find that long-context LMs are susceptible to distractibility and recency bias, and these two factors both contribute to the failure cases. Further, we observe declines in performance when task instructions are paraphrased at test time or when ICL demonstrations are repeated excessively, raising concerns about the robustness, instruction understanding, and true context utilization of current long-context LMs.We release our code and data to encourage further research that addresses these limitations.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-217" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-217', event_id='93189', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2803</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93189">Remix-DiT: Mixing Diffusion Transformers for Multi-Expert Denoising</a></strong></h5>


                        <p class="text-muted">
                            Gongfan Fang &middot; Xinyin Ma &middot; Xinchao Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Transformer-based diffusion models have achieved significant advancements across a variety of generative tasks. However, producing high-quality outputs typically necessitates large transformer models, which result in substantial training and inference overhead. In this work, we investigate an alternative approach involving multiple experts for denoising, and introduce RemixDiT, a novel method designed to enhance output quality at a low cost. The goal of RemixDiT is to craft N diffusion experts for different denoising timesteps, yet without the need for expensive training of N independent models. To achieve this, RemixDiT employs K basis models (where K &lt; N) and utilizes learnable mixing coefficients to adaptively craft expert models. This design offers two significant advantages: first, although the total model size is increased, the model produced by the mixing operation shares the same architecture as a plain model, making the overall model as efficient as a standard diffusion transformer. Second, the learnable mixing adaptively allocates model capacity across timesteps, thereby effectively improving generation quality. Experiments conducted on the ImageNet dataset demonstrate that RemixDiT achieves promising results compared to standard diffusion transformers and other multiple-expert methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-218" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-218', event_id='93816', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2804</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93816">Coherent 3D Scene Diffusion From a Single RGB Image</a></strong></h5>


                        <p class="text-muted">
                            Manuel Dahnert &middot; Angela Dai &middot; Norman M√ºller &middot; Matthias Niessner
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present a novel diffusion-based approach for coherent 3D scene reconstruction from a single RGB image. Our method utilizes an image-conditioned 3D scene diffusion model to simultaneously denoise the 3D poses and geometries of all objects within the scene.Motivated by the ill-posed nature of the task and to obtain consistent scene reconstruction results, we learn a generative scene prior by conditioning on all scene objects simultaneously to capture scene context and by allowing the model to learn inter-object relationships throughout the diffusion process.We further propose an efficient surface alignment loss to facilitate training even in the absence of full ground-truth annotation, which is common in publicly available datasets. This loss leverages an expressive shape representation, which enables direct point sampling from intermediate shape predictions.By framing the task of single RGB image 3D scene reconstruction as a conditional diffusion process, our approach surpasses current state-of-the-art methods, achieving a 12.04\% improvement in AP3D on SUN RGB-D and a 13.43\% increase in F-Score on Pix3D.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-219" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-219', event_id='94203', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2805</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94203">PrefPaint: Aligning Image Inpainting Diffusion Model with Human Preference</a></strong></h5>


                        <p class="text-muted">
                            Kendong Liu &middot; Zhiyu Zhu &middot; Chuanhao Li &middot; Hui LIU &middot; Huanqiang Zeng &middot; Junhui Hou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we make the first attempt to align diffusion models for image inpainting with human aesthetic standards via a reinforcement learning framework, significantly improving the quality and visual appeal of inpainted images. Specifically, instead of directly measuring the divergence with paired images, we train a reward model with the dataset we construct, consisting of nearly 51,000 images annotated with human preferences. Then, we adopt a reinforcement learning process to fine-tune the distribution of a pre-trained diffusion model for image inpainting in the direction of higher reward. Moreover, we theoretically deduce the upper bound on the error of the reward model, which illustrates the potential confidence of reward estimation throughout the reinforcement alignment process, thereby facilitating accurate regularization.Extensive experiments on inpainting comparison and downstream tasks, such as image extension and 3D reconstruction, demonstrate the effectiveness of our approach, showing significant improvements in the alignment of inpainted images with human preference compared with state-of-the-art methods. This research not only advances the field of image inpainting but also provides a framework for incorporating human preference into the iterative refinement of generative models based on modeling reward accuracy, with broad implications for the design of visually driven AI applications. Our code and dataset are publicly available at \url{https://prefpaint.github.io}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-220" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-220', event_id='94294', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2806</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94294">Attack-Resilient Image Watermarking Using Stable Diffusion</a></strong></h5>


                        <p class="text-muted">
                            Lijun Zhang &middot; Xiao Liu &middot; Antoni Martin &middot; Cindy Bearfield &middot; Yuriy Brun &middot; Hui Guan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Watermarking images is critical for tracking image provenance and proving ownership. With the advent of generative models, such as stable diffusion, that can create fake but realistic images, watermarking has become particularly important to make human-created images reliably identifiable. Unfortunately, the very same stable diffusion technology can remove watermarks injected using existing methods.To address this problem, we present ZoDiac, which uses a pre-trained stable diffusion model to inject a watermark into the trainable latent space, resulting in watermarks that can be reliably detected in the latent vector even when attacked. We evaluate ZoDiac on three benchmarks, MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against state-of-the-art watermark attacks, with a watermark detection rate above 98% and a false positive rate below 6.4%, outperforming state-of-the-art watermarking methods. We hypothesize that the reciprocating denoising process in diffusion models may inherently enhance the robustness of the watermark when faced with strong attacks and validate the hypothesis. Our research demonstrates that stable diffusion is a promising approach to robust watermarking, able to withstand even stable-diffusion-based attack methods. ZoDiac is open-sourced and available at https://github.com/zhanglijun95/ZoDiac.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-221" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-221', event_id='95031', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2807</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95031">Subject-driven Text-to-Image Generation via Preference-based Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Yanting Miao &middot; William Loh &middot; Suraj Kothawade &middot; Pascal Poupart &middot; Abdullah Rashwan &middot; Yeqing Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Text-to-image generative models have recently attracted considerable interest, enabling the synthesis of high-quality images from textual prompts. However, these models often lack the capability to generate specific subjects from given reference images or to synthesize novel renditions under varying conditions. Methods like DreamBooth and Subject-driven Text-to-Image (SuTI) have made significant progress in this area. Yet, both approaches primarily focus on enhancing similarity to reference images and require expensive setups, often overlooking the need for efficient training and avoiding overfitting to the reference images. In this work, we present the $\lambda$-Harmonic reward function, which provides a reliable reward signal and enables early stopping for faster training and effective regularization. By combining the Bradley-Terry preference model, the $\lambda$-Harmonic reward function also provides preference labels for subject-driven generation tasks. We propose Reward Preference Optimization (RPO), which offers a simpler setup (requiring only 3\% of the negative samples used by DreamBooth) and fewer gradient steps for fine-tuning. Unlike most existing methods, our approach does not require training a text encoder or optimizing text embeddings and achieves text-image alignment by fine-tuning only the U-Net component. Empirically, $\lambda$-Harmonic proves to be a reliable approach for model selection in subject-driven generation tasks. Based on preference labels and early stopping validation from the $\lambda$-Harmonic reward function, our algorithm achieves a state-of-the-art CLIP-I score of 0.833 and a CLIP-T score of 0.314 on DreamBench.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-222" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-222', event_id='95082', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2808</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95082">Understanding Generalizability of Diffusion Models Requires Rethinking the Hidden Gaussian Structure</a></strong></h5>


                        <p class="text-muted">
                            Xiang Li &middot; Yixiang Dai &middot; Qing Qu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this work, we study the generalizability of diffusion models by looking into the hidden properties of the learned score functions, which are essentially a series of deep denoisers trained on various noise levels. We observe that as diffusion models transition from memorization to generalization, their corresponding nonlinear diffusion denoisers exhibit increasing linearity. This discovery leads us to investigate the linear counterparts of the nonlinear diffusion models, which are a series of linear models trained to match the function mappings of the nonlinear diffusion denoisers. Surprisingly, these linear denoisers are approximately the optimal denoisers for a multivariate Gaussian distribution characterized by the empirical mean and covariance of the training dataset. This finding implies that diffusion models have the inductive bias towards capturing and utilizing the Gaussian structure (covariance information) of the training dataset for data generation. We empirically demonstrate that this inductive bias is a unique property of diffusion models in the generalization regime, which becomes increasingly evident when the model's capacity is relatively small compared to the training dataset size. In the case that the model is highly overparameterized, this inductive bias emerges during the initial training phases before the model fully memorizes its training data. Our study provides crucial insights into understanding the notable strong generalization phenomenon recently observed in real-world diffusion models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-223" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-223', event_id='95088', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2809</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95088">Time-Varying LoRA: Towards Effective Cross-Domain Fine-Tuning of Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Zhan Zhuang &middot; Yulong Zhang &middot; Xuehao Wang &middot; Jiangang Lu &middot; Ying Wei &middot; Yu Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large-scale diffusion models are adept at generating high-fidelity images and facilitating image editing and interpolation. However, they have limitations when tasked with generating images in dynamic, evolving domains. In this paper, we introduce Terra, a novel Time-varying low-rank adapter that offers a fine-tuning framework specifically tailored for domain flow generation. The key innovation of Terra lies in its construction of a continuous parameter manifold through a time variable, with its expressive power analyzed theoretically. This framework not only enables interpolation of image content and style but also offers a generation-based approach to address the domain shift problems in unsupervised domain adaptation and domain generalization. Specifically, Terra transforms images from the source domain to the target domain and generates interpolated domains with various styles to bridge the gap between domains and enhance the model generalization, respectively. We conduct extensive experiments on various benchmark datasets, empirically demonstrate the effectiveness of Terra. Our source code is publicly available on https://github.com/zwebzone/terra.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-224" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-224', event_id='95115', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2810</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95115">4Diffusion: Multi-view Video Diffusion Model for 4D Generation</a></strong></h5>


                        <p class="text-muted">
                            Haiyu Zhang &middot; Xinyuan Chen &middot; Yaohui WANG &middot; Xihui Liu &middot; Yunhong Wang &middot; Yu Qiao
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Current 4D generation methods have achieved noteworthy efficacy with the aid of advanced diffusion generative models. However, these methods lack multi-view spatial-temporal modeling and encounter challenges in integrating diverse prior knowledge from multiple diffusion models, resulting in inconsistent temporal appearance and flickers. In this paper, we propose a novel 4D generation pipeline, namely $\textbf{4Diffusion}$, aimed at generating spatial-temporally consistent 4D content from a monocular video. We first design a unified diffusion model tailored for multi-view video generation by incorporating a learnable motion module into a frozen 3D-aware diffusion model to capture multi-view spatial-temporal correlations. After training on a curated dataset, our diffusion model acquires reasonable temporal consistency and inherently preserves the generalizability and spatial consistency of the 3D-aware diffusion model. Subsequently, we propose 4D-aware Score Distillation Sampling loss, which is based on our multi-view video diffusion model, to optimize 4D representation parameterized by dynamic NeRF. This aims to eliminate discrepancies arising from multiple diffusion models, allowing for generating spatial-temporally consistent 4D content. Moreover, we devise an anchor loss to enhance the appearance details and facilitate the learning of dynamic NeRF. Extensive qualitative and quantitative experiments demonstrate that our method achieves superior performance compared to previous methods.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-225" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-225', event_id='95144', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2811</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95144">ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization</a></strong></h5>


                        <p class="text-muted">
                            Huayang Huang &middot; Yu Wu &middot; Qian Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Watermarking generative content serves as a vital tool for authentication, ownership protection, and mitigation of potential misuse. Existing watermarking methods face the challenge of balancing robustness and concealment. They empirically inject a watermark that is both invisible and robust and passively achieve concealment by limiting the strength of the watermark, thus reducing the robustness. In this paper, we propose to explicitly introduce a watermark hiding process to actively achieve concealment, thus allowing the embedding of stronger watermarks. To be specific, we implant a robust watermark in an intermediate diffusion state and then guide the model to hide the watermark in the final generated image. We employ an adversarial optimization algorithm to produce the optimal hiding prompt guiding signal for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength. The watermark can be verified by reversing the generation process. Experiments on various diffusion models demonstrate the watermark remains verifiable even under significant image tampering and shows superior invisibility compared to other state-of-the-art robust watermarking methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-226" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-226', event_id='93759', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2900</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93759">AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents</a></strong></h5>


                        <p class="text-muted">
                            Yao Fu &middot; Dong-Ki Kim &middot; Jaekyeom Kim &middot; Sungryull Sohn &middot; Lajanugen Logeswaran &middot; Kyunghoon Bae &middot; Honglak Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advances in large language models (LLMs) have empowered AI agents capable of performing various sequential decision-making tasks. However, effectively guiding LLMs to perform well in unfamiliar domains like web navigation, where they lack sufficient knowledge, has proven to be difficult with the demonstration-based in-context learning paradigm. In this paper, we introduce a novel framework, called AutoGuide, which addresses this limitation by automatically generating context-aware guidelines from offline experiences. Importantly, each context-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the context where it is applicable. As a result, our guidelines facilitate the provision of relevant knowledge for the agent's current decision-making process, overcoming the limitations of the conventional demonstration-based learning paradigm. Our evaluation demonstrates that AutoGuide significantly outperforms competitive baselines in complex benchmark domains, including real-world web navigation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-227" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-227', event_id='93099', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2901</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93099">From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When</a></strong></h5>


                        <p class="text-muted">
                            Kevin Christian Wibisono &middot; Yixin Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) like transformers demonstrate impressive in-context learning (ICL) capabilities, allowing them to makepredictions for new tasks based on prompt exemplars without parameter updates. While existing ICL theories often assume structured training data resembling ICL tasks (e.g., x-y pairs for linear regression), LLMs are typically trained unsupervised on unstructured text, such as web content, which lacks clear parallels to tasks like word analogy. To address this gap, we examine what enables ICL in models trained on unstructured data, focusing on critical sequence model requirements and training data structure. We find that many ICL capabilities canemerge simply from co-occurrence of semantically related word pairs in unstructured data; word analogy completion, for example, can provably arise purely through co-occurrence modeling, using classical language models like continuous bag of words (CBOW), without needing positional information or attention mechanisms. However, positional information becomes crucial for logic reasoning tasks requiring generalization to unseen tokens. Finally, we identify two cases where ICL fails: one in logic reasoning tasks that require generalizing to new, unseen patterns, and another in analogy completion where relevant word pairs appear only in fixed training positions. These findings suggest that LLMs' ICL abilities depend heavily on the structural elements within their training data.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-228" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-228', event_id='95234', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2902</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95234">Accuracy is Not All You Need</a></strong></h5>


                        <p class="text-muted">
                            Abhinav Dutta &middot; Sanjeev Krishnan &middot; Nipun Kwatra &middot; Ramachandran Ramjee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>When Large Language Models (LLMs) are compressed using techniques such as quantization, the predominant way to demonstrate the validity of such techniques is by measuring the model's accuracy on various benchmarks. If the accuracies of the baseline model and the compressed model are close, it is assumed that there was negligible degradation in quality. However, even when the accuracy of baseline and compressed model are similar, we observe the phenomenon of flips, wherein answers change from correct to incorrect and vice versa in proportion. We conduct a detailed study of metrics across multiple compression techniques, models and datasets, demonstrating that the behavior of compressed models as visible to end-users is often significantly different from the baseline model, even when accuracy is similar. We further evaluate compressed models qualitatively and quantitatively using MT-Bench and show that compressed models exhibiting high flips are worse than baseline models in this free-form generative task. Thus, we argue that accuracy and perplexity are necessary but not sufficient for evaluating compressed models, since these metrics hide large underlying changes that have not been observed by previous work. Hence, compression techniques should also be evaluated using distance metrics. We propose two such distance metrics, KL-Divergence and flips, and show that they are well correlated.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-229" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-229', event_id='94574', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2903</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94574">Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning</a></strong></h5>


                        <p class="text-muted">
                            Xuechen Zhang &middot; Zijian Huang &middot; Ege Onur Taga &middot; Carlee Joe-Wong &middot; Samet Oymak &middot; Jiasi Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (i.e., the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long term budget requirements. To navigate this rich design space, we propose TREACLE (Thrifty Reasoning via Context-Aware LLM and Prompt Selection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user's monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE enables cost savings of up to 85% compared to baselines, while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-230" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-230', event_id='93370', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2904</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93370">Verified Code Transpilation with LLMs</a></strong></h5>


                        <p class="text-muted">
                            Sahil Bhatia &middot; Jie Qiu &middot; Niranjan Hasabnis &middot; Sanjit Seshia &middot; Alvin Cheung
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Domain-specific languages (DSLs) have become integral to various software workflows. Such languages offer domain-specific optimizations and abstractions that improve code readability and maintainability.  However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API. While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the rewritten code. Another approach for automating this task is verified lifting, which relies on program synthesis to find programs in the target language that are functionally equivalent to the source language program. While several verified lifting tools have been developed for various application domains, they are specialized for specific source-target languages or require significant expertise in domain knowledge to make the search efficient. In this paper, leveraging recent advances in LLMs, we propose an LLM-based approach (LLMLift) to building verified lifting tools. We use the LLM's capabilities to reason about programs to translate a given program into its corresponding equivalent in the target language. Additionally, we use LLMs to generate proofs for functional equivalence. We develop lifting-based compilers for four DSLs targeting different application domains. Our approach not only outperforms previous symbolic-based tools in number of benchmarks transpiled and transpilation time, but also requires significantly less effort to build.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-231" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-231', event_id='95773', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2905</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95773">Piecewise deterministic generative models</a></strong></h5>


                        <p class="text-muted">
                            Andrea Bertazzi &middot; Dario Shariatian &middot; Umut Simsekli &middot; Eric Moulines &middot; Alain Durmus
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We introduce a novel class of generative models based on piecewise deterministic Markov processes (PDMPs), a family of non-diffusive stochastic processes consisting of deterministic motion and random jumps at random times. Similarly to diffusions, such Markov processes admit time reversals that turn out to be PDMPs as well. We apply this observation to three PDMPs considered in the literature: the Zig-Zag process, Bouncy Particle Sampler, and Randomised Hamiltonian Monte Carlo. For these three particular instances, we show that the jump rates and kernels of the corresponding time reversals admit explicit expressions depending on some conditional densities of the PDMP under consideration before and after a jump. Based on these results, we propose efficient training procedures to learn these characteristics and consider methods to approximately simulate the reverse process. Finally, we provide bounds in the total variation distance between the data distribution and the resulting distribution of our model in the case where the base distribution is the standard $d$-dimensional Gaussian distribution. Promising numerical simulations support further investigations into this class of models.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-232" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-232', event_id='95096', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2906</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95096">On the Identifiability of Hybrid Deep Generative Models: Meta-Learning as a Solution</a></strong></h5>


                        <p class="text-muted">
                            Yubo Ye &middot; Maryam Tolou &middot; Sumeet Vadhavkar &middot; Xiajun Jiang &middot; Huafeng Liu &middot; Linwei Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The interest in leveraging physics-based inductive bias in deep learning has resulted in recent development of <em>hybrid deep generative models (hybrid-DGMs)</em>  that integrates known physics-based mathematical expressions in neural generative models. To identify these hybrid-DGMs requires inferring parameters of the physics-based component along with their neural component. The identifiability of these hybrid-DGMs, however, has not yet been theoretically probed or established. How does the existing theory of the un-identifiability of general DGMs apply to hybrid-DGMs? What may be an effective approach to consutrct a hybrid-DGM with theoretically-proven identifiability? This paper provides the first theoretical probe into the identifiability of hybrid-DGMs, and present meta-learning as a novel solution to construct identifiable hybrid-DGMs. On synthetic and real-data benchmarks, we provide strong empirical evidence for the un-identifiability of existing hybrid-DGMs using unconditional priors, and strong identifiability results of the presented meta-formulations of hybrid-DGMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-233" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-233', event_id='97472', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2907</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97472">Can LLMs Solve Molecule Puzzles? A Multimodal Benchmark for Molecular Structure Elucidation</a></strong></h5>


                        <p class="text-muted">
                            Kehan Guo &middot; Bozhao Nan &middot; Yujun Zhou &middot; Taicheng Guo &middot; Zhichun Guo &middot; Mihir Surve &middot; Zhenwen Liang &middot; Nitesh Chawla &middot; Olaf Wiest &middot; Xiangliang Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large Language Models (LLMs)  have shown significant problem-solving capabilities across predictive and generative tasks in chemistry. However, their proficiency in multi-step chemical reasoning remains underexplored. We introduce a new challenge: molecular structure elucidation, which involves deducing a molecule‚Äôs structure from various types of spectral data. Solving such a molecular puzzle, akin to solving crossword puzzles, poses reasoning challenges that require integrating clues from diverse sources and engaging in iterative hypothesis testing. To address this challenging problem with LLMs, we present \textbf{MolPuzzle}, a benchmark comprising 234 instances of structure elucidation, which feature over 18,000 QA samples presented in a sequential puzzle-solving process, involving three interlinked sub-tasks: molecule understanding, spectrum interpretation, and molecule construction. Our evaluation of more than 10 LLMs reveals that the best-performing LLM, GPT-4o, performs significantly worse than humans, with only a small portion (1.4\%) of its answers exactly matching the ground truth. However, it performs nearly perfectly in the first subtask of molecule understanding, achieving accuracy close to 100\%. This discrepancy highlights the potential of developing advanced LLMs with improved chemical reasoning capabilities in the other two sub-tasks. Our MolPuzzle dataset and evaluation code are available at this  \href{https://github.com/KehanGuo2/MolPuzzle}{link}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-234" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-234', event_id='97450', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2908</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97450">Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning</a></strong></h5>


                        <p class="text-muted">
                            Jifan Zhang &middot; Lalit Jain &middot; Yang Guo &middot; Jiayi Chen &middot; Kuan Zhou &middot; Siddharth Suresh &middot; Andrew Wagenmaker &middot; Scott Sievert &middot; Timothy T Rogers &middot; Kevin Jamieson &middot; Bob Mankoff &middot; Robert Nowak
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present a novel multimodal preference dataset for creative tasks, consisting of over 250 million human votes on more than 2.2 million captions, collected through crowdsourcing rating data for The New Yorker's weekly cartoon caption contest over the past eight years. This unique dataset supports the development and evaluation of multimodal large language models and preference-based fine-tuning algorithms for humorous caption generation. We propose novel benchmarks for judging the quality of model-generated captions, utilizing both GPT4 and human judgments to establish ranking-based evaluation strategies. Our experimental results highlight the limitations of current fine-tuning methods, such as RLHF and DPO, when applied to creative tasks. Furthermore, we demonstrate that even state-of-the-art models like GPT4 and Claude currently underperform top human contestants in generating humorous captions. As we conclude this extensive data collection effort, we release the entire preference dataset to the research community, fostering further advancements in AI humor generation and evaluation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-235" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-235', event_id='96799', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2909</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96799">Multi-language Diversity Benefits Autoformalization</a></strong></h5>


                        <p class="text-muted">
                            Albert Q. Jiang &middot; Wenda Li &middot; Mateja Jamnik
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Autoformalization is the task of translating natural language materials into machine-verifiable formalisations. Progress in autoformalization research is hindered by the lack of a sizeable dataset consisting of informal-formal pairs expressing the same essence. Existing methods tend to circumvent this challenge by manually curating small corpora or using few-shot learning with large language models. But these methods suffer from data scarcity and formal language acquisition difficulty. In this work, we create mma, a large, flexible, multi-language, and multi-domain dataset of informal-formal pairs, by using a language model to translate in the reverse direction, that is, from formal mathematical statements into corresponding informal ones. Experiments show that language models fine-tuned on mma can produce up to $29-31$\% of statements acceptable with minimal corrections on the miniF2F and ProofNet benchmarks, up from $0$\% with the base model. We demonstrate that fine-tuning on multi-language formal data results in more capable autoformalization models even on single-language tasks.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-236" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-236', event_id='95287', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2910</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95287">SemCoder: Training Code Language Models with Comprehensive Semantics Reasoning</a></strong></h5>


                        <p class="text-muted">
                            Yangruibo Ding &middot; Jinjun Peng &middot; Marcus Min &middot; Gail Kaiser &middot; Junfeng Yang &middot; Baishakhi Ray
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Code Large Language Models (Code LLMs) have excelled at tasks like code completion but often miss deeper semantics such as execution effects and dynamic states. This paper aims to bridge the gap between Code LLMs' reliance on static text data and the need for semantic understanding for complex tasks like debugging and program repair. We introduce a novel strategy, <em>monologue reasoning</em>, to train Code LLMs to reason comprehensive semantics, encompassing high-level functional descriptions, local execution effects of individual statements, and overall input/output behavior, thereby linking static code text with dynamic execution states.We begin by collecting PyX, a clean Python corpus of fully executable code samples with functional descriptions and test cases. We propose training Code LLMs not only to write code but also to understand code semantics by reasoning about key properties, constraints, and execution behaviors using natural language, mimicking human verbal debugging, i.e., rubber-duck debugging. This approach led to the development of SemCoder, a Code LLM with only 6.7B parameters, which shows competitive performance with GPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder achieves 79.3% on HumanEval (GPT-3.5-turbo: 76.8%), 63.6% on CRUXEval-I (GPT-3.5-turbo: 50.3%), and 63.9% on CRUXEval-O (GPT-3.5-turbo: 59.0%). We also study the effectiveness of SemCoder's monologue-style execution reasoning compared to concrete scratchpad reasoning, showing that our approach integrates semantics from multiple dimensions more smoothly. Finally, we demonstrate the potential of applying learned semantics to improve Code LLMs' debugging and self-refining capabilities. Our data, code, and models are available at: https://github.com/ARiSE-Lab/SemCoder.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-237" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-237', event_id='94647', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2911</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94647">Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees</a></strong></h5>


                        <p class="text-muted">
                            SIJIA CHEN &middot; Yibo Wang &middot; Yi-Feng Wu &middot; Qingguo Chen &middot; Zhao Xu &middot; Weihua Luo &middot; Kaifu Zhang &middot; Lijun Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Tool-augmented large language models (LLMs) leverage tools, often in the form of APIs, to improve their reasoning capabilities on complex tasks. This enables them to act as intelligent agents interacting with the real world. The recently introduced ToolLLaMA model by Qin et al. [2023] utilizes the depth-first search-based decision tree (DFSDT) mechanism for multi-step reasoning with $16000+$ real-world APIs, effectively enhancing the performance of tool-augmented LLMs compared to traditional chain reasoning mechanisms. However, their approach only employs successful paths from decision trees (also called inference trees) for supervised fine-tuning (SFT), missing out on the potential learning opportunities from failed paths. Inspired by this, we propose an inference trajectory optimization framework based on preference learning to address this limitation. We first introduce a novel method for constructing preference data from tree-like expert trajectories, which leverages the previously ignored failed explorations in the decision trees. Specifically, we generate a step-wise preference dataset, ToolPreference, from the ToolBench dataset for tool learning. In the subsequent training phase, we first fine-tune the LLM with successful tool-usage expert trajectories and then apply direct preference optimization (DPO) with ToolPreference to update the LLM's policy, resulting in our ToolPrefer-LLaMA (TP-LLaMA) model. This approach not only enhances the utilization of original expert data but also broadens the learning space of the model. Our experiments demonstrate that by obtaining insights from errors in inference trees, TP-LLaMA significantly outperforms the baselines across almost all test scenarios by a large margin and exhibits better generalization capabilities with unseen APIs. At the same time, TP-LLaMA has also demonstrated superior reasoning efficiency compared to the baselines, making it more suitable for complex tool-usage reasoning tasks.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-238" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-238', event_id='95336', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3000</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95336">What do Graph Neural Networks learn? Insights from Tropical Geometry</a></strong></h5>


                        <p class="text-muted">
                            Tuan Anh Pham &middot; Vikas Garg
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graph neural networks (GNNs) have been analyzed from multiple perspectives, including the WL-hierarchy, which exposes limits on their expressivity to distinguish graphs. However, characterizing the class of functions that they learn has remained unresolved. We address this fundamental question for message passing GNNs under ReLU activations, i.e., the de-facto choice for most GNNs.We first show that such GNNs learn tropical rational signomial maps or continuous piecewise linear functions, establishing an equivalence with feedforward networks (FNNs). We then elucidate the role of the choice of aggregation and update functions, and derive the first general upper and lower bounds on the geometric complexity (i.e., the number of linear regions), establishing new results for popular architectures such as GraphSAGE and GIN. We also introduce and theoretically analyze several new architectures to illuminate the relative merits of the feedforward and the message passing layers, and the tradeoffs involving depth and number of trainable parameters.  Finally, we also characterize the decision boundary for node and graph classification tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-239" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-239', event_id='95396', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3001</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95396">Graphcode: Learning from multiparameter persistent homology using graph neural networks</a></strong></h5>


                        <p class="text-muted">
                            Florian Russold &middot; Michael Kerber
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce graphcodes, a novel multi-scale summary of the topological properties of a dataset that is based on the well-established theory of persistent homology. Graphcodes handle datasets that are filtered along two real-valued scale parameters. Such multi-parameter topological summaries are usually based on complicated theoretical foundations and difficult to compute; in contrast, graphcodes yield an informative and interpretable summary and can be computed as efficient as one-parameter summaries. Moreover, a graphcode is simply an embedded graph and can therefore be readily integrated in machine learning pipelines using graph neural networks. We describe such a pipeline and demonstrate that graphcodes achieve better classification accuracy than state-of-the-art approaches on various datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-240" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-240', event_id='95659', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3002</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95659">Deep Homomorphism Networks</a></strong></h5>


                        <p class="text-muted">
                            Takanori Maehara &middot; Hoang NT
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Many real-world graphs are large and have some characteristic subgraph patterns, such as triangles in social networks, cliques in web graphs, and cycles in molecular networks.Detecting such subgraph patterns is important in many applications; therefore, establishing graph neural networks (GNNs) that can detect such patterns and run fast on large graphs is demanding.In this study, we propose a new GNN layer, named \emph{graph homomorphism layer}.It enumerates local subgraph patterns that match the predefined set of patterns $\mathcal{P}^\bullet$, applies non-linear transformations to node features, and aggregates them along with the patterns. By stacking these layers, we obtain a deep GNN model called \emph{deep homomorphism network (DHN)}.The expressive power of the DHN is completely characterised by the set of patterns generated from $\mathcal{P}^\bullet$ by graph-theoretic operations;hence, it serves as a useful theoretical tool to analyse the expressive power of many GNN models.Furthermore, the model runs in the same time complexity as the graph homomorphisms, which is fast in many real-word graphs.Thus, it serves as a practical and lightweight model that solves difficult problems using domain knowledge.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-241" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-241', event_id='96038', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3003</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96038">Spectral Graph Pruning Against Over-Squashing and Over-Smoothing</a></strong></h5>


                        <p class="text-muted">
                            Adarsh Jamadandi &middot; Celia Rubio-Madrigal &middot; Rebekka Burkholz
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Message Passing Graph Neural Networks are known to suffer from two problems that are sometimes believed to be diametrically opposed: over-squashing and over-smoothing. The former results from topological bottlenecks that hamper the information flow from distant nodes and are mitigated by spectral gap maximization, primarily, by means of edge additions. However, such additions often promote over-smoothing that renders nodes of different classes less distinguishable. Inspired by the Braess phenomenon, we argue that deleting edges can address over-squashing and over-smoothing simultaneously. This insight explains how edge deletions can improve generalization, thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets. To this end, we propose a computationally effective spectral gap optimization framework to add or delete edges and demonstrate its effectiveness on the long range graph benchmark and on larger heterophilous datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-242" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-242', event_id='96065', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3004</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96065">RAGraph: A General Retrieval-Augmented Graph Learning Framework</a></strong></h5>


                        <p class="text-muted">
                            Xinke Jiang &middot; Rihong Qiu &middot; Yongxin Xu &middot; WentaoZhang &middot; Yichen Zhu &middot; Ruizhe Zhang &middot; Yuchen Fang &middot; Chu Xu &middot; Junfeng Zhao &middot; Yasha Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graph Neural Networks (GNNs) have become essential in interpreting relational data across various domains, yet, they often struggle to generalize to unseen graph data that differs markedly from training instances. In this paper, we introduce a novel framework called General Retrieval-Augmented Graph Learning (RAGraph), which brings external graph data into the general graph foundation model to improve model generalization on unseen scenarios. On the top of our framework is a toy graph vector library that we established, which captures key attributes, such as features and task-specific label information. During inference, the RAGraph adeptly retrieves similar toy graphs based on key similarities in downstream tasks, integrating the retrieved data to enrich the learning context via the message-passing prompting mechanism. Our extensive experimental evaluations demonstrate that RAGraph significantly outperforms state-of-the-art graph learning methods in multiple tasks such as node classification, link prediction, and graph classification across both dynamic and static datasets. Furthermore, extensive testing confirms that RAGraph consistently maintains high performance without the need for task-specific fine-tuning, highlighting its adaptability, robustness, and broad applicability.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-243" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-243', event_id='96733', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3005</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96733">EGonc : Energy-based Open-Set Node Classification with substitute Unknowns</a></strong></h5>


                        <p class="text-muted">
                            Qin Zhang &middot; Zelin Shi &middot; Shirui Pan &middot; Junyang Chen &middot; Huisi Wu &middot; Xiaojun Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Open-set Classification (OSC) is a critical requirement for safely deploying machine learning models in the open world, which aims to classify samples from known classes and reject samples from out-of-distribution (OOD). Existing methods exploit the feature space of trained network and attempt at estimating the uncertainty in the predictions.However, softmax-based neural networks are found to be overly confident in their predictions even on data they have never seen before andthe immense diversity of the OOD examples also makes such methods fragile.To this end, we follow the idea of estimating the underlying density of the training data to decide whether a given input is close to the in-distribution (IND) data and adopt Energy-based models (EBMs) as density estimators. A novel energy-based generative open-set node classification method, \textit{EGonc}, is proposed to achieve open-set graph learning. Specifically, we generate substitute unknowns to mimic the distribution of real open-set samples firstly, based on the information of graph structures. Then, an additional energy logit representing the virtual OOD class is learned from the residual of the feature against the principal space, and matched with the original logits by a constant scaling. This virtual logit serves as the indicator of OOD-ness. EGonc has nice theoretical properties that guarantee an overall distinguishable margin between the detection scores for IND and OOD samples. Comprehensive experimental evaluations of EGonc also demonstrate its superiority.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-244" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-244', event_id='96923', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3006</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96923">Graph Neural Networks and Arithmetic Circuits</a></strong></h5>


                        <p class="text-muted">
                            Timon Barlag &middot; Vivian Holzapfel &middot; Laura Strieker &middot; Jonni Virtema &middot; Heribert Vollmer
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types. We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-245" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-245', event_id='99329', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3007</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/99329">[Re] GNNInterpreter: A probabilistic generative model-level explanation for Graph Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Batu Helvacioglu &middot; Ana Vasilcoiu &middot; Thijs Stessen &middot; Thies Kersten
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graph Neural Networks have recently gained recognition for their performance on graph
machine learning tasks. The increasing attention on these models‚Äô trustworthiness and
decision-making mechanisms has instilled interest in the exploration of explainability tech-
niques, including the model proposed in "GNNInterpreter: A probabilistic generative model-
level explanation for Graph Neural Networks." (Wang &amp; Shen (2022)). This work aims to
reproduce the findings of the original paper, by investigation the main claims made by its
authors, namely that GNNInterpreter (i) generates faithful and realistic explanations with-
out requiring domain-specific knowledge, (ii) has the ability to work with various node and
edge features, (iii) produces explanations that are representative for the target class and
(iv) has a much lower training time compared to XGNN, the current state-of-the-art model-
level GNN explanation technique. To reproduce the results, we make use of the open-source
implementation and we test the interpreter on the same datasets and GNN models as in
the original paper. We conduct an enhanced quantitative and qualitative evaluation, and
additionally we extend the original experiments to include another real-world dataset. Our
results show that we are not able to validate the first claim, due to significant hyperpa-
rameter and seed variation, as well as due to training instability. Furthermore, we partially
validate the second claim by testing on datasets with different node and edge features, but
we reject the third claim due to GNNInterpreter‚Äôs failure to outperform XGNN in producing
dataset aligned explanations. Lastly, we are able to confirm the last claim.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-246" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-246', event_id='99341', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3008</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/99341">[Re] Reproducibility Study of ‚ÄúExplaining Temporal Graph Models Through an Explorer-Navigator Framework&quot;</a></strong></h5>


                        <p class="text-muted">
                            Helia Ghasemi &middot; Christina Isaicu &middot; Jesse Wonnink &middot; Andreas Berentzen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper seeks to reproduce and extend the results of the paper ‚ÄúExplaining Temporal Graph Models Through an Explorer-Navigator Framework‚Äù by (Xia et al., 2023). The main contribution of the original authors is a novel explainer for temporal graph networks, the Temporal GNN Explainer (T-GNNExplainer), which finds a subset of preceding events that ‚Äúexplain‚Äù a prediction made by a temporal graph model. The explorer is tested on two temporal graph models that are trained on two real-world and two synthetic datasets. The explorer is evaluated using a newly proposed metric for explanatory graph models. The authors compare the performance of their explorer to three baseline explainer methods, either adapted from a GNN explainer or developed by the authors. The authors claim that T-GNNExplainer achieves superior performance compared to the baselines when evaluated with their proposed metric. This work reproduces the original experiments by using the code (with minor adjustments), model specifications, and hyperparameters provided by the original authors. To evaluate the robustness of these claims, the method was extended to one new dataset (MOOC). Results show that the T-GNNexplainer performs best on some, but not all metrics as reported in the original findings. We conclude that the main lines of this paper hold up even though all results are less pronounced than claimed. Results show that the T-GNNExplainer does not perform similarly across different T-GNN models, precise dataset specifications are needed to obtain high performance, and there are simpler, less computationally costly explainer methods (like PBONE) that could offer competitive results.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-247" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-247', event_id='93415', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3009</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93415">DAPE: Data-Adaptive Positional Encoding for Length Extrapolation</a></strong></h5>


                        <p class="text-muted">
                            Chuanyang Zheng &middot; Yihang Gao &middot; Han Shi &middot; Minbin Huang &middot; Jingyao Li &middot; Jing Xiong &middot; Xiaozhe Ren &middot; Michael Ng &middot; Xin Jiang &middot; Zhenguo Li &middot; Yu Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Positional encoding plays a crucial role in transformers, significantly impact- ing model performance and length generalization. Prior research has introduced absolute positional encoding (APE) and relative positional encoding (RPE) to distinguish token positions in given sequences. However, both APE and RPE remain fixed after model training regardless of input data, limiting their adaptability and flexibility. Hence, we expect that the desired positional encoding should be data-adaptive and can be dynamically adjusted with the given attention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors. Experimental validation on real-world datasets (Arxiv, Books3, and CHE) demonstrates that DAPE enhances model performances in terms of trained length and length generalization, where the improvements are statistically significant. The model visualization suggests that our model can keep both local and anti-local information. Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-248" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-248', event_id='93898', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3010</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93898">Chain of Thoughtlessness? An Analysis of CoT in Planning</a></strong></h5>


                        <p class="text-muted">
                            Kaya Stechly &middot; Karthik Valmeekam &middot; Subbarao Kambhampati
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting--a method of demonstrating solution procedures--with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem.This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples.We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes.Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-249" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-249', event_id='94275', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3011</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94275">AutoPSV: Automated Process-Supervised Verifier</a></strong></h5>


                        <p class="text-muted">
                            Jianqiao Lu &middot; Zhiyang Dou &middot; Hongru WANG &middot; Zeyu Cao &middot; Jianbo Dai &middot; Yunlong Feng &middot; Zhijiang Guo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this work, we propose a novel method named \textbf{Auto}mated \textbf{P}rocess-\textbf{S}upervised \textbf{V}erifier (\textbf{\textsc{AutoPSV}}) to enhance the reasoning capabilities of large language models (LLMs) by automatically annotating the reasoning steps.\textsc{AutoPSV} begins by training a verification model on the correctness of final answers, enabling it to generate automatic process annotations. This verification model assigns a confidence score to each reasoning step, indicating the probability of arriving at the correct final answer from that point onward.We detect relative changes in the verification's confidence scores across reasoning steps to automatically annotate the reasoning process, enabling error detection even in scenarios where ground truth answers are unavailable. This alleviates the need for numerous manual annotations or the high computational costs associated with model-induced annotation approaches.We experimentally validate that the step-level confidence changes learned by the verification model trained on the final answer correctness can effectively identify errors in the reasoning steps.We demonstrate that the verification model, when trained on process annotations generated by \textsc{AutoPSV}, exhibits improved performance in selecting correct answers from multiple LLM-generated outputs.Notably, we achieve substantial improvements across five datasets in mathematics and commonsense reasoning.  The source code of \textsc{AutoPSV} is available at \url{https://github.com/rookie-joe/AutoPSV}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-250" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-250', event_id='95066', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3100</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95066">Are Graph Neural Networks Optimal Approximation Algorithms?</a></strong></h5>


                        <p class="text-muted">
                            Morris Yau &middot; Nikolaos Karalias &middot; Eric Lu &middot; Jessica Xu &middot; Stefanie Jegelka
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this work we design graph neural network architectures that capture optimalapproximation algorithms for a large class of combinatorial optimization problems,using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message-passing algorithms can representthe most powerful polynomial time algorithms for Max Constraint SatisfactionProblems assuming the Unique Games Conjecture. We leverage this result toconstruct efficient graph neural network architectures, OptGNN, that obtain high quality approximate solutions on landmark combinatorial optimization problemssuch as Max-Cut, Min-Vertex-Cover, and Max-3-SAT. Our approach achievesstrong empirical results across a wide range of real-world and synthetic datasetsagainst solvers and neural baselines. Finally, we take advantage of OptGNN‚Äôsability to capture convex relaxations to design an algorithm for producing boundson the optimal solution from the learned embeddings of OptGNN.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-251" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-251', event_id='94959', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3101</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94959">State Space Models on Temporal Graphs: A First-Principles Study</a></strong></h5>


                        <p class="text-muted">
                            Jintang Li &middot; Ruofan Wu &middot; Xinzhou Jin &middot; Boqun Ma &middot; Liang Chen &middot; Zibin Zheng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Over the past few years, research on deep graph learning has shifted from static graphs to temporal graphs in response to real-world complex systems that exhibit dynamic behaviors. In practice, temporal graphs are formalized as an ordered sequence of static graph snapshots observed at discrete time points. Sequence models such as RNNs or Transformers have long been the predominant backbone networks for modeling such temporal graphs. Yet, despite the promising results, RNNs struggle with long-range dependencies, while transformers are burdened by quadratic computational complexity. Recently, state space models (SSMs), which are framed as discretized representations of an underlying continuous-time linear dynamical system, have garnered substantial attention and achieved breakthrough advancements in independent sequence modeling. In this work, we undertake a principled investigation that extends SSM theory to temporal graphs by integrating structural information into the online approximation objective via the adoption of a Laplacian regularization term. The emergent continuous-time system introduces novel algorithmic challenges, thereby necessitating our development of GraphSSM, a graph state space model for modeling the dynamics of temporal graphs. Extensive experimental results demonstrate the effectiveness of our GraphSSM framework across various temporal graph benchmarks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-252" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-252', event_id='94794', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3102</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94794">Mixture of Link Predictors on Graphs</a></strong></h5>


                        <p class="text-muted">
                            Li Ma &middot; Haoyu Han &middot; Juanhui Li &middot; Harry Shomer &middot; Hui Liu &middot; Xiaofeng Gao &middot; Jiliang Tang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Link prediction, which aims to forecast unseen connections in graphs, is a fundamental task in graph machine learning. Heuristic methods, leveraging a range of different pairwise measures such as common neighbors and shortest paths, often rival the performance of vanilla Graph Neural Networks (GNNs). Therefore, recent advancements in GNNs for link prediction (GNN4LP) have primarily focused on integrating one or a few types of pairwise information. In this work,  we reveal that different node pairs within the same dataset necessitate varied pairwise information for accurate prediction and models that only apply the same pairwise information uniformly could achieve suboptimal performance.As a result, we propose a simple mixture of experts model Link-MoE for link prediction.  Link-MoE utilizes various GNNs as experts and  strategically selects the appropriate expert for each node pair based on various types of pairwise information. Experimental results across diverse real-world datasets demonstrate substantial performance improvement from Link-MoE. Notably,  Link-Mo achieves a relative improvement of 18.71% on the MRR metric for the Pubmed dataset and 9.59% on the Hits@100 metric for the ogbl-ppa dataset, compared to the best baselines. The code is available at https://github.com/ml-ml/Link-MoE/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-253" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-253', event_id='94628', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3103</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94628">DiGRAF: Diffeomorphic Graph-Adaptive Activation Function</a></strong></h5>


                        <p class="text-muted">
                            Krishna Sri Ipsit Mantri &middot; Xinzhi Wang &middot; Carola-Bibiane Sch√∂nlieb &middot; Bruno Ribeiro &middot; Beatrice Bevilacqua &middot; Moshe Eliasof
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we propose a novel activation function tailored specifically for graph data in Graph Neural Networks (GNNs). Motivated by the need for graph-adaptive and flexible activation functions, we introduce DiGRAF, leveraging Continuous Piecewise-Affine Based (CPAB) transformations, which we augment with an additional GNN to learn a graph-adaptive diffeomorphic activation function in an end-to-end manner. In addition to its graph-adaptivity and flexibility, DiGRAF also possesses properties that are widely recognized as desirable for activation functions, such as differentiability, boundness within the domain, and computational efficiency. We conduct an extensive set of experiments across diverse datasets and tasks, demonstrating a consistent and superior performance of DiGRAF compared to traditional and graph-specific activation functions, highlighting its effectiveness as an activation function for GNNs. Our code is available at https://github.com/ipsitmantri/DiGRAF.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-254" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-254', event_id='94570', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3104</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94570">The Map Equation Goes Neural: Mapping Network Flows with Graph Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Christopher Bl√∂cker &middot; Chester Tan &middot; Ingo Scholtes
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Community detection is an essential tool for unsupervised data exploration and revealing the organisational structure of networked systems. With a long history in network science, community detection typically relies on objective functions, optimised with custom-tailored search algorithms, but often without leveraging recent advances in deep learning. Recently, first works have started incorporating such objectives into loss functions for deep graph clustering and pooling. We consider the map equation, a popular information-theoretic objective function for unsupervised community detection, and express it in differentiable tensor form for optimisation through gradient descent. Our formulation turns the map equation compatible with any neural network architecture, enables end-to-end learning, incorporates node features, and chooses the optimal number of clusters automatically, all without requiring explicit regularisation. Applied to unsupervised graph clustering tasks, we achieve competitive performance against state-of-the-art deep graph clustering baselines in synthetic and real-world datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-255" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-255', event_id='94525', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3105</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94525">Logical characterizations of recurrent graph neural networks with reals and floats</a></strong></h5>


                        <p class="text-muted">
                            Veeti Ahvonen &middot; Damian Heiman &middot; Antti Kuusisto &middot; Carsten Lutz
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In pioneering work from 2019, Barcel√≥ and coauthors identified logics that precisely match the expressive power of constant iteration-depth graph neural networks (GNNs) relative to properties definable in first-order logic. In this article, we give exact logical characterizations of recurrent GNNs in two scenarios: (1) in the setting with floating-point numbers and (2) with reals. For floats, the formalism matching recurrent GNNs is a rule-based modal logic with counting, while for reals we use a suitable infinitary modal logic, also with counting. These results give exact matches between logics and GNNs in the recurrent setting without relativising to a background logic in either case, but using some natural assumptions about floating-point arithmetic. Applying our characterizations, we also prove that, relative to graph properties definable in monadic second-order logic (MSO), our infinitary and rule-based logics are equally expressive. This implies that recurrent GNNs with reals and floats have the same expressive power over MSO-definable properties and shows that, for such properties, also recurrent GNNs with reals are characterized by a (finitary!) rule-based modal logic. In the general case, in contrast, the expressive power with floats is weaker than with reals. In addition to logic-oriented results, we also characterize recurrent GNNs, with both reals and floats, via distributed automata, drawing links to distributed computing models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-256" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-256', event_id='94301', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3106</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94301">Rethinking Reconstruction-based Graph-Level Anomaly Detection: Limitations and a Simple Remedy</a></strong></h5>


                        <p class="text-muted">
                            Sunwoo Kim &middot; Soo Yong Lee &middot; Fanchen Bu &middot; Shinhwan Kang &middot; Kyungho Kim &middot; Jaemin Yoo &middot; Kijung Shin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graph autoencoders (Graph-AEs) learn representations of given graphs by aiming to accurately reconstruct them. A notable application of Graph-AEs is graph-level anomaly detection (GLAD), whose objective is to identify graphs with anomalous topological structures and/or node features compared to the majority of the graph population. Graph-AEs for GLAD regard a graph with a high mean reconstruction error (i.e. mean of errors from all node pairs and/or nodes) as anomalies. Namely, the methods rest on the assumption that they would better reconstruct graphs with similar characteristics to the majority. We, however, report non-trivial counter-examples, a phenomenon we call reconstruction flip, and highlight the limitations of the existing Graph-AE-based GLAD methods. Specifically, we empirically and theoretically investigate when this assumption holds and when it fails. Through our analyses, we further argue that, while the reconstruction errors for a given graph are effective features for GLAD, leveraging the multifaceted summaries of the reconstruction errors, beyond just mean, can further strengthen the features. Thus, we propose a novel and simple GLAD method, named MUSE. The key innovation of MUSE involves taking multifaceted summaries of reconstruction errors as graph features for GLAD. This surprisingly simple method obtains SOTA performance in GLAD, performing best overall among 14 methods across 10 datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-257" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-257', event_id='94172', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3107</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94172">GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules</a></strong></h5>


                        <p class="text-muted">
                            Burouj Armgaan &middot; Manthan Dalmia &middot; Sourav Medya &middot; Sayan Ranu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Instance-level explanation of graph neural networks (GNNs) is a well-studied area. These explainers, however, only explain an instance (e.g., a graph) and fail to uncover the combinatorial reasoning learned by a GNN from the training data towards making its predictions. In this work, we introduce GraphTrail, the first end-to-end, global, post-hoc GNN explainer that translates the functioning of a black-box GNN model to a boolean formula over the (sub)graph level concepts without relying on local explainers. GraphTrail is unique in automatically mining the discriminative subgraph-level concepts using Shapley values. Subsequently, the GNN predictions are mapped to a human-interpretable boolean formula over these concepts through symbolic regression. Extensive experiments across diverse datasets and GNN architectures demonstrate significant improvement over existing global explainers in mapping GNN predictions to faithful logical formulae. The robust and accurate performance of GraphTrail makes it invaluable for improving GNNs and facilitates adoption in domains with strict transparency requirements.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-258" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-258', event_id='93842', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3108</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93842">Unitary Convolutions for Learning on Graphs and Groups</a></strong></h5>


                        <p class="text-muted">
                            Bobak Kiani &middot; Lukas Fesser &middot; Melanie Weber
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Data with geometric structure is ubiquitous in machine learning often arising from fundamental symmetries in a domain, such as permutation-invariance in graphs and translation-invariance in images. Group-convolutional architectures, which encode symmetries as inductive bias, have shown great success in applications, but can suffer from instabilities as their depth increases and often struggle to learn long range dependencies in data. For instance, graph neural networks experience instability due to the convergence of node representations (over-smoothing), which can occur after only a few iterations of message-passing, reducing their effectiveness in downstream tasks. Here, we propose and study unitary group convolutions, which allow for deeper networks that are more stable during training. The main focus of the paper are graph neural networks, where we show that unitary graph convolutions provably avoid over-smoothing. Our experimental results confirm that unitary graph convolutional networks achieve competitive performance on benchmark datasets compared to state-of-the-art graph neural networks. We complement our analysis of the graph domain with the study of general unitary convolutions and analyze their role in enhancing stability in general group convolutional architectures.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-259" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-259', event_id='93739', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3109</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93739">Online Relational Inference for Evolving Multi-agent Interacting Systems</a></strong></h5>


                        <p class="text-muted">
                            Beomseok Kang &middot; Priyabrata Saha &middot; Sudarshan Sharma &middot; Biswadeep Chakraborty &middot; Saibal Mukhopadhyay
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce a novel framework, Online Relational Inference (ORI), designed to efficiently identify hidden interaction graphs in evolving multi-agent interacting systems using streaming data. Unlike traditional offline methods that rely on a fixed training set, ORI employs online backpropagation, updating the model with each new data point, thereby allowing it to adapt to changing environments in real-time. A key innovation is the use of an adjacency matrix as a trainable parameter, optimized through a new adaptive learning rate technique called AdaRelation, which adjusts based on the historical sensitivity of the decoder to changes in the interaction graph. Additionally, a data augmentation method named Trajectory Mirror (TM) is introduced to improve generalization by exposing the model to varied trajectory patterns. Experimental results on both synthetic datasets and real-world data (CMU MoCap for human motion) demonstrate that ORI significantly improves the accuracy and adaptability of relational inference in dynamic settings compared to existing methods. This approach is model-agnostic, enabling seamless integration with various neural relational inference (NRI) architectures, and offers a robust solution for real-time applications in complex, evolving systems.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-260" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-260', event_id='93683', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3110</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93683">Graph Neural Networks Do Not Always Oversmooth</a></strong></h5>


                        <p class="text-muted">
                            Bastian Epping &middot; Alexandre Ren√© &middot; Moritz Helias &middot; Michael T Schaub
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graph neural networks (GNNs) have emerged as powerful tools for processing relational data in applications. However, GNNs suffer from the problem of oversmoothing, the property that features of all nodes exponentially converge to the same vector over layers, prohibiting the design of deep GNNs. In this work we study oversmoothing in graph convolutional networks (GCNs) by using their Gaussian process (GP) equivalence in the limit of infinitely many hidden features. By generalizing methods from conventional deep neural networks (DNNs), we can describe the distribution of features at the output layer of deep GCNs in terms of a GP: as expected, we find that typical parameter choices from the literature lead to oversmoothing. The theory, however, allows us to identify a new, non-oversmoothing phase: if the initial weights of the network have sufficiently large variance, GCNs do not oversmooth, and node features remain informative even at large depth. We demonstrate the validity of this prediction in finite-size GCNs by training a linear classifier on their output. Moreover, using the linearization of the GCN GP, we generalize the concept of propagation depth of information from DNNs to GCNs. This propagation depth diverges at the transition between the oversmoothing and non-oversmoothing phase. We test the predictions of our approach and find good agreement with finite-size GCNs. Initializing GCNs near the transition to the non-oversmoothing phase, we obtain networks which are both deep and expressive.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-261" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-261', event_id='93339', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3111</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93339">Microstructures and Accuracy of Graph Recall by Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Yanbang Wang &middot; Hejie Cui &middot; Jon Kleinberg
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structural patterns) in their recall. We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from --- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-262" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-262', event_id='94548', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3200</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94548">Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks</a></strong></h5>


                        <p class="text-muted">
                            Tianyu He &middot; Darshil Doshi &middot; Aritra Das &middot; Andrey Gromov
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions $z = a  x + b  y \text{ mod } p$ labeled by the vector $(a, b) \in \mathbb{Z}_p^2$. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is *transient*, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing highly structured representations in both attention heads and MLPs; and discuss the learned algorithms. Notably, we find an algorithmic shift in deeper models, as we go from few to many in-context examples.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-263" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-263', event_id='94702', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3201</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94702">Initialization is Critical to Whether Transformers Fit Composite Functions by Reasoning or Memorizing</a></strong></h5>


                        <p class="text-muted">
                            Zhongwang Zhang &middot; Pengxiao Lin &middot; Zhiwei Wang &middot; Yaoyu Zhang &middot; Zhi-Qin Xu
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Transformers have shown impressive capabilities across various tasks, but their performance on compositional problems remains a topic of debate. In this work, we investigate the mechanisms of how transformers behave on unseen compositional tasks.  We discover that the parameter initialization scale plays a critical role in determining whether the model learns inferential (reasoning-based) solutions, which capture the underlying compositional primitives, or symmetric (memory-based) solutions, which simply memorize mappings without understanding the compositional structure. By analyzing the information flow and vector representations within the model, we reveal the distinct mechanisms underlying these solution types. We further find that inferential (reasoning-based) solutions exhibit low complexity bias, which we hypothesize is a key factor enabling them to learn individual mappings for single anchors. We validate our conclusions on various real-world datasets. Our findings provide valuable insights into the role of initialization scale in tuning the reasoning and memorizing ability and we propose the initialization rate $\gamma$ to be a convenient tunable hyper-parameter in common deep learning frameworks, where $1/d_{\mathrm{in}}^\gamma$ is the standard deviation of parameters of the layer with $d_{\mathrm{in}}$ input neurons.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-264" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-264', event_id='96455', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3202</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96455">Great Minds Think Alike: The Universal Convergence Trend of Input Salience</a></strong></h5>


                        <p class="text-muted">
                            Yipei Wang &middot; Jeffrey Siskind &middot; Xiaoqian Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Uncertainty is introduced in optimized DNNs through stochastic algorithms, forming specific distributions. Training models can be seen as random sampling from this distribution of optimized models. In this work, we study the distribution of optimized DNNs as a family of functions by leveraging a pointwise approach. We focus on the input saliency maps, as the input gradient field is decisive to the models' mathematical essence. Our investigation of saliency maps reveals a counter-intuitive trend: two stochastically optimized models tend to resemble each other more as either of their capacities increases. Therefore, we hypothesize several properties of these distributions, suggesting that (1) Within the same model architecture (e.g., CNNs, ResNets), different family variants (e.g., varying capacities) tend to align in terms of their population mean directions of the input salience. And (2) the distributions of optimized models follow a convergence trend to their shared population mean as the capacity increases. Furthermore, we also propose semi-parametric distributions based on the Saw distribution to model the convergence trend, satisfying all the counter-intuitive observations. Our experiments shed light on the significant implications of our hypotheses in various application domains, including black-box attacks, deep ensembles, etc. These findings not only enhance our understanding of DNN behaviors but also offer valuable insights for their practical application in diverse areas of deep learning.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-265" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-265', event_id='96586', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3203</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96586">Conditional Density Estimation with Histogram Trees</a></strong></h5>


                        <p class="text-muted">
                            Lincen Yang &middot; Matthijs van Leeuwen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Conditional density estimation (CDE) goes beyond regression by modeling the full conditional distribution, providing a richer understanding of the data than just the conditional mean in regression. This makes CDE particularly useful in critical application domains. However, interpretable CDE methods are understudied. Current methods typically employ kernel-based approaches, using kernel functions directly for kernel density estimation or as basis functions in linear models. In contrast, despite their conceptual simplicity and visualization suitability, tree-based methods---which are arguably more comprehensible---have been largely overlooked for CDE tasks. Thus, we propose the Conditional Density Tree (CDTree), a fully non-parametric model consisting of a decision tree in which each leaf is formed by a histogram model. Specifically, we formalize the problem of learning a CDTree using the minimum description length (MDL) principle, which eliminates the need for tuning the hyperparameter for regularization. Next, we propose an iterative algorithm that, although greedily, searches the optimal histogram for every possible node split. Our experiments demonstrate that, in comparison to existing interpretable CDE methods, CDTrees are both more accurate (as measured by the log-loss) and more robust against irrelevant features. Further, our approach leads to smaller tree sizes than existing tree-based models, which benefits interpretability.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-266" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-266', event_id='96762', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3204</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96762">LLM Circuit Analyses Are Consistent Across Training and Scale</a></strong></h5>


                        <p class="text-muted">
                            Curt Tigges &middot; Michael Hanna &middot; Qinan Yu &middot; Stella Biderman
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Most currently deployed LLMs undergo continuous training or additional finetuning. By contrast, most research into LLMs' internal mechanisms focuses on models at one snapshot in time (the end of pre-training), raising the question of whether their results generalize to real-world settings. Existing studies of mechanisms over time focus on encoder-only or toy models, which differ significantly from most deployed models. In this study, we track how model mechanisms, operationalized as circuits, emerge and evolve across 300 billion tokens of training in decoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters. We find that task abilities and the functional components that support them emerge consistently at similar token counts across scale. Moreover, although such components may be implemented by different attention heads over time, the overarching algorithm that they implement remains. Surprisingly, both these algorithms and the types of components involved therein tend to replicate across model scale. Finally, we find that circuit size correlates with model size and can fluctuate considerably over time even when the same algorithm is implemented. These results suggest that circuit analyses conducted on small models at the end of pre-training can provide insights that still apply after additional training and over model scale.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-267" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-267', event_id='97782', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3205</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97782">DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Bowen Wang &middot; Jiuyang Chang &middot; Yiming Qian &middot; Guoxin Chen &middot; Junhao Chen &middot; Zhouqiang Jiang &middot; Jiahao Zhang &middot; Yuta Nakashima &middot; Hajime Nagahara
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face challenges in the lack of interpretability when handling complex tasks in real clinical settings. We thus introduce the diagnostic reasoning dataset for clinical notes (DiReCT), aiming at evaluating the reasoning ability and interpretability of LLMs compared to human doctors. It contains 521 clinical notes, each meticulously annotated by physicians, detailing the diagnostic reasoning process from observations in a clinical note to the final diagnosis. Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant gap between their reasoning ability and that of human doctors, highlighting the critical need for models that can reason effectively in real-world clinical scenarios.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-268" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-268', event_id='93055', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3206</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93055">Towards Human-AI Complementarity with Prediction Sets</a></strong></h5>


                        <p class="text-muted">
                            Giovanni De Toni &middot; Nastaran Okati &middot; Suhas Thejaswi &middot; Eleni Straitouri &middot; Manuel Rodriguez
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Decision support systems based on prediction sets have proven to be effective at helping human experts solve classification tasks. Rather than providing single-label predictions, these systems provide sets of label predictions constructed using conformal prediction, namely prediction sets, and ask human experts to predict label values from these sets. In this paper, we first show that the prediction sets constructed using conformal prediction are, in general, suboptimal in terms of average accuracy. Then, we show that the problem of finding the optimal prediction sets under which the human experts achieve the highest average accuracy is NP-hard. More strongly, unless P = NP, we show that the problem is hard to approximate to any factor less than the size of the label set. However, we introduce a simple and efficient greedy algorithm that, for a large class of expert models and non-conformity scores, is guaranteed to find prediction sets that provably offer equal or greater performance than those constructed using conformal prediction. Further, using a simulation study with both synthetic and real expert predictions, we demonstrate that, in practice, our greedy algorithm finds near-optimal prediction sets offering greater performance than conformal prediction.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-269" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-269', event_id='93120', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3207</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93120">Human Expertise in Algorithmic Prediction</a></strong></h5>


                        <p class="text-muted">
                            Rohan Alur &middot; Manish Raghavan &middot; Devavrat Shah
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach leverages human judgment to distinguish inputs which are <em>algorithmically indistinguishable</em>, or "look the same" to predictive algorithms.  We argue that this framing clarifies the problem of human-AI collaboration in prediction tasks, as experts often form judgments by drawing on information which is not encoded in an algorithm's training data. Algorithmic indistinguishability yields a natural test for assessing whether experts incorporate this kind of "side information", and further provides a simple but principled method for selectively incorporating human feedback into algorithmic predictions. We show that this method provably improves the performance of any feasible algorithmic predictor and precisely quantify this improvement.  We find empirically that although algorithms often outperform their human counterparts <em>on average</em>, human judgment can improve algorithmic predictions on <em>specific</em> instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-270" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-270', event_id='93458', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3208</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93458">RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation</a></strong></h5>


                        <p class="text-muted">
                            Changli Wu &middot; qi chen &middot; Jiayi Ji &middot; Haowei Wang &middot; Yiwei Ma &middot; You Huang &middot; Gen Luo &middot; Hao Fei &middot; Xiaoshuai Sun &middot; Rongrong Ji
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>3D Referring Expression Segmentation (3D-RES) aims to segment 3D objects by correlating referring expressions with point clouds. However, traditional approaches frequently encounter issues like over-segmentation or mis-segmentation, due to insufficient emphasis on spatial information of instances. In this paper, we introduce a Rule-Guided Spatial Awareness Network (RG-SAN) by utilizing solely the spatial information of the target instance for supervision. This approach enables the network to accurately depict the spatial relationships among all entities described in the text, thus enhancing the reasoning capabilities. The RG-SAN consists of the Text-driven Localization Module (TLM) and the Rule-guided Weak Supervision (RWS) strategy. The TLM initially locates all mentioned instances and iteratively refines their positional information. The RWS strategy, acknowledging that only target objects have supervised positional information, employs dependency tree rules to precisely guide the core instance‚Äôs positioning. Extensive testing on the ScanRefer benchmark has shown that RG-SAN not only establishes new performance benchmarks, with an mIoU increase of 5.1 points, but also exhibits significant improvements in robustness when processing descriptions with spatial ambiguity. All codes are available at https://github.com/sosppxo/RG-SAN.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-271" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-271', event_id='94200', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3209</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94200">Referring Human Pose and Mask Estimation In the Wild</a></strong></h5>


                        <p class="text-muted">
                            Bo Miao &middot; Mingtao Feng &middot; Zijie Wu &middot; Mohammed Bennamoun &middot; Yongsheng Gao &middot; Ajmal Mian
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce Referring Human Pose and Mask Estimation (R-HPM) in the wild, where either a text or positional prompt specifies the person of interest in an image. This new task holds significant potential for human-centric applications such as assistive robotics and sports analysis. In contrast to previous works, R-HPM (i) ensures high-quality, identity-aware results corresponding to the referred person, and (ii) simultaneously predicts human pose and mask for a comprehensive representation. To achieve this, we introduce a large-scale dataset named RefHuman, which substantially extends the MS COCO dataset with additional text and positional prompt annotations. RefHuman includes over 50,000 annotated instances in the wild, each equipped with keypoint, mask, and prompt annotations. To enable prompt-conditioned estimation, we propose the first end-to-end promptable approach named UniPHD for R-HPM. UniPHD extracts multimodal representations and employs a proposed pose-centric hierarchical decoder to process (text or positional) instance queries and keypoint queries, producing results specific to the referred person. Extensive experiments demonstrate that UniPHD produces quality results based on user-friendly prompts and achieves top-tier performance on RefHuman val and MS COCO val2017.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-272" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-272', event_id='92932', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3210</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92932">Generative Semi-supervised Graph Anomaly Detection</a></strong></h5>


                        <p class="text-muted">
                            Hezhe Qiao &middot; Qingsong Wen &middot; Xiaoli Li &middot; Ee-peng Lim &middot; Guansong Pang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the extensively explored unsupervised setting with a fully unlabeled graph. We reveal that having access to the normal nodes, even just a small percentage of normal nodes, helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (namely GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate pseudo anomaly nodes, referred to as 'outlier nodes', for providing effective negative node samples in training a discriminative one-class classifier. The main challenge here lies in the lack of ground truth information about real anomaly nodes. To address this challenge, GGAD is designed to leverage two important priors about the anomaly nodes -- asymmetric local affinity and egocentric closeness -- to generate reliable outlier nodes that assimilate anomaly nodes in both graph structure and feature representations. Comprehensive experiments on six real-world GAD datasets are performed to establish a benchmark for semi-supervised GAD and show that GGAD substantially outperforms state-of-the-art unsupervised and semi-supervised GAD methods with varying numbers of training normal nodes.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-273" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-273', event_id='93261', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3211</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93261">Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval</a></strong></h5>


                        <p class="text-muted">
                            Ashwin Ramachandran &middot; Vaibhav Raj &middot; Indradyumna Roy &middot; Soumen Chakrabarti &middot; Abir De
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Graph retrieval based on subgraph isomorphism has several real-world applications such as scene graph retrieval, molecular fingerprint detection and circuit design. Roy et al. [35] proposed IsoNet, a late interaction model for subgraph matching, which first computes the node and edge embeddings of each graph independently of paired graph  and then computes a trainable alignment map. Here, we present $\texttt{IsoNet++}$, an early interaction graph neural network (GNN), based on several technical innovations. First, we compute embeddings of all nodes by passing messages within and across the two input graphs, guided by an *injective alignment* between their nodes. Second, we update this alignment in a lazy fashion over multiple *rounds*. Within each round, we run a layerwise GNN from scratch, based on the current state of the alignment. After the completion of one round of GNN, we use the last-layer embeddings to update the alignments, and proceed to the next round. Third, $\texttt{IsoNet++}$ incorporates a novel notion of node-pair partner interaction. Traditional early interaction computes attention between a node and its potential partners in the other graph, the attention then controlling messages passed across graphs. We consider *node pairs* (not single nodes) as potential partners. Existence of an edge between the nodes in one graph and non-existence in the other provide vital signals for refining the alignment. Our experiments on several datasets show that the alignments get progressively refined with successive rounds,resulting in significantly better retrieval performance than existing methods. We demonstrate that all three innovations contribute to the enhanced accuracy. Our code and datasets are publicly available at https://github.com/structlearning/isonetpp.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-274" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-274', event_id='94481', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3300</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94481">Denoising Diffusion Path: Attribution Noise Reduction with An Auxiliary Diffusion Model</a></strong></h5>


                        <p class="text-muted">
                            Yiming Lei &middot; Zilong Li &middot; Junping Zhang &middot; Hongming Shan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The explainability of deep neural networks (DNNs) is critical for trust and reliability in AI systems. Path-based attribution methods, such as integrated gradients (IG), aim to explain predictions by accumulating gradients along a path from a baseline to the target image. However, noise accumulated during this process can significantly distort the explanation. While existing methods primarily concentrate on finding alternative paths to circumvent noise, they overlook a critical issue: intermediate-step images frequently diverge from the distribution of training data, further intensifying the impact of noise. This work presents a novel Denoising Diffusion Path (DDPath) to tackle this challenge by harnessing the power of diffusionmodels for denoising. By exploiting the inherent ability of diffusion models to progressively remove noise from an image, DDPath constructs a piece-wise linear path. Each segment of this path ensures that samples drawn from a Gaussian distribution are centered around the target image. This approach facilitates a gradual reduction of noise along the path. We further demonstrate that DDPath adheres to essential axiomatic properties for attribution methods and can be seamlessly integrated with existing methods such as IG. Extensive experimental results demonstrate that DDPath can significantly reduce noise in the attributions‚Äîresulting in clearer explanations‚Äîand achieves better quantitative results than traditional path-based methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-275" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-275', event_id='94156', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3301</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94156">Selective Explanations</a></strong></h5>


                        <p class="text-muted">
                            Lucas Monteiro Paes &middot; Dennis Wei &middot; Flavio Calmon
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Feature attribution methods explain black-box machine learning (ML) models by assigning importance scores to input features. These methods can be computationally expensive for large ML models. To address this challenge, there have been increasing efforts to develop amortized explainers, where a ML model is trained to efficiently approximate computationally expensive feature attribution scores. Despite their efficiency, amortized explainers can produce misleading explanations. In this paper, we propose selective explanations to (i) detect when amortized explainers generate inaccurate explanations and (ii) improve the approximation of the explanation using a technique we call explanations with initial guess. Selective explanations allow practitioners to specify the fraction of samples that receive explanations with initial guess, offering a principled way to bridge the gap between amortized explainers (one inference) and more computationally costly approximations (multiple inferences). Our experiments on various models and datasets demonstrate that feature attributions via selective explanations strike a favorable balance between explanation quality and computational efficiency.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-276" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-276', event_id='94047', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3302</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94047">Interpretable Image Classification with Adaptive Prototype-based Vision Transformers</a></strong></h5>


                        <p class="text-muted">
                            Chiyu Ma &middot; Jon Donnelly &middot; Wenjun Liu &middot; Soroush Vosoughi &middot; Cynthia Rudin &middot; Chaofan Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form ``this looks like that.'' In our model, a prototype consists of <strong>parts</strong>, which can deform over irregular geometries to create a better comparison between images. Unlike existing models that rely on Convolutional Neural Network (CNN) backbones and spatially rigid prototypes, our model integrates Vision Transformer (ViT) backbones into prototype based models, while offering spatially deformed prototypes that not only accommodate geometric variations of objects but also provide coherent and clear prototypical feature representations with an adaptive number of prototypical parts. Our experiments show that our model can generally achieve higher performance than the existing prototype based models. Our comprehensive analyses ensure that the prototypes are consistent and the interpretations are faithful.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-277" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-277', event_id='93666', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3303</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93666">The Representation Landscape of Few-Shot Learning and Fine-Tuning in Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Diego Doimo &middot; Alessandro Serra &middot; Alessio Ansuini &middot; Alberto Cazzaniga
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. However, little is known about whether they induce similar representations inside LLMs.  We approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, we compare how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while less-defined peaks characterize the landscape of ICL representations. Our approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing us to make a step towards designing optimal methods to extract information from language models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-278" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-278', event_id='93649', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3304</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93649">Model LEGO: Creating Models Like Disassembling and Assembling Building Blocks</a></strong></h5>


                        <p class="text-muted">
                            Jiacong Hu &middot; Jing Gao &middot; Jingwen Ye &middot; Yang Gao &middot; Xingen Wang &middot; Zunlei Feng &middot; Mingli Song
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>With the rapid development of deep learning, the increasing complexity and scale of parameters make training a new model increasingly resource-intensive. In this paper, we start from the classic convolutional neural network (CNN) and explore a paradigm that does not require training to obtain new models. Similar to the birth of CNN inspired by receptive fields in the biological visual system, we draw inspiration from the information subsystem pathways in the biological visual system and propose Model Disassembling and Assembling (MDA). During model disassembling, we introduce the concept of relative contribution and propose a component locating technique to extract task-aware components from trained CNN classifiers. For model assembling, we present the alignment padding strategy and parameter scaling strategy to construct a new model tailored for a specific task, utilizing the disassembled task-aware components.The entire process is akin to playing with LEGO bricks, enabling arbitrary assembly of new models, and providing a novel perspective for model creation and reuse. Extensive experiments showcase that task-aware components disassembled from CNN classifiers or new models assembled using these components closely match or even surpass the performance of the baseline,demonstrating its promising results for model reuse. Furthermore, MDA exhibits diverse potential applications, with comprehensive experiments exploring model decision route analysis, model compression, knowledge distillation, and more.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-279" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-279', event_id='93566', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3305</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93566">Refusal in Language Models Is Mediated by a Single Direction</a></strong></h5>


                        <p class="text-muted">
                            Andy Arditi &middot; Oscar Obeso &middot; Aaquib Syed &middot; Daniel Paleka &middot; Nina Panickssery &middot; Wes Gurnee &middot; Neel Nanda
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables a model's ability to refuse, with minimal effect on other capabilities. This interpretable rank-one weight edit results in an effective jailbreak technique that is simpler and more efficient than fine-tuning. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-280" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-280', event_id='96264', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3306</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96264">Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Ling Yang &middot; Zhaochen Yu &middot; Tianjun Zhang &middot; Shiyi Cao &middot; Minkai Xu &middot; Wentao Zhang &middot; Joseph Gonzalez &middot; Bin CUI
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce Buffer of Thoughts (BoT), a novel and versatile thought-augmented reasoning approach for enhancing accuracy, efficiency and robustness of large language models (LLMs). Specifically, we propose meta-buffer to store a series of informative high-level thoughts, namely thought-template, distilled from the problem-solving processes across various tasks. Then for each problem, we retrieve a relevant thought-template and adaptively instantiate it with specific reasoning structures to conduct efficient reasoning. To guarantee the scalability and stability, we further propose buffer-manager to dynamically update the meta-buffer, thus enhancing the capacity of meta-buffer as more tasks are solved. We conduct extensive experiments on 10 challenging reasoning-intensive tasks, and achieve significant performance improvements over previous SOTA methods: 11\% on Game of 24, 20\% on Geometric Shapes and 51\% on Checkmate-in-One. Further analysis demonstrate the superior generalization ability and model robustness of our BoT, while requiring only 12\% of the cost of multi-query prompting methods (e.g., tree/graph of thoughts) on average. Code is available at: https://github.com/YangLing0818/buffer-of-thought-llm</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-281" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-281', event_id='96146', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3307</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96146">Accelerating Greedy Coordinate Gradient and General Prompt Optimization via Probe Sampling</a></strong></h5>


                        <p class="text-muted">
                            Yiran Zhao &middot; Wenyue Zheng &middot; Tianle Cai &middot; Do Xuan Long &middot; Kenji Kawaguchi &middot; Anirudh Goyal &middot; Michael Qizhe Shieh
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Safety of Large Language Models (LLMs) has become a central issue given their rapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown to be effective in constructing prompts containing adversarial suffixes to break the presumingly safe LLMs, but the optimization of GCG is time-consuming and limits its practicality. To reduce the time cost of GCG and enable more comprehensive studies of LLM safety, in this work, we study a new algorithm called $\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core of the algorithm is a mechanism that dynamically determines how similar a smaller draft model's predictions are to the target model's predictions for prompt candidates. When the target model is similar to the draft model, we rely heavily on the draft model to filter out a large number of potential prompt candidates to reduce the computation time. Probe sampling achieves up to $5.6$ times speedup using Llama2-7b-chat and leads to equal or improved attack success rate (ASR) on the AdvBench. Furthermore, probe sampling is also able to accelerate other prompt optimization techniques and adversarial attack methods, leading to acceleration of $1.8\times$ for AutoPrompt, $2.4\times$ for APE and $2.4\times$ for AutoDAN.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-282" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-282', event_id='94887', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3308</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94887">SafeWorld: Geo-Diverse Safety Alignment</a></strong></h5>


                        <p class="text-muted">
                            Da Yin &middot; Haoyi Qiu &middot; Kung-Hsiang Huang &middot; Kai-Wei Chang &middot; Nanyun Peng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In the rapidly evolving field of Large Language Models (LLMs), ensuring safety is a crucial and widely discussed topic. However, existing works often overlooks the geo-diversity of cultural and legal standards across the world. To reveal the chal5 lenges posed by geo-diverse safety standards, we introduce SafeWorld, a novel benchmark specifically designed to evaluate LLMs‚Äô ability to generate responses that are not only helpful but also culturally sensitive and legally compliant across diverse global contexts. SafeWorld encompasses 2,775 test user queries, each grounded in high-quality, human-verified cultural norms and legal policies from 50 countries and 493 regions/races. On top of it, we propose a multi-dimensional automatic safety evaluation framework that assesses the contextual appropriateness, accuracy, and comprehensiveness of responses. Our evaluations reveal that current LLMs struggle to meet these criteria effectively. To enhance LLMs‚Äô alignment with geo-diverse safety standards, we synthesize helpful preference pairs for Direct Preference Optimization (DPO) alignment. The preference pair construction aims to encourage LLMs to behave appropriately and provide precise references to relevant cultural norms and policies when necessary. Our trained SafeWorldLM outperforms all competing models, including GPT-4o on all the three evaluation dimensions by a large margin. Global human evaluators also note a nearly 20% higher winning rate in helpfulness and harmfulness evaluation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-283" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-283', event_id='94104', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3309</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94104">Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles</a></strong></h5>


                        <p class="text-muted">
                            Qi Chen &middot; Bowen Zhang &middot; Gang Wang &middot; Qi Wu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While advancements in NLP have significantly improved the performance of Large Language Models (LLMs) on tasks requiring vertical thinking, their lateral thinking capabilities remain under-explored and challenging to measure due to the complexity of assessing creative thought processes and the scarcity of relevant data. To address these challenges, we introduce SPLAT, a benchmark leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs. This benchmark, containing 975 graded situation puzzles across three difficulty levels, employs a new multi-turn player-judge framework instead of the traditional model-based evaluation, which often necessitates a stronger evaluation model. This framework simulates an interactive game where the model (player) asks the evaluation model (judge) questions about an incomplete story to infer the full scenario. The judge answers based on a detailed reference scenario or evaluates if the player's predictions align with the reference one. This approach lessens dependence on more robust evaluation models, enabling the assessment of state-of-the-art LLMs. The experiments demonstrate that a robust evaluation model, such as WizardLM-2, closely matches human judgements in both intermediate question-answering and final scenario accuracy, achieving over 80% agreement--similar to the agreement levels among humans. Furthermore, applying data and reasoning processes from our benchmark to other lateral thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to performance enhancements. This suggests that our benchmark effectively evaluates and elicits the lateral thinking abilities of LLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-284" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-284', event_id='93744', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3310</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93744">Semantics and Spatiality of Emergent Communication</a></strong></h5>


                        <p class="text-muted">
                            Rotem Ben Zion &middot; Boaz Carmeli &middot; Orr Paradise &middot; Yonatan Belinkov
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>When artificial agents are jointly trained to perform collaborative tasks using a communication channel, they develop opaque goal-oriented communication protocols. Good task performance is often considered sufficient evidence that meaningful communication is taking place, but existing empirical results show that communication strategies induced by common objectives can be counterintuitive whilst solving the task nearly perfectly. In this work, we identify a goal-agnostic prerequisite to meaningful communication, which we term semantic consistency, based on the idea that messages should have similar meanings across instances. We provide a formal definition for this idea, and use it to compare the two most common objectives in the field of emergent communication: discrimination and reconstruction. We prove, under mild assumptions, that semantically inconsistent communication protocols can be optimal solutions to the discrimination task, but not to reconstruction. We further show that the reconstruction objective encourages a stricter property, spatial meaningfulness, which also accounts for the distance between messages. Experiments with emergent communication games validate our theoretical results. These findings demonstrate an inherent advantage of distance-based communication goals, and contextualize previous empirical discoveries.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-285" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-285', event_id='93454', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3311</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93454">Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum</a></strong></h5>


                        <p class="text-muted">
                            Hadi Pouransari &middot; Chun-Liang Li &middot; Jen-Hao Chang &middot; Pavan Kumar Anasosalu Vasu &middot; Cem Koc &middot; Vaishaal Shankar &middot; Oncel Tuzel
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences. These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a predetermined target length (concat-and-chunk). Recent attention implementations mask cross-document attention, reducing the effective length of a chunk of tokens. Additionally, training on long sequences becomes computationally prohibitive due to the quadratic cost of attention. In this study, we introduce dataset decomposition, a novel variable sequence length training technique, to tackle these challenges. We decompose a dataset into a union of buckets, each containing sequences of the same size extracted from a unique document. During training, we use variable sequence length and batch-size, sampling simultaneously from all buckets with a curriculum. In contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at every step of training, our proposed method incurs a computational cost proportional to the actual document lengths at each step, resulting in significant savings in training time. We train an 8k context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach. Experiments on a web-scale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy with up to 6x faster training compared to the baseline. Our method not only enables efficient pretraining on long sequences but also scales effectively with dataset size. Lastly, we shed light on a critical yet less studied aspect of training large language models: the distribution and curriculum of sequence lengths, which results in a non-negligible difference in performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-286" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-286', event_id='96014', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3400</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96014">Fast Tree-Field Integrators: From Low Displacement Rank to Topological Transformers</a></strong></h5>


                        <p class="text-muted">
                            Krzysztof M Choromanski &middot; Arijit Sehanobish &middot; Somnath Basu Roy Chowdhury &middot; Han Lin &middot; Kumar Avinava Dubey &middot; Tamas Sarlos &middot; Snigdha Chaturvedi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present a new class of fast polylog-linear algorithms based on the theory of structured matrices (in particular <em>low displacement rank</em>) for integrating tensor fields defined on weighted trees. Several applications of the resulting <em>fast tree-field integrators</em> (FTFIs) are presented, including: (a) approximation of graph metrics with tree metrics, (b) graph classification, (c) modeling on meshes, and finally (d) <em>Topological Transformers</em> (TTs) (Choromanski et al., 2022)  for images. For Topological Transformers, we propose new relative position encoding (RPE) masking mechanisms with as few as <strong>three</strong> extra learnable parameters per Transformer layer, leading to <strong>1.0-1.5\%+</strong> accuracy gains. Importantly, most of FTFIs are <strong>exact</strong> methods, thus numerically equivalent to their brute-force counterparts. When applied to graphs with thousands of nodes, those exact algorithms provide <strong>5.7-13x</strong> speedups. We also provide an extensive theoretical analysis of our methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-287" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-287', event_id='96368', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3401</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96368">Neuc-MDS: Non-Euclidean Multidimensional Scaling Through Bilinear Forms</a></strong></h5>


                        <p class="text-muted">
                            Chengyuan Deng &middot; Jie Gao &middot; Kevin Lu &middot; Feng Luo &middot; Hongbin Sun &middot; Cheng Xin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce \textbf{N}on-\textbf{Euc}lidean-\textbf{MDS} (Neuc-MDS), which extends Multidimensional Scaling (MDS) to generate outputs that can be non-Euclidean and non-metric. The main idea is to generalize the inner product to other symmetric bilinear forms to utilize the negative eigenvalues of dissimiliarity Gram matrices. Neuc-MDS efficiently optimizes the choice of (both positive and negative) eigenvalues of the dissimilarity Gram matrix to reduce STRESS, the sum of squared pairwise error. We provide an in-depth error analysis and proofs of the optimality in minimizing lower bounds of STRESS. We demonstrate Neuc-MDS's ability to address limitations of classical MDS raised by prior research, and test it on various synthetic and real-world datasets in comparison with both linear and non-linear dimension reduction methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-288" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-288', event_id='93112', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3402</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93112">LoRANN: Low-Rank Matrix Factorization for Approximate Nearest Neighbor Search</a></strong></h5>


                        <p class="text-muted">
                            Elias J√§√§saari &middot; Ville Hyv√∂nen &middot; Teemu Roos
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Approximate nearest neighbor (ANN) search is a key component in many modern machine learning pipelines; recent use cases include retrieval-augmented generation (RAG) and vector databases. Clustering-based ANN algorithms, that use score computation methods based on product quantization (PQ), are often used in industrial-scale applications due to their scalability and suitability for distributed and disk-based implementations. However, they have slower query times than the leading graph-based ANN algorithms. In this work, we propose a new supervised score computation method based on the observation that inner product approximation is a multivariate (multi-output) regression problem that can be solved efficiently by reduced-rank regression. Our experiments show that on modern high-dimensional data sets, the proposed reduced-rank regression (RRR) method is superior to PQ in both query latency and memory usage. We also introduce LoRANN, a clustering-based ANN library that leverages the proposed score computation method. LoRANN is competitive with the leading graph-based algorithms and outperforms the state-of-the-art GPU ANN methods on high-dimensional data sets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-289" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-289', event_id='95416', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3403</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95416">Paralinguistics-Aware Speech-Empowered Large Language Models for Natural Conversation</a></strong></h5>


                        <p class="text-muted">
                            Heeseung Kim &middot; Soonshin Seo &middot; Kyeongseok Jeong &middot; Ohsung Kwon &middot; Soyoon Kim &middot; Jungwhan Kim &middot; Jaehong Lee &middot; Eunwoo Song &middot; Myungwoo Oh &middot; Jung-Woo Ha &middot; Sungroh Yoon &middot; Kang Min Yoo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech. However, an LLM-based strategy for modeling spoken dialogs remains elusive, calling for further investigation. This paper introduces an extensive speech-text LLM framework, the Unified Spoken Dialog Model (USDM), designed to generate coherent spoken responses with naturally occurring prosodic features relevant to the given input speech without relying on explicit automatic speech recognition (ASR) or text-to-speech (TTS) systems. We have verified the inclusion of prosody in speech tokens that predominantly contain semantic information and have used this foundation to construct a prosody-infused speech-text model. Additionally, we propose a generalized speech-text pretraining scheme that enhances the capture of cross-modal semantics. To construct USDM, we fine-tune our speech-text model on spoken dialog data using a multi-step spoken dialog template that stimulates the chain-of-reasoning capabilities exhibited by the underlying LLM. Automatic and human evaluations on the DailyTalk dataset demonstrate that our approach effectively generates natural-sounding spoken responses, surpassing previous and cascaded baselines. Our code and checkpoints are available at https://github.com/naver-ai/usdm.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-290" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-290', event_id='96323', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3404</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96323">Separate and Reconstruct: Asymmetric Encoder-Decoder for Speech Separation</a></strong></h5>


                        <p class="text-muted">
                            Ui-Hyeop Shin &middot; Sangyoun Lee &middot; Taehan Kim &middot; Hyung-Min Park
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In speech separation, time-domain approaches have successfully replaced the time-frequency domain with latent sequence feature from a learnable encoder. Conventionally, the feature is separated into speaker-specific ones at the final stage of the network. Instead, we propose a more intuitive strategy that separates features earlier by expanding the feature sequence to the number of speakers as an extra dimension. To achieve this, an asymmetric strategy is presented in which the encoder and decoder are partitioned to perform distinct processing in separation tasks. The encoder analyzes features, and the output of the encoder is split into the number of speakers to be separated. The separated sequences are then reconstructed by the weight-shared decoder, which also performs cross-speaker processing. Without relying on speaker information, the weight-shared network in the decoder directly learns to discriminate features using a separation objective. In addition, to improve performance, traditional methods have extended the sequence length, leading to the adoption of dual-path models, which handle the much longer sequence effectively by segmenting it into chunks. To address this, we introduce global and local Transformer blocks that can directly handle long sequences more efficiently without chunking and dual-path processing. The experimental results demonstrated that this asymmetric structure is effective and that the combination of proposed global and local Transformer can sufficiently replace the role of inter- and intra-chunk processing in dual-path structure. Finally, the presented model combining both of these achieved state-of-the-art performance with much less computation in various benchmark datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-291" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-291', event_id='94929', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3405</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94929">Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction</a></strong></h5>


                        <p class="text-muted">
                            Haoran Luo &middot; Haihong E &middot; Yuhao Yang &middot; Tianyu Yao &middot; Yikai Guo &middot; Zichen Tang &middot; Wentai Zhang &middot; Shiyao Peng &middot; Kaiyang Wan &middot; Meina Song &middot; Wei Lin &middot; Yifan Zhu &middot; Anh Tuan Luu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Beyond traditional binary relational facts, n-ary relational knowledge graphs (NKGs) are comprised of n-ary relational facts containing more than two entities, which are closer to real-world facts with broader applications. However, the construction of NKGs remains at a coarse-grained level, which is always in a single schema, ignoring the order and variable arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging and output merging to accomplish fine-grained n-ary relation extraction in different arity. Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, with high flexibility and practicality. The experimental results demonstrate that Text2NKG achieves state-of-the-art performance in F1 scores on the fine-grained n-ary relation extraction benchmark. Our code and datasets are publicly available.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-292" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-292', event_id='93423', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3406</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93423">Aligning to Thousands of Preferences via System Message Generalization</a></strong></h5>


                        <p class="text-muted">
                            Seongyun Lee &middot; Sue Hyun Park &middot; Seungone Kim &middot; Minjoon Seo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public‚Äôs preferences is optimal. A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual‚Äôs preferences. To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM‚Äôs generation behavior to better align with the user‚Äôs intentions. However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (e.g., ‚ÄúYou are a helpful assistant‚Äù), which limitstheir ability to generalize to diverse, unseen system messages. To improve this generalization, we create Multifaceted Collection, augmenting 66k user instructions into 197k system messages through hierarchical user value combinations. Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)by adding system messages that reflect unseen user values. JANUS achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), JANUS also outperforms LLaMA 3 8B Instruct by a +4.0%p, +0.1%p, +3.0%p margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public‚Äôs preference as well. Our code, dataset, benchmark, and models are available at https://lklab.kaist.ac.kr/Janus/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-293" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-293', event_id='96474', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3407</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96474">ContextCite: Attributing Model Generation to Context</a></strong></h5>


                        <p class="text-muted">
                            Benjamin Cohen-Wang &middot; Harshay Shah &middot; Kristian Georgiev &middot; Aleksander Madry
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>How do language models use information provided as context when generating a response?Can we infer whether a particular generated statement is actually grounded in the context, a misinterpretation, or fabricated?To help answer these questions, we introduce the problem of <em>context attribution</em>: pinpointing the parts of the context (if any) that <em>led</em> a model to generate a particular statement.We then present ContextCite, a simple and scalable method for context attribution that can be applied on top of any existing language model.Finally, we showcase the utility of ContextCite through three applications:(1) helping verify generated statements(2) improving response quality by pruning the context and(3) detecting poisoning attacks.We provide code for ContextCite at https://github.com/MadryLab/context-cite.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-294" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-294', event_id='93918', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3408</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93918">To Believe or Not to Believe Your LLM: IterativePrompting for Estimating Epistemic Uncertainty</a></strong></h5>


                        <p class="text-muted">
                            Yasin Abbasi Yadkori &middot; Ilja Kuzborskij &middot; Andr√°s Gy√∂rgy &middot; Csaba Szepesvari
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-295" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-295', event_id='95584', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3409</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95584">LLM-Check: Investigating Detection of Hallucinations in Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Gaurang Sriramanan &middot; Siddhant Bharti &middot; Vinu Sankar Sadasivan &middot; Shoumik Saha &middot; Priyatham Kattakinda &middot; Soheil Feizi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While Large Language Models (LLMs) have become immensely popular due to their outstanding performance on a broad range of tasks, these models are prone to producing hallucinations‚Äî outputs that are fallacious or fabricated yet often appear plausible or tenable at a glance. In this paper, we conduct a comprehensive investigation into the nature of hallucinations within LLMs and furthermore explore effective techniques for detecting such inaccuracies in various real-world settings. Prior approaches to detect hallucinations in LLM outputs, such as consistency checks or retrieval-based methods, typically assume access to multiple model responses or large databases. These techniques, however, tend to be computationally expensive in practice, thereby limiting their applicability to real-time analysis. In contrast, in this work, we seek to identify hallucinations within a single response in both white-box and black-box settings by analyzing the internal hidden states, attention maps, and output prediction probabilities of an auxiliary LLM. In addition, we also study hallucination detection in scenarios where ground-truth references are also available, such as in the setting of Retrieval-Augmented Generation (RAG). We demonstrate that the proposed detection methods are extremely compute-efficient, with speedups of up to 45x and 450x over other baselines, while achieving significant improvements in detection performance over diverse datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-296" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-296', event_id='95729', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3410</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95729">Large language model validity via enhanced conformal prediction methods</a></strong></h5>


                        <p class="text-muted">
                            John Cherian &middot; Isaac Gibbs &middot; Emmanuel Candes
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We develop new conformal inference methods for obtaining validity guarantees on the output of large language models (LLMs). Prior work in conformal language modeling identifies a subset of the text that satisfies a high-probability guarantee of correctness. These methods work by filtering claims from the LLM's original response if a scoring function evaluated on the claim fails to exceed a threshold calibrated via split conformal prediction. Existing methods in this area suffer from two deficiencies. First, the guarantee stated is not conditionally valid. The trustworthiness of the filtering step may vary based on the topic of the response. Second, because the scoring function is imperfect, the filtering step can remove many valuable and accurate claims. We address both of these challenges via two new conformal methods. First, we generalize the conditional conformal procedure of Gibbs et al. (2023) in order to adaptively issue weaker guarantees when they are required to preserve the utility of the output. Second, we show how to systematically improve the quality of the scoring function via a novel algorithm for differentiating through the conditional conformal procedure. We demonstrate the efficacy of our approach on biography and medical question-answering datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-297" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-297', event_id='93395', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3411</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93395">Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies</a></strong></h5>


                        <p class="text-muted">
                            Chaofan Tao &middot; Qian Liu &middot; Longxu Dou &middot; Niklas Muennighoff &middot; Zhongwei Wan &middot; Ping Luo &middot; Min Lin &middot; Ngai Wong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Research on scaling large language models (LLMs) has primarily focused on model parameters and training data size, overlooking the role of vocabulary size. We investigate how vocabulary size impacts LLM scaling laws by training models ranging from 33M to 3B parameters on up to 500B characters with various vocabulary configurations. We propose three complementary approaches for predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit of the loss function. Our approaches converge on the conclusion that the optimal vocabulary size depends on the compute budget, with larger models requiring larger vocabularies. Most LLMs, however, use insufficient vocabulary sizes. For example, we predict that the optimal vocabulary size of Llama2-70B should have been at least 216K, 7 times larger than its vocabulary of 32K. We validate our predictions empirically by training models with 3B parameters across different FLOPs budgets. Adopting our predicted optimal vocabulary size consistently improves downstream performance over commonly used vocabulary sizes. By increasing the vocabulary size from the conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21 FLOPs.  Our work highlights the importance of jointly considering tokenization and model scaling for efficient pre-training.  The code and demo are available at https://github.com/sail-sg/scaling-with-vocab and https://hf.co/spaces/sail/scaling-with-vocab-demo.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-298" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-298', event_id='95103', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3500</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95103">Automatically Learning Hybrid Digital Twins of Dynamical Systems</a></strong></h5>


                        <p class="text-muted">
                            Samuel Holt &middot; Tennison Liu &middot; Mihaela van der Schaar
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Digital Twins (DTs) are computational models that simulate the states and temporal dynamics of real-world systems, playing a crucial role in prediction, understanding, and decision-making across diverse domains. However, existing approaches to DTs often struggle to generalize to unseen conditions in data-scarce settings, a crucial requirement for such models. To address these limitations, our work begins by establishing the essential desiderata for effective DTs. Hybrid Digital Twins (<strong>HDTwins</strong>) represent a promising approach to address these requirements, modeling systems using a composition of both mechanistic and neural components. This hybrid architecture simultaneously leverages (partial) domain knowledge and neural network expressiveness to enhance generalization, with its modular design facilitating improved evolvability. While existing hybrid models rely on expert-specified architectures with only parameters optimized on data, <em>automatically</em> specifying and optimizing HDTwins remains intractable due to the complex search space and the need for flexible integration of domain priors. To overcome this complexity, we propose an evolutionary algorithm (<strong>HDTwinGen</strong>) that employs Large Language Models (LLMs) to autonomously propose, evaluate, and optimize HDTwins. Specifically, LLMs iteratively generate novel model specifications, while offline tools are employed to optimize emitted parameters. Correspondingly, proposed models are evaluated and evolved based on targeted feedback, enabling the discovery of increasingly effective hybrid models. Our empirical results reveal that HDTwinGen produces generalizable, sample-efficient, and evolvable models, significantly advancing DTs' efficacy in real-world applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-299" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-299', event_id='94461', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3501</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94461">Revealing Distribution Discrepancy by Sampling Transfer in Unlabeled Data</a></strong></h5>


                        <p class="text-muted">
                            Zhilin Zhao &middot; Longbing Cao &middot; Xuhui Fan &middot; Wei-Shi Zheng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>There are increasing cases where the class labels of test samples are unavailable, creating a significant need and challenge in measuring the discrepancy between training and test distributions. This distribution discrepancy complicates the assessment of whether the hypothesis selected by an algorithm on training samples remains applicable to test samples. We present a novel approach called Importance Divergence (I-Div) to address the challenge of test label unavailability, enabling distribution discrepancy evaluation using only training samples. I-Div transfers the sampling patterns from the test distribution to the training distribution by estimating density and likelihood ratios. Specifically, the density ratio, informed by the selected hypothesis, is obtained by minimizing the Kullback-Leibler divergence between the actual and estimated input distributions. Simultaneously, the likelihood ratio is adjusted according to the density ratio by reducing the generalization error of the distribution discrepancy as transformed through the two ratios. Experimentally, I-Div accurately quantifies the distribution discrepancy, as evidenced by a wide range of complex data scenarios and tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-300" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-300', event_id='94318', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3502</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94318">Scalable DBSCAN with Random Projections</a></strong></h5>


                        <p class="text-muted">
                            HaoChuan Xu &middot; Ninh Pham
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present sDBSCAN, a scalable density-based clustering algorithm in high dimensions with cosine distance. sDBSCAN leverages recent advancements in random projections given a significantly large number of random vectors to quickly identify core points and their neighborhoods, the primary hurdle of density-based clustering. Theoretically, sDBSCAN preserves the DBSCAN‚Äôs clustering structure under mild conditions with high probability. To facilitate sDBSCAN, we present sOPTICS, a scalable visual tool to guide the parameter setting of sDBSCAN. We also extend sDBSCAN and sOPTICS to L2, L1, œá2, and Jensen-Shannon distances via random kernel features. Empirically, sDBSCAN is significantly faster and provides higher accuracy than competitive DBSCAN variants on real-world million-point data sets. On these data sets, sDBSCAN and sOPTICS run in a few minutes, while the scikit-learn counterparts and other clustering competitors demand several hours orcannot run on our hardware due to memory constraints. Our code is available at https://github.com/NinhPham/sDbscan.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-301" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-301', event_id='94195', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3503</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94195">Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices</a></strong></h5>


                        <p class="text-muted">
                            Andres Potapczynski &middot; Shikai Qiu &middot; Marc Finzi &middot; Christopher Ferri &middot; Charlie Chen &middot; Micah Goldblum &middot; C. Bayan Bruss &middot; Christopher De Sa &middot; Andrew Wilson
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Dense linear layers are the dominant computational bottleneck in large neural networks, presenting a critical need for more efficient alternatives. Previous efforts to develop alternatives have focused on a small number of hand-crafted structured matrices, and have neglected to investigate whether these structures can surpass dense layers in terms of compute-optimal scaling laws when both the model size and training examples are optimally allocated. In this work, we present a unifying framework that enables searching among all linear operators expressible via an Einstein summation. This framework encompasses many previously proposed structures, such as low-rank, Kronecker, Tensor-Train, and Monarch, along with many novel structures. We develop a taxonomy of all such operators based on their computational and algebraic properties, which provides insights into their scaling laws. Combining these insights with empirical evaluation, we identify a subset of structures that achieve equal or better performance than dense layers as a function of training compute. To further improve their compute efficiency, we develop a natural extension of these performant structures that convert them into a sparse Mixture-of-Experts layer. The resulting layer significantly outperforms dense layers in compute-optimal training efficiency for GPT-2 language models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-302" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-302', event_id='94133', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3504</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94133">Diffeomorphic interpolation for efficient persistence-based topological optimization</a></strong></h5>


                        <p class="text-muted">
                            Mathieu Carri√®re &middot; Marc Theveneau &middot; Th√©o Lacombe
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Topological Data Analysis (TDA) provides a pipeline to extract quantitative and powerful topological descriptors from structured objects. This enables the definition of topological loss functions, which assert to which extent a given object exhibits some topological properties. One can then use these losses to perform topological optimization via gradient descent routines. While theoretically sounded, topological optimization faces an important challenge: gradients tend to be extremely sparse, in the sense that the loss function typically depends (locally) on only very few coordinates of the input object, yielding dramatically slow optimization schemes in practice. In this work, focusing on the central case of topological optimization for point clouds, we propose to overcome this limitation using diffeomorphic interpolation, turning sparse gradients into smooth vector fields defined on the whole space. In particular, this approach combines efficiently with subsampling techniques routinely used in TDA, as the diffeomorphism derived from the gradient computed on the subsample can be used to update the coordinates of the full and possibly large input object. We then illustrate the usefulness of our approach on black-box autoencoder (AE) regularization, where we aim at applying some topological priors on the latent spaces associated to fixed, black-box AE models without modifying their (unknown) architectures and parameters. We empirically show that, while vanilla topological optimization has to be re-run every time that new data comes out of the black-box models, learning a diffeomorphic flow can be done once and then re-applied to new data in linear time. Moreover, reverting the flow allows us to generate data by sampling the topologically-optimized latent space directly, allowing for better interpretability of the model.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-303" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-303', event_id='96561', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3505</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96561">Supervised Kernel Thinning</a></strong></h5>


                        <p class="text-muted">
                            Albert Gong &middot; Kyuseong Choi &middot; Raaz Dwivedi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The kernel thinning algorithm of Dwivedi &amp; Mackey (2024) provides a better-than-i.i.d. compression of a generic set of points. By generating high-fidelity coresets of size significantly smaller than the input points, KT is known to speed up unsupervised tasks like Monte Carlo integration, uncertainty quantification, and non-parametric hypothesis testing, with minimal loss in statistical accuracy. In this work, we generalize the KT algorithm to speed up supervised learning problems involving kernel methods. Specifically, we combine two classical algorithms---Nadaraya-Watson (NW) regression or kernel smoothing, and kernel ridge regression (KRR)---with KT to provide a quadratic speed-up in both training and inference times. We show how distribution compression with KT in each setting reduces to constructing an appropriate kernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators. We prove that KT-based regression estimators enjoy significantly superior computational efficiency over the full-data estimators and improved statistical efficiency over i.i.d. subsampling of the training data. En route, we also provide a novel multiplicative error guarantee for compressing with KT.  We validate our design choices with both simulations and real data experiments.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-304" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-304', event_id='95272', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3506</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95272">Density-based User Representation using Gaussian Process Regression for Multi-interest Personalized Retrieval</a></strong></h5>


                        <p class="text-muted">
                            Haolun Wu &middot; Ofer Meshi &middot; Masrour Zoghi &middot; Fernando Diaz &middot; Xue (Steve) Liu &middot; Craig Boutilier &middot; Maryam Karimzadehgan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Accurate modeling of the diverse and dynamic interests of users remains a significant challenge in the design of personalized recommender systems. Existing user modeling methods, like single-point and multi-point representations, have limitations w.r.t.\ accuracy, diversity, and adaptability. To overcome these deficiencies, we introduce density-based user representations (DURs), a novel method that leverages Gaussian process regression (GPR) for effective multi-interest recommendation and retrieval. Our approach, GPR4DUR, exploits DURs to capture user interest variability without manual tuning, incorporates uncertainty-awareness, and scales well to large numbers of users. Experiments using real-world offline datasets confirm the adaptability and efficiency of GPR4DUR, while online experiments with simulated users demonstrate its ability to address the exploration-exploitation trade-off by effectively utilizing model uncertainty.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-305" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-305', event_id='95890', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3507</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95890">Neuronal Competition Groups with Supervised STDP for Spike-Based Classification</a></strong></h5>


                        <p class="text-muted">
                            Gaspard Goupy &middot; Pierre Tirilly &middot; Ioan Marius Bilasco
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Spike Timing-Dependent Plasticity (STDP) is a promising substitute to backpropagation for local training of Spiking Neural Networks (SNNs) on neuromorphic hardware. STDP allows SNNs to address classification tasks by combining unsupervised STDP for feature extraction and supervised STDP for classification. Unsupervised STDP is usually employed with Winner-Takes-All (WTA) competition to learn distinct patterns. However, WTA for supervised STDP classification faces unbalanced competition challenges. In this paper, we propose a method to effectively implement WTA competition in a spiking classification layer employing first-spike coding and supervised STDP training. We introduce the Neuronal Competition Group (NCG), an architecture that improves classification capabilities by promoting the learning of various patterns per class. An NCG is a group of neurons mapped to a specific class, implementing intra-class WTA and a novel competition regulation mechanism based on two-compartment thresholds. We incorporate our proposed architecture into spiking classification layers trained with state-of-the-art supervised STDP rules. On top of two different unsupervised feature extractors, we obtain significant accuracy improvements on image recognition datasets such as CIFAR-10 and CIFAR-100. We show that our competition regulation mechanism is crucial for ensuring balanced competition and improved class separation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-306" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-306', event_id='95474', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3508</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95474">Quantifying the Gain in Weak-to-Strong Generalization</a></strong></h5>


                        <p class="text-muted">
                            Moses Charikar &middot; Chirag Pabbaraju &middot; Kirankumar Shiragur
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advances in large language models have shown capabilities that are extraordinary and near-superhuman. These models operate with such complexity that reliably evaluating and aligning them proves challenging for humans. This leads to the natural question: can guidance from weak models (like humans) adequately direct the capabilities of strong models? In a recent and somewhat surprising work, Burns et al. (2023) empirically demonstrated that when strong models (like GPT-4) are finetuned using labels generated by weak supervisors (like GPT-2), the strong models outperform their weaker counterparts---a phenomenon they term <em>weak-to-strong generalization</em>.In this work, we present a theoretical framework for understanding weak-to-strong generalization. Specifically, we show that the improvement in performance achieved by strong models over their weaker counterparts is quantified by the <em>misfit error</em> incurred by the strong model on labels generated by the weaker model. Our theory reveals several curious algorithmic insights. For instance, we can predict the amount by which the strong model will improve over the weak model, and also choose among different weak models to train the strong model, based on its misfit error. We validate our theoretical findings through various empirical assessments.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-307" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-307', event_id='96793', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3509</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96793">Meta-Exploiting Frequency Prior for Cross-Domain Few-Shot Learning</a></strong></h5>


                        <p class="text-muted">
                            Fei Zhou &middot; Peng Wang &middot; Lei Zhang &middot; Zhenghua Chen &middot; Wei Wei &middot; Chen Ding &middot; Guosheng Lin &middot; Yanning Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Meta-learning offers a promising avenue for few-shot learning (FSL), enabling models to glean a generalizable feature embedding through episodic training on synthetic FSL tasks in a source domain. Yet, in practical scenarios where the target task diverges from that in the source domain, meta-learning based method is susceptible to over-fitting. To overcome this, we introduce a novel framework, Meta-Exploiting Frequency Prior for Cross-Domain Few-Shot Learning, which is crafted to comprehensively exploit the cross-domain transferable image prior that each image can be decomposed into complementary low-frequency content details and high-frequency robust structural characteristics. Motivated by this insight, we propose to decompose each query image into its high-frequency and low-frequency components, and parallel incorporate them into the feature embedding network to enhance the final category prediction. More importantly, we introduce a feature reconstruction prior and a prediction consistency prior to separately encourage the consistency of the intermediate feature as well as the final category prediction between the original query image and its decomposed frequency components. This allows for collectively guiding the network's meta-learning process with the aim of learning generalizable image feature embeddings, while not introducing any extra computational cost in the inference phase. Our framework establishes new state-of-the-art results on multiple cross-domain few-shot learning benchmarks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-308" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-308', event_id='96402', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3510</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96402">Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging</a></strong></h5>


                        <p class="text-muted">
                            Zhenyi Lu &middot; Chenghao Fan &middot; Wei Wei &middot; Xiaoye Qu &middot; Dangyang Chen &middot; Yu Cheng
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on $20$ datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-309" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-309', event_id='96121', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3511</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96121">A Metalearned Neural Circuit for Nonparametric Bayesian Inference</a></strong></h5>


                        <p class="text-muted">
                            Jake Snell &middot; Gianluca Bencomo &middot; Tom Griffiths
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Most applications of machine learning to classification assume a closed set of balanced classes. This is at odds with the real world, where class occurrence statistics often follow a long-tailed power-law distribution and it is unlikely that all classes are seen in a single sample. Nonparametric Bayesian models naturally capture this phenomenon, but have significant practical barriers to widespread adoption, namely implementation complexity and computational inefficiency. To address this, we present a method for extracting the inductive bias from a nonparametric Bayesian model and transferring it to an artificial neural network. By simulating data with a nonparametric Bayesian prior, we can metalearn a sequence model that performs inference over an unlimited set of classes. After training, this "neural circuit" has distilled the corresponding inductive bias and can successfully perform sequential inference over an open set of classes. Our experimental results show that the metalearned neural circuit achieves comparable or better performance than particle filter-based methods for inference in these models while being faster and simpler to use than methods that explicitly incorporate Bayesian nonparametric inference.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-310" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-310', event_id='94795', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3600</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94795">Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion</a></strong></h5>


                        <p class="text-muted">
                            Ye He &middot; Kevin Rojas &middot; Molei Tao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Denoising Diffusion Monte Carlo (DDMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DDMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DDMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DDMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit still suffering from the curse of dimensionality. Consequently, for low dimensional distributions, ZOD-MC is a very efficient sampler, with performance exceeding latest samplers, including also-denoising-diffusion-based RDMC and RSDMC. Last, we experimentally demonstrate the insensitivity of ZOD-MC to increasingly higher barriers between modes or discontinuity in non-convex potential.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-311" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-311', event_id='97799', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3601</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97799">NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples</a></strong></h5>


                        <p class="text-muted">
                            Baiqi Li &middot; Zhiqiu Lin &middot; WENXUAN PENG &middot; Jean de Dieu Nyandwi &middot; Daniel Jiang &middot; Zixian Ma &middot; Simran Khanuja &middot; Ranjay Krishna &middot; Graham Neubig &middot; Deva Ramanan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new NaturalBench benchmark for reliably evaluating VLMs with over 10,000 human-verified VQA samples. Crucially, we adopt a vision-centric design by pairing each question with two images that yield different answers, preventing ``blind'' solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can largely be solved with language priors like commonsense knowledge. Popular VLMs like InstructBLIP, LLaVA-NeXT, ShareGPT4V, and XGen-MM (BLIP-3) only achieve 1%-15% above random chance performance. Even the best (closed-source) GPT-4o lags significantly behind human performance (which is above 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. We show that debiasing can be crucial for VLM performance. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-312" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-312', event_id='93787', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3602</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93787">Unified Domain Generalization and Adaptation for Multi-View 3D Object Detection</a></strong></h5>


                        <p class="text-muted">
                            Gyusam Chang &middot; Jiwon Lee &middot; Donghyun Kim &middot; Jinkyu Kim &middot; Dongwook Lee &middot; Daehyun Ji &middot; Sujin Jang &middot; Sangpil Kim
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Recent advances in 3D object detection leveraging multi-view cameras have demonstrated their practical and economical value in various challenging vision tasks.However, typical supervised learning approaches face challenges in achieving satisfactory adaptation toward unseen and unlabeled target datasets (i.e., direct transfer) due to the inevitable geometric misalignment between the source and target domains.In practice, we also encounter constraints on resources for training models and collecting annotations for the successful deployment of 3D object detectors.In this paper, we propose Unified Domain Generalization and Adaptation (UDGA), a practical solution to mitigate those drawbacks.We first propose Multi-view Overlap Depth Constraint that leverages the strong association between multi-view, significantly alleviating geometric gaps due to perspective view changes.Then, we present a Label-Efficient Domain Adaptation approach to handle unfamiliar targets with significantly fewer amounts of labels (i.e., 1$\%$ and 5$\%)$, while preserving well-defined source knowledge for training efficiency.Overall, UDGA framework enables stable detection performance in both source and target domains, effectively bridging inevitable domain gaps, while demanding fewer annotations.We demonstrate the robustness of UDGA with large-scale benchmarks: nuScenes, Lyft, and Waymo, where our framework outperforms the current state-of-the-art methods.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-313" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-313', event_id='93810', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3603</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93810">(FL)$^2$: Overcoming Few Labels in Federated Semi-Supervised Learning</a></strong></h5>


                        <p class="text-muted">
                            Seungjoo Lee &middot; Thanh-Long V. Le &middot; Jaemin Shin &middot; Sung-Ju Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Federated Learning (FL) is a distributed machine learning framework that trains accurate global models while preserving clients' privacy-sensitive data. However, most FL approaches assume that clients possess labeled data, which is often not the case in practice. Federated Semi-Supervised Learning (FSSL) addresses this label deficiency problem, targeting situations where only the server has a small amount of labeled data while clients do not. However, a significant performance gap exists between Centralized Semi-Supervised Learning (SSL) and FSSL. This gap arises from confirmation bias, which is more pronounced in FSSL due to multiple local training epochs and the separation of labeled and unlabeled data. We propose $(FL)^2$, a robust training method for unlabeled clients using sharpness-aware consistency regularization. We show that regularizing the original pseudo-labeling loss is suboptimal, and hence we carefully select unlabeled samples for regularization. We further introduce client-specific adaptive thresholding and learning status-aware aggregation to adjust the training process based on the learning progress of each client. Our experiments on three benchmark datasets demonstrate that our approach significantly improves performance and bridges the gap with SSL, particularly in scenarios with scarce labeled data.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-314" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-314', event_id='95634', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3604</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95634">QBB: Quantization with Binary Bases for LLMs</a></strong></h5>


                        <p class="text-muted">
                            Adrian Bulat &middot; Yassine Ouali &middot; Georgios Tzimiropoulos
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Current post-training quantization methods for LLMs compress the weights down to 4-bits, with moderate to low degradation in accuracy. However, further reducing the number of bits or accelerating the network while avoiding large accuracy drops, especially for smaller, sub 7B models, remains an actively researched and open problem. To address this, in this work, we introduce Quantization with Binary Bases (QBB), a new approach for low-bit quantization that effectively removes (nearly) all multiplications, reducing the implementation to summations. Our novel approach works by decomposing the original weights into a set of binary (1-bit) matrices using an iterative process. For a given layer, starting from a weight matrix, we first construct an initial approximation using an analytical solution, where each new binary matrix, paired with a scaling vector, approximates the residual error of the previous estimation. Secondly, using gradient descent and a progressive learning curriculum, we find the optimal set of binary matrices and scaling vectors that minimize the $\ell_2$ distance between the produced approximation and original weights. Thirdly, as previous steps are input agnostic, we holistically optimize the scaling vectors alone, calibrating them in student-teacher fashion, with the teacher providing both the data,  by autoregressive generation starting from a random token, and the target logits. When evaluated across multiple LLM families, our approach matches and outperforms all prior works, setting a new state-of-the-art result using a summation-only based approach.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-315" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-315', event_id='96821', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3605</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96821">On the cohesion and separability of average-link for hierarchical agglomerative clustering</a></strong></h5>


                        <p class="text-muted">
                            Eduardo Laber &middot; Miguel Batista
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Average-link is widely recognized as one of the most popular and effective methods for building hierarchical agglomerative clustering. The available theoretical analyses show that this method has a much better approximation than other popular heuristics, as single-linkage and complete-linkage, regarding variants of Dasgupta's cost function [STOC 2016]. However, these analyses do not separate average-link from a random hierarchy and they are not appealing for metric spaces since every hierarchical clustering has a $1/2$  approximation with regard to the variant of Dasgupta's functionthat is employed for dissimilarity measures [Moseley and Yang 2020]. In this paper, we present a comprehensive study of the performance of \avglink \, in metric spaces, regarding several natural criteria that capture separability and cohesion, and are more interpretable than Dasgupta's cost function and its variants.  We also present experimental results with real datasets that, together with our theoretical analyses, suggest that average-link is a better choice than other related methods when both cohesion and separability are important goals.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-316" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-316', event_id='94429', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3606</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94429">Boosting Transferability and Discriminability for Time Series Domain Adaptation</a></strong></h5>


                        <p class="text-muted">
                            Mingyang Liu &middot; Xinyang Chen &middot; Yang Shu &middot; Xiucheng Li &middot; Weili Guan &middot; Liqiang Nie
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Unsupervised domain adaptation excels in transferring knowledge from a labeled source domain to an unlabeled target domain, playing a critical role in time series applications. Existing time series domain adaptation methods either ignore frequency features or treat temporal and frequency features equally, which makes it challenging to fully exploit the advantages of both types of features. In this paper, we delve into transferability and discriminability, two crucial properties in transferable representation learning. It's insightful to note that frequency features are more discriminative within a specific domain, while temporal features show better transferability across domains. Based on the findings, we propose <strong>A</strong>dversarial <strong>CO</strong>-learning <strong>N</strong>etworks (<strong>ACON</strong>), to enhance transferable representation learning through a collaborative learning manner in three aspects: (1) Considering the multi-periodicity in time series, multi-period frequency feature learning is proposed to enhance the discriminability of frequency features; (2) Temporal-frequency domain mutual learning is proposed to enhance the discriminability of temporal features in the source domain and improve the transferability of frequency features in the target domain; (3) Domain adversarial learning is conducted in the correlation subspaces of temporal-frequency features instead of original feature spaces to further enhance the transferability of both features. Extensive experiments conducted on a wide range of time series datasets and five common applications demonstrate the state-of-the-art performance of ACON. Code is available at <a href="https://github.com/mingyangliu1024/ACON">https://github.com/mingyangliu1024/ACON</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-317" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-317', event_id='94447', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3607</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94447">$\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning</a></strong></h5>


                        <p class="text-muted">
                            Runqian Wang &middot; Soumya Ghosh &middot; David Cox &middot; Diego Antognini &middot; Aude Oliva &middot; Rogerio Feris &middot; Leonid Karlinsky
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Low-rank adapters (LoRA) and their variants are popular parameter-efficient fine-tuning (PEFT) techniques that closely match full model fine-tune performance while requiring only a small number of additional parameters. These additional LoRA parameters are specific to the base model being adapted. When the base model needs to be deprecated and replaced with a new one, all the associated LoRA modules need to be re-trained. Such re-training requires access to the data used to train the LoRA for the original base model. This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data. To address this challenge, we propose $\textit{Trans-LoRA}$ --- a novel method for lossless, nearly data-free transfer of LoRAs across base models. Our approach relies on synthetic data to transfer LoRA modules. Using large language models, we design a synthetic data generator to approximate the data-generating process of the $\textit{observed}$ task data subset. Training on the resulting synthetic dataset transfers LoRA modules to new models. We show the effectiveness of our approach using both LLama and Gemma model families. Our approach achieves lossless (mostly improved) LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-318" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-318', event_id='94891', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3608</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94891">Learning from Offline Foundation Features with Tensor Augmentations</a></strong></h5>


                        <p class="text-muted">
                            Emir Konuk &middot; Christos Matsoukas &middot; Moein Sorkhei &middot; Phitchapha Lertsiravarameth &middot; Kevin Smith
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We introduce Learning from Offline Foundation Features with Tensor Augmentations (LOFF-TA), an efficient training scheme designed to harness the capabilities of foundation models in limited resource settings where their direct development is not feasible. LOFF-TA involves training a compact classifier on cached feature embeddings  from a frozen foundation model, resulting in up to  $37\times$ faster training and up to $26\times$ reduced GPU memory usage. Because the embeddings of augmented images would be too numerous to store, yet the augmentation process is essential for training, we propose to apply tensor augmentations to the cached embeddings of the original non-augmented images. LOFF-TA makes it possible to leverage the power of foundation models, regardless of their size, in settings with limited computational capacity. Moreover, LOFF-TA can be used to apply foundation models to high-resolution images without increasing compute.    In certain scenarios, we find that training with LOFF-TA yields better results than directly fine-tuning the foundation model.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-319" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-319', event_id='94914', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3609</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94914">Learning Where to Edit Vision Transformers</a></strong></h5>


                        <p class="text-muted">
                            Yunqiao Yang &middot; Long-Kai Huang &middot; Shengzhuang Chen &middot; Kede Ma &middot; Ying Wei
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Model editing aims to data-efficiently correct predictive errors of large pre-trained models while ensuring generalization to neighboring failures and locality to minimize unintended effects on unrelated examples. While significant progress has been made in editing Transformer-based large language models, effective strategies for editing vision Transformers (ViTs) in computer vision remain largely untapped. In this paper, we take initial steps towards correcting predictive errors of ViTs, particularly those arising from subpopulation shifts. Taking a locate-then-edit approach, we first address the <code>where-to-edit</code> challenge by meta-learning a hypernetwork on CutMix-augmented data generated for editing reliability. This trained hypernetwork produces generalizable binary masks that identify a sparse subset of structured model parameters,  responsive to real-world failure samples. Afterward, we solve the <code>how-to-edit</code> problem by simply fine-tuning the identified parameters using a variant of gradient descent to achieve successful edits. To validate our method, we construct an editing benchmark that introduces subpopulation shifts towards natural underrepresented images and AI-generated images, thereby revealing the limitations of pre-trained ViTs for object recognition. Our approach not only achieves superior performance on the proposed benchmark but also allows for adjustable trade-offs between generalization and locality. Our code is available at https://github.com/hustyyq/Where-to-Edit.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-320" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-320', event_id='95353', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3610</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95353">Zero-Shot Transfer of Neural ODEs</a></strong></h5>


                        <p class="text-muted">
                            Tyler Ingebrand &middot; Adam Thorpe &middot; Ufuk Topcu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Autonomous systems often encounter environments and scenarios beyond the scope of their training data, which underscores a critical challenge: the need to generalize and adapt to unseen scenarios in real time. This challenge necessitates new mathematical and algorithmic tools that enable adaptation and zero-shot transfer. To this end, we leverage the theory of function encoders, which enables zero-shot transfer by combining the flexibility of neural networks with the mathematical principles of Hilbert spaces. Using this theory, we first present a method for learning a space of dynamics spanned by a set of neural ODE basis functions. After training, the proposed approach can rapidly identify dynamics in the learned space using an efficient inner product calculation. Critically, this calculation requires no gradient calculations or retraining during the online phase. This method enables zero-shot transfer for autonomous systems at runtime and opens the door for a new class of adaptable control algorithms. We demonstrate state-of-the-art system modeling accuracy for two MuJoCo robot environments and show that the learned models can be used for more efficient MPC control of a quadrotor.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-321" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-321', event_id='95759', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3611</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95759">DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity</a></strong></h5>


                        <p class="text-muted">
                            Baekrok Shin &middot; Junsoo Oh &middot; Hanseul Cho &middot; Chulhee Yun
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Warm-starting neural network training by initializing networks with previously learned weights is appealing, as practical neural networks are often deployed under a continuous influx of new data. However, it often leads to <em>loss of plasticity</em>, where the network loses its ability to learn new information, resulting in worse generalization than training from scratch. This occurs even under stationary data distributions, and its underlying mechanism is poorly understood. We develop a framework emulating real-world neural network training and identify noise memorization as the primary cause of plasticity loss when warm-starting on stationary data. Motivated by this, we propose <strong>Direction-Aware SHrinking (DASH)</strong>, a method aiming to mitigate plasticity loss by selectively forgetting memorized noise while preserving learned features. We validate our approach on vision tasks, demonstrating improvements in test accuracy and training efficiency.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-322" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-322', event_id='97575', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3700</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97575">SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers</a></strong></h5>


                        <p class="text-muted">
                            Shraman Pramanick &middot; Rama Chellappa &middot; Subhashini Venugopalan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Seeking answers to questions within long scientific research articles is a crucial area of study that aids readers in quickly addressing their inquiries. However, existing question-answering (QA) datasets based on scientific papers are limited in scale and focus solely on textual content. To address this limitation, we introduce SPIQA (Scientific Paper Image Question Answering), the first large-scale QA dataset specifically designed to interpret complex figures and tables within the context of scientific research articles across various domains of computer science. Leveraging the breadth of expertise and ability of multimodal large language models (MLLMs) to understand figures, we employ automatic and manual curation to create the dataset. We craft an information-seeking task involving multiple images that cover a wide variety of plots, charts, tables, schematic diagrams, and result visualizations. SPIQA comprises 270K questions divided into training, validation, and three different evaluation splits. Through extensive experiments with 12 prominent foundational models, we evaluate the ability of current multimodal systems to comprehend the nuanced aspects of research articles. Additionally, we propose a Chain-of-Thought (CoT) evaluation strategy with in-context retrieval that allows fine-grained, step-by-step assessment and improves model performance. We further explore the upper bounds of performance enhancement with additional textual information, highlighting its promising potential for future research and the dataset‚Äôs impact on revolutionizing how we interact with scientific literature.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-323" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-323', event_id='97456', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3701</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97456">MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning</a></strong></h5>


                        <p class="text-muted">
                            Yifan Jiang &middot; jiarui zhang &middot; Kexuan Sun &middot; Zhivar Sourati Hassan  Zadeh &middot; Kian Ahrabian &middot; Kaixin Ma &middot; Filip Ilievski &middot; Jay Pujara
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While multi-modal large language models (MLLMs) have shown significant progress across popular visual reasoning benchmarks, whether they possess abstract visual reasoning abilities remains an open question. Similar to the Sudoku puzzles, abstract visual reasoning (AVR) problems require finding high-level patterns (e.g., repetition constraints on numbers) that control the input shapes (e.g., digits) in a specific task configuration (e.g., matrix). However, existing AVR benchmarks only consider a limited set of patterns (addition, conjunction), input shapes (rectangle, square), and task configurations (3 √ó 3 matrices). And they fail to capture all abstract reasoning patterns in human cognition necessary for addressing real-world tasks, such as geometric properties and object boundary understanding in real-world navigation. To evaluate MLLMs‚Äô AVR abilities systematically, we introduce MARVEL founded on the core knowledge system in human cognition, a multi-dimensional AVR benchmark with 770 puzzles composed of six core knowledge patterns, geometric and abstract shapes, and five different task configurations. To inspect whether the model performance is grounded in perception or reasoning, MARVEL complements the standard AVR question with perception questions in a hierarchical evaluation framework. We conduct comprehensive experiments on MARVEL with ten representative MLLMs in zero-shot and few-shot settings. Our experiments reveal that all MLLMs show near-random performance on MARVEL, with significant performance gaps (40%) compared to humans across all patterns and task configurations. Further analysis of perception questions reveals that MLLMs struggle to comprehend the visual features (near-random performance). Although closed-source MLLMs, such as GPT-4V, show a promising understanding of reasoning patterns (on par with humans) after adding textual descriptions, this advantage is hindered by their weak perception abilities. We release our entirecode and dataset at https://github.com/1171-jpg/MARVEL_AVR.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-324" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-324', event_id='96251', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3702</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96251">Probabilistic Conformal Distillation for Enhancing Missing Modality Robustness</a></strong></h5>


                        <p class="text-muted">
                            Mengxi Chen &middot; Fei Zhang &middot; Zihua Zhao &middot; Jiangchao Yao &middot; Ya Zhang &middot; Yanfeng Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multimodal models trained on modality-complete data are plagued with severe performance degradation when encountering modality-missing data. Prevalent cross-modal knowledge distillation-based methods precisely align the representation of modality-missing data and that of its modality-complete counterpart to enhance robustness. However, due to the irreparable information asymmetry, this determinate alignment is too stringent, easily inducing modality-missing features to capture spurious factors erroneously. In this paper, a novel multimodal Probabilistic Conformal Distillation (PCD) method is proposed, which considers the inherent indeterminacy in this alignment. Given a modality-missing input, our goal is to learn the unknown Probability Density Function (PDF) of the mapped variables in the modality-complete space, rather than relying on the brute-force point alignment. Specifically, PCD models the modality-missing feature as a probabilistic distribution, enabling it to satisfy two characteristics of the PDF. One is the extremes of probabilities of modality-complete feature points on the PDF, and the other is the geometric consistency between the modeled distributions and the peak points of different PDFs. Extensive experiments on a range of benchmark datasets demonstrate the superiority of PCD over state-of-the-art methods. Code is available at: https://github.com/mxchen-mc/PCD.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-325" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-325', event_id='95960', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3704</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95960">Artemis:  Towards Referential Understanding in Complex Videos</a></strong></h5>


                        <p class="text-muted">
                            Jihao Qiu &middot; Yuan Zhang &middot; Xi Tang &middot; Lingxi Xie &middot; Tianren Ma &middot; Pengyu Yan &middot; DAVID DOERMANN &middot; Qixiang Ye &middot; Yunjie Tian
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Videos carry rich visual information including object description, action, interaction, etc., but the existing multimodal large language models (MLLMs) fell short in referential understanding scenarios such as video-based referring. In this paper, we present Artemis, an MLLM that pushes video-based referential understanding to a finer level. Given a video, Artemis receives a natural-language question with a bounding box in any video frame and describes the referred target in the entire video. The key to achieving this goal lies in extracting compact, target-specific video features, where we set a solid baseline by tracking and selecting spatiotemporal features from the video. We train Artemis on the newly established ViderRef45K dataset with 45K video-QA pairs and design a computationally efficient, three-stage training procedure. Results are promising both quantitatively and qualitatively. Additionally, we show that Artemis can be integrated with video grounding and text summarization tools to understand more complex scenarios. Code and data are available at https://github.com/NeurIPS24Artemis/Artemis.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-326" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-326', event_id='95829', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3705</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95829">HEALNet: Multimodal Fusion for Heterogeneous Biomedical Data</a></strong></h5>


                        <p class="text-muted">
                            Konstantin Hemker &middot; Nikola Simidjievski &middot; Mateja Jamnik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Technological advances in medical data collection, such as high-throughput genomic sequencing and digital high-resolution histopathology, have contributed to the rising requirement for multimodal biomedical modelling, specifically for image, tabular and graph data. Most multimodal deep learning approaches use modality-specific architectures that are often trained separately and cannot capture the crucial cross-modal information that motivates the integration of different data sources. This paper presents the <strong>H</strong>ybrid <strong>E</strong>arly-fusion <strong>A</strong>ttention <strong>L</strong>earning <strong>Net</strong>work (HEALNet) ‚Äì a flexible multimodal fusion architecture, which: a) preserves modality-specific structural information, b) captures the cross-modal interactions and structural information in a shared latent space, c) can effectively handle missing modalities during training and inference, and d) enables intuitive model inspection by learning on the raw data input instead of opaque embeddings. We conduct multimodal survival analysis on Whole Slide Images and Multi-omic data on four cancer datasets from The Cancer Genome Atlas (TCGA). HEALNet achieves state-of-the-art performance compared to other end-to-end trained fusion models, substantially improving over unimodal and multimodal baselines whilst being robust in scenarios with missing modalities. The code is available at https://github.com/konst-int-i/healnet.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-327" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-327', event_id='95751', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3706</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95751">Dense Connector for MLLMs</a></strong></h5>


                        <p class="text-muted">
                            Huanjin Yao &middot; Wenhao Wu &middot; Taojiannan Yang &middot; YuXin Song &middot; Mengxi Zhang &middot; Haocheng Feng &middot; Yifan Sun &middot; Zhiheng Li &middot; Wanli Ouyang &middot; Jingdong Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p><em>Do we fully leverage the potential of visual encoder in Multimodal Large Language Models (MLLMs)?</em> The recent outstanding performance of MLLMs in multimodal understanding has garnered broad attention from both academia and industry. In the current MLLM rat race, the focus seems to be predominantly on the linguistic side. We witness the rise of larger and higher-quality instruction datasets, as well as the involvement of larger-sized LLMs. Yet, scant attention has been directed towards the visual signals utilized by MLLMs, often assumed to be the final high-level features extracted by a frozen visual encoder. In this paper, we introduce the <strong>Dense Connector</strong> - a simple, effective, and plug-and-play vision-language connector that significantly enhances existing MLLMs by leveraging multi-layer visual features, with minimal additional computational overhead. Building on this, we also propose the Efficient Dense Connector, which achieves performance comparable to LLaVA-v1.5 with only 25% of the visual tokens. Furthermore, our model, trained solely on images, showcases remarkable zero-shot capabilities in video understanding as well. Experimental results across various vision encoders, image resolutions, training dataset scales, varying sizes of LLMs (2.7B‚Üí70B), and diverse architectures of MLLMs (e.g., LLaVA-v1.5, LLaVA-NeXT and Mini-Gemini) validate the versatility and scalability of our approach, achieving state-of-the-art performance across 19 image and video benchmarks. We hope that this work will provide valuable experience and serve as a basic module for future MLLM development. Code is available at https://github.com/HJYao00/DenseConnector.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-328" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-328', event_id='95720', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3707</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95720">SpatialRGPT: Grounded Spatial Reasoning in Vision-Language Models</a></strong></h5>


                        <p class="text-muted">
                            An-Chieh Cheng &middot; Hongxu Yin &middot; Yang Fu &middot; Qiushan Guo &middot; Ruihan Yang &middot; Jan Kautz &middot; Xiaolong Wang &middot; Sifei Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Vision Language Models (VLMs) have demonstrated remarkable performance in 2D vision and language tasks. However, their ability to reason about spatial arrangements remains limited. In this work, we introduce Spatial Region GPT (SpatialRGPT) to enhance VLMs‚Äô spatial perception and reasoning capabilities. SpatialRGPT advances VLMs‚Äô spatial understanding through two key innovations: (i) a data curation pipeline that enables effective learning of regional representation from 3D scene graphs, and (ii) a flexible ``plugin'' module for integrating depth information into the visual encoder of existing VLMs. During inference, when provided with user-specified region proposals, SpatialRGPT can accurately perceive their relative directions and distances. Additionally, we propose SpatialRGBT-Bench, a benchmark with ground-truth 3D annotations encompassing indoor, outdoor, and simulated environments, for evaluating 3D spatial cognition in Vision-Language Models (VLMs). Our results demonstrate that SpatialRGPT significantly enhances performance in spatial reasoning tasks, both with and without local region prompts. The model also exhibits strong generalization capabilities, effectively reasoning about complex spatial relations and functioning as a region-aware dense reward annotator for robotic tasks. Code, dataset, and benchmark are released at https://www.anjiecheng.me/SpatialRGPT.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-329" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-329', event_id='95713', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3708</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95713">Visual Perception by Large Language Model‚Äôs Weights</a></strong></h5>


                        <p class="text-muted">
                            Feipeng Ma &middot; Hongwei Xue &middot; Yizhou Zhou &middot; Guangting Wang &middot; Fengyun Rao &middot; Shilin Yan &middot; Yueyi Zhang &middot; Siying Wu &middot; Mike Zheng Shou &middot; Xiaoyan Sun
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Existing Multimodal Large Language Models (MLLMs) follow the paradigm that perceives visual information by aligning visual features with the input space of Large Language Models (LLMs) and concatenating visual tokens with text tokens to form a unified sequence input for LLMs. These methods demonstrate promising results on various vision-language tasks but are limited by the high computational effort due to the extended input sequence resulting from the involvement of visual tokens. In this paper, instead of input space alignment, we propose a novel parameter space alignment paradigm that represents visual information as model weights. For each input image, we use a vision encoder to extract visual features, convert features into perceptual weights, and merge the perceptual weights with LLM's weights. In this way, the input of LLM does not require visual tokens, which reduces the length of the input sequence and greatly improves efficiency. Following this paradigm, we propose VLoRA with the perceptual weights generator. The perceptual weights generator is designed to convert visual features to perceptual weights with low-rank property, exhibiting a form similar to LoRA. The experimental results show that our VLoRA achieves comparable performance on various benchmarks for MLLMs, while significantly reducing the computational costs for both training and inference. Code and models are released at \url{https://github.com/FeipengMa6/VLoRA}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-330" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-330', event_id='95674', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3709</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95674">Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight</a></strong></h5>


                        <p class="text-muted">
                            Ziyuan Huang &middot; Kaixiang Ji &middot; Biao Gong &middot; Zhiwu Qing &middot; Qinglong Zhang &middot; Kecheng Zheng &middot; Jian Wang &middot; Jingdong Chen &middot; Ming Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>This paper introduces Chain-of-Sight, a vision-language bridge module that accelerates the pre-training of Multimodal Large Language Models (MLLMs). Our approach employs a sequence of visual resamplers that capture visual details at various spacial scales.This architecture not only leverages global and local visual contexts effectively, but also facilitates the flexible extension of visual tokens through a compound token scaling strategy, allowing up to a 16x increase in the token count post pre-training.Consequently, Chain-of-Sight requires significantly fewer visual tokens in the pre-training phase compared to the fine-tuning phase. This intentional reduction of visual tokens during pre-training notably accelerates the pre-training process, cutting down the wall-clock training time by $\sim$73\%.Empirical results on a series of vision-language benchmarks reveal that the pre-train acceleration through Chain-of-Sight is achieved without sacrificing performance, matching or surpassing the standard pipeline of utilizing all visual tokens throughout the entire training process. Further scaling up the number of visual tokens for pre-training leads to stronger performances, competitive to existing approaches in a series of benchmarks.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-331" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-331', event_id='95478', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3710</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95478">Why are Visually-Grounded Language Models Bad at Image Classification?</a></strong></h5>


                        <p class="text-muted">
                            Yuhui Zhang &middot; Alyssa Unell &middot; Xiaohan Wang &middot; Dhruba Ghosh &middot; Yuchang Su &middot; Ludwig Schmidt &middot; Serena Yeung
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We find that existing proprietary and public VLMs, despite often using CLIP as a vision encoder and having many more parameters, significantly underperform CLIP on standard image classification benchmarks like ImageNet. To understand the reason, we explore several hypotheses concerning the inference algorithms, training objectives, and data processing in VLMs. Our analysis reveals that the primary cause is data-related: critical information for image classification is encoded in the VLM's latent space but can only be effectively decoded with enough training data. Specifically, there is a strong correlation between the frequency of class exposure during VLM training and instruction-tuning and the VLM's performance in those classes; when trained with sufficient data, VLMs can match the accuracy of state-of-the-art classification models. Based on these findings, we enhance a VLM by integrating classification-focused datasets into its training, and demonstrate that the enhanced classification performance of the VLM transfers to its general capabilities, resulting in an improvement of 11.8% on the newly collected ImageWikiQA dataset.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-332" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-332', event_id='95301', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3711</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95301">Continual Audio-Visual Sound Separation</a></strong></h5>


                        <p class="text-muted">
                            Weiguo Pian &middot; Yiyang Nan &middot; Shijian Deng &middot; Shentong Mo &middot; Yunhui Guo &middot; Yapeng Tian
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In this paper, we introduce a novel continual audio-visual sound separation task, aiming to continuously separate sound sources for new classes while preserving performance on previously learned classes, with the aid of visual guidance. This problem is crucial for practical visually guided auditory perception as it can significantly enhance the adaptability and robustness of audio-visual sound separation models, making them more applicable for real-world scenarios where encountering new sound sources is commonplace. The task is inherently challenging as our models must not only effectively utilize information from both modalities in current tasks but also preserve their cross-modal association in old tasks to mitigate catastrophic forgetting during audio-visual continual learning. To address these challenges, we propose a novel approach named ContAV-Sep ($\textbf{Cont}$inual $\textbf{A}$udio-$\textbf{V}$isual Sound $\textbf{Sep}$aration). ContAV-Sep presents a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the cross-modal semantic similarity through incremental tasks and retain previously acquired knowledge of semantic similarity in old models, mitigating the risk of catastrophic forgetting. The CrossSDC can seamlessly integrate into the training process of different audio-visual sound separation frameworks. Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic forgetting and achieve significantly better performance compared to other continual learning baselines for audio-visual sound separation. Code is available at: https://github.com/weiguoPian/ContAV-Sep_NeurIPS2024.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-333" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-333', event_id='93212', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3800</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93212">RSA: Resolving Scale Ambiguities in Monocular Depth Estimators through Language Descriptions</a></strong></h5>


                        <p class="text-muted">
                            Ziyao Zeng &middot; Yangchao Wu &middot; Hyoungseob Park &middot; Daniel Wang &middot; Fengyu Yang &middot; Stefano Soatto &middot; DONG LAO &middot; Byung-Woo Hong &middot; Alex Wong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose a method for metric-scale monocular depth estimation. Inferring depth from a single image is an ill-posed problem due to the loss of scale from perspective projection during the image formation process. Any scale chosen is a bias, typically stemming from training on a dataset; hence, existing works have instead opted to use relative (normalized, inverse) depth. Our goal is to recover metric-scaled depth maps through a linear transformation. The crux of our method lies in the observation that certain objects (e.g., cars, trees, street signs) are typically found or associated with certain types of scenes (e.g., outdoor). We explore whether language descriptions can be used to transform relative depth predictions to those in metric scale. Our method, RSA , takes as input a text caption describing objects present in an image and outputs the parameters of a linear transformation which can be applied globally to a relative depth map to yield metric-scaled depth predictions. We demonstrate our method on recent general-purpose monocular depth models on indoors (NYUv2, VOID) and outdoors (KITTI). When trained on multiple datasets, RSA can serve as a general alignment module in zero-shot settings. Our method improves over common practices in aligning relative to metric depth and results in predictions that are comparable to an upper bound of fitting relative depth to ground truth via a linear transformation. Code is available at: https://github.com/Adonis-galaxy/RSA.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-334" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-334', event_id='93219', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3801</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93219">HAWK: Learning to Understand Open-World Video Anomalies</a></strong></h5>


                        <p class="text-muted">
                            Jiaqi Tang &middot; Hao LU &middot; RUIZHENG WU &middot; Xiaogang Xu &middot; Ke Ma &middot; Cheng Fang &middot; Bin Guo &middot; Jiangbo Lu &middot; Qifeng Chen &middot; Yingcong Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs. However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction. Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios.In this paper, we introduce HAWK, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely. Recognizing the difference in motion information between abnormal and normal videos, HAWK explicitly integrates motion modality to enhance anomaly identification. To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality. Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation. Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions. The final results demonstrate that HAWK achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering. Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-335" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-335', event_id='93362', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3802</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93362">Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers</a></strong></h5>


                        <p class="text-muted">
                            Haifeng Huang &middot; Yilun Chen &middot; Zehan Wang &middot; Rongjie Huang &middot; Runsen Xu &middot; Tai WANG &middot; Luping Liu &middot; Xize Cheng &middot; Yang Zhao &middot; Jiangmiao Pang &middot; Zhou Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advancements in 3D Large Language Models (LLMs) have demonstrated promising capabilities for 3D scene understanding. However, previous methods exhibit deficiencies in general referencing and grounding capabilities for intricate scene comprehension. In this paper, we introduce the use of object identifiers and object-centric representations to interact with scenes at the object level. Specifically, we decompose the input 3D scene into a set of object proposals, each assigned a unique identifier token, which enables efficient object referencing and grounding during user-assistant interactions. Given the scarcity of scene-language data, we model the scene embeddings as a sequence of explicit object-level embeddings, derived from semantic-rich 2D or 3D representations. By employing object identifiers, we transform diverse 3D scene-language tasks into a unified question-answering format, facilitating joint training without the need for additional task-specific heads. With minimal fine-tuning on all downstream tasks, our model significantly outperforms existing methods on benchmarks including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-336" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-336', event_id='93497', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3803</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93497">Empowering Visible-Infrared Person Re-Identification with Large Foundation Models</a></strong></h5>


                        <p class="text-muted">
                            Zhangyi Hu &middot; Bin Yang &middot; Mang Ye
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Visible-Infrared Person Re-identification (VI-ReID) is a challenging cross-modal retrieval task due to significant modality differences, primarily caused by the absence of detailed color information in the infrared modality. The development of large foundation models like Large Language Models (LLMs) and Vision Language Models (VLMs) motivates us to investigate a feasible solution to empower VI-ReID performance with off-the-shelf large foundation models. To this end, we propose a novel Text-enhanced VI-ReID framework driven by Large Foundation Models (TVI-LFM). The basic idea is to enrich the representation of the infrared modality with textual descriptions automatically generated by VLMs. Specifically, we incorporate a pre-trained VLM to extract textual features from texts generated by VLM and augmented by LLM, and incrementally fine-tune the text encoder to minimize the domain gap between generated texts and original visual modalities. Meanwhile, to enhance the infrared modality with extracted textual representations, we leverage modality alignment capabilities of VLMs and VLM-generated feature-level filters. This allows the text model to learn complementary features from the infrared modality, ensuring the semantic structural consistency between the fusion modality and the visible modality. Furthermore, we introduce modality joint learning to align features of all modalities, ensuring that textual features maintain stable semantic representation of overall pedestrian appearance during complementary information learning. Additionally, a modality ensemble retrieval strategy is proposed to leverage complementary strengths of each query modality to improve retrieval effectiveness and robustness. Extensive experiments demonstrate that our method significantly improves retrieval performance on three expanded cross-modal re-identification datasets, paving the way for utilizing large foundation models in downstream data-demanding multi-modal retrieval tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-337" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-337', event_id='94309', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3805</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94309">What matters when building vision-language models?</a></strong></h5>


                        <p class="text-muted">
                            Hugo Lauren√ßon &middot; Leo Tronchon &middot; Matthieu Cord &middot; Victor Sanh
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-338" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-338', event_id='94669', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3806</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94669">MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and Correspondence</a></strong></h5>


                        <p class="text-muted">
                            Fuming You &middot; Minghui Fang &middot; Li Tang &middot; Rongjie Huang &middot; Yongqi Wang &middot; Zhou Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Motion-to-music and music-to-motion have been studied separately, each attracting substantial research interest within their respective domains. The interaction between human motion and music is a reflection of advanced human intelligence, and establishing a unified relationship between them is particularly important. However, to date, there has been no work that considers them jointly to explore the modality alignment within. To bridge this gap, we propose a novel framework, termed MoMu-Diffusion, for long-term and synchronous motion-music generation. Firstly, to mitigate the huge computational costs raised by long sequences, we propose a novel Bidirectional Contrastive Rhythmic Variational Auto-Encoder (BiCoR-VAE) that extracts the modality-aligned latent representations for both motion and music inputs. Subsequently, leveraging the aligned latent spaces, we introduce a multi-modal diffusion Transformer model and a cross-guidance sampling strategy to enable various generation tasks, including cross-modal, multi-modal, and variable-length generation. Extensive experiments demonstrate that MoMu-Diffusion surpasses recent state-of-the-art methods both qualitatively and quantitatively, and can synthesize realistic, diverse, long-term, and beat-matched music or motion sequences. The generated motion-music samples are available at https://momu-diffusion.github.io/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-339" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-339', event_id='94750', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3807</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94750">Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning</a></strong></h5>


                        <p class="text-muted">
                            Mingcheng Li &middot; Dingkang Yang &middot; Yang Liu &middot; Shunli Wang &middot; Jiawei Chen &middot; Shuaibing Wang &middot; Jinjie Wei &middot; Yue Jiang &middot; Qingyao Xu &middot; Xiaolu Hou &middot; Mingyang Sun &middot; Ziyun Qian &middot; Dongliang Kou &middot; Lihua Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multimodal Sentiment Analysis (MSA) is an important research area that aims to understand and recognize human sentiment through multiple modalities. The complementary information provided by multimodal fusion promotes better sentiment analysis compared to utilizing only a single modality. Nevertheless, in real-world applications, many unavoidable factors may lead to situations of uncertain modality missing, thus hindering the effectiveness of multimodal modeling and degrading the model‚Äôs performance. To this end, we propose a Hierarchical Representation Learning Framework (HRLF) for the MSA task under uncertain missing modalities. Specifically, we propose a fine-grained representation factorization module that sufficiently extracts valuable sentiment information by factorizing modality into sentiment-relevant and modality-specific representations through crossmodal translation and sentiment semantic reconstruction. Moreover, a hierarchical mutual information maximization mechanism is introduced to incrementally maximize the mutual information between multi-scale representations to align and reconstruct the high-level semantics in the representations. Ultimately, we propose a hierarchical adversarial learning mechanism that further aligns and adapts the latent distribution of sentiment-relevant representations to produce robust joint multimodal representations. Comprehensive experiments on three datasets demonstrate that HRLF significantly improves MSA performance under uncertain modality missing cases.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-340" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-340', event_id='94788', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3808</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94788">Jointly Modeling Inter- &amp; Intra-Modality Dependencies for Multi-modal Learning</a></strong></h5>


                        <p class="text-muted">
                            Divyam Madaan &middot; Taro Makino &middot; Sumit Chopra &middot; Kyunghyun Cho
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Supervised multi-modal learning involves mapping multiple modalities to a target label. Previous studies in this field have concentrated on capturing in isolation either the inter-modality dependencies (the relationships between different modalities and the label) or the intra-modality dependencies (the relationships within a single modality and the label). We argue that these conventional approaches that rely solely on either inter- or intra-modality dependencies may not be optimal in general. We view the multi-modal learning problem from the lens of generative models where we consider the target as a source of multiple modalities and the interaction between them. Towards that end, we propose inter- \&amp; intra-modality modeling (I2M2) framework, which captures and integrates both the inter- and intra-modality dependencies, leading to more accurate predictions. We evaluate our approach using real-world healthcare and vision-and-language datasets with state-of-the-art models, demonstrating superior performance over traditional methods focusing only on one type of modality dependency. The code is available at https://github.com/divyam3897/I2M2.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-341" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-341', event_id='94826', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3809</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94826">Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning</a></strong></h5>


                        <p class="text-muted">
                            Alex Jinpeng Wang &middot; Linjie Li &middot; Yiqi Lin &middot; Min Li &middot; Lijuan Wang &middot; Mike Zheng Shou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Training models with longer in-context lengths is a significant challenge for multimodal machine learning due to substantial GPU memory and computational costs. This exploratory study does not present state-of-the-art models; rather, it introduces an innovative method designed to increase in-context text length in multi-modality large language models (MLLMs) efficiently. We present \ModelFullName (\ModelName), which processes long in-context text using visual tokens. This technique significantly reduces GPU memory usage and floating point operations (FLOPs). For instance, our method expands the pre-training in-context length from 256 to 2048 tokens with fewer FLOPs for a 56 billion parameter MOE model. Experimental results demonstrate that \ModelName enhances OCR capabilities and delivers superior performance on common downstream benchmarks for in-context few-shot evaluation. Additionally, \ModelName proves effective for long context inference, achieving results comparable to full text input while maintaining computational efficiency.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-342" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-342', event_id='94944', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3810</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94944">No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models</a></strong></h5>


                        <p class="text-muted">
                            Ang√©line Pouget &middot; Lucas Beyer &middot; Emanuele Bugliarello &middot; Xiao Wang &middot; Andreas Steiner &middot; Xiaohua Zhai &middot; Ibrahim Alabdulmohsin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We study cultural and socioeconomic diversity in contrastive vision-language models (VLMs). Using a broad range of benchmark datasets and evaluation metrics, we bring to attention several important findings. First, the common filtering of training data to English image-text pairs disadvantages communities of lower socioeconomic status and negatively impacts cultural understanding. Notably, this performance gap is not captured by - and even at odds with - the currently popular evaluation metrics derived from the Western-centric ImageNet and COCO datasets. Second, pretraining with global, unfiltered data before fine-tuning on English content can improve cultural understanding without sacrificing performance on said popular benchmarks. Third, we introduce the task of geo-localization as a novel evaluation metric to assess cultural diversity in VLMs. Our work underscores the value of using diverse data to create more inclusive multimodal systems and lays the groundwork for developing VLMs that better represent global perspectives.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-343" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-343', event_id='95296', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3811</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95296">What Makes CLIP More Robust to Long-Tailed Pre-Training Data? A Controlled Study for Transferable Insights</a></strong></h5>


                        <p class="text-muted">
                            Xin Wen &middot; Bingchen Zhao &middot; Yilun Chen &middot; Jiangmiao Pang &middot; Xiaojuan Qi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Severe data imbalance naturally exists among web-scale vision-language datasets. Despite this, we find CLIP pre-trained thereupon exhibits notable robustness to the data imbalance compared to supervised learning, and demonstrates significant effectiveness in learning generalizable representations. With an aim to investigate the reasons behind this finding, we conduct controlled experiments to study various underlying factors, and reveal that CLIP's pretext task forms a dynamic classification problem wherein only a subset of classes is present in training. This isolates the bias from dominant classes and implicitly balances the learning signal. Furthermore, the robustness and discriminability of CLIP improve with more descriptive language supervision, larger data scale, and broader open-world concepts, which are inaccessible to supervised learning. Our study not only uncovers the mechanisms behind CLIP's generalizability beyond data imbalance but also provides transferable insights for the research community. The findings are validated in both supervised and self-supervised learning, enabling models trained on imbalanced data to achieve CLIP-level performance on diverse recognition tasks. Code and data are available at: https://github.com/CVMI-Lab/clip-beyond-tail.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-344" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-344', event_id='93146', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3900</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93146">A Sober Look at the Robustness of CLIPs to Spurious Features</a></strong></h5>


                        <p class="text-muted">
                            Qizhou Wang &middot; Yong Lin &middot; Yongqiang Chen &middot; Ludwig Schmidt &middot; Bo Han &middot; Tong Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large vision language models, such as CLIP, demonstrate impressive robustness to spurious features than single-modal models trained on ImageNet. However, existing test datasets are typically curated based on ImageNet-trained models, which aim to capture the spurious features inherited in ImageNet. Benchmarking CLIP models based on the ImageNet-oriented spurious features may not be sufficient to reflect the extent to which CLIP models are robust to spurious correlations within CLIP training data, e.g., LAION. To this end, we craft a new challenging dataset named CounterAnimal designed to reveal the reliance of CLIP models on realistic spurious features. Specifically, we split animal photos into groups according to the backgrounds, and then identify a pair of groups for each class where a CLIP model shows high-performance drops across the two groups. Our evaluations show that the spurious features captured by CounterAnimal are generically learned by CLIP models with different backbones and pre-train data, yet have limited influence for ImageNet models. We provide theoretical insights that the CLIP objective cannot offer additional robustness. Furthermore, we also re-evaluate strategies such as scaling up parameters and high-quality pre-trained data. We find that they still help mitigate the spurious features, providing a promising path for future developments.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-345" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-345', event_id='93122', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3901</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93122">Automated Multi-level Preference for MLLMs</a></strong></h5>


                        <p class="text-muted">
                            Mengxi Zhang &middot; Wenhao Wu &middot; Yu Lu &middot; YuXin Song &middot; KANG RONG &middot; Huanjin Yao &middot; Jianbo Zhao &middot; Fanglong Liu &middot; Haocheng Feng &middot; Jingdong Wang &middot; Yifan Sun
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Current multimodal Large Language Models (MLLMs) suffer from ''hallucination'', occasionally generating responses that are not grounded in the input images. To tackle this challenge, one promising path is to utilize reinforcement learning from human feedback (RLHF), which steers MLLMs towards learning superior responses while avoiding inferior ones. We rethink the common practice of using binary preferences (<em>i.e.</em>, superior, inferior), and find that adopting multi-level preferences (<em>e.g.</em>, superior, medium, inferior) is better for two benefits: 1) It narrows the gap between adjacent levels, thereby encouraging MLLMs to discern subtle differences. 2) It further integrates cross-level comparisons (beyond adjacent-level comparisons), thus providing a broader range of comparisons with hallucination examples. To verify our viewpoint, we present the Automated Multi-level Preference (<strong>AMP</strong>) framework for MLLMs. To facilitate this framework, we first develop an automated dataset generation pipeline that provides high-quality multi-level preference datasets without any human annotators. Furthermore, we design the Multi-level Direct Preference Optimization (MDPO) algorithm to robustly conduct complex multi-level preference learning. Additionally, we propose a new hallucination benchmark, MRHal-Bench. Extensive experiments across public hallucination and general benchmarks, as well as our MRHal-Bench, demonstrate the effectiveness of our proposed method. Code is available at https://github.com/takomc/amp.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-346" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-346', event_id='93063', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3902</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93063">Unified Lexical Representation for Interpretable Visual-Language Alignment</a></strong></h5>


                        <p class="text-muted">
                            Yifan Li &middot; Yikai Wang &middot; Yanwei Fu &middot; Dongyu Ru &middot; Zheng Zhang &middot; Tong He
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's groundbreaking work. Although CLIP performs well, the typical direct latent feature alignment lacks clarity in its representation and similarity scores. On the other hand, lexical representation, a vector whose element represents the similarity between the sample and a word from the vocabulary, is a natural sparse representation and interpretable, providing exact matches for individual words.However, lexical representations are difficult to learn due to no ground-truth supervision and false-discovery issues, and thus requires complex design to train effectively.In this paper, we introduce LexVLA, a more interpretable VLA framework by learning a unified lexical representation for both modalities without complex design. We use DINOv2 as our visual model for its local-inclined features and Llama 2, a generative language model, to leverage its in-context lexical prediction ability.To avoid the false discovery, we propose an overuse penalty to refrain the lexical representation from falsely frequently activating meaningless words.We demonstrate that these two pre-trained uni-modal models can be well-aligned by fine-tuning on the modest multi-modal dataset and avoid intricate training configurations. On cross-modal retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset, outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those trained from scratch on even bigger datasets (e.g., 1.1B data, including CC-12M).We conduct extensive experiments to analyze LexVLA. Codes are available at https://github.com/Clementine24/LexVLA.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-347" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-347', event_id='92955', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3903</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92955">Deep Correlated Prompting for Visual Recognition with Missing Modalities</a></strong></h5>


                        <p class="text-muted">
                            lianyu hu &middot; Tongkai Shi &middot; Wei Feng &middot; Fanhua Shang &middot; Liang Wan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large-scale multimodal models have shown excellent performance over a series of tasks powered by the large corpus of paired multimodal training data. Generally, they are always assumed to receive modality-complete inputs. However, this simple assumption may not always hold in the real world due to privacy constraints or collection difficulty, where models pretrained on modality-complete data easily demonstrate degraded performance on missing-modality cases. To handle this issue, we refer to prompt learning to adapt large pretrained multimodal models to handle missing-modality scenarios by regarding different missing cases as different types of input. Instead of only prepending independent prompts to the intermediate layers, we present to leverage the correlations between prompts and input features and excavate the relationships between different layers of prompts to carefully design the instructions. We also incorporate the complementary semantics of different modalities to guide the prompting design for each modality. Extensive experiments on three commonly-used datasets consistently demonstrate the superiority of our method compared to the previous approaches upon different missing scenarios. Plentiful ablations are further given to show the generalizability and reliability of our method upon different modality-missing ratios and types.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-348" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-348', event_id='96361', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3904</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96361">Evaluating alignment between humans and neural network representations in image-based learning tasks</a></strong></h5>


                        <p class="text-muted">
                            Can Demircan &middot; Tankred Saanum &middot; Leonardo Pettini &middot; Marcel Binz &middot; Blazej Baczkowski &middot; Christian Doeller &middot; Mona Garvert &middot; Eric Schulz
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Humans represent scenes and objects in rich feature spaces, carrying information that allows us to generalise about category memberships and abstract functions with few examples. What determines whether a neural network model generalises like a human? We tested how well the representations of $86$ pretrained neural network models mapped to human learning trajectories across two tasks where humans had to learn continuous relationships and categories of natural images. In these tasks, both human participants and neural networks successfully identified the relevant stimulus features within a few trials, demonstrating effective generalisation. We found that while training dataset size was a core determinant of alignment with human choices, contrastive training with multi-modal data (text and imagery) was a common feature of currently publicly available models that predicted human generalisation. Intrinsic dimensionality of representations had different effects on alignment for different model types. Lastly, we tested three sets of human-aligned representations and found no consistent improvements in predictive accuracy compared to the baselines. In conclusion, pretrained neural networks can serve to extract representations for cognitive models, as they appear to capture some fundamental aspects of cognition that are transferable across tasks. Both our paradigms and modelling approach offer a novel way to quantify alignment between neural networks and humans and extend cognitive science into more naturalistic domains.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-349" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-349', event_id='96344', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3905</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96344">NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction</a></strong></h5>


                        <p class="text-muted">
                            Zixuan Gong &middot; Guangyin Bao &middot; Qi Zhang &middot; Zhongwei Wan &middot; Duoqian Miao &middot; Shoujin Wang &middot; Lei Zhu &middot; Changwei Wang &middot; Rongtao Xu &middot; Liang Hu &middot; Ke Liu &middot; Yu Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Reconstruction of static visual stimuli from non-invasion brain activity fMRI achieves great success, owning to advanced deep learning models such as CLIP and Stable Diffusion. However, the research on fMRI-to-video reconstruction remains limited since decoding the spatiotemporal perception of continuous visual experiences is formidably challenging. We contend that the key to addressing these challenges lies in accurately decoding both high-level semantics and low-level perception flows, as perceived by the brain in response to video stimuli. To the end, we propose NeuroClips, an innovative framework to decode high-fidelity and smooth video from fMRI. NeuroClips utilizes a semantics reconstructor to reconstruct video keyframes, guiding semantic accuracy and consistency, and employs a perception reconstructor to capture low-level perceptual details, ensuring video smoothness. During inference, it adopts a pre-trained T2V diffusion model injected with both keyframes and low-level perception flows for video reconstruction. Evaluated on a publicly available fMRI-video dataset, NeuroClips achieves smooth high-fidelity video reconstruction of up to 6s at 8FPS, gaining significant improvements over state-of-the-art models in various metrics, e.g., a 128% improvement in SSIM and an 81% improvement in spatiotemporal metrics. Our project is available at https://github.com/gongzix/NeuroClips.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-350" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-350', event_id='96333', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3906</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96333">Global Distortions from Local Rewards: Neural Coding Strategies in Path-Integrating Neural Systems</a></strong></h5>


                        <p class="text-muted">
                            Francisco Acosta &middot; Fatih Dinc &middot; William Redman &middot; Manu Madhav &middot; David Klindt &middot; Nina Miolane
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Grid cells in the mammalian brain are fundamental to spatial navigation, and therefore crucial to how animals perceive and interact with their environment. Traditionally, grid cells are thought support path integration through highly symmetric hexagonal lattice firing patterns. However, recent findings show that their firing patterns become distorted in the presence of significant spatial landmarks such as rewarded locations. This introduces a novel perspective of dynamic, subjective, and action-relevant interactions between spatial representations and environmental cues. Here, we propose a practical and theoretical framework to quantify and explain these interactions. To this end, we train path-integrating recurrent neural networks (piRNNs) on a spatial navigation task, whose goal is to predict the agent's position with a special focus on rewarded locations. Grid-like neurons naturally emerge from the training of piRNNs, which allows us to investigate how the two aspects of the task, space and reward, are integrated in their firing patterns. We find that geometry, but not topology, of the grid cell population code becomes distorted. Surprisingly, these distortions are global in the firing patterns of the grid cells despite local changes in the reward. Our results indicate that after training with location-specific reward information, the preserved representational topology supports successful path integration, whereas the emergent heterogeneity in individual responses due to global distortions may encode dynamically changing environmental cues. By bridging the gap between computational models and the biological reality of spatial navigation under reward information, we offer new insights into how neural systems prioritize environmental landmarks in their spatial navigation code.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-351" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-351', event_id='96245', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3907</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96245">Flexible task abstractions emerge in linear networks with fast and bounded units</a></strong></h5>


                        <p class="text-muted">
                            Kai Sandbrink &middot; Jan Bauer &middot; Alexandra Proca &middot; Andrew Saxe &middot; Christopher Summerfield &middot; Ali Hummos
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Animals survive in dynamic environments changing at arbitrary timescales, but such data distribution shifts are a challenge to neural networks. To adapt to change, neural systems may change a large number of parameters, which is a slow process involving forgetting past information. In contrast, animals leverage distribution changes to segment their stream of experience into tasks and associate them with internal task abstracts. Animals can then respond flexibly by selecting the appropriate task abstraction. However, how such flexible task abstractions may arise in neural systems remains unknown. Here, we analyze a linear gated network where the weights and gates are jointly optimized via gradient descent, but with neuron-like constraints on the gates including a faster timescale, non-negativity, and bounded activity. We observe that the weights self-organize into modules specialized for tasks or sub-tasks encountered, while the gates layer forms unique representations that switch the appropriate weight modules (task abstractions). We analytically reduce the learning dynamics to an effective eigenspace, revealing a virtuous cycle: fast adapting gates drive weight specialization by protecting previous knowledge, while weight specialization in turn increases the update rate of the gating layer. Task switching in the gating layer accelerates as a function of curriculum block size and task training, mirroring key findings in cognitive neuroscience. We show that the discovered task abstractions support generalization through both task and subtask composition, and we extend our findings to a non-linear network switching between two tasks. Overall, our work offers a theory of cognitive flexibility in animals as arising from joint gradient descent on synaptic and neural gating in a neural network architecture.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-352" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-352', event_id='96170', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3908</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96170">Inferring stochastic low-rank recurrent neural networks from neural data</a></strong></h5>


                        <p class="text-muted">
                            Matthijs Pals &middot; A Erdem Saƒütekin &middot; Felix Pei &middot; Manuel Gloeckler &middot; Jakob H Macke
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>A central aim in computational neuroscience is to relate the activity of large populations of neurons to an underlying dynamical system. Models of these neural dynamics should ideally be both interpretable and fit the observed data well. Low-rank recurrent neural networks (RNNs) exhibit such interpretability by having tractable dynamics. However, it is unclear how to best fit low-rank RNNs to data consisting of noisy observations of an underlying stochastic system. Here, we propose to fit stochastic low-rank RNNs with variational sequential Monte Carlo methods. We validate our method on several datasets consisting of both continuous and spiking neural data, where we obtain lower dimensional latent dynamics than current state of the art methods. Additionally, for low-rank models with piecewise linear nonlinearities, we show how to efficiently identify all fixed points in polynomial rather than exponential cost in the number of units, making analysis of the inferred dynamics tractable for large RNNs. Our method both elucidates the dynamical systems underlying experimental recordings and provides a generative model whose trajectories match observed variability.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-353" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-353', event_id='95893', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3909</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95893">Latent Learning Progress Drives Autonomous Goal Selection in Human Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Gaia Molinaro &middot; C√©dric Colas &middot; Pierre-Yves Oudeyer &middot; Anne Collins
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Humans are autotelic agents who learn by setting and pursuing their own goals. However, the precise mechanisms guiding human goal selection remain unclear. Learning progress, typically measured as the observed change in performance, can provide a valuable signal for goal selection in both humans and artificial agents. We hypothesize that human choices of goals may also be driven by <em>latent learning progress</em>, which humans can estimate through knowledge of their actions and the environment ‚Äì even without experiencing immediate changes in performance. To test this hypothesis, we designed a hierarchical reinforcement learning task in which human participants (N = 175) repeatedly chose their own goals and learned goal-conditioned policies. Our behavioral and computational modeling results confirm the influence of latent learning progress on goal selection and uncover inter-individual differences, partially mediated by recognition of the task's hierarchical structure. By investigating the role of latent learning progress in human goal selection, we pave the way for more effective and personalized learning experiences as well as the advancement of more human-like autotelic machines.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-354" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-354', event_id='95754', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3910</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95754">LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model</a></strong></h5>


                        <p class="text-muted">
                            Zecheng Hao &middot; Xinyu Shi &middot; Yujia Liu &middot; Zhaofei Yu &middot; Tiejun Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more energy-efficient manner. However, despite previous efforts to optimize the learning algorithm of SNNs through various methods, SNNs still lag behind ANNs in terms of performance. The recently proposed multi-threshold model provides more possibilities for further enhancing the learning capability of SNNs. In this paper, we rigorously analyze the relationship among the multi-threshold model, vanilla spiking model and quantized ANNs from a mathematical perspective, then propose a novel LM-HT model, which is an equidistant multi-threshold model that can dynamically regulate the global input current and membrane potential leakage on the time dimension. The LM-HT model can also be transformed into a vanilla single threshold model through reparameterization, thereby achieving more flexible hardware deployment. In addition, we note that the LM-HT model can seamlessly integrate with ANN-SNN Conversion framework under special initialization. This novel hybrid learning framework can effectively improve the relatively poor performance of converted SNNs under low time latency. Extensive experimental results have demonstrated that our model can outperform previous state-of-the-art works on various types of datasets, which promote SNNs to achieve a brand-new level of performance comparable to quantized ANNs. Code is available at https://github.com/hzc1208/LMHT_SNN.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-355" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-355', event_id='95187', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3911</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95187">Trading Place for Space: Increasing Location Resolution Reduces Contextual Capacity in Hippocampal Codes</a></strong></h5>


                        <p class="text-muted">
                            Spencer Rooke &middot; Zhaoze Wang &middot; Ronald Di Tullio &middot; Vijay Balasubramanian
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Many animals learn cognitive maps of their environment - a simultaneous representation of context, experience, and position.  Place cells in the hippocampus, named for their explicit encoding of position, are believed to be a neural substrate of these maps, with place cell "remapping" explaining how this system can represent different contexts. Briefly, place cells alter their firing properties, or "remap", in response to changes in experiential or sensory cues. Substantial sensory changes, produced, e.g., by moving between environments, cause large subpopulations of place cells to change their tuning entirely. While many studies have looked at the physiological basis of remapping, we lack explicit calculations of how the contextual capacity of the place cell system changes as a function of place field firing properties. Here, we propose a geometric approach to understanding population level activity of place cells.  Using known firing field statistics, we investigate how changes to place cell firing properties affect the distances between representations of different environments within firing rate space.  Using this approach, we find that the number of contexts storable by the hippocampus grows exponentially with the number of place cells, and calculate this exponent for environments of different sizes. We identify a fundamental trade-off between high resolution encoding of position and the number of storable contexts. This trade-off is tuned by place cell width, which might explain the change in firing field scale along the dorsal-ventral axis of the hippocampus. We demonstrate that clustering of place cells near likely points of confusion, such as boundaries, increases the contextual capacity of the place system within our framework and conclude by discussing how our geometric approach could be extended to include other cell types and abstract spaces.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-356" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-356', event_id='96039', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4000</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96039">Addressing Spatial-Temporal Heterogeneity: General Mixed Time Series Analysis via Latent Continuity Recovery and Alignment</a></strong></h5>


                        <p class="text-muted">
                            Jiawei Chen &middot; Êò•Êôñ Ëµµ
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Mixed time series (MiTS) comprising both continuous variables (CVs) and discrete variables (DVs) are frequently encountered yet under-explored in time series analysis. Essentially, CVs and DVs exhibit different temporal patterns and distribution types. Overlooking these heterogeneities would lead to insufficient and imbalanced representation learning, bringing biased results. This paper addresses the problem with two insights: 1) DVs may originate from intrinsic latent continuous variables (LCVs), which lose fine-grained information due to extrinsic discretization; 2) LCVs and CVs share similar temporal patterns and interact spatially. Considering these similarities and interactions, we propose a general MiTS analysis framework MiTSformer, which recovers LCVs behind DVs for sufficient and balanced spatial-temporal modeling by designing two essential inductive biases: 1) hierarchically aggregating multi-scale temporal context information to enrich the information granularity of DVs; 2) adaptively learning the aggregation processes via the adversarial guidance from CVs. Subsequently, MiTSformer captures complete spatial-temporal dependencies within and across LCVs and CVs via cascaded self- and cross-attention blocks. Empirically, MiTSformer achieves consistent SOTA on five mixed time series analysis tasks, including classification, extrinsic regression, anomaly detection, imputation, and long-term forecasting. The code is available at https://github.com/chunhuiz/MiTSformer.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-357" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-357', event_id='93065', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4001</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93065">Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Yufei Guo &middot; Yuanpei Chen &middot; Zecheng Hao &middot; Weihang Peng &middot; Zhou Jie &middot; Yuhan Zhang &middot; Xiaode Liu &middot; Zhe Ma
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The Spiking Neural Network (SNN) is a biologically inspired neural network infrastructure that has recently garnered significant attention. It utilizes binary spike activations to transmit information, thereby replacing multiplications with additions and resulting in high energy efficiency. However, training an SNN directly poses a challenge due to the undefined gradient of the firing spike process. Although prior works have employed various surrogate gradient training methods that use an alternative function to replace the firing process during back-propagation, these approaches ignore an intrinsic problem: gradient vanishing. To address this issue, we propose a shortcut back-propagation method in the paper, which advocates for transmitting the gradient directly from the loss to the shallow layers. This enables us to present the gradient to the shallow layers directly, thereby significantly mitigating the gradient vanishing problem. Additionally, this method does not introduce any burden during the inference phase.To strike a balance between final accuracy and ease of training, we also propose an evolutionary training framework and implement it by inducing a balance coefficient that dynamically changes with the training epoch, which further improves the network's performance. Extensive experiments conducted over static and dynamic datasets using several popular network structures reveal that our method consistently outperforms state-of-the-art methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-358" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-358', event_id='93138', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4002</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93138">Learning Cortico-Muscular Dependence through Orthonormal Decomposition of Density Ratios</a></strong></h5>


                        <p class="text-muted">
                            Shihan Ma &middot; Bo Hu &middot; Tianyu Jia &middot; Alexander Clarke &middot; Blanka Zicher &middot; Arnault Caillet &middot; Dario Farina &middot; Jose C Principe
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The cortico-spinal neural pathway is fundamental for motor control and movement execution, and in humans it is typically studied using concurrent electroencephalography (EEG) and electromyography (EMG) recordings. However, current approaches for capturing high-level and contextual connectivity between these recordings have important limitations. Here, we present a novel application of statistical dependence estimators based on orthonormal decomposition of density ratios to model the relationship between cortical and muscle oscillations. Our method extends from traditional scalar-valued measures by learning eigenvalues, eigenfunctions, and projection spaces of density ratios from realizations of the signal, addressing the interpretability, scalability, and local temporal dependence of cortico-muscular connectivity. We experimentally demonstrate that eigenfunctions learned from cortico-muscular connectivity can accurately classify movements and subjects. Moreover, they reveal channel and temporal dependencies that confirm the activation of specific EEG channels during movement.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-359" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-359', event_id='96212', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4003</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96212">Symbolic Regression with a Learned Concept Library</a></strong></h5>


                        <p class="text-muted">
                            Arya Grayeli &middot; Atharva Sehgal &middot; Omar Costilla Reyes &middot; Miles Cranmer &middot; Swarat Chaudhuri
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present a novel method for symbolic regression (SR), the task of searching for compact programmatic hypotheses that best explain a dataset. The problem is commonly solved using genetic algorithms; we show that we can enhance such methods by inducing a library of abstract textual concepts. Our algorithm, called LaSR, uses zero-shot queries to a large language model (LLM) to discover and evolve concepts occurring in known high-performing hypotheses. We discover new hypotheses using a mix of standard evolutionary steps and LLM-guided steps (obtained through zero-shot LLM queries) conditioned on discovered concepts. Once discovered,  hypotheses are used in a new round of concept abstraction and evolution. We validate LaSR on the Feynman equations, a popular SR benchmark, as well as a set of synthetic tasks. On these benchmarks, LaSR substantially outperforms a variety of state-of-the-art SR approaches based on deep learning and evolutionary algorithms. Moreover, we show that LASR can be used to discover a new and powerful scaling law for LLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-360" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-360', event_id='93697', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4004</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93697">Active design of two-photon holographic stimulation for identifying neural population dynamics</a></strong></h5>


                        <p class="text-muted">
                            Andrew Wagenmaker &middot; Lu Mi &middot; Marton Rozsa &middot; Matthew Bull &middot; Karel Svoboda &middot; Kayvon Daie &middot; Matthew Golub &middot; Kevin Jamieson
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain.  In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous two-photon calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-361" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-361', event_id='93965', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4005</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93965">Exploring Behavior-Relevant and Disentangled Neural Dynamics with Generative Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Yule Wang &middot; Chengrui Li &middot; Weihan Li &middot; Anqi Wu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Understanding the neural basis of behavior is a fundamental goal in neuroscience. Current research in large-scale neuro-behavioral data analysis often relies on decoding models, which quantify behavioral information in neural data but lack details on behavior encoding. This raises an intriguing scientific question: "how can we enable in-depth exploration of neural representations in behavioral tasks, revealing interpretable neural dynamics associated with behaviors". However, addressing this issue is challenging due to the varied behavioral encoding across different brain regions and mixed selectivity at the population level. To tackle this limitation, our approach, named ("BeNeDiff"), first identifies a fine-grained and disentangled neural subspace using a behavior-informed latent variable model. It then employs state-of-the-art generative diffusion models to synthesize behavior videos that interpret the neural dynamics of each latent factor. We validate the method on multi-session datasets containing widefield calcium imaging recordings across the dorsal cortex. Through guiding the diffusion model to activate individual latent factors, we verify that the neural dynamics of latent factors in the disentangled neural subspace provide interpretable quantifications of the behaviors of interest. At the same time, the neural subspace in BeNeDiff demonstrates high disentanglement and neural reconstruction quality.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-362" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-362', event_id='94041', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4006</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94041">Flexible mapping of abstract domains by grid cells via self-supervised extraction and projection of generalized velocity signals</a></strong></h5>


                        <p class="text-muted">
                            Abhiram Iyer &middot; Sarthak Chandra &middot; Sugandha Sharma &middot; Ila Fiete
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Grid cells in the medial entorhinal cortex create remarkable periodic maps of explored space during navigation. Recent studies show that they form similar maps of abstract cognitive spaces. Examples of such abstract environments include auditory tone sequences in which the pitch is continuously varied or images in which abstract features are continuously deformed (e.g., a cartoon bird whose legs stretch and shrink). Here, we hypothesize that the brain generalizes how it maps spatial domains to mapping abstract spaces. To sidestep the computational cost of learning representations for each high-dimensional sensory input, the brain extracts self-consistent, low-dimensional descriptions of displacements across abstract spaces, leveraging the spatial velocity integration of grid cells to efficiently build maps of different domains.Our neural network model for abstract velocity extraction factorizes the content of these abstract domains from displacements within the domains to generate content-independent and self-consistent, low-dimensional velocity estimates. Crucially, it uses a self-supervised geometric consistency constraint that requires displacements along closed loop trajectories to sum to zero, an integration that is itself performed by the downstream grid cell circuit over learning. This process results in high fidelity estimates of velocities and allowed transitions in abstract domains, a crucial prerequisite for efficient map generation in these high-dimensional environments. We also show how our method outperforms traditional dimensionality reduction and deep-learning based motion extraction networks on the same set of tasks.This is the first neural network model to explain how grid cells can flexibly represent different abstract spaces and makes the novel prediction that they should do so while maintaining their population correlation and manifold structure across domains. Fundamentally, our model sheds light on the mechanistic origins of cognitive flexibility and transfer of representations across vastly different domains in brains, providing a potential self-supervised learning (SSL) framework for leveraging similar ideas in transfer learning and data-efficient generalization in machine learning and robotics.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-363" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-363', event_id='94113', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4007</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94113">Brain-JEPA: Brain Dynamics Foundation Model with Gradient Positioning and Spatiotemporal Masking</a></strong></h5>


                        <p class="text-muted">
                            Zijian Dong &middot; Ruilin Li &middot; Yilei Wu &middot; Thuan Tinh Nguyen &middot; Joanna Chong &middot; Fang Ji &middot; Nathanael Tong &middot; Christopher Chen &middot; Juan Helen Zhou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce <em>Brain-JEPA</em>, a brain dynamics foundation model with the Joint-Embedding Predictive Architecture (JEPA). This pioneering model achieves state-of-the-art performance in demographic prediction, disease diagnosis/prognosis, and trait prediction through fine-tuning. Furthermore, it excels in off-the-shelf evaluations (e.g., linear probing) and demonstrates superior generalizability across different ethnic groups, surpassing the previous large model for brain activity significantly. Brain-JEPA incorporates two innovative techniques: <strong>Brain Gradient Positioning</strong> and <strong>Spatiotemporal Masking</strong>. Brain Gradient Positioning introduces a functional coordinate system for brain functional parcellation, enhancing the positional encoding of different Regions of Interest (ROIs). Spatiotemporal Masking, tailored to the unique characteristics of fMRI data, addresses the challenge of heterogeneous time-series patches. These methodologies enhance model performance and advance our understanding of the neural circuits underlying cognition. Overall, Brain-JEPA is paving the way to address pivotal questions of building brain functional coordinate system and masking brain activity at the AI-neuroscience interface, and setting a potentially new paradigm in brain activity analysis through downstream adaptation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-364" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-364', event_id='94169', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4008</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94169">To Learn or Not to Learn, That is the Question ‚Äî A Feature-Task Dual Learning Model of Perceptual Learning</a></strong></h5>


                        <p class="text-muted">
                            Xiao Liu &middot; Muyang Lyu &middot; Cong Yu &middot; Si Wu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Perceptual learning refers to the practices through which participants learn to improve their performance in perceiving sensory stimuli. Two seemingly conflicting phenomena of specificity and transfer have been widely observed in perceptual learning. Here, we propose a dual-learning model to reconcile these two phenomena. The model consists of two learning processes. One is task-based learning, which is fast and enables the brain to adapt to a task rapidly by using existing feature representations. The other is feature-based learning, which is slow and enables the brain to improve feature representations to match the statistical change of the environment. Associated with different training paradigms, the interactions between these two learning processes induce the rich phenomena of perceptual learning. Specifically, in the training paradigm where the same stimulus condition is presented excessively, feature-based learning is triggered, which incurs specificity, while in the paradigm where the stimulus condition varies during the training, task-based learning dominates to induce the transfer effect. As the number of training sessions under the same stimulus condition increases, a transition from transfer to specificity occurs. We demonstrate that the dual-learning model can account for both the specificity and transfer phenomena observed in classical psychophysical experiments. We hope that this study gives us insight into understanding how the brain balances the accomplishment of a new task and the consumption of learning effort.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-365" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-365', event_id='94209', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4009</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94209">Compositional Generalization Across Distributional Shifts with Sparse Tree Operations</a></strong></h5>


                        <p class="text-muted">
                            Paul Soulos &middot; Henry Conklin &middot; Mattia Opper &middot; Paul Smolensky &middot; Jianfeng Gao &middot; Roland Fernandez
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Neural networks continue to struggle with compositional generalization, and this issue is exacerbated by a lack of massive pre-training. One successful approach for developing neural systems which exhibit human-like compositional generalization is $\textit{hybrid}$ neurosymbolic techniques. However, these techniques run into the core issues that plague symbolic approaches to AI: scalability and flexibility. The reason for this failure is that at their core, hybrid neurosymbolic models perform symbolic computation and relegate the scalable and flexible neural computation to parameterizing a symbolic system. We investigate a $\textit{unified}$ neurosymbolic system where transformations in the network can be interpreted simultaneously as both symbolic and neural computation. We extend a unified neurosymbolic architecture called the Differentiable Tree Machine in two central ways. First, we significantly increase the model‚Äôs efficiency through the use of sparse vector representations of symbolic structures. Second, we enable its application beyond the restricted set of tree2tree problems to the more general class of seq2seq problems. The improved model retains its prior generalization capabilities and, since there is a fully neural path through the network, avoids the pitfalls of other neurosymbolic techniques that elevate symbolic computation over neural computation.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-366" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-366', event_id='94632', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4010</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94632">Latent Diffusion for Neural Spiking Data</a></strong></h5>


                        <p class="text-muted">
                            Jaivardhan Kapoor &middot; Auguste Schulz &middot; Julius Vetter &middot; Felix Pei &middot; Richard Gao &middot; Jakob H Macke
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Modern datasets in neuroscience enable unprecedented inquiries into the relationship between complex behaviors and the activity of many simultaneously recorded neurons. While latent variable models can successfully extract low-dimensional embeddings from such recordings, using them to generate realistic spiking data, especially in a behavior-dependent manner, still poses a challenge. Here, we present Latent Diffusion for Neural Spiking data (LDNS), a diffusion-based generative model with a low-dimensional latent space: LDNS employs an autoencoder with structured state-space (S4) layers to project discrete high-dimensional spiking data into continuous time-aligned latents. On these inferred latents, we train expressive (conditional) diffusion models, enabling us to sample neural activity with realistic single-neuron and population spiking statistics. We validate LDNS on synthetic data, accurately recovering latent structure, firing rates, and spiking statistics. Next, we demonstrate its flexibility by generating variable-length data that mimics human cortical activity during attempted speech. We show how to equip LDNS with an expressive observation model that accounts for single-neuron dynamics not mediated by the latent state, further increasing the realism of generated samples. Finally, conditional LDNS trained on motor cortical activity during diverse reaching behaviors can generate realistic spiking data given reach direction or unseen reach trajectories. In summary, LDNS simultaneously enables inference of low-dimensional latents and realistic conditional generation of neural spiking datasets, opening up further possibilities for simulating experimentally testable hypotheses.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-367" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-367', event_id='94983', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4011</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94983">Exploring the trade-off between deep-learning and explainable models for brain-machine interfaces</a></strong></h5>


                        <p class="text-muted">
                            Luis Cubillos &middot; Guy Revach &middot; Matthew Mender &middot; Joseph Costello &middot; Hisham Temmar &middot; Aren Hite &middot; Diksha Anoop Kumar Zutshi &middot; Dylan Wallace &middot; Xiaoyong Ni &middot; Madison Kelberman &middot; Matt Willsey &middot; Ruud Van Sloun &middot; Nir Shlezinger &middot; Parag Patil &middot; Anne Draelos &middot; Cynthia Chestek
                        </p>

                    </div>
                    <div class="abstract">
                        <p>People with brain or spinal cord-related paralysis often need to rely on others for basic tasks, limiting their independence. A potential solution is brain-machine interfaces (BMIs), which could allow them to voluntarily control external devices (e.g., robotic arm) by decoding brain activity to movement commands. In the past decade, deep-learning decoders have achieved state-of-the-art results in most BMI applications, ranging from speech production to finger control. However, the 'black-box' nature of deep-learning decoders could lead to unexpected behaviors, resulting in major safety concerns in real-world physical control scenarios. In these applications, explainable but lower-performing decoders, such as the Kalman filter (KF), remain the norm. In this study, we designed a BMI decoder based on KalmanNet, an extension of the KF that augments its operation with recurrent neural networks to compute the Kalman gain. This results in a varying ‚Äútrust‚Äù that shifts between inputs and dynamics. We used this algorithm to predict finger movements from the brain activity of two monkeys. We compared KalmanNet results offline (pre-recorded data, $n=13$ days) and online (real-time predictions, $n=5$ days) with a simple KF and two recent deep-learning algorithms: tcFNN (non-ReFIT version) and LSTM. KalmanNet achieved comparable or better results than other deep learning models in offline and online modes, relying on the dynamical model for stopping while depending more on neural inputs for initiating movements. We further validated this mechanism by implementing a heteroscedastic KF that used the same strategy, and it also approached state-of-the-art performance while remaining in the explainable domain of standard KFs. However, we also see two downsides to KalmanNet. KalmanNet shares the limited generalization ability of existing deep-learning decoders, and its usage of the KF as an inductive bias limits its performance in the presence of unseen noise distributions. Despite this trade-off, our analysis successfully integrates traditional controls and modern deep-learning approaches to motivate high-performing yet still explainable BMI designs.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-368" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-368', event_id='96095', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4100</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96095">Multiple Physics Pretraining for Spatiotemporal Surrogate Models</a></strong></h5>


                        <p class="text-muted">
                            Michael McCabe &middot; Bruno R√©galdo-Saint Blancard &middot; Liam Parker &middot; Ruben Ohana &middot; Miles Cranmer &middot; Alberto Bietti &middot; Michael Eickenberg &middot; Siavash Golkar &middot; Geraud Krawezik &middot; Francois Lanusse &middot; Mariel Pettee &middot; Tiberiu Tesileanu &middot; Kyunghyun Cho &middot; Shirley Ho
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling of spatiotemporal systems with transformers. In MPP, rather than training one model on a specific physical system, we train a backbone model to predict the dynamics of multiple heterogeneous physical systems simultaneously in order to learn features that are broadly useful across systems and facilitate transfer. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on systems with previously unseen physical components or higher dimensional systems compared to training from scratch or finetuning pretrained video foundation models. We open-source our code and model weights trained at multiple scales for reproducibility.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-369" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-369', event_id='95731', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4101</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95731">Poseidon: Efficient Foundation Models for PDEs</a></strong></h5>


                        <p class="text-muted">
                            Maximilian Herde &middot; Bogdan Raonic &middot; Tobias Rohner &middot; Roger K√§ppeli &middot; Roberto Molinaro &middot; Emmanuel de B√©zenac &middot; Siddhartha Mishra
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce Poseidon, a foundation model for learning the solution operators of PDEs. It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations. A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed. Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics. It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators. We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy. Poseidon also generalizes very well to new physics that is not seen during pretraining. Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks. Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model. Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-370" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-370', event_id='93155', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4102</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93155">Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs</a></strong></h5>


                        <p class="text-muted">
                            Md Ashiqur Rahman &middot; Robert Joseph George &middot; Mogab Elleithy &middot; Daniel Leibovici &middot; Zongyi Li &middot; Boris Bonev &middot; Colin White &middot; Julius Berner &middot; Raymond A. Yeh &middot; Jean Kossaifi &middot; Kamyar Azizzadenesheli &middot; Animashree Anandkumar
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs) due to complex geometries, interactions between physical variables, and the limited amounts of high-resolution training data. To address these issues, we propose <em>Codomain Attention Neural Operator</em> (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems. Specifically, we extend positional encoding, self-attention, and normalization layers to function spaces. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings. On complex downstream tasks with limited data, such as fluid flow simulations, fluid-structure interactions, and Rayleigh-B√©nard convection, we found CoDA-NO to outperform existing methods by over 36%.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-371" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-371', event_id='93990', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4103</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93990">Generalizing Weather Forecast to Fine-grained Temporal Scales via Physics-AI Hybrid Modeling</a></strong></h5>


                        <p class="text-muted">
                            Wanghan Xu &middot; Fenghua Ling &middot; zhangwenlong &middot; Tao Han &middot; Hao Chen &middot; Wanli Ouyang &middot; LEI BAI
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Data-driven artificial intelligence (AI) models have made significant advancements in weather forecasting, particularly in medium-range and nowcasting. However, most data-driven weather forecasting models are black-box systems that focus on learning data mapping rather than fine-grained physical evolution in the time dimension. Consequently, the limitations in the temporal scale of datasets prevent these models from forecasting at finer time scales. This paper proposes a physics-AI hybrid model (i.e., WeatherGFT) which Generalizes weather forecasts to Finer-grained Temporal scales beyond training dataset. Specifically, we employ a carefully designed PDE kernel to simulate physical evolution on a small time scale (e.g., 300 seconds) and use a parallel neural networks with a learnable router for bias correction. Furthermore, we introduce a lead time-aware training framework to promote the generalization of the model at different lead times. The weight analysis of physics-AI modules indicates that physics conducts major evolution while AI performs corrections adaptively. Extensive experiments show that WeatherGFT trained on an hourly dataset, achieves state-of-the-art performance across multiple lead times and exhibits the capability to generalize 30-minute forecasts.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-372" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-372', event_id='93730', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4104</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93730">Validating Climate Models with Spherical Convolutional Wasserstein Distance</a></strong></h5>


                        <p class="text-muted">
                            Robert Garrett &middot; Trevor Harris &middot; Zhuo Wang &middot; Bo Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The validation of global climate models is crucial to ensure the accuracy and efficacy of model output. We introduce the spherical convolutional Wasserstein distance to more comprehensively measure differences between climate models and reanalysis data. This new similarity measure accounts for spatial variability using convolutional projections and quantifies local differences in the distribution of climate variables. We apply this method to evaluate the historical model outputs of the Coupled Model Intercomparison Project (CMIP) members by comparing them to observational and reanalysis data products. Additionally, we investigate the progression from CMIP phase 5 to phase 6 and find modest improvements in the phase 6 models regarding their ability to produce realistic climatologies.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-373" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-373', event_id='93149', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4105</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93149">Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Joel Oskarsson &middot; Tomas Landelius &middot; Marc Deisenroth &middot; Fredrik Lindsten
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In recent years, machine learning has established itself as a powerful tool for high-resolution weather forecasting. While most current machine learning models focus on deterministic forecasts, accurately capturing the uncertainty in the chaotic weather system calls for probabilistic modeling. We propose a probabilistic weather forecasting model called Graph-EFM, combining a flexible latent-variable formulation with the successful graph-based forecasting framework. The use of a hierarchical graph construction allows for efficient sampling of spatially coherent forecasts. Requiring only a single forward pass per time step, Graph-EFM allows for fast generation of arbitrarily large ensembles. We experiment with the model on both global and limited area forecasting. Ensemble forecasts from Graph-EFM achieve equivalent or lower errors than comparable deterministic models, with the added benefit of accurately capturing forecast uncertainty.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-374" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-374', event_id='94512', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4106</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94512">A Recipe for Charge Density Prediction</a></strong></h5>


                        <p class="text-muted">
                            Xiang Fu &middot; Andrew Rosen &middot; Kyle Bystrom &middot; Rui Wang &middot; Albert Musaelian &middot; Boris Kozinsky &middot; Tess Smidt &middot; Tommi Jaakkola
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In density functional theory, charge density is the core attribute of atomic systems from which all chemical properties can be derived. Machine learning methods are promising in significantly accelerating charge density prediction, yet existing approaches either lack accuracy or scalability. We propose a recipe that can achieve both. In particular, we identify three key ingredients: (1) representing the charge density with atomic and virtual orbitals (spherical fields centered at atom/virtual coordinates); (2) using expressive and learnable orbital basis sets (basis function for the spherical fields); and (3) using high-capacity equivariant neural network architecture. Our method achieves state-of-the-art accuracy while being more than an order of magnitude faster than existing methods. Furthermore, our method enables flexible efficiency-accuracy trade-offs by adjusting the model/basis sizes.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-375" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-375', event_id='94197', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4107</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94197">Higher-Rank Irreducible Cartesian Tensors for Equivariant Message Passing</a></strong></h5>


                        <p class="text-muted">
                            Viktor Zaverkin &middot; Francesco Alesiani &middot; Takashi Maruyama &middot; Federico Errica &middot; Henrik Christiansen &middot; Makoto Takamoto &middot; Nicolas Weber &middot; Mathias Niepert
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The ability to perform fast and accurate atomistic simulations is crucial for advancing the chemical sciences. By learning from high-quality data, machine-learned interatomic potentials achieve accuracy on par with ab initio and first-principles methods at a fraction of their computational cost. The success of machine-learned interatomic potentials arises from integrating inductive biases such as equivariance to group actions on an atomic system, e.g., equivariance to rotations and reflections. In particular, the field has notably advanced with the emergence of equivariant message passing. Most of these models represent an atomic system using spherical tensors, tensor products of which require complicated numerical coefficients and can be computationally demanding. Cartesian tensors offer a promising alternative, though state-of-the-art methods lack flexibility in message-passing mechanisms, restricting their architectures and expressive power. This work explores higher-rank irreducible Cartesian tensors to address these limitations. We integrate irreducible Cartesian tensor products into message-passing neural networks and prove the equivariance and traceless property of the resulting layers. Through empirical evaluations on various benchmark data sets, we consistently observe on-par or better performance than that of state-of-the-art spherical and Cartesian models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-376" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-376', event_id='96678', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4108</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96678">Treeffuser: probabilistic prediction via conditional diffusions with gradient-boosted trees</a></strong></h5>


                        <p class="text-muted">
                            Nicolas Beltran Velez &middot; Alessandro A Grande &middot; Achille Nazaret &middot; Alp Kucukelbir &middot; David Blei
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Probabilistic prediction aims to compute predictive distributions rather than single point predictions. These distributions enable practitioners to quantify uncertainty, compute risk, and detect outliers. However, most probabilistic methods assume parametric responses, such as Gaussian or Poisson distributions. When these assumptions fail, such models lead to bad predictions and poorly calibrated uncertainty.  In this paper, we propose Treeffuser, an easy-to-use method for probabilistic prediction on tabular data. The idea is to learn a conditional diffusion model where the score function is estimated using gradient-boosted trees. The conditional diffusion model makes Treeffuser flexible and non-parametric, while the gradient-boosted trees make it robust and easy to train on CPUs. Treeffuser learns well-calibrated predictive distributions and can handle a wide range of regression tasks---including those with multivariate, multimodal, and skewed responses. We study Treeffuser on synthetic and real data and show that it outperforms existing methods, providing better calibrated probabilistic predictions. We further demonstrate its versatility with an application to inventory allocation under uncertainty using sales data from Walmart. We implement Treeffuser in https://github.com/blei-lab/treeffuser.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-377" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-377', event_id='96481', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4109</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96481">An Efficient High-dimensional Gradient Estimator for Stochastic Differential Equations</a></strong></h5>


                        <p class="text-muted">
                            Shengbo Wang &middot; Jose Blanchet &middot; Peter W Glynn
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Overparameterized stochastic differential equation (SDE) models have achieved remarkable success in various complex environments, such as PDE-constrained optimization, stochastic control and reinforcement learning, financial engineering, and neural SDEs. These models often feature system evolution coefficients that are parameterized by a high-dimensional vector $\theta \in \mathbb{R}^n$, aiming to optimize expectations of the SDE, such as a value function, through stochastic gradient ascent. Consequently, designing efficient gradient estimators for which the computational complexity scales well with $n$ is of significant interest. This paper introduces a novel unbiased stochastic gradient estimator‚Äîthe generator gradient estimator‚Äîfor which the computation time remains stable in $n$. In addition to establishing the validity of our methodology for general SDEs with jumps, we also perform numerical experiments that test our estimator in linear-quadratic control problems parameterized by high-dimensional neural networks. The results show a significant improvement in efficiency compared to the widely used pathwise differentiation method: Our estimator achieves near-constant computation times, increasingly outperforms its counterpart as $n$ increases, and does so without compromising estimation variance. These empirical findings highlight the potential of our proposed methodology for optimizing SDEs in contemporary applications.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-378" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-378', event_id='95142', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4110</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95142">Shaving Weights with Occam&#x27;s Razor: Bayesian Sparsification for Neural Networks using the Marginal Likelihood</a></strong></h5>


                        <p class="text-muted">
                            Rayen Dhahri &middot; Alexander Immer &middot; Bertrand Charpentier &middot; Stephan G√ºnnemann &middot; Vincent Fortuin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to naively deploy on consumer hardware. While much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM), a sparsification framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. In addition, we demonstrate that the pre-computed posterior precision from the Laplace approximation can be re-used to define a cheap pruning criterion, which outperforms many existing (more expensive) approaches. We demonstrate the effectiveness of our framework, especially at high sparsity levels, across a range of different neural network architectures and datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-379" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-379', event_id='93389', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4111</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93389">Preferential Normalizing Flows</a></strong></h5>


                        <p class="text-muted">
                            Petrus Mikkola &middot; Luigi Acerbi &middot; Arto Klami
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Eliciting a high-dimensional probability distribution from an expert via noisy judgments is notoriously challenging, yet useful for many applications, such as prior elicitation and reward modeling. We introduce a method for eliciting the expert's belief density as a normalizing flow based solely on preferential questions such as comparing or ranking alternatives. This allows eliciting in principle arbitrarily flexible densities, but flow estimation is susceptible to the challenge of collapsing or diverging probability mass that makes it difficult in practice. We tackle this problem by introducing a novel functional prior for the flow, motivated by a decision-theoretic argument, and show empirically that the belief density can be inferred as the function-space maximum a posteriori estimate. We demonstrate our method by eliciting multivariate belief densities of simulated experts, including the prior belief of a general-purpose large language model over a real-world dataset.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-380" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-380', event_id='96874', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4200</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96874">Trajectory Diffusion for ObjectGoal Navigation</a></strong></h5>


                        <p class="text-muted">
                            Xinyao Yu &middot; Sixian Zhang &middot; Xinhang Song &middot; Xiaorong Qin &middot; Shuqiang Jiang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Object goal navigation requires an agent to navigate to a specified object in an unseen environment based on visual observations and user-specified goals. Human decision-making in navigation is sequential, planning a most likely sequence of actions toward the goal. However, existing ObjectNav methods, both end-to-end learning methods and modular methods, rely on single-step planning. They output the next action based on the current model input, which easily overlooks temporal consistency and leads to myopic planning.To this end, we aim to learn sequence planning for ObjectNav. Specifically, we propose trajectory diffusion to learn the distribution of trajectory sequences conditioned on the current observation and the goal. We utilize DDPM and automatically collected optimal trajectory segments to train the trajectory diffusion.Once the trajectory diffusion model is trained, it can generate a temporally coherent sequence of future trajectory for agent based on its current observations.Experimental results on the Gibson and MP3D datasets demonstrate that the generated trajectories effectively guide the agent, resulting in more accurate and efficient navigation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-381" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-381', event_id='97424', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4201</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97424">Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning</a></strong></h5>


                        <p class="text-muted">
                            Haoyi Zhu &middot; Yating Wang &middot; Di Huang &middot; Weicai Ye &middot; Wanli Ouyang &middot; Tong He
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In robot learning, the observation space is crucial due to the distinct characteristics of different modalities, which can potentially become a bottleneck alongside policy design. In this study, we explore the influence of various observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. We introduce OBSBench, a benchmark comprising two simulators and 125 tasks, along with standardized pipelines for various encoders and policy baselines. Extensive experiments on diverse contact-rich manipulation tasks reveal a notable trend: point cloud-based methods, even those with the simplest designs, frequently outperform their RGB and RGB-D counterparts. This trend persists in both scenarios: training from scratch and utilizing pre-training. Furthermore, our findings demonstrate that point cloud observations often yield better policy performance and significantly stronger generalization capabilities across various geometric and visual conditions. These outcomes suggest that the 3D point cloud is a valuable observation modality for intricate robotic tasks. We also suggest that incorporating both appearance and coordinate information can enhance the performance of point cloud methods. We hope our work provides valuable insights and guidance for designing more generalizable and robust robotic models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-382" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-382', event_id='93780', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4202</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93780">Practical Shuffle Coding</a></strong></h5>


                        <p class="text-muted">
                            Julius Kunze &middot; Daniel Severo &middot; Jan-Willem van de Meent &middot; James Townsend
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present a general method for lossless compression of unordered data structures, including multisets and graphs. It is a variant of shuffle coding that is many orders of magnitude faster than the original and enables 'one-shot' compression of single unordered objects. Our method achieves state-of-the-art compression rates on various large-scale network graphs at speeds of megabytes per second, efficiently handling even a multi-gigabyte plain graph with one billion edges. We release an implementation that can be easily adapted to different data types and statistical models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-383" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-383', event_id='94777', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4203</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94777">Log-concave Sampling from a Convex Body with a Barrier: a Robust and Unified Dikin Walk</a></strong></h5>


                        <p class="text-muted">
                            Yuzhou Gu &middot; Nikki Lijing Kuang &middot; Yian Ma &middot; Zhao Song &middot; Lichen Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We consider the problem of sampling from a $d$-dimensional log-concave distribution $\pi(\theta) \propto \exp(-f(\theta))$ for $L$-Lipschitz $f$, constrained to a convex body (described by $n$ hyperplanes) equipped with a barrier function, contained in a ball of radius $R$ with a $w$-warm start. We propose a \emph{robust} sampling framework that computes spectral approximations to the Hessian of the barrier functions in each iteration. We prove that for the polytope constraints, sampling with the Lee-Sidford barrier function mixes within $\widetilde O((d^2+dL^2R^2)\log(w/\delta))$ steps with a per step cost of $\widetilde O(nd^{\omega-1})$, where $\omega\approx 2.37$ is the fast matrix multiplication exponent. Compared to the prior work of Mangoubi and Vishnoi, our approach gives faster mixing time as we are able to design a generalized soft-threshold Dikin walk beyond log-barrier.We further extend our result to show how to sample from a $d$-dimensional spectrahedron, the constrained set of a semidefinite program, specified by the set $\{x\in \mathbb{R}^d: \sum_{i=1}^d x_i A_i \succeq C \}$ where $A_1,\ldots,A_d, C$ are $n\times n$ real symmetric matrices. We design a walk that mixes in $\widetilde O((nd+dL^2R^2)\log(w/\delta))$ steps with a per iteration cost of $\widetilde O(n^\omega+n^2d^{3\omega-5})$. We improve the mixing time bound of prior best Dikin walk due to Narayanan and Rakhlin that mixes in $\widetilde O((n^2d^3+n^2dL^2R^2)\log(w/\delta))$ steps.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-384" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-384', event_id='93352', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4204</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93352">Physics-Informed Variational State-Space Gaussian Processes</a></strong></h5>


                        <p class="text-muted">
                            Oliver Hamelijnck &middot; Arno Solin &middot; Theodoros Damoulas
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Differential equations are important mechanistic models that are integral to many scientific and engineering applications. With the abundance of available data there has been a growing interest in data-driven physics-informed models. Gaussian processes (GPs) are particularly suited to this task as they can model complex, non-linear phenomena whilst incorporating prior knowledge and quantifying uncertainty. Current approaches have found some success but are limited as they either achieve poor computational scalings or focus only on the temporal setting. This work addresses these issues by introducing a variational spatio-temporal state-space GP that handles linear and non-linear physical constraints while achieving efficient linear-in-time computation costs. We demonstrate our methods in a range of synthetic and real-world settings and outperform the current state-of-the-art in both predictive and computational performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-385" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-385', event_id='93960', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4205</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93960">Nonstationary Sparse Spectral Permanental Process</a></strong></h5>


                        <p class="text-muted">
                            Zicheng Sun &middot; Yixuan Zhang &middot; Zenan Ling &middot; Xuhui Fan &middot; Feng Zhou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Existing permanental processes often impose constraints on kernel types or stationarity, limiting the model's expressiveness. To overcome these limitations, we propose a novel approach utilizing the sparse spectral representation of nonstationary kernels. This technique relaxes the constraints on kernel types and stationarity, allowing for more flexible modeling while reducing computational complexity to the linear level. Additionally, we introduce a deep kernel variant by hierarchically stacking multiple spectral feature mappings, further enhancing the model's expressiveness to capture complex patterns in data. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our approach, particularly in scenarios with pronounced data nonstationarity. Additionally, ablation studies are conducted to provide insights into the impact of various hyperparameters on model performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-386" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-386', event_id='98319', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4206</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/98319">Pre-trained Gaussian Processes for Bayesian Optimization</a></strong></h5>


                        <p class="text-muted">
                            Zi Wang &middot; George Dahl &middot; Kevin Swersky &middot; Chansoo Lee &middot; Zachary Nado &middot; Justin Gilmer &middot; Jasper Snoek &middot; Zoubin Ghahramani
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Bayesian optimization (BO) has become a popular strategy for global optimization of expensive real-world functions. Contrary to a common expectation that BO is suited to optimizing black-box functions, it actually requires domain knowledge about those functions to deploy BO successfully. Such domain knowledge often manifests in Gaussian process (GP) priors that specify initial beliefs on functions. However, even with expert knowledge, it is non-trivial to quantitatively define a prior. This is especially true for hyperparameter tuning problems on complex machine learning models, where landscapes of tuning objectives are often difficult to comprehend. We seek an alternative practice for setting these functional priors. In particular, we consider the scenario where we have data from similar functions that allow us to pre-train a tighter distribution a priori. We detail what pre-training entails for GPs using a KL divergence based loss function, and propose a new pre-training based BO framework named HyperBO. Theoretically, we show bounded posterior predictions and near-zero regrets for HyperBO without assuming the "ground truth" GP prior is known. To verify our approach in realistic setups, we collect a large multi-task hyperparameter tuning dataset by training tens of thousands of configurations of near-state-of-the-art deep learning models on popular image and text datasets, as well as a protein sequence dataset. Our results show that on average, HyperBO is able to locate good hyperparameters at least 3 times more efficiently than the best competing methods on both our new tuning dataset and existing multi-task BO benchmarks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-387" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-387', event_id='93406', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4207</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93406">Enhancing Diversity in Bayesian Deep Learning via Hyperspherical Energy Minimization of CKA</a></strong></h5>


                        <p class="text-muted">
                            David Smerkous &middot; Qinxun Bai &middot; Fuxin Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Particle-based Bayesian deep learning often requires a similarity metric to compare two networks. However, naive similarity metrics lack permutation invariance and are inappropriate for comparing networks. Centered Kernel Alignment (CKA) on feature kernels has been proposed to compare deep networks but has not been used as an optimization objective in Bayesian deep learning. In this paper, we explore the use of CKA in Bayesian deep learning to generate diverse ensembles and hypernetworks that output a network posterior. Noting that CKA projects kernels onto a unit hypersphere and that directly optimizing the CKA objective leads to diminishing gradients when two networks are very similar. We propose adopting the approach of hyperspherical energy (HE) on top of CKA kernels to address this drawback and improve training stability. Additionally, by leveraging CKA-based feature kernels, we derive feature repulsive terms applied to synthetically generated outlier examples. Experiments on both diverse ensembles and hypernetworks show that our approach significantly outperforms baselines in terms of uncertainty quantification in both synthetic and realistic outlier detection tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-388" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-388', event_id='93772', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4208</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93772">Bayesian Adaptive Calibration and Optimal Design</a></strong></h5>


                        <p class="text-muted">
                            Rafael Oliveira &middot; Dino Sejdinovic &middot; David Howard &middot; Edwin Bonilla
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The process of calibrating computer models of natural phenomena is essential for applications in the physical sciences, where plenty of domain knowledge can be embedded into simulations and then calibrated against real observations. Current machine learning approaches, however, mostly rely on rerunning simulations over a fixed set of designs available in the observed data, potentially neglecting informative correlations across the design space and requiring a large amount of simulations. Instead, we consider the calibration process from the perspective of Bayesian adaptive experimental design and propose a data-efficient algorithm to run maximally informative simulations within a batch-sequential process. At each round, the algorithm jointly estimates the parameters posterior distribution and optimal designs by maximising a variational lower bound of the expected information gain. The simulator is modelled as a sample from a Gaussian process, which allows us to correlate simulations and real data with the unknown calibration parameters. We show the benefits of our method when compared to related approaches across synthetic and real-data problems.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-389" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-389', event_id='94356', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4209</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94356">Probabilistic size-and-shape functional mixed models</a></strong></h5>


                        <p class="text-muted">
                            Fangyi Wang &middot; Karthik Bharath &middot; Oksana Chkrebtii &middot; Sebastian Kurtek
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The reliable recovery and uncertainty quantification of a fixed effect function $\mu$ in a functional mixed model, for modeling population- and object-level variability in noisily observed functional data, is a notoriously challenging task: variations along the $x$ and $y$ axes are confounded with additive measurement error, and cannot in general be disentangled. The question then as to what properties of $\mu$ may be reliably recovered becomes important. We demonstrate that it is possible to recover the size-and-shape of a square-integrable $\mu$ under a Bayesian functional mixed model. The size-and-shape of $\mu$ is a geometric property invariant to a family of space-time unitary transformations, viewed as rotations of the Hilbert space, that jointly transform the $x$ and $y$ axes. A random object-level unitary transformation then captures size-and-shape preserving deviations of $\mu$ from an individual function, while a random linear term and measurement error capture size-and-shape altering deviations. The model is regularized by appropriate priors on the unitary transformations, posterior summaries of which may then be suitably interpreted as optimal data-driven rotations of a fixed orthonormal basis for the Hilbert space. Our numerical experiments demonstrate utility of the proposed model, and superiority over the current state-of-the-art.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-390" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-390', event_id='94813', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4210</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94813">Conjugate Bayesian Two-step Change Point Detection for Hawkes Process</a></strong></h5>


                        <p class="text-muted">
                            Zeyue Zhang &middot; Xiaoling LU &middot; Feng Zhou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The Bayesian two-step change point detection method is popular for the Hawkes process due to its simplicity and intuitiveness. However, the non-conjugacy between the point process likelihood and the prior requires most existing Bayesian two-step change point detection methods to rely on non-conjugate inference methods. These methods lack analytical expressions, leading to low computational efficiency and impeding timely change point detection. To address this issue, this work employs data augmentation to propose a conjugate Bayesian two-step change point detection method for the Hawkes process, which proves to be more accurate and efficient. Extensive experiments on both synthetic and real data demonstrate the superior effectiveness and efficiency of our method compared to baseline methods. Additionally, we conduct ablation studies to explore the robustness of our method concerning various hyperparameters.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-391" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-391', event_id='93179', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4211</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93179">High Rank Path Development: an approach to learning the filtration of stochastic processes</a></strong></h5>


                        <p class="text-muted">
                            Jiajie Tao &middot; Hao Ni &middot; Chong Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Since the weak convergence for stochastic processes does not account for the growth of information over time which is represented by the underlying filtration, a slightly erroneous stochastic model in weak topology may cause huge loss in multi-periods decision making problems. To address such discontinuities, Aldous introduced the extended weak convergence, which can fully characterise all essential properties, including the filtration, of stochastic processes; however, it was considered to be hard to find efficient numerical implementations. In this paper, we introduce a novel metric called High Rank PCF Distance (HRPCFD) for extended weak convergence based on the high rank path development method from rough path theory, which also defines the characteristic function for measure-valued processes. We then show that such HRPCFD admits many favourable analytic properties which allows us to design an efficient algorithm for training HRPCFD from data and construct the HRPCF-GAN by using HRPCFD as the discriminator for conditional time series generation. Our numerical experiments on both hypothesis testing and generative modelling validate the out-performance of our approach compared with several state-of-the-art methods, highlighting its potential in broad applications of synthetic time series generation and in addressing classic financial and economic challenges, such as optimal stopping or utility maximisation problems. Code is available at https://github.com/DeepIntoStreams/High-Rank-PCF-GAN.git.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-392" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-392', event_id='96071', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4300</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96071">EASI: Evolutionary Adversarial Simulator Identification for Sim-to-Real Transfer</a></strong></h5>


                        <p class="text-muted">
                            Haoyu Dong &middot; Huiqiao Fu &middot; Wentao Xu &middot; Zhehao Zhou &middot; Chunlin Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Reinforcement Learning (RL) controllers have demonstrated remarkable performance in complex robot control tasks. However, the presence of reality gap often leads to poor performance when deploying policies trained in simulation directly onto real robots. Previous sim-to-real algorithms like Domain Randomization (DR) requires domain-specific expertise and suffers from issues such as reduced control performance and high training costs. In this work, we introduce Evolutionary Adversarial Simulator Identification (EASI), a novel approach that combines Generative Adversarial Network (GAN) and Evolutionary Strategy (ES) to address sim-to-real challenges. Specifically, we consider the problem of sim-to-real as a search problem, where ES acts as a generator in adversarial competition with a neural network discriminator, aiming to find physical parameter distributions that make the state transitions between simulation and reality as similar as possible. The discriminator serves as the fitness function, guiding the evolution of the physical parameter distributions. EASI features simplicity, low cost, and high fidelity, enabling the construction of a more realistic simulator with minimal requirements for real-world data, thus aiding in transferring simulated-trained policies to the real world. We demonstrate the performance of EASI in both sim-to-sim and sim-to-real tasks, showing superior performance compared to existing sim-to-real algorithms.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-393" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-393', event_id='95974', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4301</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95974">Reasoning Multi-Agent Behavioral Topology for Interactive Autonomous Driving</a></strong></h5>


                        <p class="text-muted">
                            Haochen Liu &middot; Li Chen &middot; Yu Qiao &middot; Chen Lv &middot; Hongyang Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Autonomous driving system aims for safe and social-consistent driving through the behavioral integration among interactive agents. However, challenges remain due to multi-agent scene uncertainty and heterogeneous interaction. Current dense and sparse behavioral representations struggle with inefficiency and inconsistency in multi-agent modeling, leading to instability of collective behavioral patterns when integrating prediction and planning (IPP). To address this, we initiate a topological formation that serves as a compliant behavioral foreground to guide downstream trajectory generations. Specifically, we introduce Behavioral Topology (BeTop), a pivotal topological formulation that explicitly represents the consensual behavioral pattern among multi-agent future. BeTop is derived from braid theory to distill compliant interactive topology from multi-agent future trajectories. A synergistic learning framework (BeTopNet) supervised by BeTop facilitates the consistency of behavior prediction and planning within the predicted topology priors. Through imitative contingency learning, BeTop also effectively manages behavioral uncertainty for prediction and planning. Extensive verification on large-scale real-world datasets, including nuPlan and WOMD, demonstrates that BeTop achieves state-of-the-art performance in both prediction and planning tasks. Further validations on the proposed interactive scenario benchmark showcase planning compliance in interactive cases. Code and model is available at https://github.com/OpenDriveLab/BeTop.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-394" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-394', event_id='93992', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4302</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93992">Variational Distillation of Diffusion Policies into Mixture of Experts</a></strong></h5>


                        <p class="text-muted">
                            Hongyi Zhou &middot; Denis Blessing &middot; Ge Li &middot; Onur Celik &middot; Xiaogang Jia &middot; Gerhard Neumann &middot; Rudolf Lioutikov
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This work introduces Variational Diffusion Distillation (VDD), a novel method that distills denoising diffusion policies into Mixtures of Experts (MoE) through variational inference. Diffusion Models are the current state-of-the-art in generative modeling due to their exceptional ability to accurately learn and represent complex, multi-modal distributions. This ability allows Diffusion Models to replicate the inherent diversity in human behavior, making them the preferred models in behavior learning such as Learning from Human Demonstrations (LfD).However, diffusion models come with some drawbacks, including the intractability of likelihoods and long inference times due to their iterative sampling process. The inference times, in particular, pose a significant challenge to real-time applications such as robot control.In contrast, MoEs effectively address the aforementioned issues while retaining the ability to represent complex distributions but are notoriously difficult to train.VDD is the first method that distills pre-trained diffusion models into MoE models, and hence, combines the expressiveness of Diffusion Models with the benefits of Mixture Models.Specifically, VDD leverages a decompositional upper bound of the variational objective that allows the training of each expert separately, resulting in a robust optimization scheme for MoEs.VDD demonstrates across nine complex behavior learning tasks, that it is able to: i) accurately distill complex distributions learned by the diffusion model, ii) outperform existing state-of-the-art distillation methods, and iii) surpass conventional methods for training MoE. The code and videos are available at https://intuitive-robots.github.io/vdd-website.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-395" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-395', event_id='96185', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4303</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96185">DPIC: Decoupling Prompt and Intrinsic Characteristics for LLM Generated Text Detection</a></strong></h5>


                        <p class="text-muted">
                            XIAO YU &middot; Yuang Qi &middot; Kejiang Chen &middot; Guoqiang Chen &middot; Xi Yang &middot; Pengyuan Zhu &middot; Xiuwei Shang &middot; Weiming Zhang &middot; Nenghai Yu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) have the potential to generate texts that pose risks of misuse, such as plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Consequently, detecting whether a text is generated by LLMs has become increasingly important. Existing high-quality detection methods usually require access to the interior of the model to extract the intrinsic characteristics. However, since we do not have access to the interior of the black-box model, we must resort to surrogate models, which impacts detection quality. In order to achieve high-quality detection of black-box models, we would like to extract deep intrinsic characteristics of the black-box model generated texts. We view the generation process as a coupled process of prompt and intrinsic characteristics of the generative model. Based on this insight, we propose to decouple prompt and intrinsic characteristics (DPIC) for LLM-generated text detection method. Specifically, given a candidate text, DPIC employs an auxiliary LLM to reconstruct the prompt corresponding to the candidate text, then uses the prompt to regenerate text by the auxiliary LLM, which makes the candidate text and the regenerated text align with their prompts, respectively. Then, the similarity between the candidate text and the regenerated text is used as a detection feature, thus eliminating the prompt in the detection process, which allows the detector to focus on the intrinsic characteristics of the generative model. Compared to the baselines, DPIC has achieved an average improvement of 6.76\% and 2.91\% in detecting texts from different domains generated by GPT4 and Claude3, respectively.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-396" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-396', event_id='96850', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4304</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96850">ANT: Adaptive Noise Schedule for Time Series Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Seunghan Lee &middot; Kibok Lee &middot; Taeyoung Park
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Advances in diffusion models for generative artificial intelligence have recently propagated to the time series (TS) domain, demonstrating state-of-the-art performance on various tasks. However, prior works on TS diffusion models often borrow the framework of existing works proposed in other domains without considering the characteristics of TS data, leading to suboptimal performance. In this work, wepropose Adaptive Noise schedule for Time series diffusion models (ANT), which automatically predetermines proper noise schedules for given TS datasets based on their statistics representing non-stationarity. Our intuition is that an optimal noise schedule should satisfy the following desiderata: 1) It linearly reduces the non-stationarity of TS data so that all diffusion steps are equally meaningful, 2) the data is corrupted to the random noise at the final step, and 3) the number of steps is sufficiently large. The proposed method is practical for use in that it eliminates the necessity of finding the optimal noise schedule with a small additional cost to compute the statistics for given datasets, which can be done offline before training. We validate the effectiveness of our method across various tasks, including TS forecasting, refinement, and generation, on datasets from diverse domains. Code is available at this repository: https://github.com/seunghan96/ANT.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-397" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-397', event_id='96119', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4305</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96119">Scaling Law for Time Series Forecasting</a></strong></h5>


                        <p class="text-muted">
                            Jingzhe Shi &middot; Qinwei Ma &middot; Huan Ma &middot; Lei Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Scaling law that rewards large datasets, complex models and enhanced data granularity has been observed in various fields of deep learning. Yet, studies on time series forecasting have cast doubt on scaling behaviors of deep learning methods for time series forecasting: while more training data improves performance, more capable models do not always outperform less capable models, and longer input horizon may hurt performance for some models. We propose a theory for scaling law for time series forecasting that can explain these seemingly abnormal behaviors. We take into account the impact of dataset size and model complexity, as well as time series data granularity, particularly focusing on the look-back horizon, an aspect that has been unexplored in previous theories. Furthermore, we empirically evaluate various models using a diverse set of time series forecasting datasets, which (1) verifies the validity of scaling law on dataset size and model complexity within the realm of time series forecasting, and (2) validates our theoretical framework, particularly regarding the influence of look back horizon. We hope our findings may inspire new models targeting time series forecasting datasets of limited size, as well as large foundational datasets and models for time series forecasting in future works.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-398" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-398', event_id='95770', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4306</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95770">TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables</a></strong></h5>


                        <p class="text-muted">
                            Yuxuan Wang &middot; Haixu Wu &middot; Jiaxiang Dong &middot; Guo Qin &middot; Haoran Zhang &middot; Yong Liu &middot; Yun-Zhong Qiu &middot; Jianmin Wang &middot; Mingsheng Long
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deep models have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous variables can provide valuable external information for endogenous variables. Thus, unlike well-established multivariate or univariate forecasting paradigms that either treat all the variables equally or ignore exogenous information, this paper focuses on a more practical setting: time series forecasting with exogenous variables. We propose a novel approach, TimeXer, to ingest external information to enhance the forecasting of endogenous variables. With deftly designed embedding layers, TimeXer empowers the canonical Transformer with the ability to reconcile endogenous and exogenous information, where patch-wise self-attention and variate-wise cross-attention are used simultaneously. Moreover, global endogenous tokens are learned to effectively bridge the causal information underlying exogenous series into endogenous temporal patches. Experimentally, TimeXer achieves consistent state-of-the-art performance on twelve real-world forecasting benchmarks and exhibits notable generality and scalability. Code is available at this repository: https://github.com/thuml/TimeXer.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-399" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-399', event_id='95653', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4307</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95653">Conformalized Time Series with Semantic Features</a></strong></h5>


                        <p class="text-muted">
                            Baiting Chen &middot; Zhimei Ren &middot; Lu Cheng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Conformal prediction is a powerful tool for uncertainty quantification, but its application to time-series data is constrained by the violation of the exchangeability assumption. Current solutions for time-series prediction typically operate in the output space and rely on manually selected weights to address distribution drift, leading to overly conservative predictions. To enable dynamic weight learning in the semantically rich latent space, we introduce a novel approach called Conformalized Time Series with Semantic Features (CT-SSF). CT-SSF utilizes the inductive bias in deep representation learning to dynamically adjust weights, prioritizing semantic features relevant to the current prediction. Theoretically, we show that CT-SSF surpasses previous methods defined in the output space. Experiments on synthetic and benchmark datasets demonstrate that CT-SSF significantly outperforms existing state-of-the-art (SOTA) conformal prediction techniques in terms of prediction efficiency while maintaining a valid coverage guarantee.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-400" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-400', event_id='93973', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4308</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93973">Con4m: Context-aware Consistency Learning Framework for Segmented Time Series Classification</a></strong></h5>


                        <p class="text-muted">
                            Junru Chen &middot; Tianyu Cao &middot; Jing Xu &middot; Jiahe Li &middot; Zhilong Chen &middot; Tao Xiao &middot; YANG YANG
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Time Series Classification (TSC) encompasses two settings: classifying entire sequences or classifying segmented subsequences. The raw time series for segmented TSC usually contain Multiple classes with Varying Duration of each class (MVD). Therefore, the characteristics of MVD pose unique challenges for segmented TSC, yet have been largely overlooked by existing works. Specifically, there exists a natural temporal dependency between consecutive instances (segments) to be classified within MVD. However, mainstream TSC models rely on the assumption of independent and identically distributed (i.i.d.), focusing on independently modeling each segment. Additionally, annotators with varying expertise may provide inconsistent boundary labels, leading to unstable performance of noise-free TSC models. To address these challenges, we first formally demonstrate that valuable contextual information enhances the discriminative power of classification instances. Leveraging the contextual priors of MVD at both the data and label levels, we propose a novel consistency learning framework Con4m, which effectively utilizes contextual information more conducive to discriminating consecutive segments in segmented TSC tasks, while harmonizing inconsistent boundary labels for training. Extensive experiments across multiple datasets validate the effectiveness of Con4m in handling segmented TSC tasks on MVD. The source code is available at https://github.com/MrNobodyCali/Con4m.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-401" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-401', event_id='99349', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4309</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/99349">Studying How to Efficiently and Effectively Guide Models with Explanations - A Reproducibility Study</a></strong></h5>


                        <p class="text-muted">
                            Adrian Sauter &middot; Milan Miletiƒá &middot; Ryan Ott &middot; Rohith Prabakaran
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Model guidance describes the approach of regularizing the explanations of a deep neu-
ral network model towards highlighting the correct features to ensure that the model is
‚Äúright for the right reasons‚Äù. Rao et al. (2023) conducted an in-depth evaluation of ef-
fective and efficient model guidance for object classification across various loss functions,
attributions methods, models, and ‚Äôguidance depths‚Äô to study the effectiveness of differ-
ent methods. Our work aims to (1) reproduce the main results obtained by Rao et al.
(2023), and (2) propose several extensions to their research. We conclude that the major
part of the original work is reproducible, with certain minor exceptions, which we discuss
in this paper. In our extended work, we point to an issue with the Energy Pointing Game
(EPG) metric used for evaluation and propose an extension for increasing its robustness.
In addition, we observe the EPG metric‚Äôs predisposition towards favoring larger bounding
boxes, a bias we address by incorporating a corrective penalty term into the original En-
ergy loss function. Furthermore, we revisit the feasibility of using segmentation masks in
light of the original study‚Äôs finding that minimal annotated data can significantly boost
model performance. Our findings suggests that Energy loss inherently guides models to
on-object features without the requirement for segmentation masks. Finally, we explore
the role of contextual information in object detection and, contrary to the assumption
that focusing solely on object-specific features suffices for accurate classification, our find-
ings suggest the importance of contextual cues in certain scenarios. 
Code available at: https://anonymous.4open.science/r/model<em>guidance</em>repro_study.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-402" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-402', event_id='98321', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4310</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/98321">An Analysis of Robustness of Non-Lipschitz Networks</a></strong></h5>


                        <p class="text-muted">
                            Maria-Florina Balcan &middot; Avrim Blum &middot; Dravyansh Sharma &middot; Hongyang Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite significant advances, deep networks remain highly susceptible to adversarial attack.  One fundamental challenge is that small input perturbations can often produce large movements in the network‚Äôs final-layer feature space.  In this paper, we define an attack model that abstracts this challenge, to help understand its intrinsic properties.  In our model, the adversary may move data an arbitrary distance in feature space but only in random low-dimensional subspaces.  We prove such adversaries can be quite powerful: defeating any algorithm that must classify any input it is given.  However, by allowing the algorithm to abstain on unusual inputs, we show such adversaries can be overcome when classes are reasonably well-separated in feature space. We further provide strong theoretical guarantees for setting algorithm parameters to optimize over accuracy-abstention trade-offs using data-driven methods. Our results provide new robustness guarantees for nearest-neighbor style algorithms, and also have application to contrastive learning, where we empirically demonstrate the ability of such algorithms to obtain high robust accuracy with low abstention rates.  Our model is also motivated by strategic classification, where entities being classified aim to manipulate their observable features to produce a preferred classification, and we provide new insights into that area as well.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-403" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-403', event_id='96895', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4311</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96895">Cooperate or Collapse:  Emergence of Sustainable Cooperation in a Society of LLM Agents</a></strong></h5>


                        <p class="text-muted">
                            Giorgio Piatti &middot; Zhijing Jin &middot; Max Kleiman-Weiner &middot; Bernhard Sch√∂lkopf &middot; Mrinmaya Sachan &middot; Rada Mihalcea
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage "Universalization"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-404" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-404', event_id='95705', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4401</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95705">Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models</a></strong></h5>


                        <p class="text-muted">
                            Yuancheng Xu &middot; Jiarui Yao &middot; Manli Shu &middot; Yanchao Sun &middot; Zichu Wu &middot; Ning Yu &middot; Tom Goldstein &middot; Furong Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, but their versatility raises security concerns. This study takes the first step in exposing VLMs‚Äô susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is a traditional Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is a novel Persuasion Attack, leveraging VLMs‚Äô text generation capabilities to craft persuasive and seemingly rational narratives for misinformation, such as portraying junk food as healthy. We show that Shadowcast effectively achieves the attacker‚Äôs intentions using as few as 50 poison samples. Crucially, the poisoned samples demonstrate transferability across different VLM architectures, posing a significant concern in black-box settings. Moreover, Shadowcast remains potent under realistic conditions involving various text prompts, training data augmentation, and image compression techniques. This work reveals how poisoned VLMs can disseminate convincing yet deceptive misinformation to everyday, benign users, emphasizing the importance of data integrity for responsible VLM deployments. Our code is available at: https://github.com/umd-huang-lab/VLM-Poisoning.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-405" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-405', event_id='95826', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4402</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95826">Training for Stable Explanation for Free</a></strong></h5>


                        <p class="text-muted">
                            Chao Chen &middot; Chenghua Guo &middot; Rufeng Chen &middot; Guixiang Ma &middot; Ming Zeng &middot; Xiangwen Liao &middot; Xi Zhang &middot; Sihong Xie
                        </p>

                    </div>
                    <div class="abstract">
                        <p>To foster trust in machine learning models, explanations must be faithful and stable for consistent insights. Existing relevant works rely on the $\ell_p$ distance for stability assessment, which diverges from human perception. Besides, existing adversarial training (AT) associated with intensive computations may lead to an arms race. To address these challenges, we introduce a novel metric to assess the stability of top-$k$ salient features. We introduce R2ET which trains for stable explanation by efficient and effective regularizer,and analyze R2ET by multi-objective optimization to prove numerical and statistical stability of explanations. Moreover, theoretical connections between R2ET and certified robustness justify R2ET's stability in all attacks. Extensive experiments across various data modalities and model architectures show that R2ET achieves superior stability against stealthy attacks, and generalizes effectively across different explanation methods. The code can be found at https://github.com/ccha005/R2ET.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-406" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-406', event_id='95896', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4403</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95896">Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            ShengYun Peng &middot; Pin-Yu Chen &middot; Matthew Hull &middot; Duen Horng Chau
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Safety alignment is crucial to ensure that large language models (LLMs) behave in ways that align with human preferences and prevent harmful actions during inference. However, recent studies show that the alignment can be easily compromised through finetuning with only a few adversarially designed training examples. We aim to measure the risks in finetuning LLMs through navigating the LLM safety landscape. We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as ‚Äúsafety basin‚Äù: random perturbations to model weights maintain the safety level of the original aligned model within its local neighborhood. However, outside this local region, safety is fully compromised, exhibiting a sharp, step-like drop. This safety basin contrasts sharply with the LLM capability landscape, where model performance peaks at the origin and gradually declines as random perturbation increases. Our discovery inspires us to propose the new VISAGE safety metric that measures the safety in LLM finetuning by probing its safety landscape. Visualizing the safety landscape of the aligned model enables us to understand how finetuning compromises safety by dragging the model away from the safety basin. The LLM safety landscape also highlights the system prompt‚Äôs critical role in protecting a model, and that such protection transfers to its perturbed variants within the safety basin. These observations from our safety landscape research provide newinsights for future work on LLM safety community. Our code is publicly available at https://github.com/ShengYun-Peng/llm-landscape.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-407" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-407', event_id='95917', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4404</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95917">Erasing Undesirable Concepts in Diffusion Models with Adversarial Preservation</a></strong></h5>


                        <p class="text-muted">
                            Anh Bui &middot; Tung-Long Vuong &middot; Khanh Doan &middot; Trung Le &middot; Paul Montague &middot; Tamas Abraham &middot; Dinh Phung
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion models excel at generating visually striking content from text but can inadvertently produce undesirable or harmful content when trained on unfiltered internet data. A practical solution is to selectively removing target concepts from the model, but this may impact the remaining concepts. Prior approaches have tried to balance this by introducing a loss term to preserve neutral content or a regularization term to minimize changes in the model parameters, yet resolving this trade-off remains challenging. In this work, we propose to identify and preserving concepts most affected by parameter changes, termed as <em>adversarial concepts</em>. This approach ensures stable erasure with minimal impact on the other concepts. We demonstrate the effectiveness of our method using the Stable Diffusion model, showing that it outperforms state-of-the-art erasure methods in eliminating unwanted content while maintaining the integrity of other unrelated elements. Our code is available at \url{https://github.com/tuananhbui89/Erasing-Adversarial-Preservation}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-408" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-408', event_id='95958', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4405</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95958">Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously</a></strong></h5>


                        <p class="text-muted">
                            Yihan Wang &middot; Yifan Zhu &middot; Xiao-Shan Gao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Availability attacks provide a tool to prevent the unauthorized use of private data and commercial datasets by generating imperceptible noise and crafting unlearnable examples before release. Ideally, the obtained unlearnability can prevent algorithms from training usable models. When supervised learning (SL) algorithms have failed, a malicious data collector possibly resorts to contrastive learning (CL) algorithms to bypass the protection.Through evaluation, we have found that most existing methods are unable to achieve both supervised and contrastive unlearnability, which poses risks to data protection by availability attacks.Different from recent methods based on contrastive learning, we employ contrastive-like data augmentations in supervised learning frameworks to obtain attacks effective for both SL and CL.Our proposed AUE and AAP attacks achieve state-of-the-art worst-case unlearnability across SL and CL algorithms with less computation consumption, showcasing prospects in real-world applications. The code is available at https://github.com/EhanW/AUE-AAP.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-409" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-409', event_id='96006', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4406</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96006">ZeroMark: Towards Dataset Ownership Verification without Disclosing Watermark</a></strong></h5>


                        <p class="text-muted">
                            Junfeng Guo &middot; Yiming Li &middot; Ruibo Chen &middot; Yihan Wu &middot; chenxi liu &middot; Heng Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>High-quality public datasets significantly prompt the prosperity of deep neural networks (DNNs). Currently, dataset ownership verification (DOV), which consists of dataset watermarking and ownership verification, is the only feasible solution to protect their copyright by preventing unauthorized use. In this paper, we revisit existing DOV methods and find that they all mainly focused on the first stage by designing different types of dataset watermarks and directly exploiting watermarked samples as the verification samples for ownership verification. As such, their success relies on an underlying assumption that verification is a \emph{one-time} and \emph{privacy-preserving} process, which does not necessarily hold in practice. To alleviate this problem, we propose \emph{ZeroMark} to conduct ownership verification without disclosing dataset-specified watermarks. Our method is inspired by our empirical and theoretical findings of the intrinsic property of DNNs trained on the watermarked dataset. Specifically, ZeroMark first generates the closest boundary version of given benign samples and calculates their boundary gradients under the label-only black-box setting. After that, it examines whether the given suspicious method has been trained on the protected dataset by performing a hypothesis test, based on the cosine similarity measured on the boundary gradients and the watermark pattern. Extensive experiments on benchmark datasets verify the effectiveness of our ZeroMark and its resistance to potential adaptive attacks. The codes for reproducing our main experiments are publicly available at \href{https://github.com/JunfengGo/ZeroMark.git}{GitHub}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-410" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-410', event_id='96060', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4407</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96060">Breaking the False Sense of Security in Backdoor Defense through Re-Activation Attack</a></strong></h5>


                        <p class="text-muted">
                            Mingli Zhu &middot; Siyuan Liang &middot; Baoyuan Wu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deep neural networks face persistent challenges in defending against backdoor attacks, leading to an ongoing battle between attacks and defenses. While existing backdoor defense strategies have shown promising performance on reducing attack success rates, can we confidently claim that the backdoor threat has truly been eliminated from the model? To address it, we re-investigate the characteristics of the backdoored models after defense (denoted as defense models). Surprisingly, we find that the original backdoors still exist in defense models derived from existing post-training defense strategies, and the backdoor existence is measured by a novel metric called backdoor existence coefficient. It implies that the backdoors just lie dormant rather than being eliminated. To further verify this finding, we empirically show that these dormant backdoors can be easily re-activated during inference stage, by manipulating the original trigger with well-designed tiny perturbation using universal adversarial attack. More practically, we extend our backdoor re-activation to black-box scenario, where the defense model can only be queried by the adversary during inference stage, and develop two effective methods, i.e., query-based and transfer-based backdoor re-activation attacks. The effectiveness of the proposed methods are verified on both image classification and multimodal contrastive learning (i.e., CLIP) tasks. In conclusion, this work uncovers a critical vulnerability that has never been explored in existing defense strategies, emphasizing the urgency of designing more robust and advanced backdoor defense mechanisms in the future.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-411" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-411', event_id='96211', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4408</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96211">The Best of Both Worlds: On the Dilemma of Out-of-distribution Detection</a></strong></h5>


                        <p class="text-muted">
                            Qingyang Zhang &middot; Qiuxuan Feng &middot; Joey Tianyi Zhou &middot; Yatao Bian &middot; Qinghua Hu &middot; Changqing Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Out-of-distribution (OOD) detection is essential for model trustworthiness which aims to sensitively identity semantic OOD samples and robustly generalize for covariate-shifted OOD samples. However, we discover that the superior OOD detection performance of state-of-the-art methods is achieved by secretly sacrificing the OOD generalization ability. The classification accuracy frequently collapses catastrophically when even slight noise is encountered. Such a phenomenon violates the motivation of trustworthiness and significantly limits the model's deployment in the real world. What is the hidden reason behind such a limitation? In this work, we theoretically demystify the "\textit{sensitive-robust}" dilemma that lies in previous OOD detection methods. Consequently, a theory-inspired algorithm is induced to overcome such a dilemma. By decoupling the uncertainty learning objective from a Bayesian perspective, the conflict between OOD detection and OOD generalization is naturally harmonized and a dual-optimized performance could be expected. Empirical studies show that our method achieves superior performance on commonly used benchmarks. To our best knowledge, this work is the first principled OOD detection method that achieves state-of-the-art OOD detection performance without sacrificing OOD generalization ability. Our code is available at https://github.com/QingyangZhang/DUL.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-412" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-412', event_id='96362', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4409</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96362">Improved Generation of Adversarial Examples Against Safety-aligned LLMs</a></strong></h5>


                        <p class="text-muted">
                            Qizhang Li &middot; Yiwen Guo &middot; Wangmeng Zuo &middot; Hao Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Adversarial prompts (or say, adversarial examples) generated using gradient-based methods exhibit outstanding performance in performing automatic jailbreak attacks against safety-aligned LLMs. Nevertheless, due to the discrete nature of texts, the input gradient of LLMs struggles to precisely reflect the magnitude of loss change that results from token replacements in the prompt, leading to limited attack success rates against safety-aligned LLMs, even in the <em>white-box</em> setting. In this paper, we explore a new perspective on this problem, suggesting that it can be alleviated by leveraging innovations inspired in transfer-based attacks that were originally proposed for attacking <em>black-box</em> image classification models. For the first time, we appropriate the ideologies of effective methods among these transfer-based attacks, <em>i.e.</em>, Skip Gradient Method and Intermediate Level Attack, into gradient-based adversarial prompt generation and achieve significant performance gains without introducing obvious computational cost. Meanwhile, by discussing mechanisms behind the gains, new insights are drawn, and proper combinations of these methods are also developed. Our empirical results show that 87% of the query-specific adversarial suffixes generated by the developed combination can induce Llama-2-7B-Chat to produce the output that exactly matches the target string on AdvBench. This match rate is 33% higher than that of a very strong baseline known as GCG, demonstrating advanced discrete optimization for adversarial prompt generation against LLMs. In addition, without introducing obvious cost, the combination achieves &gt;30% absolute increase in attack success rates compared with GCG when generating both query-specific (38% ->68%) and universal adversarial prompts (26.68% -> 60.32%) for attacking the Llama-2-7B-Chat model on AdvBench.Code at: https://github.com/qizhangli/Gradient-based-Jailbreak-Attacks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-413" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-413', event_id='96556', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4410</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96556">Federated Model Heterogeneous Matryoshka Representation Learning</a></strong></h5>


                        <p class="text-muted">
                            Liping Yi &middot; Han Yu &middot; Chao Ren &middot; Gang Wang &middot; xiaoguang Liu &middot; Xiaoxiao Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Model heterogeneous federated learning (MHeteroFL) enables FL clients to collaboratively train models with heterogeneous structures in a distributed fashion. However, existing MHeteroFL methods rely on training loss to transfer knowledge between the client model and the server model, resulting in limited knowledge exchange. To address this limitation, we propose the **Fed**erated model heterogeneous **M**atryoshka **R**epresentation **L**earning (**FedMRL**) approach for supervised learning tasks. It adds an auxiliary small homogeneous model shared by clients with heterogeneous local models. (1) The generalized and personalized representations extracted by the two models' feature extractors are fused by a personalized lightweight representation projector. This step enables representation fusion to adapt to local data distribution. (2) The fused representation is then used to construct Matryoshka representations with multi-dimensional and multi-granular embedded representations learned by the global homogeneous model header and the local heterogeneous model header. This step facilitates multi-perspective representation learning and improves model learning capability. Theoretical analysis shows that FedMRL achieves a $O(1/T)$ non-convex convergence rate. Extensive experiments on benchmark datasets demonstrate its superior model accuracy with low communication and computational costs compared to seven state-of-the-art baselines. It achieves up to 8.48% and 24.94% accuracy improvement compared with the state-of-the-art and the best same-category baseline, respectively.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-414" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-414', event_id='96716', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4411</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96716">Diffusion-based  Layer-wise Semantic Reconstruction for Unsupervised Out-of-Distribution Detection</a></strong></h5>


                        <p class="text-muted">
                            Ying Yang &middot; De Cheng &middot; Chaowei Fang &middot; Yubiao Wang &middot; Changzhe Jiao &middot; Lechao Cheng &middot; Nannan Wang &middot; Xinbo Gao
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Unsupervised out-of-distribution (OOD) detection aims to identify out-of-domain data by learning only from unlabeled In-Distribution (ID) training samples, which is crucial for developing a safe real-world machine learning system. Current reconstruction-based method provides a good alternative approach, by measuring the reconstruction error between the input and its corresponding generative counterpart in the pixel/feature space. However, such generative methods face the key dilemma, $i.e.$, improving the reconstruction power of the generative model, while keeping compact representation of the ID data. To address this issue, we propose the diffusion-based layer-wise semantic reconstruction approach for unsupervised OOD detection. The innovation of our approach is that we leverage the diffusion model's intrinsic data reconstruction ability to distinguish ID samples from OOD samples in the latent feature space. Moreover, to set up a comprehensive and discriminative feature representation, we devise a multi-layer semantic feature extraction strategy. Through distorting the extracted features with Gaussian noises and applying the diffusion model for feature reconstruction, the separation of ID and OOD samples is implemented according to the reconstruction errors. Extensive experimental results on multiple benchmarks built upon various datasets demonstrate that our method achieves state-of-the-art performance in terms of detection accuracy and speed.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-415" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-415', event_id='95643', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4500</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95643">Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models</a></strong></h5>


                        <p class="text-muted">
                            Yuxin Wen &middot; Leo Marchyok &middot; Sanghyun Hong &middot; Jonas Geiping &middot; Tom Goldstein &middot; Nicholas Carlini
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>It is commonplace to produce application-specific models by fine-tuning large pre-trained models using a small bespoke dataset. The widespread availability of foundation model checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This black-box privacy attack aims to amplify the privacy leakage that arises when fine-tuning a model: when a victim fine-tunes a backdoored model, their training data will be leaked at a significantly higher rate than if they had fine-tuned a typical model. We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different fine-tuning methods and inference strategies to thoroughly analyze this new threat. Our findings highlight a critical privacy concern within the machine learning community and call for a re-evaluation of safety protocols in the use of open-source pre-trained models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-416" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-416', event_id='95487', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4501</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95487">Beyond Slow Signs in High-fidelity Model Extraction</a></strong></h5>


                        <p class="text-muted">
                            Hanna Foerster &middot; Robert Mullins &middot; I Shumailov &middot; Jamie Hayes
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deep neural networks, costly to train and rich in intellectual property value, areincreasingly threatened by model extraction attacks that compromise their confiden-tiality. Previous attacks have succeeded in reverse-engineering model parametersup to a precision of float64 for models trained on random data with at most threehidden layers using cryptanalytical techniques. However, the process was identifiedto be very time consuming and not feasible for larger and deeper models trained onstandard benchmarks. Our study evaluates the feasibility of parameter extractionmethods of Carlini et al. [1] further enhanced by Canales-Mart√≠nez et al. [2] formodels trained on standard benchmarks. We introduce a unified codebase thatintegrates previous methods and reveal that computational tools can significantlyinfluence performance. We develop further optimisations to the end-to-end attackand improve the efficiency of extracting weight signs by up to 14.8 times com-pared to former methods through the identification of easier and harder to extractneurons. Contrary to prior assumptions, we identify extraction of weights, notextraction of weight signs, as the critical bottleneck. With our improvements, a16,721 parameter model with 2 hidden layers trained on MNIST is extracted withinonly 98 minutes compared to at least 150 minutes previously. Finally, addressingmethodological deficiencies observed in previous studies, we propose new ways ofrobust benchmarking for future model extraction attacks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-417" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-417', event_id='95271', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4502</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95271">Controlling Counterfactual Harm in Decision Support Systems Based on Prediction Sets</a></strong></h5>


                        <p class="text-muted">
                            Eleni Straitouri &middot; Suhas Thejaswi &middot; Manuel Rodriguez
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Decision support systems based on prediction sets help humans solve multiclass classification tasks by narrowing down the set of potential label values to a subset of them, namely a prediction set, and asking them to always predict label values from the prediction sets. While this type of systems have been proven to be effective at improving the average accuracy of the predictions made by humans, by restricting human agency, they may cause harm---a human who has succeeded at predicting the ground-truth label of an instance on their own may have failed had they used these systems. In this paper, our goal is to control how frequently a decision support system based on prediction sets may cause harm, by design. To this end, we start by characterizing the above notion of harm using the theoretical framework of structural causal models. Then, we show that, under a natural, albeit unverifiable, monotonicity assumption, we can estimate how frequently a system may cause harm using only predictions made by humans on their own. Further, we also show that, under a weaker monotonicity assumption, which can be verified experimentally, we can bound how frequently a system may cause harm again using only predictions made by humans on their own. Building upon these assumptions, we introduce a computational framework to design decision support systems based on prediction sets that are guaranteed to cause harm less frequently than a user-specified value using conformal risk control. We validate our framework using real human predictions from two different human subject studies and show that, in decision support systems based on prediction sets, there is a trade-off between accuracy and counterfactual harm.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-418" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-418', event_id='95152', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4503</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95152">LACIE: Listener-Aware Finetuning for Calibration in Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Elias Stengel-Eskin &middot; Peter Hase &middot; Mohit Bansal
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>When answering questions, large language models (LLMs) can convey not only an answer to the question, but a level of confidence about the answer being correct. This includes explicit markers of confidence (e.g. giving a numeric confidence score) as well as implicit markers, like using an authoritative tone or elaborating with additional knowledge of a subject. For LLMs to be trustworthy sources of knowledge, the confidence they convey should match their actual expertise on a topic; however, this is currently not the case, with most models tending towards overconfidence. To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that directly models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener. Specifically, we cast calibration as a preference optimization problem, creating data via a two-agent speaker-listener game, where a speaker model‚Äôs outputs are judged by a simulated listener. We then finetune three different LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the models resulting from this multi-agent optimization are better calibrated on TriviaQA with respect to a simulated listener. Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM‚Äôs answers to trivia questions, finding that training with LACIE results in 47% fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers. Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis indicates that LACIE leads to a better separation in confidence between correct and incorrect examples. Qualitatively, we find that a LACIE-trained model hedges more when uncertain and adopts implicit cues to signal certainty when it is correct, such as using an authoritative tone or including details. Finally, finetuning with our listener- aware method leads to an emergent increase in model abstention (e.g. saying ‚ÄúI don‚Äôt know‚Äù) for answers that are likely to be wrong, trading recall for precision.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-419" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-419', event_id='94981', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4504</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94981">RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models</a></strong></h5>


                        <p class="text-muted">
                            Maya Varma &middot; Jean-Benoit Delbrouck &middot; Zhihong Chen &middot; Akshay Chaudhari &middot; Curtis Langlotz
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) primarily operate at the global image-level rather than intervening directly on fine-grained image features and (ii) are predominantly designed for unimodal settings. In this work, we present RaVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level. Given a fine-tuned VLM, RaVL first discovers spurious correlations by leveraging a region-level clustering approach to identify precise image features contributing to zero-shot classification errors. Then, RaVL mitigates the identified spurious correlation with a novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with various model architectures, data domains, and learned spurious correlations. Our results show that RaVL accurately discovers (191% improvement over the closest baseline) and mitigates (8.2% improvement on worst-group image classification accuracy) spurious correlations. Qualitative evaluations on general-domain and medical-domain VLMs confirm our findings.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-420" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-420', event_id='94902', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4505</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94902">Weak Supervision Performance Evaluation via Partial Identification</a></strong></h5>


                        <p class="text-muted">
                            Felipe Maia Polo &middot; Subha Maity &middot; Mikhail Yurochkin &middot; Moulinath Banerjee &middot; Yuekai Sun
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Programmatic Weak Supervision (PWS) enables supervised model training without direct access to ground truth labels, utilizing weak labels from heuristics, crowdsourcing, or pre-trained models. However, the absence of ground truth complicates model evaluation, as traditional metrics such as accuracy, precision, and recall cannot be directly calculated. In this work, we present a novel method to address this challenge by framing model evaluation as a partial identification problem and estimating performance bounds using Fr√©chet bounds. Our approach derives reliable bounds on key metrics without requiring labeled data, overcoming core limitations in current weak supervision evaluation techniques. Through scalable convex optimization, we obtain accurate and computationally efficient bounds for metrics including accuracy, precision, recall, and F1-score, even in high-dimensional settings. This framework offers a robust approach to assessing model quality without ground truth labels, enhancing the practicality of weakly supervised learning for real-world applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-421" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-421', event_id='94850', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4506</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94850">Fine-Tuning Personalization in Federated Learning to Mitigate Adversarial Clients</a></strong></h5>


                        <p class="text-muted">
                            Youssef Allouah &middot; Abdellah El Mrini &middot; Rachid Guerraoui &middot; Nirupam Gupta &middot; Rafael Pinot
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Federated learning (FL) is an appealing paradigm that allows a group of machines(a.k.a. clients) to learn collectively while keeping their data local. However, dueto the heterogeneity between the clients‚Äô data distributions, the model obtainedthrough the use of FL algorithms may perform poorly on some client‚Äôs data.Personalization addresses this issue by enabling each client to have a differentmodel tailored to their own data while simultaneously benefiting from the otherclients‚Äô data. We consider an FL setting where some clients can be adversarial, andwe derive conditions under which full collaboration fails. Specifically, we analyzethe generalization performance of an interpolated personalized FL framework in thepresence of adversarial clients, and we precisely characterize situations when fullcollaboration performs strictly worse than fine-tuned personalization. Our analysisdetermines how much we should scale down the level of collaboration, accordingto data heterogeneity and the tolerable fraction of adversarial clients. We supportour findings with empirical results on mean estimation and binary classificationproblems, considering synthetic and benchmark image classification datasets</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-422" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-422', event_id='94754', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4507</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94754">When Your AIs Deceive You: Challenges of Partial Observability in Reinforcement Learning from Human Feedback</a></strong></h5>


                        <p class="text-muted">
                            Leon Lang &middot; Davis Foote &middot; Stuart J Russell &middot; Anca Dragan &middot; Erik Jenner &middot; Scott Emmons
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Past analyses of reinforcement learning from human feedback (RLHF) assume that the human evaluators fully observe the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deceptive inflation and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. Under the new assumption that the human's partial observability is known and accounted for, we then analyze how much information the feedback process provides about the return function. We show that sometimes, the human's feedback determines the return function uniquely up to an additive constant, but in other realistic cases, there is irreducible ambiguity. We propose exploratory research directions to help tackle these challenges and experimentally validate both the theoretical concerns and potential mitigations, and caution against blindly applying RLHF in partially observable settings.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-423" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-423', event_id='94472', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4508</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94472">Optimistic Verifiable Training by Controlling Hardware Nondeterminism</a></strong></h5>


                        <p class="text-muted">
                            Megha Srivastava &middot; Simran Arora &middot; Dan Boneh
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning and backdoors, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which can be difficult to scale, and ``optimistic'' methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thresholding procedure, to successfully control for  nondeterminism. Across three different NVIDIA GPUs (A40, Titan XP, RTX 2080 Ti), we achieve exact training replication at FP32 precision for both full-training and fine-tuning of ResNet-50 (23M) and GPT-2 (117M) models. Our  verifiable training scheme significantly decreases the storage and time costs compared to proof-based systems.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-424" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-424', event_id='94422', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4509</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94422">Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models</a></strong></h5>


                        <p class="text-muted">
                            Lu Yu &middot; Haiyang Zhang &middot; Changsheng Xu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Due to the impressive zero-shot capabilities, pre-trained vision-language models (e.g. CLIP), have attracted widespread attention and adoption across various domains. Nonetheless, CLIP has been observed to be susceptible to adversarial examples. Through experimental analysis, we have observed a phenomenon wherein adversarial perturbations induce shifts in text-guided attention. Building upon this observation, we propose a simple yet effective strategy: Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR). This framework incorporates two components: the Attention Refinement module and the Attention-based Model Constraint module. Our goal is to maintain the generalization of the CLIP model and enhance its adversarial robustness: The Attention Refinement module aligns the text-guided attention obtained from the target model via adversarial examples with the text-guided attention acquired from the original model via clean examples. This alignment enhances the model‚Äôs robustness. Additionally, the Attention-based Model Constraint module acquires text-guided attention from both the target and original models using clean examples. Its objective is to maintain model performance on clean samples while enhancing overall robustness. The experiments validate that our method yields a 9.58% enhancement in zero-shot robust accuracy over the current state-of-the-art techniques across 16 datasets. Our code is available at https://github.com/zhyblue424/TGA-ZSR.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-425" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-425', event_id='94345', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4510</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94345">Neural Model Checking</a></strong></h5>


                        <p class="text-muted">
                            Mirco Giacobbe &middot; Daniel Kroening &middot; Abhinandan Pal &middot; Michael Tautschnig
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce a machine learning approach to model checking temporal logic, with application to formal hardware verification. Model checking answers the question of whether every execution of a given system satisfies a desired temporal logic specification. Unlike testing, model checking provides formal guarantees. Its application is expected standard in silicon design and the EDA industry has invested decades into the development of performant symbolic model checking algorithms. Our new approach combines machine learning and symbolic reasoning by using neural networks as formal proof certificates for linear temporal logic. We train our neural certificates from randomly generated executions of the system and we then symbolically check their validity using satisfiability solving which, upon the affirmative answer, establishes that the system provably satisfies the specification. We leverage the expressive power of neural networks to represent proof certificates as well as the fact that checking a certificate is much simpler than finding one. As a result, our machine learning procedure for model checking is entirely unsupervised, formally sound, and practically effective. We experimentally demonstrate that our method outperforms the state-of-the-art academic and commercial model checkers on a set of standard hardware designs written in SystemVerilog.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-426" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-426', event_id='94324', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4511</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94324">Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement</a></strong></h5>


                        <p class="text-muted">
                            Zhehao Huang &middot; Xinwen Cheng &middot; JingHao Zheng &middot; Haoran Wang &middot; Zhengbao He &middot; Tao Li &middot; Xiaolin Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Machine unlearning (MU) has emerged to enhance the privacy and trustworthiness of deep neural networks. Approximate MU is a practical method for large-scale models. Our investigation into approximate MU starts with identifying the steepest descent direction, minimizing the output Kullback-Leibler divergence to exact MU inside a parameters' neighborhood. This probed direction decomposes into three components: weighted forgetting gradient ascent, fine-tuning retaining gradient descent, and a weight saliency matrix. Such decomposition derived from Euclidean metric encompasses most existing gradient-based MU methods. Nevertheless, adhering to Euclidean space may result in sub-optimal iterative trajectories due to the overlooked geometric structure of the output probability space. We suggest embedding the unlearning update into a manifold rendered by the remaining geometry, incorporating second-order Hessian from the remaining data. It helps prevent effective unlearning from interfering with the retained performance. However, computing the second-order Hessian for large-scale models is intractable. To efficiently leverage the benefits of Hessian modulation, we propose a fast-slow parameter update strategy to implicitly approximate the up-to-date salient unlearning direction.Free from specific modal constraints, our approach is adaptable across computer vision unlearning tasks, including classification and generation. Extensive experiments validate our efficacy and efficiency. Notably, our method successfully performs class-forgetting on ImageNet using DiT and forgets a class on CIFAR-10 using DDPM in just 50 steps, compared to thousands of steps required by previous methods. Code is available at <a href="https://github.com/K1nght/Unified-Unlearning-w-Remain-Geometry">Unified-Unlearning-w-Remain-Geometry</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-427" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-427', event_id='97863', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4600</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97863">NetworkGym: Reinforcement Learning Environments for Multi-Access Traffic Management in Network Simulation</a></strong></h5>


                        <p class="text-muted">
                            Momin Haider &middot; Ming Yin &middot; Menglei Zhang &middot; Arpit Gupta &middot; Jing Zhu &middot; Yu-Xiang Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Mobile devices such as smartphones, laptops, and tablets can often connect to multiple access networks (e.g., Wi-Fi, LTE, and 5G) simultaneously.Recent advancements facilitate seamless integration of these connections below the transport layer, enhancing the experience for apps that lack inherent multi-path support.This optimization hinges on dynamically determining the traffic distribution across networks for each device, a process referred to as multi-access traffic splitting.This paper introduces NetworkGym, a high-fidelity network environment simulator that facilitates generating multiple network traffic flows and multi-access traffic splitting.This simulator facilitates training and evaluating different RL-based solutions for the multi-access traffic splitting problem.Our initial explorations demonstrate that the majority of existing state-of-the-art offline RL algorithms (e.g. CQL) fail to outperform certain hand-crafted heuristic policies on average.This illustrates the urgent need to evaluate offline RL algorithms against a broader range of benchmarks, rather than relying solely on popular ones such as D4RL.We also propose an extension to the TD3+BC algorithm, named Pessimistic TD3 (PTD3), and demonstrate that it outperforms many state-of-the-art offline RL algorithms.PTD3's behavioral constraint mechanism, which relies on value-function pessimism, is theoretically motivated and relatively simple to implement.We open source our code and offline datasets at github.com/hmomin/networkgym.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-428" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-428', event_id='92958', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4601</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92958">Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses</a></strong></h5>


                        <p class="text-muted">
                            Xiaosen Zheng &middot; Tianyu Pang &middot; Chao Du &middot; Qian Liu &middot; Jing Jiang &middot; Min Lin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recently, Anil et al. (2024) show that many-shot (up to hundreds of) demonstrations can jailbreak state-of-the-art LLMs by exploiting their long-context capability. Nevertheless, is it possible to use few-shot demonstrations to efficiently jailbreak LLMs within limited context sizes? While the vanilla few-shot jailbreaking may be inefficient, we propose improved techniques such as injecting special system tokens like [/INST] and employing demo-level random search from a collected demo pool. These simple techniques result in surprisingly effective jailbreaking against aligned LLMs (even with advanced defenses). For example, our method achieves &gt;80% (mostly &gt;95%) ASRs on Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are enhanced by strong defenses such as perplexity detection and/or SmoothLLM, which is challenging for suffix-based jailbreaking. In addition, we conduct comprehensive and elaborate (e.g., making sure to use correct system prompts) evaluations against other aligned LLMs and advanced defenses, where our method consistently achieves nearly 100% ASRs. Our code is available at https://github.com/sail-sg/I-FSJ.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-429" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-429', event_id='92999', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4602</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92999">From Trojan Horses to Castle Walls: Unveiling Bilateral Data Poisoning Effects in Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Zhuoshi Pan &middot; Yuguang Yao &middot; Gaowen Liu &middot; Bingquan Shen &middot; H. Vicky Zhao &middot; Ramana Kompella &middot; Sijia Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While state-of-the-art diffusion models (DMs) excel in image generation, concerns regarding their security persist. Earlier research highlighted DMs' vulnerability to data poisoning attacks, but these studies placed stricter requirements than conventional methods like 'BadNets' in image classification. This is because the art necessitates modifications to the diffusion training and sampling procedures. Unlike the prior work, we investigate whether BadNets-like data poisoning methods can directly degrade the generation by DMs. In other words, if only the training dataset is contaminated (without manipulating the diffusion process), how will this affect the performance of learned DMs? In this setting, we uncover bilateral data poisoning effects that not only serve an adversarial purpose (compromising the functionality of DMs) but also offer a defensive advantage (which can be leveraged for defense in classification tasks against poisoning attacks). We show that a BadNets-like data poisoning attack remains effective in DMs for producing incorrect images (misaligned with the intended text conditions). Meanwhile, poisoned DMs exhibit an increased ratio of triggers, a phenomenon we refer to as 'trigger amplification', among the generated images. This insight can be then used to enhance the detection of poisoned training data. In addition, even under a low poisoning ratio, studying the poisoning effects of DMs is also valuable for designing robust image classifiers against such attacks. Last but not least, we establish a meaningful linkage between data poisoning and the phenomenon of data replications by exploring DMs' inherent data memorization tendencies. Code is available at https://github.com/OPTML-Group/BiBadDiff.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-430" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-430', event_id='93091', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4603</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93091">Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates</a></strong></h5>


                        <p class="text-muted">
                            Kaifeng Lyu &middot; Haoyu Zhao &middot; Xinran Gu &middot; Dingli Yu &middot; Anirudh Goyal &middot; Sanjeev Arora
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Public LLMs such as the Llama 2-Chat underwent alignment training and were considered safe. Recently Qi et al. (2024) reported that even benign fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. We focus on the setting where a public model is fine-tuned before serving users for specific usage, where the model should improve on the downstream task while maintaining alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the ‚ÄúPure Tuning, Safe Testing‚Äù (PTST) strategy --- fine-tune models without a safety prompt, but include it at test time. This seemingly counterintuitive strategy incorporates an intended distribution shift to encourage alignment preservation. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-431" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-431', event_id='93462', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4604</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93462">Bridging OOD Detection and Generalization: A Graph-Theoretic View</a></strong></h5>


                        <p class="text-muted">
                            Han Wang &middot; Sharon Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In the context of modern machine learning, models deployed in real-world scenarios often encounter diverse data shifts like covariate and semantic shifts, leading to challenges in both out-of-distribution (OOD) generalization and detection. Despite considerable attention to these issues separately, a unified framework for theoretical understanding and practical usage is lacking. To bridge the gap, we introduce a graph-theoretic framework to jointly tackle both OOD generalization and detection problems. By leveraging the graph formulation, data representations are obtained through the factorization of the graph's adjacency matrix, enabling us to derive provable error quantifying OOD generalization and detection performance. Empirical results showcase competitive performance in comparison to existing methods, thereby validating our theoretical underpinnings.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-432" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-432', event_id='93519', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4605</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93519">Pseudo-Private Data Guided Model Inversion Attacks</a></strong></h5>


                        <p class="text-muted">
                            Xiong Peng &middot; Bo Han &middot; Feng Liu &middot; Tongliang Liu &middot; Mingyuan Zhou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In model inversion attacks (MIAs), adversaries attempt to recover private training data by exploiting access to a well-trained target model. Recent advancements have improved MIA performance using a two-stage generative framework. This approach first employs a generative adversarial network to learn a fixed distributional prior, which is then used to guide the inversion process during the attack. However, in this paper, we observed a phenomenon that such a fixed prior would lead to a low probability of sampling actual private data during the inversion process due to the inherent distribution gap between the prior distribution and the private data distribution, thereby constraining attack performance. To address this limitation, we propose increasing the density around high-quality pseudo-private data‚Äîrecovered samples through model inversion that exhibit characteristics of the private training data‚Äîby slightly tuning the generator. This strategy effectively increases the probability of sampling actual private data that is close to these pseudo-private data during the inversion process. After integrating our method, the generative model inversion pipeline is strengthened, leading to improvements over state-of-the-art MIAs. This paves the way for new research directions in generative MIAs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-433" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-433', event_id='93668', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4606</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93668">Dual-Personalizing Adapter for Federated Foundation Models</a></strong></h5>


                        <p class="text-muted">
                            yiyuan yang &middot; Guodong Long &middot; Tao Shen &middot; Jing Jiang &middot; Michael Blumenstein
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning diverse instruction data. Notably, federated foundation models (FedFM) emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to FedFM for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications, and conventional methods for test-time distribution shifts in personalized FL are less effective for FedFM due to their failure to adapt to complex distribution shift scenarios and the requirement to train all parameters. To bridge this gap, we refine the setting in FedFM, termed test-time personalization, which aims to learn personalized federated foundation models on clients while effectively handling test-time distribution shifts simultaneously. To address challenges in this setting, we explore a simple yet effective solution, a Federated Dual-Personalizing Adapter (FedDPA) architecture. By co-working with a foundation model, a global adapter and a local adapter jointly tackle the test-time distribution shifts and client-specific personalization. Additionally, we introduce an instance-wise dynamic weighting mechanism that dynamically integrates the global and local adapters for each test instance during inference, facilitating effective test-time personalization. The effectiveness of the proposed method has been evaluated on benchmark datasets across different NLP tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-434" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-434', event_id='93799', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4607</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93799">Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack</a></strong></h5>


                        <p class="text-muted">
                            Tiansheng Huang &middot; Sihao Hu &middot; Ling Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The new paradigm of fine-tuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the fine-tuning to produce an alignment-broken model. We conduct an empirical analysis and uncovera \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users  fine-tuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from  un-sanitized user data in the fine-tuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts. Our code is available at  https://github.com/git-disl/Vaccine.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-435" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-435', event_id='93801', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4608</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93801">Be Confident in What You Know: Bayesian Parameter Efficient Fine-Tuning of Vision Foundation Models</a></strong></h5>


                        <p class="text-muted">
                            Deep Pandey &middot; Spandan Pyakurel &middot; Qi Yu
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Large transformer-based foundation models have been commonly used as pre-trained models that can be adapted to different challenging datasets and settings with state-of-the-art generalization performance. Parameter efficient fine-tuning ($\texttt{PEFT}$) provides promising generalization performance in adaptation while incurring minimum computational overhead. However, adaptation of these foundation models through $\texttt{PEFT}$ leads to accurate but severely underconfident models, especially in few-shot learning settings. Moreover, the adapted models lack accurate fine-grained uncertainty quantification capabilities limiting their broader applicability in critical domains. To fill out this critical gap, we develop a novel lightweight {Bayesian Parameter Efficient Fine-Tuning} (referred to as $\texttt{Bayesian-PEFT}$) framework for large transformer-based foundation models. The framework integrates state-of-the-art $\texttt{PEFT}$ techniques with two Bayesian components to address the under-confidence issue while ensuring reliable prediction under challenging few-shot settings. The first component performs base rate adjustment to strengthen the prior belief corresponding to the knowledge gained through pre-training, making the model more confident in its predictions; the second component builds an evidential ensemble that leverages belief regularization to ensure diversity among different ensemble components.Our thorough theoretical analysis justifies that the  Bayesian components can ensure reliable and accurate few-shot adaptations with well-calibrated uncertainty quantification. Extensive experiments across diverse datasets, few-shot learning scenarios, and multiple $\texttt{PEFT}$ techniques demonstrate the outstanding prediction and calibration performance by $\texttt{Bayesian-PEFT}$.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-436" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-436', event_id='94189', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4609</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94189">Reconstruct and Match: Out-of-Distribution Robustness via Topological Homogeneity</a></strong></h5>


                        <p class="text-muted">
                            Chaoqi Chen &middot; Luyao Tang &middot; Hui Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Since deep learning models are usually deployed in non-stationary environments, it is imperative to improve their robustness to out-of-distribution (OOD) data. A common approach to mitigate distribution shift is to regularize internal representations or predictors learned from in-distribution (ID) data to be domain invariant. Past studies have primarily learned pairwise invariances, ignoring the intrinsic structure and high-order dependencies of the data. Unlike machines, human recognizes objects by first dividing them into major components and then identifying the topological relation of these components. Motivated by this, we propose Reconstruct and Match (REMA), a general learning framework for object recognition tasks to endow deep models with the capability of capturing the topological homogeneity of objects without human prior knowledge or fine-grained annotations. To identify major components from objects, REMA introduces a selective slot-based reconstruction module to dynamically map dense pixels into a sparse and discrete set of slot vectors in an unsupervised manner. Then, to model high-order dependencies among these components, we propose a hypergraph-based relational reasoning module that models the intricate relations of nodes (slots) with structural constraints. Experiments on standard benchmarks show that REMA outperforms state-of-the-art methods in OOD generalization and test-time adaptation settings.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-437" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-437', event_id='94258', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4610</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94258">Confidence Calibration of Classifiers with Many Classes</a></strong></h5>


                        <p class="text-muted">
                            Adrien Le Coz &middot; St√©phane Herbin &middot; Faouzi Adjed
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>For classification models based on neural networks, the maximum predicted class probability is often used as a confidence score. This score rarely predicts well the probability of making a correct prediction and requires a post-processing calibration step. However, many confidence calibration methods fail for problems with many classes. To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier.  This approach allows for more efficient use of standard calibration methods. We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-438" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-438', event_id='94295', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4611</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94295">Large Language Model Unlearning via Embedding-Corrupted Prompts</a></strong></h5>


                        <p class="text-muted">
                            Chris Liu &middot; Yaxuan Wang &middot; Jeffrey Flanigan &middot; Yang Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains. Yet controlling what a large language model should not know is important for ensuring alignment and thus safe use. However, accurately and efficiently unlearning knowledge from an LLM remains challenging due to the potential collateral damage caused by the fuzzy boundary between retention and forgetting, and the large computational requirements for optimization across state-of-the-art models with hundreds of billions of parameters. In this work, we present \textbf{Embedding-COrrupted (ECO) Prompts}, a lightweight unlearning framework for large language models to address both the challenges of knowledge entanglement and unlearning efficiency. Instead of relying on the LLM itself to unlearn, we enforce an unlearned state during inference by employing a prompt classifier to identify and safeguard prompts to forget. We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline and corrupt prompts flagged by the classifier during inference. We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting. Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at \textit{nearly zero side effects} in general domains and domains closely related to the unlearned ones. Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases. We have made our code publicly available at \url{https://github.com/chrisliu298/llm-unlearn-eco}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-439" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-439', event_id='97638', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4700</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97638">4DBInfer:  A 4D Benchmarking Toolbox for Graph-Centric Predictive Modeling on RDBs</a></strong></h5>


                        <p class="text-muted">
                            Minjie Wang &middot; Quan Gan &middot; David Wipf &middot; Zheng Zhang &middot; Christos Faloutsos &middot; Weinan Zhang &middot; Muhan Zhang &middot; Zhenkun Cai &middot; Jiahang Li &middot; Zunyao Mao &middot; Yakun Song &middot; Jianheng Tang &middot; Yanlin Zhang &middot; Guang Yang &middot; Chuan Lei &middot; Xiao Qin &middot; Ning Li &middot; Han Zhang &middot; Yanbo Wang &middot; Zizhao Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Given a relational database (RDB), how can we predict  missing column values in some target table of interest? Although RDBs store vast amounts of rich, informative data spread across interconnected tables, the progress of predictive machine learning models as applied to such tasks arguably falls well behind advances in other domains such as computer vision or natural language processing.  This deficit stems, at least in part, from the lack of established/public RDB benchmarks as needed for training and evaluation purposes.  As a result, related model development thus far often defaults to tabular approaches trained on ubiquitous single-table benchmarks, or on the relational side, graph-based alternatives such as GNNs applied to a completely different set of graph datasets devoid of tabular characteristics.  To more precisely target RDBs lying at the nexus of these two complementary regimes, we explore a broad class of baseline models predicated on: (i) converting multi-table datasets into graphs using various strategies equipped with efficient subsampling, while preserving tabular characteristics; and (ii) trainable models with well-matched inductive biases that output predictions based on these input subgraphs.  Then, to address the dearth of suitable public benchmarks and reduce siloed comparisons, we assemble a diverse collection of (i) large-scale RDB datasets and (ii) coincident predictive tasks.  From a delivery standpoint, we  operationalize the above four dimensions (4D) of exploration within a unified, scalable open-source toolbox called 4DBInfer; please see https://github.com/awslabs/multi-table-benchmark .</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-440" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-440', event_id='97447', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4701</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97447">$\texttt{ConflictBank}$: A Benchmark for Evaluating the Influence of Knowledge Conflicts in LLMs</a></strong></h5>


                        <p class="text-muted">
                            Zhaochen Su &middot; Jun Zhang &middot; Xiaoye Qu &middot; Tong Zhu &middot; Yanshu Li &middot; Jiashuo Sun &middot; Juntao Li &middot; Min Zhang &middot; Yu Cheng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) have achievedimpressive advancements across numerous disciplines, yet the critical issue of knowledge conflicts, a major source of hallucinations, has rarely been studied. Only a few research explored the conflicts between the inherent knowledge of LLMs and the retrieved contextual knowledge. However, a thorough assessment of knowledge conflict in LLMs is still missing. Motivated by this research gap, we present ConflictBank, the first comprehensive benchmark developed to systematically evaluate knowledge conflicts from three aspects: (i) conflicts encountered in retrieved knowledge, (ii) conflicts within the models' encoded knowledge, and (iii) the interplay between these conflict forms. Our investigation delves into four model families and twelve LLM instances, meticulously analyzing conflicts stemming from misinformation, temporal discrepancies, and semantic divergences. Based on our proposed novel construction framework, we create 7,453,853 claim-evidence pairs and 553,117 QA pairs. We present numerous findings on model scale, conflict causes, and conflict types.We hope our ConflictBank benchmark will help the community better understand model behavior in conflicts and develop more reliable LLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-441" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-441', event_id='96912', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4702</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96912">Near-Optimal Distributionally Robust Reinforcement Learning with General $L_p$ Norms</a></strong></h5>


                        <p class="text-muted">
                            Pierre Clavier &middot; Laixi Shi &middot; Erwan Le Pennec &middot; Eric Mazumdar &middot; Adam Wierman &middot; Matthieu Geist
                        </p>

                    </div>
                    <div class="abstract">
                        <p>To address the challenges of sim-to-real gap and sample efficiency in reinforcement learning (RL), this work studies distributionally robust Markov decision processes (RMDPs) --- optimize the worst-case performance when the deployed environment is within an uncertainty set around some nominal MDP. Despite recent efforts, the sample complexity of RMDPs has remained largely undetermined. While the statistical implications of distributional robustness in RL have been explored in some specific cases, the generalizability of the existing findings remains unclear, especially in comparison to standard RL.  Assuming access to a generative model that samples from the nominal MDP, we examine the sample complexity of RMDPs using a class of generalized $L_p$ norms as the 'distance' function for the uncertainty set, under two commonly adopted $sa$-rectangular and $s$-rectangular conditions. Our results imply that RMDPs can be more sample-efficient to solve than standard MDPs using generalized $L_p$ norms in both $sa$- and $s$-rectangular cases, potentially inspiring more empirical research. We provide a near-optimal upper bound and a matching minimax lower bound for the $sa$-rectangular scenarios. For $s$-rectangular cases, we improve the state-of-the-art upper bound and also derive a lower bound using $L_\infty$ norm that verifies the tightness.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-442" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-442', event_id='96740', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4703</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96740">GS-Hider: Hiding Messages into 3D Gaussian Splatting</a></strong></h5>


                        <p class="text-muted">
                            Xuanyu Zhang &middot; Jiarui Meng &middot; Runyi Li &middot; Zhipei Xu &middot; yongbing zhang &middot; Jian Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>3D Gaussian Splatting (3DGS) has already become the emerging research focus in the fields of 3D scene reconstruction and novel view synthesis. Given that training a 3DGS requires a significant amount of time and computational cost, it is crucial to protect the copyright, integrity, and privacy of such 3D assets. Steganography, as a crucial technique for encrypted transmission and copyright protection, has been extensively studied. However, it still lacks profound exploration targeted at 3DGS. Unlike its predecessor NeRF, 3DGS possesses two distinct features: 1) explicit 3D representation; and 2) real-time rendering speeds. These characteristics result in the 3DGS point cloud files being public and transparent, with each Gaussian point having a clear physical significance. Therefore, ensuring the security and fidelity of the original 3D scene while embedding information into the 3DGS point cloud files is an extremely challenging task. To solve the above-mentioned issue, we first propose a steganography framework for 3DGS, dubbed GS-Hider, which can embed 3D scenes and images into original GS point clouds in an invisible manner and accurately extract the hidden messages. Specifically, we design a coupled secured feature attribute to replace the original 3DGS's spherical harmonics coefficients and then use a scene decoder and a message decoder to disentangle the original RGB scene and the hidden message. Extensive experiments demonstrated that the proposed GS-Hider can effectively conceal multimodal messages without compromising rendering quality and possesses exceptional security, robustness, capacity, and flexibility. Our project is available at: https://xuanyuzhang21.github.io/project/gshider.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-443" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-443', event_id='96681', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4704</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96681">Linear Regression using Heterogeneous Data Batches</a></strong></h5>


                        <p class="text-muted">
                            Ayush Jain &middot; Rajat Sen &middot; Weihao Kong &middot; Abhimanyu Das &middot; Alon Orlitsky
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In many learning applications, data are collected from multiple sources, each providing a \emph{batch} of samples that by itself is insufficient to learn its input-output relationship. A common approach assumes that the sources fall in one of several unknown subgroups, each with an unknown input distribution and input-output relationship. We consider one of this setup's most fundamental and important manifestations where the output is a noisy linear combination of the inputs, and there are $k$ subgroups, each with its own regression vector. Prior work [KSS$^+$20] showed that with abundant small-batches, the regression vectors can be learned with only few, $\tilde\Omega( k^{3/2})$, batches of medium-size with $\tilde\Omega(\sqrt k)$ samples each. However, the paper requires that the input distribution for all $k$ subgroups be isotropic Gaussian, and states that removing this assumption is an ``interesting and challenging problem". We propose a novel gradient-based algorithm that improves on the existing results in several ways. It extends the applicability of the algorithm by: (1) allowing the subgroups' underlying input distributions to be different, unknown, and heavy-tailed; (2) recovering all subgroups followed by a significant proportion of batches even for infinite $k$; (3) removing the separation requirement between the regression vectors; (4) reducing the number of batches and allowing smaller batch sizes.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-444" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-444', event_id='96619', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4705</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96619">DiTFastAttn: Attention Compression for Diffusion Transformer Models</a></strong></h5>


                        <p class="text-muted">
                            Zhihang Yuan &middot; Hanling Zhang &middot; Lu Pu &middot; Xuefei Ning &middot; Linfeng Zhang &middot; Tianchen Zhao &middot; Shengen Yan &middot; Guohao Dai &middot; Yu Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Diffusion Transformers (DiT) excel at image and video generation but face computational challenges due to the quadratic complexity of self-attention operators. We propose DiTFastAttn, a post-training compression method to alleviate the computational bottleneck of DiT.We identify three key redundancies in the attention computation during DiT inference: (1) spatial redundancy, where many attention heads focus on local information; (2) temporal redundancy, with high similarity between the attention outputs of neighboring steps; (3) conditional redundancy, where conditional and unconditional inferences exhibit significant similarity. We propose three techniques to reduce these redundancies: (1) $\textit{Window Attention with Residual Sharing}$ to reduce spatial redundancy; (2) $\textit{Attention Sharing across Timesteps}$ to exploit the similarity between steps; (3) $\textit{Attention Sharing across CFG}$ to skip redundant computations during conditional generation.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-445" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-445', event_id='96386', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4706</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96386">Stratified Prediction-Powered Inference for Effective Hybrid Evaluation of Language Models</a></strong></h5>


                        <p class="text-muted">
                            Adam Fisch &middot; Joshua Maynez &middot; R. Hofer &middot; Bhuwan Dhingra &middot; Amir Globerson &middot; William Cohen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Prediction-powered inference (PPI) is a method that improves statistical estimates based on limited human-labeled data.  PPI achieves this by combining small amounts of human-labeled data with larger amounts of data labeled by a reasonably accurate---but potentially biased---automatic system, in a way that results in tighter confidence intervals for certain parameters of interest (e.g., the mean performance of a language model). In this paper, we propose a method called Stratified Prediction-Powered Inference (StratPPI), in which we show that the basic PPI estimates can be considerably improved by employing simple data stratification strategies. Without making any assumptions on the underlying automatic labeling system or data distribution, we derive an algorithm for computing provably valid confidence intervals for parameters of any dimensionality that is based on stratified sampling. In particular, we show both theoretically and empirically that, with appropriate choices of stratification and sample allocation, our approach can provide substantially tighter confidence intervals than  unstratified  approaches. Specifically, StratPPI  is expected to improve in cases where the performance of the autorater varies across different conditional distributions of the target data.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-446" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-446', event_id='96378', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4707</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96378">Near-Optimal Streaming Heavy-Tailed Statistical Estimation with Clipped SGD</a></strong></h5>


                        <p class="text-muted">
                            Aniket Das &middot; Dheeraj Nagaraj &middot; Soumyabrata Pal &middot; Arun Suggala &middot; Prateek Varshney
                        </p>

                    </div>
                    <div class="abstract">
                        <p>$\newcommand{\Tr}{\mathsf{Tr}}$We consider the problem of high-dimensional heavy-tailed statistical estimation in the streaming setting, which is much harder than the traditional batch setting due to memory constraints. We cast this problem as stochastic convex optimization with heavy tailed stochastic gradients, and prove that the widely used Clipped-SGD algorithm attains near-optimal sub-Gaussian statistical rates whenever the second moment of the stochastic gradient noise is finite. More precisely, with $T$ samples, we show that Clipped-SGD, for smooth and strongly convex objectives, achieves an error of $\sqrt{\frac{\Tr(\Sigma)+\sqrt{\Tr(\Sigma)\\|\Sigma\\|_2}\ln(\tfrac{\ln(T)}{\delta})}{T}}$ with probability $1-\delta$, where $\Sigma$ is the covariance of the clipped gradient. Note that the fluctuations (depending on $\tfrac{1}{\delta}$) are of lower order than the term $\Tr(\Sigma)$.This improves upon the current best rate of$\sqrt{\frac{\Tr(\Sigma)\ln(\tfrac{1}{\delta})}{T}}$ for Clipped-SGD, known \emph{only} for smooth and strongly convex objectives. Our results also extend to smooth convex and lipschitz convex objectives. Key to our result is a novel iterative refinement strategy for martingale concentration, improving upon the PAC-Bayes approach of \citet{catoni2018dimension}.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-447" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-447', event_id='96353', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4708</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96353">Any2Policy: Learning Visuomotor Policy with Any-Modality</a></strong></h5>


                        <p class="text-muted">
                            Yichen Zhu &middot; Zhicai Ou &middot; Feifei Feng &middot; Jian Tang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Humans can communicate and observe media with different modalities, such as texts, sounds, and images. For robots to be more generalizable embodied agents, they should be capable of following instructions and perceiving the world with adaptation to diverse modalities. Current robotic learning methodologies often focus on single-modal task specification and observation, thereby limiting their ability to process rich multi-modal information. Addressing this limitation, we present an end-to-end general-purpose multi-modal system named Any-to-Policy Embodied Agents. This system empowers robots to handle tasks using various modalities, whether in combinations like text-image, audio-image, text-point cloud, or in isolation. Our innovative approach involves training a versatile modality network that adapts to various inputs and connects with policy networks for effective control. Because of the lack of existing multi-modal robotics datasets for evaluation, we assembled a comprehensive real-world dataset encompassing 30 robotic tasks. Each task in this dataset is richly annotated across multiple modalities, providing a robust foundation for assessment. We conducted extensive validation of our proposed unified modality embodied agent using several simulation benchmarks, including Franka Kitchen, Meta-World, and Maniskill2, as well as in our real-world settings. Our experiments showcase the promising capability of building embodied agents that can adapt to diverse multi-modal in a unified framework.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-448" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-448', event_id='96066', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4709</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96066">Be like a Goldfish, Don&#x27;t Memorize! Mitigating Memorization in Generative LLMs</a></strong></h5>


                        <p class="text-muted">
                            Abhimanyu Hans &middot; John Kirchenbauer &middot; Yuxin Wen &middot; Neel Jain &middot; Hamid Kazemi &middot; Prajwal Singhania &middot; Siddharth Singh &middot; Gowthami Somepalli &middot; Jonas Geiping &middot; Abhinav Bhatele &middot; Tom Goldstein
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models can memorize and repeat their training data, causing privacy and copyright risks. To mitigate memorization, we introduce a subtle modification to the next-token training objective that we call the goldfish loss. During training, a randomly sampled subsets of tokens are excluded from the loss computation. These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set. We run extensive experiments training billion-scale LLaMA-2 models, both pre-trained and trained from scratch, and demonstrate significant reductions in extractable memorization with little to no impact on downstream benchmarks.<em>Code and checkpoints: https://github.com/ahans30/goldfish-loss</em></p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-449" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-449', event_id='95957', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4710</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95957">Probing the Decision Boundaries of In-context Learning in Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Siyan Zhao &middot; Tung Nguyen &middot; Aditya Grover
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In-context learning is an emergent paradigm in large language models (LLMs) that enables them to generalize to new tasks and domains by simply prompting these models with a few exemplars without explicit parameter updates. Many attempts have been made to understand in-context learning in LLMs as a function of model scale, pretraining data, and other factors. In this work, we propose a new mechanism to probe and understand in-context learning from the lens of decision boundaries for in-context binary classification. Decision boundaries are straightforward to visualize and provide important information about the qualitative behavior of the inductive biases of standard classifiers. To our surprise, we find that the decision boundaries learned by current LLMs in simple binary classification tasks are often irregularly non-smooth, regardless of task linearity. This paper investigates the factors influencing these decision boundaries and explores methods to enhance their generalizability. We assess various approaches, including training-free and fine-tuning methods for LLMs, the impact of model architecture, and the effectiveness of active prompting techniques for smoothing decision boundaries in a data-efficient manner. Our findings provide a deeper understanding of in-context learning dynamics and offer practical improvements for enhancing robustness and generalizability of in-context learning.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-450" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-450', event_id='95950', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4711</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95950">UniAR: A Unified model for predicting human Attention and Responses on visual content</a></strong></h5>


                        <p class="text-muted">
                            Peizhao Li &middot; Junfeng He &middot; Gang Li &middot; Rachit Bhargava &middot; Shaolei Shen &middot; Nachiappan Valliappan &middot; Youwei Liang &middot; Hongxiang Gu &middot; Venky Ramachandran &middot; Golnaz farhadi &middot; Yang Li &middot; Kai Kohlhoff &middot; Vidhya Navalpakkam
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Progress in human behavior modeling involves understanding both implicit, early-stage perceptual behavior, such as human attention, and explicit, later-stage behavior, such as subjective preferences or likes. Yet most prior research has focused on modeling implicit and explicit human behavior in isolation; and often limited to a specific type of visual content. We propose UniAR -- a unified model of human attention and preference behavior across diverse visual content. UniAR leverages a multimodal transformer to predict subjective feedback, such as satisfaction or aesthetic quality, along with the underlying human attention or interaction heatmaps and viewing order. We train UniAR on diverse public datasets spanning natural images, webpages, and graphic designs, and achieve SOTA performance on multiple benchmarks across various image domains and behavior modeling tasks. Potential applications include providing instant feedback on the effectiveness of UIs/visual content, and enabling designers and content-creation models to optimize their creation for human-centric improvements.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-451" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-451', event_id='94671', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4800</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94671">The Poisson Midpoint Method for Langevin Dynamics:  Provably Efficient Discretization for Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Saravanan Kandasamy &middot; Dheeraj Nagaraj
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Langevin Dynamics is a Stochastic Differential Equation (SDE) central to sampling and generative modeling and is implemented via time discretization. Langevin Monte Carlo (LMC), based on the Euler-Maruyama discretization, is the simplest and most studied algorithm. LMC can suffer from slow convergence - requiring a large number of steps of small step-size to obtain good quality samples. This becomes stark in the case of diffusion models where a large number of steps gives the best samples, but the quality degrades rapidly with smaller number of steps. Randomized Midpoint Method has been recently proposed as a better discretization of Langevin dynamics for sampling from strongly log-concave distributions. However, important applications such as diffusion models involve non-log concave densities and contain time varying drift. We propose its variant, the Poisson Midpoint Method, which approximates a small step-size LMC with large step-sizes. We prove that this can obtain a quadratic speed up of LMC under very weak assumptions. We apply our method to diffusion models for image generation and show that it maintains the quality of DDPM with 1000 neural network calls with just 50-80 neural network calls and outperforms ODE based methods with similar compute.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-452" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-452', event_id='94692', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4801</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94692">Fairness without Harm: An Influence-Guided Active Sampling Approach</a></strong></h5>


                        <p class="text-muted">
                            Jinlong Pang &middot; Jialu Wang &middot; Zhaowei Zhu &middot; Yuanshun Yao &middot; Chen Qian &middot; Yang Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The pursuit of fairness in machine learning (ML), ensuring that the models do not exhibit biases toward protected demographic groups, typically results in a compromise scenario. This compromise can be explained by a Pareto frontier where given certain resources (e.g., data), reducing the fairness violations often comes at the cost of lowering the model accuracy. In this work, we aim to train models that mitigate group fairness disparity without causing harm to model accuracy.Intuitively, acquiring more data is a natural and promising approach to achieve this goal by reaching a better Pareto frontier of the fairness-accuracy tradeoff. The current data acquisition methods, such as fair active learning approaches, typically require annotating sensitive attributes. However, these sensitive attribute annotations should be protected due to privacy and safety concerns. In this paper, we propose a tractable active data sampling algorithm that does not rely on training group annotations, instead only requiring group annotations on a small validation set. Specifically, the algorithm first scores each new example by its influence on fairness and accuracy evaluated on the validation dataset, and then selects a certain number of examples for training. We theoretically analyze how acquiring more data can improve fairness without causing harm, and validate the possibility of our sampling approach in the context of risk disparity. We also provide the upper bound of generalization error and risk disparity as well as the corresponding connections.Extensive experiments on real-world data demonstrate the effectiveness of our proposed algorithm. Our code is available at <a href="https://github.com/UCSC-REAL/FairnessWithoutHarm">github.com/UCSC-REAL/FairnessWithoutHarm</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-453" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-453', event_id='94867', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4802</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94867">Satformer: Accurate and Robust Traffic Data Estimation for Satellite Networks</a></strong></h5>


                        <p class="text-muted">
                            Liang Qin &middot; Xiyuan Liu &middot; Wenting Wei &middot; Liang Chengbin &middot; Huaxi Gu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The operations and maintenance of satellite networks heavily depend on traffic measurements. Due to the large-scale and highly dynamic nature of satellite networks, global measurement encounters significant challenges in terms of complexity and overhead. Estimating global network traffic data from partial traffic measurements is a promising solution. However, the majority of current estimation methods concentrate on low-rank linear decomposition, which is unable to accurately estimate. The reason lies in its inability to capture the intricate nonlinear spatio-temporal relationship found in large-scale, highly dynamic traffic data. This paper proposes Satformer, an accurate and robust method for estimating traffic data in satellite networks. In Satformer, we innovatively incorporate an adaptive sparse spatio-temporal attention mechanism. In the mechanism, more attention is paid to specific local regions of the input tensor to improve the model's sensitivity on details and patterns. This method enhances its capability to capture nonlinear spatio-temporal relationships. Experiments on small, medium, and large-scale satellite networks datasets demonstrate that Satformer outperforms mathematical and neural baseline methods notably. It provides substantial improvements in reducing errors and maintaining robustness, especially for larger networks. The approach shows promise for deployment in actual systems.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-454" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-454', event_id='94875', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4803</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94875">LLM-AutoDA: Large Language Model-Driven Automatic Data Augmentation for Long-tailed Problems</a></strong></h5>


                        <p class="text-muted">
                            Pengkun Wang &middot; Zhe Zhao &middot; HaiBin Wen &middot; Fanfu Wang &middot; Binwu Wang &middot; Qingfu Zhang &middot; Yang Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The long-tailed distribution is the underlying nature of real-world data, and it presents unprecedented challenges for training deep learning models. Existing long-tailed learning paradigms based on re-balancing or data augmentation have partially alleviated the long-tailed problem. However, they still have limitations, such as relying on manually designed augmentation strategies, having a limited search space, and using fixed augmentation strategies. To address these limitations, this paper proposes a novel LLM-based long-tailed data augmentation framework called LLM-AutoDA, which leverages large-scale pretrained models to automatically search for the optimal augmentation strategies suitable for long-tailed data distributions. In addition, it applies this strategy to the original imbalanced data to create an augmented dataset and fine-tune the underlying long-tailed learning model. The performance improvement on the validation set serves as a reward signal to update the generation model, enabling the generation of more effective augmentation strategies in the next iteration. We conducted extensive experiments on multiple mainstream long-tailed learning benchmarks. The results show that LLM-AutoDA outperforms state-of-the-art data augmentation methods and other re-balancing methods significantly.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-455" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-455', event_id='95018', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4804</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95018">In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization</a></strong></h5>


                        <p class="text-muted">
                            Ruiqi Zhang &middot; Jingfeng Wu &middot; Peter Bartlett
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We study the \emph{in-context learning} (ICL) ability of a \emph{Linear Transformer Block} (LTB) that combines a linear attention component and a linear multi-layer perceptron (MLP) component. For ICL of linear regression with a Gaussian prior and a \emph{non-zero mean}, we show that LTB can achieve nearly Bayes optimal ICL risk. In contrast, using only linear attention must incur an irreducible additive  approximation error. Furthermore, we establish a correspondence between LTB and one-step gradient descent estimators with learnable initialization ($\mathsf{GD}-\beta$), in the sense that every $\mathsf{GD}-\beta$ estimator can be implemented by an LTB estimator and every optimal LTB estimator that minimizes the in-class ICL risk is effectively a $\mathsf{GD}-\beta$ estimator.Finally, we show that $\mathsf{GD}-\beta$ estimators can be efficiently optimized with gradient flow, despite a non-convex training objective.Our results reveal that LTB achieves ICL by implementing $\mathsf{GD}-\beta$, and they highlight the role of MLP layers in reducing approximation error.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-456" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-456', event_id='95101', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4805</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95101">State-free Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Mingyu Chen &middot; Aldo Pacchiano &middot; Xuezhou Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In this work, we study the \textit{state-free RL} problem, where the algorithm does not have the states information before interacting with the environment. Specifically, denote the reachable state set by $\mathcal{S}^\Pi := \{ s|\max_{\pi\in \Pi}q^{P, \pi}(s)>0 \}$, we design an algorithm which requires no information on the state space $S$ while having a regret that is completely independent of $\mathcal{S}$ and only depend on $\mathcal{S}^\Pi$. We view this as a concrete first step towards \textit{parameter-free RL}, with the goal of designing RL algorithms that require no hyper-parameter tuning.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-457" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-457', event_id='95117', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4806</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95117">D√©j√† Vu Memorization in Vision‚ÄìLanguage Models</a></strong></h5>


                        <p class="text-muted">
                            Bargav Jayaraman &middot; Chuan Guo &middot; Kamalika Chaudhuri
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Vision-Language Models (VLMs) have emerged as the state-of-the-art representation learning solution, with myriads of downstream applications such as image classification, retrieval and generation. A natural question is whether these models memorize their training data, which also has implications for generalization. We propose a new method for measuring memorization in VLMs, which we call d√®j√° vu memorization. For VLMs trained on image-caption pairs, we show that the model indeed retains information about individual objects in the training images beyond what can be inferred from correlations or the image caption. We evaluate d√®j√° vu memorization at both sample and population level, and show that it is significant for OpenCLIP trained on as many as 50M image-caption pairs. Finally, we show that text randomization considerably mitigates memorization risk while only moderately impacting the model‚Äôs downstream task performance.  The code is available here: https://github.com/facebookresearch/VLMDejaVu.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-458" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-458', event_id='95150', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4807</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95150">QVAE-Mole: The Quantum VAE with Spherical Latent Variable Learning for 3-D Molecule Generation</a></strong></h5>


                        <p class="text-muted">
                            Huaijin Wu &middot; Xinyu Ye &middot; Junchi Yan
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Molecule generation ideally in its 3-D form has enjoyed wide applications in material, chemistry, life science, etc. We propose the first quantum parametric circuit for 3-D molecule generation for its potential quantum advantage especially considering the arrival of Noisy Intermediate-Scale Quantum (NISQ) era. We choose the Variational AutoEncoder (VAE) scheme for its simplicity and one-shot generation ability, which we believe is more quantum-friendly compared with the auto-regressive generative models or diffusion models as used in classic approaches. Specifically, we present a quantum encoding scheme designed for 3-D molecules with qubits complexity $\mathcal{O}(C\log n)$ ($n$ is the number of atoms) and adopt a von Mises-Fisher (vMF) distributed latent space to meet the inherent coherence of the quantum system. We further design to encode conditions into quantum circuits for property-specified generation. Experimentally, our model could generate plausible 3-D molecules and achieve competitive quantitative performance with significantly reduced circuit parameters compared with their classic counterparts. The source code will be released upon publication.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-459" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-459', event_id='95278', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4808</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95278">Generative Hierarchical Materials Search</a></strong></h5>


                        <p class="text-muted">
                            Sherry Yang &middot; Simon Batzner &middot; Ruiqi Gao &middot; Muratahan Aykol &middot; Alexander Gaunt &middot; Brendan C McMorrow &middot; Danilo Jimenez Rezende &middot; Dale Schuurmans &middot; Igor Mordatch &middot; Ekin Dogus Cubuk
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Generative models trained at scale can now produce novel text, video, and more recently, scientific data such as crystal structures. The ultimate goal for materials discovery, however, goes beyond generation: we desire a fully automated system that proposes, generates, and verifies crystal structures given a high-level user instruction. In this work, we formulate end-to-end language-to-structure generation as a multi-objective optimization problem, and propose Generative Hierarchical Materials Search (GenMS) for controllable generation of crystal structures. GenMS consists of (1) a language model that takes high-level natural language as input and generates intermediate textual information about a crystal (e.g., chemical formulae), and (2) a diffusion model that takes intermediate information as input and generates low-level continuous value crystal structures. GenMS additionally uses a graph neural network to predict properties (e.g., formation energy) from the generated crystal structures. During inference, GenMS leverages all three components to conduct a forward tree search over the space of possible structures. Experiments show that GenMS outperforms other alternatives both in satisfying user request and in generating low-energy structures. GenMS is able to generate complex structures such as double perovskites (or elpasolites), layered structures, and spinels, solely from natural language input.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-460" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-460', event_id='95504', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4809</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95504">Theoretical Investigations and Practical Enhancements on Tail Task Risk Minimization in Meta Learning</a></strong></h5>


                        <p class="text-muted">
                            Yiqin Lv &middot; Qi Wang &middot; Dong Liang &middot; Zheng Xie
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Meta learning is a promising paradigm in the era of large models and task distributional robustness has become an indispensable consideration in real-world scenarios.Recent advances have examined the effectiveness of tail task risk minimization in fast adaptation robustness improvement \citep{wang2023simple}.This work contributes to more theoretical investigations and practical enhancements in the field.Specifically, we reduce the distributionally robust strategy to a max-min optimization problem, constitute the Stackelberg equilibrium as the solution concept, and estimate the convergence rate.In the presence of tail risk, we further derive the generalization bound, establish connections with estimated quantiles, and practically improve the studied strategy.Accordingly, extensive evaluations demonstrate the significance of our proposal in boosting robustness.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-461" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-461', event_id='95615', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4810</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95615">EfficientCAPER: An End-to-End Framework for Fast and Robust Category-Level Articulated Object Pose Estimation</a></strong></h5>


                        <p class="text-muted">
                            Xinyi Yu &middot; Haonan Jiang &middot; Li Zhang &middot; Lin Yuanbo Wu &middot; Linlin Ou &middot; Liu Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Human life is populated with articulated objects. Pose estimation for category-level articulated objects is a significant challenge due to their inherent complexity and diverse kinematic structures. Current methods for this task usually meet the problems of insufficient consideration of kinematic constraints, self-occlusion, and optimization requirements. In this paper, we propose EfficientCAPER, an end-to-end Category-level Articulated object Pose EstimatoR, eliminating the need for optimization functions as post-processing and utilizing the kinematic structure for joint-centric pose modeling, thus enhancing the efficiency and applicability. Given a partial point cloud as input, the EfficientCAPER firstly estimates the pose for the free part of an articulated object using decoupled rotation representation. Next, we canonicalize the input point cloud to estimate constrained parts' poses by predicting the joint parameters and states as replacements. Evaluations on three diverse datasets, ArtImage, ReArtMix, and RobotArm, show EfficientCAPER's effectiveness and generalization ability to real-world scenarios. The framework exhibits excellent static pose estimation performance for articulated objects, contributing to the advancement of category-level pose estimation. Codes will be made publicly available.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-462" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-462', event_id='95812', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4811</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95812">Blind Image Restoration via Fast Diffusion Inversion</a></strong></h5>


                        <p class="text-muted">
                            Hamadi Chihaoui &middot; Abdelhak Lemkhenter &middot; Paolo Favaro
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Image Restoration (IR) methods based on a pre-trained diffusion model have demonstrated state-of-the-art performance. However, they have two fundamental limitations: 1) they often assume that the degradation operator is completely known and 2) they alter the diffusion sampling process, which may result in restored images that do not lie onto the data manifold. To address these issues, we propose Blind Image Restoration via fast Diffusion inversion (BIRD) a blind IR method that jointly optimizes for the degradation model parameters and the restored image. To ensure that the restored images lie onto the data manifold, we propose a novel sampling technique on a pre-trained diffusion model. A key idea in our method is not to modify the reverse sampling, i.e., not to alter all the intermediate latents, once an initial noise is sampled. This is ultimately equivalent to casting the IR task as an optimization problem in the space of the input noise. Moreover, to mitigate the computational cost associated with inverting a fully unrolled diffusion model, we leverage the inherent capability of these models to skip ahead in the forward diffusion process using large time steps. We experimentally validate BIRD on several image restoration tasks and show that it achieves state of the art performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-463" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-463', event_id='94584', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4900</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94584">Navigating Chemical Space with Latent Flows</a></strong></h5>


                        <p class="text-muted">
                            Guanghao Wei &middot; Yining Huang &middot; Chenru Duan &middot; Yue Song &middot; Yuanqi Du
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent progress of deep generative models in the vision and language domain has stimulated significant interest in more structured data generation such as molecules. However, beyond generating new random molecules, efficient exploration and a comprehensive understanding of the vast chemical space are of great importance to molecular science and applications in drug design and materials discovery.In this paper, we propose a new framework, ChemFlow, to traverse chemical space through navigating the latent space learned by molecule generative models through flows. We introduce a dynamical system perspective that formulates the problem as learning a vector field that transports the mass of the molecular distribution to the region with desired molecular properties or structure diversity. Under this framework, we unify previous approaches on molecule latent space traversal and optimization and propose alternative competing methods incorporating different physical priors. We validate the efficacy of ChemFlow on molecule manipulation and single- and multi-objective molecule optimization tasks under both supervised and unsupervised molecular discovery settings.Codes and demos are publicly available on GitHub at <a href="https://github.com/garywei944/ChemFlow">https://github.com/garywei944/ChemFlow</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-464" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-464', event_id='94547', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4901</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94547">Unified Covariate Adjustment for Causal Inference</a></strong></h5>


                        <p class="text-muted">
                            Yonghan Jung &middot; Jin Tian &middot; Elias Bareinboim
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Causal effect identification and estimation are two crucial tasks in causal inference. Although causal effect identification has been theoretically resolved, many existing estimators only address a subset of scenarios, known as the sequential back-door adjustment (SBD) (Pearl and Robins, 1995) or g-formula (Robins, 1986). Recent efforts for developing general-purpose estimators with broader coverage, incorporating the front-door adjustment (FD) (Pearl, 2000) and more, lack scalability due to the high computational cost of summing over high-dimensional variables. In this paper, we introduce a novel approach that achieves broad coverage of causal estimands beyond the SBD, incorporating various sum-product functionals like the FD, while maintaining scalability -- estimated in polynomial time relative to the number of variables and samples. Specifically, we present the class of UCA for which a scalable and doubly robust estimator is developed. In particular, we illustrate the expressiveness of UCA for a wide spectrum of causal estimands (e.g., SBD, FD, and more) in causal inference. We then develop an estimator that exhibits computational efficiency and doubly robustness. The scalability and robustness of the proposed framework are verified through simulations.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-465" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-465', event_id='94340', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4903</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94340">Adversarial Representation Engineering: A General Model Editing Framework for Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Yihao Zhang &middot; Zeming Wei &middot; Jun Sun &middot; Meng Sun
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Since the rapid development of Large Language Models (LLMs) has achieved remarkable success, understanding and rectifying their internal complex mechanisms has become an urgent issue. Recent research has attempted to interpret their behaviors through the lens of inner representation. However, developing practical and efficient methods for applying these representations for general and flexible model editing remains challenging. In this work, we explore how to leverage insights from representation engineering to guide the editing of LLMs by deploying a representation sensor as an editing oracle. We first identify the importance of a robust and reliable sensor during editing, then propose an \textbf{A}dversarial \textbf{R}epresentation \textbf{E}ngineering (\textbf{ARE}) framework to provide a unified and interpretable approach for conceptual model editing without compromising baseline performance. Experiments on multiple tasks demonstrate the effectiveness of ARE in various model editing scenarios. Our code and data are available at \url{https://github.com/Zhang-Yihao/Adversarial-Representation-Engineering}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-466" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-466', event_id='94322', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4904</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94322">Bayesian Identification of the Hamiltonian Inductive Bias in Dynamical Systems</a></strong></h5>


                        <p class="text-muted">
                            Stefano Cortinovis &middot; Mark van der Wilk
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In the context of learning continuous-time system dynamics, models incorporating the Hamiltonian inductive bias have been shown to generalise better when modelling conservative data. However, current methods rely on either prior knowledge or trial and error to identify whether imposing such a structure is appropriate. In this work, we demonstrate the effectiveness of Bayesian model selection to do so automatically at training time. We develop a Gaussian process-based model that parameterises the range of structural properties, such as energy conservation or dissipation, that a physical system may exhibit. We train this model with a variational inference scheme and show that the inductive biases inferred from data align with the true underlying properties of the system, thereby providing evidence that the marginal likelihood constitutes a sensible objective for automatic inductive bias selection in dynamical systems.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-467" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-467', event_id='94226', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4905</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94226">Tactile DreamFusion: Exploiting Tactile Sensing for 3D Generation</a></strong></h5>


                        <p class="text-muted">
                            Ruihan Gao &middot; Kangle Deng &middot; Gengshan Yang &middot; Wenzhen Yuan &middot; Jun-Yan Zhu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>3D generation methods have shown visually compelling results powered by diffusion image priors. However, they often fail to produce realistic geometric details, resulting in overly smooth surfaces or geometric details inaccurately baked in albedo maps. To address this, we introduce a new method that incorporates touch as an additional modality to improve the geometric details of generated 3D assets. We design a lightweight 3D texture field to synthesize visual and tactile textures, guided by diffusion-based distribution matching losses on both visual and tactile domains. Our method ensures the consistency between visual and tactile textures while preserving photorealism. We further present a multi-part editing pipeline that enables us to synthesize different textures across various regions. To our knowledge, we are the first to leverage high-resolution tactile sensing to enhance geometric details for 3D generation tasks. We evaluate our method on both text-to-3D and image-to-3D settings. Our experiments demonstrate that our method provides customized and realistic fine geometric textures while maintaining accurate alignment between two modalities of vision and touch.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-468" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-468', event_id='94096', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4906</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94096">Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using Discrete State Space Diffusion Model</a></strong></h5>


                        <p class="text-muted">
                            Wenjia Xie &middot; Hao Wang &middot; Luankang Zhang &middot; Rui Zhou &middot; Defu Lian &middot; Enhong Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Sequential recommendation (SR) aims to predict items that users may be interested in based on their historical behavior sequences. We revisit SR from a novel information-theoretic perspective and find that conventional sequential modeling methods fail to adequately capture the randomness and unpredictability of user behavior. Inspired by fuzzy information processing theory, this paper introduces the DDSR model, which uses fuzzy sets of interaction sequences to overcome the limitations and better capture the evolution of users' real interests. Formally based on diffusion transition processes in discrete state spaces, which is unlike common diffusion models such as DDPM that operate in continuous domains. It is better suited for discrete data, using structured transitions instead of arbitrary noise introduction to avoid information loss. Additionally, to address the inefficiency of matrix transformations due to the vast discrete space, we use semantic labels derived from quantization or RQ-VAE to replace item IDs, enhancing efficiency and improving cold start issues. Testing on three public benchmark datasets shows that DDSR outperforms existing state-of-the-art methods in various settings, demonstrating its potential and effectiveness in handling SR tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-469" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-469', event_id='94076', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4907</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94076">Rethinking Optimal Transport in Offline Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Arip Asadulaev &middot; Rostislav Korst &middot; Aleksandr Korotin &middot; Vage Egiazarian &middot; Andrey Filchenkov &middot; Evgeny Burnaev
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose a novel algorithm for offline reinforcement learning using optimal transport. Typically, in offline reinforcement learning, the data is provided by various experts and some of them can be sub-optimal. To extract an efficient policy, it is necessary to \emph{stitch} the best behaviors from the dataset. To address this problem, we rethink offline reinforcement learning as an optimal transportation problem. And based on this, we present an algorithm that aims to find a policy that maps states to a \emph{partial} distribution of the best expert actions for each given state. We evaluate the performance of our algorithm on continuous control problems from the D4RL suite and demonstrate improvements over existing methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-470" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-470', event_id='94055', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4908</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94055">GraphVis: Boosting LLMs with Visual Knowledge Graph Integration</a></strong></h5>


                        <p class="text-muted">
                            Yihe Deng &middot; Chenchen Ye &middot; Zijie Huang &middot; Mingyu Derek Ma &middot; Yiwen Kou &middot; Wei Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The rapid evolution of large language models (LLMs) has expanded their capabilities across various data modalities, extending from well-established image data to increasingly popular graph data. Given the limitation of LLMs in hallucinations and inaccuracies in recalling factual knowledge, Knowledge Graph (KG) has emerged as a crucial data modality to support more accurate reasoning by LLMs. However, integrating structured knowledge from KGs into LLMs remains challenging, as most current KG-enhanced LLM methods directly convert the KG into linearized text triples, which is not as expressive as the original structured data. To address this, we introduce GraphVis, which conserves the intricate graph structure through the visual modality to enhance the comprehension of KGs with the aid of Large Vision Language Models (LVLMs). Our approach incorporates a unique curriculum fine-tuning scheme which first instructs LVLMs to recognize basic graphical features from the images, and subsequently incorporates reasoning on QA tasks with the visual graphs. This cross-modal methodology not only markedly enhances performance on standard textual QA  but also shows improved zero-shot VQA performance by utilizing synthetic graph images to augment the data for VQA tasks. We present comprehensive evaluations across commonsense reasoning QA benchmarks, where GraphVis provides an average improvement of 11.1% over its base model and outperforms existing KG-enhanced LLM approaches. Across VQA benchmarks such as ScienceQA that share similar scientific diagram images, GraphVis provides a notable gain of 4.32%.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-471" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-471', event_id='93591', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4909</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93591">Exploration by Learning Diverse Skills through Successor State Representations</a></strong></h5>


                        <p class="text-muted">
                            Paul-Antoine LE TOLGUENEC &middot; Yann BESSE &middot; Florent Teichteil-Koenigsbuch &middot; Dennis Wilson &middot; Emmanuel Rachelson
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The ability to perform different skills can encourage agents to explore. In this work, we aim to construct a set of diverse skills that uniformly cover the state space. We propose a formalization of this search for diverse skills, building on a previous definition based on the mutual information between states and skills. We consider the distribution of states reached by a policy conditioned on each skill and leverage the successor state representation to maximize the difference between these skill distributions. We call this approach LEADS: Learning Diverse Skills through Successor State Representations. We demonstrate our approach on a set of maze navigation and robotic control tasks which show that our method is capable of constructing a diverse set of skills which exhaustively cover the state space without relying on reward or exploration bonuses. Our findings demonstrate that this new formalization promotes more robust and efficient exploration by combining mutual information maximization and exploration bonuses.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-472" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-472', event_id='93418', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4910</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93418">Sequoia: Scalable and Robust Speculative Decoding</a></strong></h5>


                        <p class="text-muted">
                            Zhuoming Chen &middot; Avner May &middot; Ruslan Svirschevski &middot; Yu-Hsun Huang &middot; Max Ryabinin &middot; Zhihao Jia &middot; Beidi Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p>As the usage of large language models (LLMs) grows, it becomes increasingly important to serve them quickly and efficiently. While speculative decoding has recently emerged as a promising direction for accelerating LLM serving, existing methods are limited in their ability to scale to larger speculation budgets and adapt to different hyperparameters. This paper introduces Sequoia, a scalable and robust algorithm for speculative decoding. To improve scalability, Sequoia introduces a dynamic programming algorithm to find an optimal tree structure for the speculated tokens. To achieve robust speculative decoding, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 GPU by up to $4.04\times$, $3.73\times$, and $2.27 \times$. To serve Llama3-70B-Instruct on a single L40 GPU through offloading, Sequoia reduces the per-token decoding latency to 0.60 s/token, $9.5\times$ faster than DeepSpeed-Zero-Inference.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-473" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-473', event_id='93404', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4911</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93404">Treatment of Statistical Estimation Problems in Randomized Smoothing for Adversarial Robustness</a></strong></h5>


                        <p class="text-muted">
                            Vaclav Voracek
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Randomized smoothing is a popular certified defense against adversarial attacks. In its essence, we need to solve a problem of  statistical estimation which is usually very time-consuming since we need to perform numerous (usually $10^5$) forward passes of the classifier for every point to be certified. In this paper, we review the statistical estimation problems for randomized smoothing to find out if the computational burden is necessary.  In particular, we consider the (standard) task of adversarial robustness where we need to decide if a point is robust at a certain radius or not using as few samples as possible while maintaining statistical guarantees.  We present estimation procedures employing confidence sequences enjoying the same statistical guarantees as the standard methods, with the optimal sample complexities for the estimation task and empirically demonstrate their good performance. Additionally, we provide a randomized version of Clopper-Pearson confidence intervals resulting in strictly stronger certificates.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-474" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-474', event_id='94305', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4920</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94305">Introducing Spectral Attention for Long-Range Dependency in Time Series Forecasting</a></strong></h5>


                        <p class="text-muted">
                            Bong Gyun Kang &middot; Dongjun Lee &middot; HyunGi Kim &middot; Dohyun Chung &middot; Sungroh Yoon
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Sequence modeling faces challenges in capturing long-range dependencies across diverse tasks. Recent linear and transformer-based forecasters have shown superior performance in time series forecasting. However, they are constrained by their inherent inability to effectively address long-range dependencies in time series data, primarily due to using fixed-size inputs for prediction. Furthermore, they typically sacrifice essential temporal correlation among consecutive training samples by shuffling them into mini-batches. To overcome these limitations, we introduce a fast and effective Spectral Attention mechanism, which preserves temporal correlations among samples and facilitates the handling of long-range information while maintaining the base model structure. Spectral Attention preserves long-period trends through a low-pass filter and facilitates gradient to flow between samples. Spectral Attention can be seamlessly integrated into most sequence models, allowing models with fixed-sized look-back windows to capture long-range dependencies over thousands of steps. Through extensive experiments on 11 real-world time series datasets using 7 recent forecasting models, we consistently demonstrate the efficacy of our Spectral Attention mechanism, achieving state-of-the-art results.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-475" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-475', event_id='95780', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4921</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95780">Sigmoid Gating is More Sample Efficient than Softmax Gating in Mixture of Experts</a></strong></h5>


                        <p class="text-muted">
                            Huy Nguyen &middot; Nhat Ho &middot; Alessandro Rinaldo
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The softmax gating function is arguably the most popular choice in mixture of experts modeling. Despite its widespread use in practice, the softmax gating may lead to unnecessary competition among experts, potentially causing the undesirable phenomenon of representation collapse due to its inherent structure. In response, the sigmoid gating function has been recently proposed as an alternative and has been demonstrated empirically to achieve superior performance. However, a rigorous examination of the sigmoid gating function is lacking in current literature. In this paper, we verify theoretically that the sigmoid gating, in fact, enjoys a higher sample efficiency than the softmax gating for the statistical task of expert estimation. Towards that goal, we consider a regression framework in which the unknown regression function is modeled as a mixture of experts, and study the rates of convergence of the least squares estimator under the over-specified case in which the number of fitted experts is larger than the true value. We show that two gating regimes naturally arise and, in each of them, we formulate an identifiability condition for the expert functions and derive the corresponding convergence rates. In both cases, we find that experts formulated as feed-forward networks with commonly used activation such as $\mathrm{ReLU}$ and $\mathrm{GELU}$ enjoy faster convergence rates under the sigmoid gating than those under softmax gating. Furthermore, given the same choice of experts, we demonstrate that the sigmoid gating function requires a smaller sample size than its softmax counterpart to attain the same error of expert estimation and, therefore, is more sample efficient.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-476" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-476', event_id='92925', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4922</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92925">PERIA: Perceive, Reason, Imagine, Act via Holistic Language and Vision Planning for Manipulation</a></strong></h5>


                        <p class="text-muted">
                            Fei Ni &middot; Jianye Hao &middot; Shiguang Wu &middot; Longxin Kou &middot; Yifu Yuan &middot; Zibin Dong &middot; Jinyi Liu &middot; MingZhi Li &middot; Yuzheng Zhuang &middot; YAN ZHENG
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Long-horizon manipulation tasks with general instructions often implicitly encapsulate multiple sub-tasks, posing significant challenges in instruction following.While language planning is a common approach to decompose general instructions into stepwise sub-instructions, text-only guidance may lack expressiveness and lead to potential ambiguity. Considering that humans often imagine and visualize sub-instructions reasoning out before acting, the imagined subgoal images can provide more intuitive guidance and enhance the reliability of decomposition. Inspired by this, we propose <strong>PERIA</strong>(<strong>PE</strong>rceive, <strong>R</strong>eason, <strong>I</strong>magine, <strong>A</strong>ct), a novel framework that integrates holistic language planning and vision planning for long-horizon manipulation tasks with complex instructions, leveraging both logical and intuitive aspects of task decomposition.Specifically, we first perform a lightweight multimodal alignment on the encoding side to empower the MLLM to perceive visual details and language instructions. The MLLM is then jointly instruction-tuned with a pretrained image-editing model to unlock capabilities of simultaneous reasoning of language instructions and generation of imagined subgoals. Furthermore, we introduce a consistency alignment loss to encourage coherent subgoal images and align with their corresponding instructions, mitigating potential hallucinations and semantic conflicts between the two planning manners.Comprehensive evaluations across three task domains demonstrate that PERIA, benefiting from holistic language and vision planning, significantly outperforms competitive baselines in both instruction following accuracy and task success rate on complex manipulation tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-477" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-477', event_id='93284', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4923</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93284">SPRINQL: Sub-optimal Demonstrations driven Offline Imitation Learning</a></strong></h5>


                        <p class="text-muted">
                            Huy Hoang &middot; Tien Mai &middot; Pradeep Varakantham
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We focus on offline imitation learning (IL), which aims to mimic an expert's behavior using demonstrations without any interaction with the environment. One of the main challenges in offline IL is the limited support of expert demonstrations, which typically cover only a small fraction of the state-action space. While it may not be feasible to obtain numerous expert demonstrations, it is often possible to gather a larger set of sub-optimal demonstrations. For example, in treatment optimization problems, there are varying levels of doctor treatments available for different chronic conditions. These range from treatment specialists and experienced general practitioners to less experienced general practitioners. Similarly, when robots are trained to imitate humans in routine tasks, they might learn from individuals with different levels of expertise and efficiency. In this paper, we propose an offline IL approach that leverages the larger set of sub-optimal demonstrations while effectively mimicking expert trajectories. Existing offline IL methods based on behavior cloning or distribution matching often face issues such as overfitting to the limited set of expert demonstrations or inadvertently imitating sub-optimal trajectories from the larger dataset. Our approach, which is based on inverse soft-Q learning, learns from both expert and sub-optimal demonstrations. It assigns higher importance (through learned weights) to aligning with expert demonstrations and lower importance to aligning with sub-optimal ones. A key contribution of our approach, called SPRINQL, is transforming the offline IL problem into a convex optimization over the space of Q functions. Through comprehensive experimental evaluations, we demonstrate that the SPRINQL algorithm achieves state-of-the-art (SOTA) performance on offline IL benchmarks. Code is available at https://github.com/hmhuy0/SPRINQL .</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-478" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-478', event_id='93324', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4924</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93324">Transductive Active Learning: Theory and Applications</a></strong></h5>


                        <p class="text-muted">
                            Jonas H√ºbotter &middot; Bhavya &middot; Lenart Treven &middot; Yarden As &middot; Andreas Krause
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We study a generalization of classical active learning to real-world settings with concrete prediction targets where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region.We analyze a family of decision rules that sample adaptively to minimize uncertainty about prediction targets.We are the first to show, under general regularity assumptions, that such decision rules converge uniformly to the smallest possible uncertainty obtainable from the accessible data.We demonstrate their strong sample efficiency in two key applications: active fine-tuning of large neural networks and safe Bayesian optimization, where they achieve state-of-the-art performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-479" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-479', event_id='93353', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4925</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93353">Gorilla: Large Language Model Connected with Massive APIs</a></strong></h5>


                        <p class="text-muted">
                            Shishir G Patil &middot; Tianjun Zhang &middot; Xin Wang &middot; Joseph Gonzalez
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large Language Models (LLMs) have seen an impressive wave of advances, withmodels now excelling in a variety of tasks, such as mathematical reasoning andprogram synthesis. However, their potential to effectively use tools via API callsremains unfulfilled. This is a challenging task even for today‚Äôs state-of-the-artLLMs such as GPT-4 largely due to their unawareness of what APIs are availableand how to use them in a frequently updated tool set. We develop Gorilla, afinetuned LLaMA model that surpasses the performance of GPT-4 on writing APIcalls. Trained with the novel Retriever Aware Training (RAT), when combinedwith a document retriever, Gorilla demonstrates a strong capability to adapt totest-time document changes, allowing flexible user updates or version changes.It also substantially mitigates the issue of hallucination, commonly encounteredwhen prompting LLMs directly. To evaluate the model‚Äôs ability, we introduceAPIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, andTensorHub APIs. The successful integration of the retrieval system with Gorillademonstrates the potential for LLMs to use tools more accurately, keep up withfrequently updated documentation, and consequently increase the reliability andapplicability of their outputs. Gorilla‚Äôs code, model, data, and demo are availableat: https://gorilla.cs.berkeley.edu</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-480" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-480', event_id='94709', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4930</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94709">Make Your LLM Fully Utilize the Context</a></strong></h5>


                        <p class="text-muted">
                            Shengnan An &middot; Zexiong Ma &middot; Zeqi Lin &middot; Nanning Zheng &middot; Jian-Guang Lou &middot; Weizhu Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the <em>lost-in-the-middle</em> challenge.We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information.Based on this intuition, our study presents <strong>information-intensive (IN2) training</strong>, a purely data-driven solution to overcome lost-in-the-middle.Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) <strong>fine-grained information awareness</strong> on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the <strong>integration and reasoning</strong> of information from two or more short segments.Through applying this information-intensive training on Mistral-7B, we present <strong>FILM-7B</strong> (FIll-in-the-Middle).To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval).The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window.Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU).</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-481" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-481', event_id='93114', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4931</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93114">Solving Zero-Sum Markov Games with Continous State via Spectral Dynamic Embedding</a></strong></h5>


                        <p class="text-muted">
                            Chenhao Zhou &middot; Zebang Shen &middot; zhang chao &middot; Hanbin Zhao &middot; Hui Qian
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In this paper, we propose a provably efficient natural policy gradient algorithm called Spectral Dynamic Embedding Policy Optimization (\SDEPO) for two-player zero-sum stochastic Markov games with continuous state space and finite action space.  In the policy evaluation procedure of our algorithm, a novel kernel embedding method is employed to construct a finite-dimensional linear approximations to the state-action value function.  We explicitly analyze the approximation error in policy evaluation, and show that \SDEPO\  achieves an $\tilde{O}(\frac{1}{(1-\gamma)^3\epsilon})$ last-iterate convergence to the $\epsilon-$optimal Nash equilibrium, which is independent of the cardinality of the state space.  The complexity result matches the best-known results for global convergence of policy gradient algorithms for single agent setting.  Moreover, we also propose a practical variant of \SDEPO\ to deal with continuous action space and empirical results demonstrate the practical superiority of the proposed method.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-482" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-482', event_id='96548', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4932</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96548">Locating What You Need: Towards Adapting Diffusion Models to OOD Concepts In-the-Wild</a></strong></h5>


                        <p class="text-muted">
                            Jianan Yang &middot; Chenchao Gao &middot; Zhiqing Xiao &middot; Junbo Zhao &middot; Sai Wu &middot; Gang Chen &middot; Haobo Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The recent large-scale text-to-image generative models have attained unprecedented performance, while people established <em>adaptor</em> modules like LoRA and DreamBooth to extend this performance to even more unseen concept tokens. However, we empirically find that this workflow often fails to accurately depict the <em>out-of-distribution</em> concepts. This failure is highly related to the low quality of training data. To resolve this, we present a framework called Controllable Adaptor Towards Out-of-Distribution Concepts (CATOD). Our framework follows the active learning paradigm which includes high-quality data accumulation and adaptor training, enabling a finer-grained enhancement of generative results. The <em>aesthetics</em> score and <em>concept-matching</em> score are two major factors that impact the quality of synthetic results. One key component of CATOD is the weighted scoring system that automatically balances between these two scores and we also offer comprehensive theoretical analysis for this point. Then, it determines how to select data and schedule the adaptor training based on this scoring system. The extensive results show that CATOD significantly outperforms the prior approaches with an 11.10 boost on the CLIP score and a 33.08% decrease on the CMMD metric.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-483" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-483', event_id='95606', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4933</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95606">Association Pattern-aware Fusion for Biological Entity Relationship Prediction</a></strong></h5>


                        <p class="text-muted">
                            Lingxiang Jia &middot; Yuchen Ying &middot; Zunlei Feng &middot; Zipeng Zhong &middot; Shaolun Yao &middot; Jiacong Hu &middot; Mingjiang Duan &middot; Xingen Wang &middot; Jie Song &middot; Mingli Song
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deep learning-based methods significantly advance the exploration of associations among triple-wise biological entities (e.g., drug-target protein-adverse reaction), thereby facilitating drug discovery and safeguarding human health. However, existing researches only focus on entity-centric information mapping and aggregation, neglecting the crucial role of potential association patterns among different entities. To address the above limitation, we propose a novel association pattern-aware fusion method for biological entity relationship prediction, which effectively integrates the related association pattern information into entity representation learning. Additionally, to enhance the missing information of the low-order message passing, we devise a bind-relation module that considers the strong bind of low-order entity associations. Extensive experiments conducted on three biological datasets quantitatively demonstrate that the proposed method achieves about 4%-23% hit@1 improvements compared with state-of-the-art baselines. Furthermore, the interpretability of association patterns is elucidated in detail, thus revealing the intrinsic biological mechanisms and promoting it to be deployed in real-world scenarios. Our data and code are available at https://github.com/hry98kki/PatternBERP.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-484" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-484', event_id='93785', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4934</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93785">Should We Really Edit Language Models? On the Evaluation of Edited Language Models</a></strong></h5>


                        <p class="text-muted">
                            Qi Li &middot; Xiang Liu &middot; Zhenheng Tang &middot; Peijie Dong &middot; Zeyu Li &middot; Xinglin Pan &middot; Xiaowen Chu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality,  with many methods excelling across these criteria. Some recent works disclose the pitfalls of these editing methods such as knowledge distortion or conflict. However, the general abilities of post-edited language models remain unexplored. In this paper, we perform a comprehensive evaluation on various editing methods and different language models, and have following findings.(1) Existing editing methods lead to inevitable performance deterioration on general benchmarks, indicating that existing editing methods maintain the general abilities of the model within only a few dozen edits.When the number of edits is slightly large, the intrinsic knowledge structure of the model is disrupted or even completely damaged. (2) Instruction-tuned models are more robust to editing, showing less performance drop on general knowledge after editing. (3) Language model with large scale is more resistant to editing compared to small model.(4) The safety of the edited model, is significantly weakened, even for those safety-aligned models.Our findings indicate that current editing methods are only suitable for small-scale knowledge updates within language models, which motivates further research on more practical and reliable editing methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-485" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-485', event_id='94304', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4935</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94304">Multi-Head Mixture-of-Experts</a></strong></h5>


                        <p class="text-muted">
                            Xun Wu &middot; Shaohan Huang &middot; Wenhui Wang &middot; Shuming Ma &middot; Li Dong &middot; Furu Wei
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Sparse Mixtures of Experts (SMoE) scales model capacity without significant increases in computational costs. However, it exhibits the low expert activation issue, i.e., only a small subset of experts are activated for optimization, leading to suboptimal performance and limiting its effectiveness in learning a larger number of experts in complex tasks. In this paper, we propose Multi-Head Mixture-of-Experts (MH-MoE). MH-MoE split each input token into multiple sub-tokens, then these sub-tokens are assigned to and processed by a diverse set of experts in parallel, and seamlessly reintegrated into the original token form. The above operations enables MH-MoE to significantly enhance expert activation while collectively attend to information from various representation spaces within different experts to deepen context understanding. Besides, it's worth noting that our MH-MoE is straightforward to implement and decouples from other SMoE frameworks, making it easy to integrate with these frameworks for enhanced performance. Extensive experimental results across different parameter scales (300M to 7B) and three pre-training tasks‚ÄîEnglish-focused language modeling, multi-lingual language modeling and masked multi-modality modeling‚Äîalong with multiple downstream validation tasks, demonstrate the effectiveness of MH-MoE.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-486" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-486', event_id='95636', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4936</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95636">Dual-Perspective Activation: Efficient Channel Denoising via Joint Forward-Backward Criterion for Artificial Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Tian Qiu &middot; Chenchao Gao &middot; Zunlei Feng &middot; Jie Lei &middot; Bingde Hu &middot; Xingen Wang &middot; Yi Gao &middot; Mingli Song
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The design of Artificial Neural Network (ANN) is inspired by the working patterns of the human brain. Connections in biological neural networks are sparse, as they only exist between few neurons. Meanwhile, the sparse representation in ANNs has been shown to possess significant advantages. Activation responses of ANNs are typically expected to promote sparse representations, where key signals get activated while irrelevant/redundant signals are suppressed. It can be observed that samples of each category are only correlated with sparse and specific channels in ANNs. However, existing activation mechanisms often struggle to suppress signals from other irrelevant channels entirely, and these signals have been verified to be detrimental to the network's final decision. To address the issue of channel noise interference in ANNs, a novel end-to-end trainable Dual-Perspective Activation (DPA) mechanism is proposed. DPA efficiently identifies irrelevant channels and applies channel denoising under the guidance of a joint criterion established online from both forward and backward propagation perspectives while preserving activation responses from relevant channels. Extensive experiments demonstrate that DPA successfully denoises channels and facilitates sparser neural representations. Moreover, DPA is parameter-free, fast, applicable to many mainstream ANN architectures, and achieves remarkable performance compared to other existing activation counterparts across multiple tasks and domains. Code is available at https://github.com/horrible-dong/DPA.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-487" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-487', event_id='93632', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4937</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93632">Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech Foundation Models</a></strong></h5>


                        <p class="text-muted">
                            Yuchen Hu &middot; CHEN CHEN &middot; Chao-Han Yang &middot; Chengwei Qin &middot; Pin-Yu Chen &middot; Eng-Siong Chng &middot; Chao Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose an unsupervised adaptation framework, Self-TAught Recognizer (STAR), which leverages unlabeled data to enhance the robustness of automatic speech recognition (ASR) systems in diverse target domains, such as noise and accents. STAR is developed for prevalent speech foundation models based on Transformer-related architecture with auto-regressive decoding (e.g., Whisper, Canary). Specifically, we propose a novel indicator that empirically integrates step-wise information during decoding to assess the token-level quality of pseudo labels without ground truth, thereby guiding model updates for effective unsupervised adaptation. Experimental results show that STAR achieves an average of 13.5% relative reduction in word error rate across 14 target domains, and it sometimes even approaches the upper-bound performance of supervised adaptation. Surprisingly, we also observe that STAR prevents the adapted model from the common catastrophic forgetting problem without recalling source-domain data. Furthermore, STAR exhibits high data efficiency that only requires less than one-hour unlabeled data, and seamless generality to alternative large speech models and speech translation tasks. Our code aims to open source to the research communities.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-488" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-488', event_id='95154', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4938</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95154">HuRef: HUman-REadable Fingerprint for Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Boyi Zeng &middot; Lizheng Wang &middot; Yuncong Hu &middot; Yi Xu &middot; Chenghu Zhou &middot; Xinbing Wang &middot; Yu Yu &middot; Zhouhan Lin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In thisstudy, we introduce HuRef, a human-readable fingerprint for LLMs that uniquely identifies the base model without interfering with training or exposing model parameters to the public.We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, with negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning, and RLHF, which makes it a sufficient conditionto identify the base model.The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension permutation or matrix rotation, which significantly change it without affecting performance. To address this, leveraging the Transformer structure, we systematically analyze potential attacks and define three invariant terms that identify an LLM's base model. Due to the potential risk of information leakage, we cannot publish invariant terms directly. Instead, we map them to a Gaussian vector using an encoder, then convert it into a natural image using StyleGAN2, and finally publish the image. In our black-box setting, all fingerprinting steps are internally conducted by the LLMs owners. To ensure the published fingerprints are honestly generated, we introduced Zero-Knowledge Proof (ZKP).Experimental results across various LLMs demonstrate the effectiveness of our method. The code is available at https://github.com/LUMIA-Group/HuRef.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-489" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-489', event_id='96661', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4939</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96661">The High Line: Exact Risk and Learning Rate Curves of Stochastic Adaptive Learning Rate Algorithms</a></strong></h5>


                        <p class="text-muted">
                            Elizabeth Collins-Woodfin &middot; Inbar Seroussi &middot; Bego√±a Garc√≠a Malaxechebarr√≠a &middot; Andrew W. Mackenzie &middot; Elliot Paquette &middot; Courtney Paquette
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We develop a framework for analyzing the training and learning rate dynamics on a large class of high-dimensional optimization problems, which we call the high line, trained using one-pass stochastic gradient descent (SGD) with adaptive learning rates. We give exact expressions for the risk and learning rate curves in terms of a deterministic solution to a system of ODEs. We then investigate in detail two adaptive learning rates  -- an idealized exact line search and AdaGrad-Norm -- on the least squares problem. When the data covariance matrix has strictly positive eigenvalues, this idealized exact line search strategy can exhibit arbitrarily slower convergence when compared to the optimal fixed learning rate with SGD. Moreover we exactly characterize the limiting learning rate (as time goes to infinity) for line search in the setting where the data covariance has only two distinct eigenvalues. For noiseless targets, we further demonstrate that the AdaGrad-Norm learning rate converges to a deterministic constant inversely proportional to the average eigenvalue of the data covariance matrix, and identify a phase transition when the covariance density of eigenvalues follows a power law distribution. We provideour code for evaluation at https://github.com/amackenzie1/highline2024.</p>
</p>
                    </div>

                </div>
            
        </div>



    




    
        </div>
    

</main>
<!--END BLOCK CONTENT-->


<!--Footer for the edit button-->

    <div id="editFooter" class="noprint" style="width:70px;">

        <button class="editFooterButton btn btn-outline-primary" title="Enable editing of content where possible"
                id="editpage"
                onclick="start_edit();"><i class="far fa-edit"></i></button>
        <button class="editFooterButton btn btn-outline-primary" title="Save edited content and reload the page"
                id="noeditpage"
                onclick="stop_edit();" style="display:none"><i class="fas fa-save"></i>
        </button>
    </div>


<script>

    $(function () {
        if ($(".editable").length == 0) {
            $("#editFooter").hide();
        }
    })
</script>

<script src="/static/core/js/fastclick.min.js" ></script>

<!--We don't know if there are editable tags on the page until after the django template engine has rendered the page. So,
test in javascript for "editable" tags and if present, load the ckeditor engine dynamically. -->

<script>
    if (document.getElementsByClassName('editable').length > 0) {
        var script = document.createElement("script");
        script.type = "text/javascript";
        script.src = "/static/core/ckeditor/4.18/ckeditor.js";    // use this for linked script
        script.text = "alert('voila!');"               // use this for inline script
        document.body.appendChild(script);
    }

</script>


<script>
    function fetchContent() {
        $(".editable").each(function (index) {
            debugger
            var myself = this;
            var docvID = this.getAttribute('id').replace("id_", "");
            var blurbtext = this.getAttribute("blurbtext");
            $.ajax({
                url: "/Admin/RetrieveDocumentVersion",
                type: "POST",
                data: {
                    docvID: docvID,
                    blurbtext: blurbtext,
                    csrfmiddlewaretoken: csrftoken,
                },
                success: function (data, textStatus, jqXHR) {
                    myself.setAttribute("contenteditable", "true");
                    myself.innerHTML = data;
                    CKEDITOR.inline(myself.id);
                },
            });
        })
    }

    $("#nopageedit").hide();

    function start_edit() {

        $(".editable").addClass("warning-ring");

        //At the beginning of an edit, we need to replace the content of the
        //editable div with it's databased content in order to preserve the
        //template tags. We want the tag, not the rendered tag.

        /* You must remove any countdown.js timers on the page before replacing the page with it's
        document version otherwise, Javascript will throw an exception.  */


        $("[class$='-countdown']").parent().remove();
        fetchContent();
        $(".editable").attr("onblur", "ckeditorsave(this)");
        window.status.bold();
        window.status = "Click outside the editable area to save. Changes are LIVE!! Refresh page to discard changes.";
        $("#editpage").hide();
        $("#noeditpage").show();
    }


    function stop_edit() {
        ckeditorsave();
        $("#noeditpage").hide();
        $("#editpage").show();
        window.location.reload();
    }

    function ckeditorsave(event) {
        for (var name in CKEDITOR.instances) {
            if (CKEDITOR.instances[name].checkDirty()) {
                editor = CKEDITOR.instances[name];
                saveEditable(editor);
            }
        }
    }

    function saveEditable(editor) {
        var content = editor.getData();
        var contentId = editor.name;
        var pageId = window.location.pathname;
        var originalContent = "N/A";
        var documentversion = editor.container.getAttribute("id").replace("id_", "");
        var blurbtext = editor.container.getAttribute("blurbtext");
        if (contentId.match(/-aloha$/gi)) {
            contentId = contentId.replace(/-aloha/gi, '');
        }  /*I'm not sure what this does but it seems like it would matter*/
        var request = jQuery.ajax({
            url: "/Admin/SaveDocument",
            type: "POST",
            async: false,
            data: {
                content: content,
                originalContent: originalContent,
                contentId: contentId,
                pageId: pageId,
                documentversion: documentversion,
                blurbtext: blurbtext,
                csrfmiddlewaretoken: csrftoken
            },
            success: function (data) {
                if (data['message']) {
                    alert(data['message']);
                }
            },
            error: function (xqXHR, textStatus) {
                window.status = textStatus;
                debugger;
            }

        });

    };


</script>

<script>
    jQuery(document).ajaxSend(function (event, xhr, settings) {
        function getCookie(name) {
            var cookieValue = null;
            if (document.cookie && document.cookie != '') {
                var cookies = document.cookie.split(';');
                for (var i = 0; i < cookies.length; i++) {
                    var cookie = jQuery.trim(cookies[i]);
                    // Does this cookie string begin with the name we want?
                    if (cookie.substring(0, name.length + 1) == (name + '=')) {
                        cookieValue = decodeURIComponent(cookie.substring(name.length + 1));
                        break;
                    }
                }
            }
            return cookieValue;
        }

        function sameOrigin(url) {
            // url could be relative or scheme relative or absolute
            var host = document.location.host; // host + port
            var protocol = document.location.protocol;
            var sr_origin = '//' + host;
            var origin = protocol + sr_origin;
            // Allow absolute or scheme relative URLs to same origin
            return (url == origin || url.slice(0, origin.length + 1) == origin + '/') ||
                (url == sr_origin || url.slice(0, sr_origin.length + 1) == sr_origin + '/') ||
                // or any other URL that isn't scheme relative or absolute i.e relative.
                !(/^(\/\/|http:|https:).*/.test(url));
        }

        function safeMethod(method) {
            return (/^(GET|HEAD|OPTIONS|TRACE)$/.test(method));
        }

        if (!safeMethod(settings.type) && sameOrigin(settings.url)) {
            xhr.setRequestHeader("X-CSRFToken", getCookie('csrftoken'));
        }
    });
</script>





<div id="successful-page-load" class='hidden'>Successful Page Load</div>





    
        <link href="/static/conf_gdpr/css/conf_gdpr.css" rel="stylesheet">
        <div id="cookie-bar" style="z-index: 8">
            <table class="gdpr-statement">
                <col>
                <col style="width:120px">
                <tr>
                    <td style="padding:5px">
                        NeurIPS uses cookies for essential functions only. We do not sell your personal
                        information.
                        <a href="/public/PrivacyPolicy">Our Privacy Policy &raquo;&nbsp;</a>
                    </td>
                    <td>
                        <button float-end class="btn btn-light btn-sm btn btn-outline-dark" onClick="accept_cookies();">Accept
                            Cookies
                        </button>
                    </td>
                </tr>
            </table>
        </div>

        <script>
            function accept_cookies() {

                $.ajax({
                    method: "POST",
                    url: "/conf_gdpr/accept",
                    data: {
                        csrfmiddlewaretoken: csrftoken,
                    },
                }).done(function (data) {
                    console.log(data);
                    $("#cookie-bar").fadeOut();
                }).fail(function (jqXHR, textStatus) {
                    alert(textStatus);
                });
            }
        </script>

    







<br>
<div class="noprint">
    <footer id="bootstrap-footer" class="text-center text-lg-start bg-light text-muted noprint">

        <div class="text-center p-1 border-top border-dark">
        </div>
        <!-- Section: Links  -->
        <section class="pt-1">
            <div class="container text-center text-md-start mt-3">
                <!-- Grid row -->
                <div class="row mt-3">
                    <!-- Grid column -->
                    <div class="col-md-3 col-lg-3 col-xl-3 mx-auto mb-3">
                        <!-- Content -->
                        <h6 class="text-uppercase fw-bold mb-4">
                            <img src="/static/core/img/NeurIPS-logo.svg" alt="NeurIPS logo" height='30'>
                        </h6>
                        <p>
                            The NeurIPS Logo above may be used on presentations. Right-click and choose
                            download. It is a vector graphic and may be used at any scale.
                        </p>

                    </div>


                    <!-- Grid column -->
                    <div class="col-md-5 col-lg-4 col-xl-3 mx-auto mb-4" style="max-width: 300px;">
                        <!-- Links -->
                        <h6 class="text-uppercase fw-bold mb-4 text-center">
                            Useful links
                        </h6>
                        <div>
             <ul>
	<li><a href="/Conferences/2024/Press">Press</a></li>
	<li><a href="/Exhibitors/exhibitorinfo">Exhibitor Information</a></li>
</ul>

            </div>
                    </div>
                    <!-- Grid column -->

                    <!-- Grid column -->
                    <div class="col-md-4 col-lg-3 col-xl-3 mx-auto mb-md-0 mb-4">
                        <!-- Links -->
                        <h6 class="text-uppercase fw-bold mb-4">Contact</h6>
                        
                            <p>
                                <i class="fas fa-home me-3"></i> 1269 Law St, San Diego CA 92109
                            </p>
                        
                        <p>
                            <i class="fas fa-envelope me-3"></i> <a href="/Help/Contact">Email</a>
                        </p>
                        
                        


                    </div>
                    <!-- Grid column -->
                </div>
                <!-- Grid row -->
            </div>
        </section>
        <!-- Section: Links  -->

        <!-- Copyright -->
        <div class="text-center p-4" style="background-color: rgba(0, 0, 0, 0.05);">
            <div>
             <p><a href="https://proceedings.neurips.cc">NeurIPS Proceedings</a></p>

            </div>
        </div>
        <!-- Copyright -->
    </footer>
</div>
<!-- Footer -->


<!-- Footer -->

</body>
</html>
