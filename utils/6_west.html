





<!DOCTYPE html>
<html lang="en" style="scroll-padding-top: 70px;"> 

<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="/static/virtual/js/virtual.js"></script>
    <meta name="google-site-verification" content="0jwPnVXIAk4FvFdT37dwMmd-kjHF86e5DKwvqlStUW0">


    
    <link rel="stylesheet" href="/static/core/css/core.css" type="text/css">
    <link rel="stylesheet" href="/static/virtual/css/virtual.css" type="text/css">
     <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">

    <link rel="stylesheet" href="/static/core/css/custom.css" type="text/css">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.14.0-beta2/dist/css/bootstrap-select.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Exo:wght@400;700&family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
      "tex2jax": {
        "inlineMath": [["$","$"], ["\\(","\\)"]],
        "displayMath": [["\\[","\\]"]],
        "processEscapes": true
      }
    }
    );
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <!--This script keeps local links inside the web app rather than opening them
in Safari, and has nothing to do with editing or Aloha.-->

<script >
	(function(document,navigator,standalone) {
		// prevents links from apps from opening in mobile safari
		// this javascript must be the first script in your <head>
		if ((standalone in navigator) && navigator[standalone]) {
			var curnode, location=document.location, stop=/^(a|html)$/i;
			document.addEventListener('click', function(e) {
				curnode=e.target;
				while (!(stop).test(curnode.nodeName)) {
					curnode=curnode.parentNode;
				}
				// Conditions to do this only on links to your own app
				// if you want all links, use if('href' in curnode) instead.
				if(
					'href' in curnode && // is a link
					(chref=curnode.href).replace(location.href,'').indexOf('#') && // is not an anchor
					(	!(/^[a-z\+\.\-]+:/i).test(chref) ||                       // either does not have a proper scheme (relative links)
						chref.indexOf(location.protocol+'//'+location.host)===0 ) // or is in the same protocol and domain
				) {
					e.preventDefault();
					location.href = curnode.href;
				}
			},false);
		}
	})(document,window.navigator,'standalone');
</script>        

<!-- This style sets the minimum size of a blurb to 260 px unless there is a
template context variable blurb_min_height that sets it otherwise. If blurbs
aren't all about the same size, they don't flow well when the window is
resized.-->


<style>
/*This is here rather that in a .css file for a reason.*/
    @media screen and (min-width: 767px) {
        .blurb {
            min-height:260px;
        }
    }
</style>
    

<script src="https://code.jquery.com/jquery-3.6.1.min.js"
        integrity="sha256-o88AwQnZB+VDvE9tvIXrMQaPlFFSUTR+nldQm1LuPXQ=" crossorigin="anonymous">
</script>

<script>
    if (typeof jQuery === 'undefined') {
        var script = document.createElement('script');
        script.type = 'text/javascript';
        script.src = "/static/core/js/jquery-3.6.1.min.js";
        document.head.appendChild(script);
    }
</script>


    <script>
        var $ = jQuery;
        /*Store a pointer to jquery2, so I can reference it later.  Aloha loads jquery 1.7 and much
        of bootstrap 3 is not compatible. This comment is deprecated. */
    </script>

    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4" crossorigin="anonymous"></script>

    <script src="/static/core/js/ajax-csrf-snippet.js"></script>
    <script src="https://kit.fontawesome.com/be44b7e05d.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.14.0-beta3/dist/js/bootstrap-select.min.js"></script>


    <style>
        body {
            font-family: Exo;}
    </style>








        


    <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">
    <link href="https://fonts.googleapis.com/css2?family=Exo:wght@400;700&family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/static/virtual/css/virtual.css">
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/corejs-typeahead/1.3.1/typeahead.bundle.min.js" integrity="sha512-lEb9Vp/rkl9g2E/LdHIMFTqz21+LA79f84gqP75fbimHqVTu6483JG1AwJlWLLQ8ezTehty78fObKupq3HSHPQ==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script>
    <script src="/static/core/js/ajax-csrf-snippet.js" ></script>
    <script src="/static/virtual/js/virtual.js"></script>
    


    
    <title>Track: Poster Session 6 West</title>
    <script src='https://slideslive.com/embed_presentation.js'></script>

    <title>NeurIPS 2024</title>
</head>

<body>




<div class="noprint">
    
        <!--Navbar start-->
<header>
    <a href="#child-menu" class="off-screen">Skip to yearly menu bar</a>
    <a href="#main" class="off-screen">Skip to main content</a>
    <div id="id_navbar" class="navbar navbar-expand-sm navbar-dark" aria-label="Main Navigation"
         style="background-color:#212529">
        <h2 class="off-screen">Main Navigation</h2>
        <div class="container-fluid">
            <div><a class="navbar-brand" href="/">

                <img src="/static/core/img/neurips-navbar-logo.svg" alt="conference_logo" height="40"></a></div>


            <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                    data-bs-target="#navbarToggler1"
                    aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarToggler1">
                <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                    
    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" 
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            NeurIPS
        </a>
        <ul class="dropdown-menu dropdown-menu-dark">
            
    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/FAQ">
                    <span >
                        Help/FAQ
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Help/Contact">
                    <span >
                        Contact NeurIPS
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Conferences/2023/EthicsGuidelines">
                    <span >
                        Code of Ethics
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/public/CodeOfConduct">
                    <span >
                        Code of Conduct
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Profile/create">
                    <span >
                        Create Profile
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/public/JournalToConference">
                    <span >
                        Journal To Conference Track
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/public/DiversityInclusion">
                    <span >
                        Diversity &amp; Inclusion
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="https://proceedings.neurips.cc/">
                    <span >
                        Proceedings
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Conferences/FutureMeetings">
                    <span >
                        Future Meetings
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Conferences/2024/Press">
                    <span >
                        Press
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Exhibitors/exhibitorinfo">
                    <span >
                        Exhibitor Information
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/public/PrivacyPolicy">
                    <span >
                        Privacy Policy
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Downloads">
                    <span >
                        Downloads
                    </span>
                </a>
                
            </li>

        

    



        </ul>
    </li>
    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/MyStuff">
                    <span >
                        My Stuff
                    </span>
                </a>
                
            </li>

        

    



                </ul>

                <form class="d-flex mx-2" aria-label="Search" role="search" action="/search">
                    <div class="input-group" role="search" style="outline-color:green;">
                        <input type="text" class="form-control" placeholder="Search" name="q"
                               value=""
                               aria-label="Search" aria-describedby="btnGroupAddon"
                                id="navbar-search">
                        <div class="input-group-text btn-primary" id="btnGroupAddon">
                            <button style="border: none; background-color: transparent; padding: 0;" type="submit">
                                <i class="fa-solid fa-magnifying-glass"></i>
                            </button>
                        </div>
                    </div>
                </form>

                
                    
                    <div class="btn-group d-none d-sm-block nav-item" role="group"
                         aria-label="Button group with nested dropdown">
                        <div class="btn-group" role="group">
                            <button type="button" class="btn btn-light dropdown-toggle" data-bs-toggle="dropdown"
                               id="id_navbar_username" aria-expanded="false">
                                Mei
                            </button>
                            <ul class="dropdown-menu dropdown-menu-end dropdown-menu-dark">
                                
                                <li>
                                    <a class="dropdown-item" href="/MyStuff"> <i class="fa-regular fa-user"></i> &nbsp;My
                                        Stuff</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li>
                                    <a class="dropdown-item" href="/virtual/2024/mycalendar"> <i class="fa-regular fa-user"></i> &nbsp;My
                                        Bookmarks</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li>
                                    <a class="dropdown-item" href="/Profile/change-password"> <i
                                            class="fa-solid fa-lock"></i> &nbsp;Change Password</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li>
                                    <a class="dropdown-item" href="/resetpassword"> <i class="fa-solid fa-unlock"></i>
                                        &nbsp;Reset Password</a>
                                </li>

                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/EditProfile"> <i
                                        class="fa-regular fa-address-card"></i> &nbsp;Edit Profile </a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/MergeAccounts"> <i
                                        class="fa-solid fa-code-merge"></i> &nbsp;Merge Profiles </a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item"
                                       href="/set_timezone?nextp=/virtual/2024/session/108371"> <i
                                        class="fa-solid fa-earth-americas"></i>
                                    &nbsp;TZ: America/Chicago</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/logout"> <i
                                        class="fa-solid fa-right-from-bracket"></i> &nbsp;Log Out</a></li>
                            </ul>
                        </div>
                    </div>
                    <br>
                    
                    <div class="btn-group d-block d-sm-none " role="group"
                         aria-label="Button group with nested dropdown">

                        <div class="btn-group" role="group">
                            <button class="btn btn-light dropdown-toggle" data-bs-toggle="dropdown"
                                    aria-expanded="false">
                                Mei
                            </button>
                            <ul class="dropdown-menu dropdown-menu-dark">

                                <li>
                                    <a class="dropdown-item" href="/MyStuff"> <span
                                            class="glyphicon glyphicon-cog"></span> My Stuff</a>
                                </li>
                                <li>
                                    <a class="dropdown-item" href="/virtual/2024/mycalendar"> <span
                                            class="glyphicon glyphicon-cog"></span> My Bookmarks</a>
                                </li>
                                <li>
                                    <a class="dropdown-item" href="/Profile/change-password"> <span
                                            class="glyphicon glyphicon-cog"></span> Change Password</a>
                                </li>

                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/EditProfile"><span
                                        class="fa-solid fa-pen-to-square"></span> Edit Profile </a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item"
                                       href="/set_timezone?nextp=/virtual/2024/session/108371">TZ: America/Chicago</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/logout"><span
                                        class="fa-solid fa-right-from-bracket"></span> Log Out</a></li>
                            </ul>
                        </div>
                    </div>
                

            </div>
        </div>
    </div>
</header>
<!--Navbar end-->
    
</div><!--noprint div-->

<!--This holds the whole page including the navbar-->

<main id="main">
    
        <div class="container-fluid">
            <!--Navbar start-->

<div class="dropdown" id="child-menu">
    <nav class="align-middle navbar navbar-expand-md  rounded-bottom"
         style="min-height: 57px; background-image: url(/static/virtual/img/navbackground.png); background-repeat: repeat-x;">
        <div class="container-fluid">

            <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                    data-bs-target="#navbarToggler987"
                    aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarToggler987">
                <ul class="navbar-nav me-auto mb-lg-0">
                    


    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle border-3  btn btn-primary text-white p-1" style= "background-color: #070bff; font-size: 1.2 em;"
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Select Year: (2024)
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2024">2024
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2023">2023
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2022">2022
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2021">2021
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2020">2020
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2019">2019
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2018">2018
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2017">2017
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2016">2016
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2015">2015
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2014">2014
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2013">2013
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2012">2012
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2011">2011
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2010">2010
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2009">2009
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2008">2008
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2007">2007
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2006">2006
                </a>
                
            </li>
        

    



        </ul>
    </li>
    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/index.html">
                    <span >
                        Start Here
                    </span>
                </a>
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/calendar">
                    <span >
                        Schedule
                    </span>
                </a>
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/tutorial">
                    <span >
                        Tutorials
                    </span>
                </a>
                
            </li>

        

    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" 
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Main Conference
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/eventlistwithbios/Invited%20Talk">
                    <span >
                        Invited Talks
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/oral">
                    <span >
                        Orals
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/spotlight-posters-2024">
                    <span >
                        Spotlights
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/papers.html">
                    <span >
                        Papers
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="https://neurips2024.vizhub.ai">
                    <span >
                        Paper Visualization
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/Competition">
                    <span >
                        Competitions
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/datasets-benchmarks-2024">
                    <span >
                        Datasets &amp; Benchmarks
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/journal_track_2024">
                    <span >
                        Journal Track
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/creative-ai-2024">
                    <span >
                        Creative AI Track
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/awards_detail">
                    <span >
                        Outstanding Paper Awards
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/affinity%20workshop">
                    <span >
                        Affinity Workshops
                    </span>
                </a>
                
            </li>

        

    



        </ul>
    </li>
    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" 
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Community
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/affinity_events">
                    <span >
                        Affinity Events
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/community?show=mentorship">
                    <span >
                        Mentorship Event
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/session/101095">
                    <span >
                        Bridging the Future
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/social">
                    <span >
                        Socials
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/careers">
                    <span >
                        Careers
                    </span>
                </a>
                
            </li>

        

    



        </ul>
    </li>
    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/workshop">
                    <span >
                        Workshops
                    </span>
                </a>
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/sponsor_list">
                    <span >
                        Exhibitors
                    </span>
                </a>
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/search">
                    <span >
                        <i class="fas fa-search"></i>
                    </span>
                </a>
                
            </li>

        

    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" 
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Help
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/FAQ">
                    <span >
                        FAQ
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="https://chat.neurips.cc/channel/HelpDesk">
                    <span >
                        Helpdesk in RocketChat
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/organizers">
                    <span >
                        Organizers
                    </span>
                </a>
                
            </li>

        

    



        </ul>
    </li>
    



                </ul>
            </div>
        </div>
    </nav>
</div>
    <!--Navbar end-->
        </div>
        <br><br>
    
    
        
        <div class="container">
    
    

    

    

        <div class="container">
            
                

    <span id="the-bookmark-1" class="bump20 bookmark-right fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='the-bookmark-1', event_id='108371', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>



            
            <!-- Title -->
            <div class="" style="">
                <div class="card-header">
                    <h3 class="text-center ">Poster Session</h3>
                    <h2 class="card-title main-title text-center" style="">
                        Poster Session 6 West
                    </h2>
                    
                        <h5 class="text-center text-muted">West Ballroom A-D</h5>
                    
                    


                    
                    

                    


                    <div class="text-center">
                        

                        <div>
                            
                        </div>
                        <div>
                            
                        </div>
                        <div>
                            
                        </div>

                        
                        
                        

                        

                        

                        

                            
                                <div>Fri 13 Dec 6:30 p.m. CST 
                                    &mdash; 9:30 p.m. CST  

    <span id="bookmark-number-0" class="bump20 " title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-0', event_id='108371', bookmark_event_number='1',
                  alt_bookmark_element_id='the-bookmark-1');">
        
            <span class="green">(Bookmark)</span>
        
    </span>



                                    

                                </div>
                                <div class="schedule-html-detail"></div>

                            

                            

                        
                        

                        
                    </div>
                    <div class=" text-center text-muted text-monospace ">
                        <div> 
                        </div>
                    </div>
                </div>
            </div>
            <div id="details" class="pp-card m-3 collapse">
                <div class="card-body p-3">

                    <div id="abstractExample">
                        <span class="font-weight-bold">Abstract:</span> 
                    </div>


                </div>
            </div>
        </div>
        <!-- SlidesLive -->

        
            <div class="container">
                <div class="col-xs-12 my-auto p-2 admin centered">
                    
                </div>
            </div>

            

            
         

        <!--Children in this session -->
        <p></p>

        <div class="container" style="padding-bottom: 30px; padding-top:30px">
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-1" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-1', event_id='96813', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5000</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96813">Partial Transportability for Domain Generalization</a></strong></h5>


                        <p class="text-muted">
                            Kasra Jalaldoust &middot; Alexis Bellot &middot; Elias Bareinboim
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>A fundamental task in AI is providing performance guarantees for predictions made in unseen domains. In practice, there can be substantial uncertainty about the distribution of new data, and corresponding variability in the performance of existing predictors. Building on the theory of partial identification and transportability, this paper introduces new results for bounding the value of a functional of the target distribution, such as the generalization error of a classifiers, given data from source domains and assumptions about the data generating mechanisms, encoded in causal diagrams. Our contribution is to provide the first general estimation technique for transportability problems, adapting existing parameterization schemes such Neural Causal Models to encode the structural constraints necessary for cross-population inference. We demonstrate the expressiveness and consistency of this procedure and further propose a gradient-based optimization scheme for making scalable inferences in practice. Our results are corroborated with experiments.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-2" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-2', event_id='96784', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5001</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96784">Automated Efficient Estimation using Monte Carlo Efficient Influence Functions</a></strong></h5>


                        <p class="text-muted">
                            Raj Agrawal &middot; Sam Witty &middot; Andy Zane &middot; Elias Bingham
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Many practical problems involve estimating low dimensional statistical quantities with high-dimensional models and datasets. Several approaches address these estimation tasks based on the theory of influence functions, such as debiased/double ML or targeted minimum loss estimation. We introduce \textit{Monte Carlo Efficient Influence Functions} (MC-EIF), a fully automated technique for approximating efficient influence functions that integrates seamlessly with existing differentiable probabilistic programming systems. MC-EIF automates efficient statistical estimation for a broad class of models and functionals that previously required rigorous custom analysis. We prove that MC-EIF is consistent, and that estimators using MC-EIF achieve optimal $\sqrt{N}$ convergence rates. We show empirically that estimators using MC-EIF are at parity with estimators using analytic EIFs. Finally, we present a novel capstone example using MC-EIF for optimal portfolio selection.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-3" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-3', event_id='96766', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5002</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96766">Graph Structure Inference with BAM: Neural Dependency Processing via Bilinear Attention</a></strong></h5>


                        <p class="text-muted">
                            Philipp Froehlich &middot; Heinz Koeppl
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Detecting dependencies among variables is a fundamental task across scientific disciplines. We propose a novel neural network model for graph structure inference, which aims to learn a mapping from observational data to the corresponding underlying dependence structures. The model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference. Central to our approach is a novel bilinear attention mechanism (BAM) operating on covariance matrices of transformed data while respecting the geometry of the manifold of symmetric positive definite (SPD) matrices. Inspired by graphical lasso methods, our model optimizes over continuous graph representations in the SPD space, where inverse covariance matrices encode conditional independence relations. Empirical evaluations demonstrate the robustness of our method in detecting diverse dependencies, excelling in undirected graph estimation and showing competitive performance in completed partially directed acyclic graph estimation via a novel two-step approach. The trained model effectively detects causal relationships and generalizes well across different functional forms of nonlinear dependencies.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-4" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-4', event_id='96180', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5003</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96180">Tangent Space Causal Inference: Leveraging Vector Fields for Causal Discovery in Dynamical Systems</a></strong></h5>


                        <p class="text-muted">
                            Kurt Butler &middot; Daniel Waxman &middot; Petar Djuric
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Causal discovery with time series data remains a challenging yet increasingly important task across many scientific domains. Convergent cross mapping (CCM) and related methods have been proposed to study time series that are generated by dynamical systems, where traditional approaches like Granger causality are unreliable. However, CCM often yields inaccurate results depending upon the quality of the data. We propose the Tangent Space Causal Inference (TSCI) method for detecting causalities in dynamical systems. TSCI works by considering vector fields as explicit representations of the systems' dynamics and checks for the degree of synchronization between the learned vector fields. The TSCI approach is model-agnostic and can be used as a drop-in replacement for CCM and its generalizations. We first present a basic version of the TSCI algorithm, which is shown to be more effective than the basic CCM algorithm with very little additional computation. We additionally present augmented versions of TSCI that leverage the expressive power of latent variable models and deep learning. We validate our theory on standard systems, and we demonstrate improved causal inference performance across a number of benchmark tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-5" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-5', event_id='95916', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5004</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95916">Consistency of Neural Causal Partial Identification</a></strong></h5>


                        <p class="text-muted">
                            Jiyuan Tan &middot; Jose Blanchet &middot; Vasilis Syrgkanis
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent progress in Neural Causal Models (NCMs) showcased how identification and partial identification of causal effects can be automatically carried out via training of neural generative models that respect the constraints encoded in a given causal graph [Xia et al. 2022, Balazadeh et al. 2022]. However, formal consistency of these methods has only been proven for the case of discrete variables or only for linear causal models. In this work, we prove the consistency of partial identification via NCMs in a general setting with both continuous and categorical variables. Further, our results highlight the impact of the design of the underlying neural network architecture in terms of depth and connectivity as well as the importance of applying Lipschitz regularization in the training phase. In particular, we provide a counterexample showing that without Lipschitz regularization this method may not be asymptotically consistent. Our results are enabled by new results on the approximability of Structural Causal Models (SCMs) via neural generative models, together with an analysis of the sample complexity of the resulting architectures and how that translates into an error in the constrained optimization problem that defines the partial identification bounds.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-6" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-6', event_id='95380', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5005</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95380">Identifying Causal Effects Under Functional Dependencies</a></strong></h5>


                        <p class="text-muted">
                            Yizuo Chen &middot; Adnan Darwiche
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We study the identification of causal effects, motivated by two improvements to identifiability which can be attained if one knows that some variables in a causal graph are functionally determined by their parents (without needing to know the specific functions). First, an unidentifiable causal effect may become identifiable when certain variables are functional. Second, certain functional variables can be excluded from being observed without affecting the identifiability of a causal effect, which may significantly reduce the number of needed variables in observational data. Our results are largely based on an elimination procedure which removes functional variables from a causal graph while preserving key properties in the resulting causal graph, including the identifiability of causal effects.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-7" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-7', event_id='94488', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5006</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94488">On the Complexity of Identification in Linear Structural Causal Models</a></strong></h5>


                        <p class="text-muted">
                            Julian Drfler &middot; Benito van der Zander &middot; Markus Blser &middot; Maciej Liskiewicz
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Learning the unknown causal parameters of a linear structural causal model is a fundamental task in causal analysis. The task, known as the problem of identification, asks to estimate the parameters of the model from acombination of assumptions on the graphical structure of the model and observational data, represented as a non-causal covariance matrix.In this paper, we give a new sound and complete algorithm for generic identification which runs in polynomial space. By a standard simulation result, namely $\mathsf{PSPACE} \subseteq \mathsf{EXP}$,this algorithm has exponential running time which vastly improves the state-of-the-art double exponential time method using a Grbner basis approach. The paper also presents evidence that parameter identification is computationally hard in general. In particular, we prove, that the taskasking whether, for a given feasible correlation matrix, there are exactly one or two or more parameter sets explaining the observed matrix, is hard for $\forall \mathbb{R}$, the co-class of the existential theory of the reals. In particular, this problem is $\mathsf{coNP}$-hard.To our best knowledge, this is the first hardness result for some notion of identifiability.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-8" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-8', event_id='93760', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5007</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93760">A Simple yet Scalable Granger Causal Structural Learning Approach for Topological Event Sequences</a></strong></h5>


                        <p class="text-muted">
                            Mingjia Li &middot; Shuo Liu &middot; Hong Qian &middot; Aimin Zhou
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In modern telecommunication networks, faults manifest as alarms, generating thousands of events daily. Network operators need an efficient method to identify the root causes of these alarms to mitigate potential losses. This task is challenging due to the increasing scale of telecommunication networks and the interconnected nature of devices, where one fault can trigger a cascade of alarms across multiple devices within a topological network. Recent years have seen a growing focus on causal approaches to addressing this problem, emphasizing the importance of learning a Granger causal graph from topological event sequences. Such causal graphs delineate the relations among alarms and can significantly aid engineers in identifying and rectifying faults. However, existing methods either ignore the topological relationships among devices or suffer from relatively low scalability and efficiency, failing to deliver high-quality responses in a timely manner. To this end, this paper proposes $S^2GCSL$, a simple yet scalable Granger causal structural learning approach for topological event sequences. $S^2GCSL$ utilizes a linear kernel to model activation interactions among various event types within a topological network, and employs gradient descent to efficiently optimize the likelihood function. Notably, it can seamlessly incorporate expert knowledge as constraints within the optimization process, which enhances the interpretability of the outcomes. Extensive experimental results on both large-scale synthetic and real-world problems verify the scalability and efficacy of $S^2GCSL$.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-9" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-9', event_id='97846', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5008</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97846">A Benchmark Suite for Systematically Evaluating Reasoning Shortcuts</a></strong></h5>


                        <p class="text-muted">
                            Samuele Bortolotti &middot; Emanuele Marconato &middot; Tommaso Carraro &middot; Paolo Morettin &middot; Emile van Krieken &middot; Antonio Vergari &middot; Stefano Teso &middot; Andrea Passerini
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The advent of powerful neural classifiers has increased interest in problems that require both learning and reasoning.These problems are critical for understanding important properties of models, such as trustworthiness, generalization, interpretability, and compliance to safety and structural constraints. However, recent research observed that tasks requiring both learning and reasoning on background knowledge often suffer from reasoning shortcuts (RSs): predictors can solve the downstream reasoning task without associating the correct concepts to the high-dimensional data. To address this issue, we introduce rsbench, a comprehensive benchmark suite designed to systematically evaluate the impact of RSs on models by providing easy access to highly customizable tasks affected by RSs. Furthermore, rsbench implements common metrics for evaluating concept quality and introduces novel formal verification procedures for assessing the presence of RSs in learning tasks. Using rsbench, we highlight that obtaining high quality concepts in both purely neural and neuro-symbolic models is a far-from-solved problem. rsbench is available at: https://unitn-sml.github.io/rsbench.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-10" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-10', event_id='97837', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5009</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97837">UrbanDataLayer: A Unified Data Pipeline for Urban Science</a></strong></h5>


                        <p class="text-muted">
                            Yiheng Wang &middot; Tianyu Wang &middot; YuYing Zhang &middot; Hongji Zhang &middot; Haoyu Zheng &middot; Guanjie Zheng &middot; Linghe Kong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The rapid progression of urbanization has generated a diverse array of urban data, facilitating significant advancements in urban science and urban computing. Current studies often work on separate problems case by case using diverse data, e.g., air quality prediction, built-up areas classification. This fragmented approach hinders the urban research field from advancing at the pace observed in Computer Vision and Natural Language Processing, due to two primary reasons. On one hand, the diverse data processing steps lead to the lack of large-scale benchmark and therefore decelerate iterative methodology improvement on single problem. On the other hand, the disparity in multi-modal data formats hinders the combination of the related modal data to stimulate more research findings. To address these challenges, we propose UrbanDataLayer (UDL), a suite of standardized data structure and pipeline for city data engineering, providing a unified data format for researchers. This allows researchers easily build up large-scale benchmark and combine multi-modal data, thus expediting the development of multi-modal urban foundation models. To verify the effectiveness of our work, we present four distinct urban problem tasks utilizing the proposed data layer. UrbanDataLayer aims to enhance standardization and operational efficiency within the urban science research community.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-11" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-11', event_id='97664', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5100</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97664">Lean Workbook: A large-scale Lean problem set formalized from natural language math problems</a></strong></h5>


                        <p class="text-muted">
                            Huaiyuan Ying &middot; Zijian Wu &middot; Yihan Geng &middot; JIayu Wang &middot; Dahua Lin &middot; Kai Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at \url{https://github.com/InternLM/InternLM-Math} and our data at \url{https://huggingface.co/datasets/InternLM/Lean-Workbook}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-12" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-12', event_id='97669', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5101</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97669">Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs</a></strong></h5>


                        <p class="text-muted">
                            Rudolf Laine &middot; Bilal Chughtai &middot; Jan Betley &middot; Kaivalya Hariharan &middot; Mikita Balesni &middot; Jrmy Scheurer &middot; Marius Hobbhahn &middot; Alexander Meinke &middot; Owain Evans
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>AI assistants such as ChatGPT are trained to respond to users by saying, "I am a large language model.This raises questions. Do such models "know'' that they are LLMs and reliably act on this knowledge? Are they "aware" of their current circumstances, such as being deployed to the public?We refer to a model's knowledge of itself and its circumstances as <strong>situational awareness</strong>.To quantify situational awareness in LLMs, we introduce a range of behavioral tests, based on question answering and instruction following. These tests form the <strong>Situational Awareness Dataset (SAD)</strong>, a benchmark comprising 7 task categories and over 13,000 questions.The benchmark tests numerous abilities, including the capacity of LLMs to (i) recognize their own generated text, (ii) predict their own behavior, (iii) determine whether a prompt is from internal evaluation or real-world deployment, and (iv) follow instructions that depend on self-knowledge.We evaluate 16 LLMs on SAD, including both base (pretrained) and chat models.While all models perform better than chance, even the highest-scoring model (Claude 3 Opus) is far from a human baseline on certain tasks. We also observe that performance on SAD is only partially predicted by metrics of general knowledge. Chat models, which are finetuned to serve as AI assistants, outperform their corresponding base models on SAD but not on general knowledge tasks.The purpose of SAD is to facilitate scientific understanding of situational awareness in LLMs by breaking it down into quantitative abilities. Situational awareness is important because it enhances a model's capacity for autonomous planning and action. While this has potential benefits from automation, it also introduces novel risks related to AI safety and control.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-13" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-13', event_id='97685', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5102</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97685">MathPile: A Billion-Token-Scale Pretraining Corpus for Math</a></strong></h5>


                        <p class="text-muted">
                            Zengzhi Wang &middot; Xuefeng Li &middot; Rui Xia &middot; Pengfei Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce MathPile, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of less is more, firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates and conducted continual pre-training experiments, booting the performance on common mathematical reasoning benchmarks. We aim for our MathPile to boost language models mathematical reasoning and plan to open-source its different versions and processing scripts to advance the field.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-14" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-14', event_id='97703', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5103</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97703">CRAG - Comprehensive RAG Benchmark</a></strong></h5>


                        <p class="text-muted">
                            Xiao Yang &middot; Kai Sun &middot; Hao Xin &middot; Yushi Sun &middot; Nikita Bhalla &middot; Xiangsen Chen &middot; Sajal Choudhary &middot; Rongze Gui &middot; Ziran Jiang &middot; Ziyu Jiang &middot; Lingkun Kong &middot; Brian Moran &middot; Jiaqi Wang &middot; Yifan Xu &middot; An Yan &middot; Chenyu Yang &middot; Eting Yuan &middot; Hanwen Zha &middot; Nan Tang &middot; Lei Chen &middot; Nicolas Scheffer &middot; Yue Liu &middot; Nirav Shah &middot; Rakesh Wanga &middot; Anuj Kumar &middot; Scott Yih &middot; Xin Dong
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve $\le 34\%$ accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-15" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-15', event_id='97724', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5104</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97724">NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates</a></strong></h5>


                        <p class="text-muted">
                            Hexuan Deng &middot; Wenxiang Jiao &middot; Xuebo Liu &middot; Min Zhang &middot; Zhaopeng Tu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite their remarkable abilities in various tasks, large language models (LLMs) still struggle with real-time information (e.g., new facts and terms) due to the knowledge cutoff in their development process. However, existing benchmarks focus on outdated content and limited fields, facing difficulties in real-time updating and leaving new terms unexplored. To address this problem, we propose an adaptive benchmark, NewTerm, for real-time evaluation of new terms. We design a highly automated construction method to ensure high-quality benchmark construction with minimal human effort, allowing flexible updates for real-time information. Empirical results on various LLMs demonstrate over 20% performance reduction caused by new terms. Additionally, while updates to the knowledge cutoff of LLMs can cover some of the new terms, they are unable to generalize to more distant new terms. We also analyze which types of terms are more challenging and why LLMs struggle with new terms, paving the way for future research. Finally, we construct NewTerm 2022 and 2023 to evaluate the new terms updated each year and will continue updating annually. The benchmark and codes can be found at https://anonymous.4open.science/r/NewTerms.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-16" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-16', event_id='97745', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5105</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97745">3DCoMPaT200: Language Grounded Large-Scale 3D Vision Dataset for Compositional Recognition</a></strong></h5>


                        <p class="text-muted">
                            Mahmoud Ahmed &middot; Xiang Li &middot; Arpit Prajapati &middot; Mohamed Elhoseiny
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Understanding objects in 3D at the part level is essential for humans and robots to navigate and interact with the environment. Current datasets for part-level 3D object understanding encompass a limited range of categories. For instance, the ShapeNet-Part and PartNet datasets only include 16, and 24 object categories respectively. The 3DCoMPaT dataset, specifically designed for compositional understanding of parts and materials, contains only 42 object categories. To foster richer and fine-grained part-level 3D understanding, we introduce 3DCoMPaT200, a large-scale dataset tailored for compositional understanding of object parts and materials, with 200 object categories with approximately 5 times larger object vocabulary compared to 3DCoMPaT and almost 4 times larger part categories. Concretely, 3DCoMPaT200 significantly expands upon 3DCoMPaT, featuring 1031 fine-grained part categories and 293 distinct material classes for compositional application to 3D object parts. Additionally, to address the complexities of compositional 3D modeling, we propose a novel task of Compositional Part Shape Retrieval using ULIP to provide a strong 3D foundational model for 3D Compositional Understanding. This method evaluates the model shape retrieval performance given one, three, or six parts described in text format. These results show that the model's performance improves with an increasing number of style compositions, highlighting the critical role of the compositional dataset. Such results underscore the dataset's effectiveness in enhancing models' capability to understand complex 3D shapes from a compositional perspective. Code and Data can be found here: https://github.com/3DCoMPaT200/3DCoMPaT200/</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-17" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-17', event_id='97778', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5106</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97778">Streaming Detection of Queried Event Start</a></strong></h5>


                        <p class="text-muted">
                            Cristobal Eyzaguirre &middot; Eric Tang &middot; Shyamal Buch &middot; Adrien Gaidon &middot; Jiajun Wu &middot; Juan Carlos Niebles
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Robotics, autonomous driving, augmented reality, and many embodied computer vision applications must quickly react to user-defined events unfolding in real time. We address this setting by proposing a novel task for multimodal video understanding---Streaming Detection of Queried Event Start (SDQES).The goal of SDQES is to identify the beginning of a complex event as described by a natural language query, with high accuracy and low latency. We introduce a new benchmark based on  the Ego4D dataset, as well as new task-specific metrics to study streaming multimodal detection of diverse events in an egocentric video setting.Inspired by parameter-efficient fine-tuning methods in NLP and for video tasks, we propose adapter-based baselines that enable image-to-video transfer learning, allowing for efficient online video modeling.We evaluate three vision-language backbones and three adapter architectures on both short-clip and untrimmed video settings.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-18" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-18', event_id='97801', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5107</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97801">Instruction Tuning Large Language Models to Understand Electronic Health Records</a></strong></h5>


                        <p class="text-muted">
                            Zhenbang Wu &middot; Anant Dadu &middot; Michael Nalls &middot; Faraz Faghri &middot; Jimeng Sun
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) have demonstrated remarkable capabilities in solving diverse tasks following human instructions. However, it is challenging to develop a conversational AI assistant for electronic medical health (EHR) data because (1) there is no large-scale instruction-following dataset and (2) existing model architectures are ineffective for handling complex and heterogeneous EHR data.Our paper introduces MIMIC-Instr, a dataset comprising over 400K open-ended instruction-following data based on the MIMIC-IV EHR database. This dataset covers a broad range of topics and can be used to instruction-tune general-purpose LLMs for diverse clinical use cases. Additionally, we propose Llemr, a general framework designed to empower LLMs to process and interpret EHRs with complex data schemas effectively. Llemr exhibits competitive capabilities in answering diverse patient-related based on EHR data.Furthermore, our evaluations on clinical predictive modeling benchmarks show that the fine-tuned Llemr can match the performance of state-of-the-art (SOTA) baselines with curated features.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-19" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-19', event_id='97807', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5108</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97807">TorchSpatial: A Location Encoding Framework and Benchmark for Spatial Representation Learning</a></strong></h5>


                        <p class="text-muted">
                            Nemin Wu &middot; Qian Cao &middot; Zhangyu Wang &middot; Zeping Liu &middot; Yanlin Qi &middot; Jielu Zhang &middot; Joshua Ni &middot; X. Yao &middot; Hongxu Ma &middot; Lan Mu &middot; Stefano Ermon &middot; Tanuja Ganu &middot; Akshay Nambi &middot; Ni Lao &middot; Gengchen Mai
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Spatial representation learning (SRL) aims at learning general-purpose neural network representations from various types of spatial data (e.g., points, polylines, polygons, networks, images, etc.) in their native formats. Learning good spatial representations is a fundamental problem for various downstream applications such as species distribution modeling, weather forecasting, trajectory generation, geographic question answering, etc. Even though SRL has become the foundation of almost all geospatial artificial intelligence (GeoAI) research, we have not yet seen significant efforts to develop an extensive deep learning framework and benchmark to support SRL model development and evaluation. To fill this gap, we propose TorchSpatial, a learning framework and benchmark for location (point) encoding, which is one of the most fundamental data types of spatial representation learning. TorchSpatial contains three key components: 1) a unified location encoding framework that consolidates 15 commonly recognized location encoders, ensuring scalability and reproducibility of the implementations; 2) the LocBench benchmark tasks encompassing 7 geo-aware image classification and 4 geo-aware image regression datasets; 3) a comprehensive suite of evaluation metrics to quantify geo-aware models overall performance as well as their geographic bias, with a novel Geo-Bias Score metric. Finally, we provide a detailed analysis and insights into the model performance and geographic bias of different location encoders. We believe TorchSpatial will foster future advancement of spatial representation learning and spatial fairness in GeoAI research. The TorchSpatial model framework, LocBench, and Geo-Bias Score evaluation framework are available at https://github.com/seai-lab/TorchSpatial.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-20" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-20', event_id='97814', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5109</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97814">DataComp-LM: In search of the next generation of training sets for language models</a></strong></h5>


                        <p class="text-muted">
                            Amro Abbas &middot; Alon Albalak &middot; Kushal Arora &middot; Hritik Bansal &middot; Yonatan Bitton &middot; Yair Carmon &middot; Khyathi Chandu &middot; Mayee Chen &middot; Giannis Daras &middot; Achal Dave &middot; Alex Dimakis &middot; Alaaeldin El-Nouby &middot; Fartash Faghri &middot; Alex Fang &middot; Samir Yitzhak Gadre &middot; Josh Gardner &middot; Saurabh Garg &middot; Dhruba Ghosh &middot; Aaron Gokaslan &middot; Dirk Groeneveld &middot; Etash Guha &middot; Suchin Gururangan &middot; Reinhard Heckel &middot; Cheng-Yu Hsieh &middot; Gabriel Ilharco &middot; Maor Ivgi &middot; Jenia Jitsev &middot; Matt Jordan &middot; Sham Kakade &middot; Sedrick Scott Keh &middot; Maciej Kilian &middot; Pang Wei Koh &middot; Thomas Kollar &middot; Jeffrey Li &middot; Kyle Lo &middot; Kalyani Marathe &middot; Jean Mercat &middot; Niklas Muennighoff &middot; Marianna Nezhurina &middot; Thao Nguyen &middot; Sewoong Oh &middot; Hadi Pouransari &middot; Sarah Pratt &middot; Sunny Sanyal &middot; Ludwig Schmidt &middot; Vaishaal Shankar &middot; Rulin Shao &middot; Georgios Smyrnis &middot; Luca Soldaini &middot; Shuran Song &middot; Alexander Toshev &middot; Igor Vasiljevic &middot; Stephanie Wang &middot; Mitchell Wortsman &middot; Rui Xin &middot; Luke Zettlemoyer &middot; Hanlin Zhang &middot; Jieyu Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce DataComp for Language Models, a testbed for controlled dataset experiments with the goal of improving language models.As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations.Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing atmodel scales ranging from 412M to 7B parameters.As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set.The resulting dataset, DCLM-Baseline, enables training a 7B parameter language model from scratch to 63% 5-shot accuracy on MMLU with 2T training tokens.Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6 percentage point improvement on MMLU while being trained with half the compute.Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-21" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-21', event_id='97823', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5110</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97823">MassSpecGym: A benchmark for the discovery and identification of molecules</a></strong></h5>


                        <p class="text-muted">
                            Roman Bushuiev &middot; Anton Bushuiev &middot; Niek de Jonge &middot; Adamo Young &middot; Fleming Kretschmer &middot; Raman Samusevich &middot; Janne Heirman &middot; Fei Wang &middot; Luke Zhang &middot; Kai Dhrkop &middot; Marcus Ludwig &middot; Nils Haupt &middot; Apurva Kalia &middot; Corinna Brungs &middot; Robin Schmid &middot; Russell Greiner &middot; Bo Wang &middot; David Wishart &middot; Liping Liu &middot; Juho Rousu &middot; Wout Bittremieux &middot; Hannes Rost &middot; Tytus Mak &middot; Soha Hassoun &middot; Florian Huber &middot; Justin J.J. van der Hooft &middot; Michael Stravs &middot; Sebastian Bcker &middot; Josef Sivic &middot; Tom Pluskal
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The discovery and identification of molecules in biological and environmental samples is crucial for advancing biomedical and chemical sciences. Tandem mass spectrometry (MS/MS) is the leading technique for high-throughput elucidation of molecular structures. However, decoding a molecular structure from its mass spectrum is exceptionally challenging, even when performed by human experts. As a result, the vast majority of acquired MS/MS spectra remain uninterpreted, thereby limiting our understanding of the underlying (bio)chemical processes. Despite decades of progress in machine learning applications for predicting molecular structures from MS/MS spectra, the development of new methods is severely hindered by the lack of standard datasets and evaluation protocols. To address this problem, we propose MassSpecGym -- the first comprehensive benchmark for the discovery and identification of molecules from MS/MS data. Our benchmark comprises the largest publicly available collection of high-quality MS/MS spectra and defines three MS/MS annotation challenges: \textit{de novo} molecular structure generation, molecule retrieval, and spectrum simulation. It includes new evaluation metrics and a generalization-demanding data split, therefore standardizing the MS/MS annotation tasks and rendering the problem accessible to the broad machine learning community. MassSpecGym is publicly available at \url{https://github.com/pluskal-lab/MassSpecGym}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-22" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-22', event_id='97659', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5200</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97659">RelBench: A Benchmark for Deep Learning on Relational Databases</a></strong></h5>


                        <p class="text-muted">
                            Joshua Robinson &middot; Rishabh Ranjan &middot; Weihua Hu &middot; Kexin Huang &middot; Jiaqi Han &middot; Alejandro Dobles &middot; Matthias Fey &middot; Jan Eric Lenssen &middot; Yiwen Yuan &middot; Zecheng Zhang &middot; Xinwei He &middot; Jure Leskovec
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present RelBench, a public benchmark for solving predictive tasks in relational databases with deep learning.  RelBench provides databases and tasks spanning diverse domains, scales, and database dimensions, and is intended to be a foundational infrastructure for future research in this direction. We use RelBench to conduct the first comprehensive empirical study of graph neural network (GNN) based predictive models on relational data, as recently proposed by Fey et al. 2024.  End-to-end learned GNNs are capable fully exploiting the predictive signal encoded in links between entities, marking a significant shift away from the dominant paradigm of manual feature engineering combined with tabular machine learning. To thoroughly evaluate GNNs against the prior gold-standard we conduct a user study, where an experienced data scientist manually engineers features for each task. In this study, GNNs learn better models whilst reducing human work needed by more than an order of magnitude. This result demonstrates the power of GNNs for solving predictive tasks in relational databases, opening up new research opportunities.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-23" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-23', event_id='95200', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5201</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95200">Virtual Scanning: Unsupervised Non-line-of-sight Imaging from Irregularly Undersampled Transients</a></strong></h5>


                        <p class="text-muted">
                            Xingyu Cui &middot; Huanjing Yue &middot; Song Li &middot; Xiangjun Yin &middot; Yusen Hou &middot; Yun Meng &middot; Kai Zou &middot; Xiaolong Hu &middot; Jingyu Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Non-line-of-sight (NLOS) imaging allows for seeing hidden scenes around corners through active sensing.Most previous algorithms for NLOS reconstruction require dense transients acquired through regular scans over a large relay surface, which limits their applicability in realistic scenarios with irregular relay surfaces.In this paper, we propose an unsupervised learning-based framework for NLOS imaging from irregularly undersampled transients~(IUT).Our method learns implicit priors from noisy irregularly undersampled transients without requiring paired data, which is difficult and expensive to acquire and align. To overcome the ambiguity of the measurement consistency constraint in inferring the albedo volume, we design a virtual scanning process that enables the network to learn within both range and null spaces for high-quality reconstruction.We devise a physics-guided SURE-based denoiser to enhance robustness to ubiquitous noise in low-photon imaging conditions. Extensive experiments on both simulated and real-world data validate the performance and generalization of our method.Compared with the state-of-the-art (SOTA) method, our method achieves higher fidelity, greater robustness, and remarkably faster inference times by orders of magnitude.The code and model are available at https://github.com/XingyuCuii/Virtual-Scanning-NLOS.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-24" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-24', event_id='97656', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5202</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97656">WindsorML - High-Fidelity Computational Fluid Dynamics Dataset For Automotive Aerodynamics</a></strong></h5>


                        <p class="text-muted">
                            Neil Ashton &middot; Jordan Angel &middot; Aditya Ghate &middot; Gaetan Kenway &middot; Man Long Wong &middot; Cetin Kiris &middot; Astrid Walle &middot; Danielle Maddix &middot; Gary Page
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper presents a new open-source high-fidelity dataset for Machine Learning (ML) containing 355 geometric variants of the Windsor body, to help the development and testing of ML surrogate models for external automotive aerodynamics. Each Computational Fluid Dynamics (CFD) simulation was run with a GPU-native high-fidelity Wall-Modeled Large-Eddy Simulations (WMLES) using a Cartesian immersed-boundary method using more than 280M cells to ensure the greatest possible accuracy. The dataset contains geometry variants that exhibits a wide range of flow characteristics that are representative of those observed on road-cars. The dataset itself contains the 3D time-averaged volume \&amp; boundary data as well as the geometry and force \&amp; moment coefficients. This paper discusses the validation of the underlying CFD methods as well as contents and structure of the dataset. To the authors knowledge, this represents the first, large-scale high-fidelity CFD dataset for the Windsor body with a permissive open-source license (CC-BY-SA).</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-25" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-25', event_id='97654', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5203</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97654">Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition</a></strong></h5>


                        <p class="text-muted">
                            Edoardo Debenedetti &middot; Javier Rando &middot; Daniel Paleka &middot; Silaghi Florin &middot; Dragos Albastroiu &middot; Niv Cohen &middot; Yuval Lemberg &middot; Reshmi Ghosh &middot; Rui Wen &middot; Ahmed Salem &middot; Giovanni Cherubin &middot; Santiago Zanella-Beguelin &middot; Robin Schmid &middot; Victor Klemm &middot; Takahiro Miki &middot; Chenhao Li &middot; Stefan Kraft &middot; Mario Fritz &middot; Florian Tramer &middot; Sahar Abdelnabi &middot; Lea Schnherr
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language model systems face important security risks from maliciously crafted messages that aim to overwrite the system's original instructions or leak private data. To study this problem, we organized a capture-the-flag competition at IEEE SaTML 2024, where the flag is a secret string in the LLM system prompt. The competition was organized in two phases. In the first phase, teams developed defenses to prevent the model from leaking the secret. During the second phase, teams were challenged to extract the secrets hidden for defenses proposed by the other teams. This report summarizes the main insights from the competition. Notably, we found that all defenses were bypassed at least once, highlighting the difficulty of designing a successful defense and the necessity for additional research to protect LLM systems. To foster future research in this direction, we compiled a dataset with over 137k multi-turn attack chats and open-sourced the platform.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-26" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-26', event_id='97641', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5204</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97641">SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset</a></strong></h5>


                        <p class="text-muted">
                            Juntao Dai &middot; Tianle Chen &middot; Xuyao Wang &middot; Ziran Yang &middot; Taiye Chen &middot; Jiaming Ji &middot; Yaodong Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>To mitigate the risk of harmful outputs from large vision models (LVMs), we introduce the <em>SafeSora</em> dataset to promote research on aligning text-to-video generation with human values. This dataset encompasses human preferences in text-to-video generation tasks along two primary dimensions: helpfulness and harmlessness. To capture in-depth human preferences and facilitate structured reasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and harmlessness into 12 sub-categories, serving as the basis for pilot annotations. The <em>SafeSora</em> dataset includes 14,711 unique prompts, 57,333 unique videos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations labeled by humans. We further demonstrate the utility of the <em>SafeSora</em> dataset through several applications, including training the text-video moderation model and aligning LVMs with human preference by fine-tuning a prompt augmentation module or the diffusion model. These applications highlight its potential as the foundation for text-to-video alignment research, such as human preference modeling and the development and validation of alignment algorithms. Our project is available at https://sites.google.com/view/safe-sora.Warning: this paper contains example data that may be offensive or harmful.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-27" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-27', event_id='97636', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5205</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97636">Benchmark Repositories for Better Benchmarking</a></strong></h5>


                        <p class="text-muted">
                            Rachel Longjohn &middot; Markelle Kelly &middot; Sameer Singh &middot; Padhraic Smyth
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In machine learning research, it is common to evaluate algorithms via their performance on standard benchmark datasets. While a growing body of work establishes guidelines for---and levies criticisms at---data and benchmarking practices in machine learning, comparatively less attention has been paid to the repositories where these datasets are stored, documented, and shared. In this paper, we analyze the landscape of these <em>benchmark repositories</em> and the role they can play in improving benchmarking. This role includes addressing issues with both datasets themselves (e.g., representational harms, construct validity) and the manner in which evaluation is carried out using such datasets (e.g., overemphasis on a few datasets and metrics, lack of reproducibility). To this end, we identify and discuss a set of considerations surrounding the design and use of benchmark repositories, with a focus on improving benchmarking practices in machine learning.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-28" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-28', event_id='97614', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5206</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97614">CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models</a></strong></h5>


                        <p class="text-muted">
                            Peng Xia &middot; Ze Chen &middot; Juanxi Tian &middot; Yangrui Gong &middot; Ruibo Hou &middot; Yue Xu &middot; Zhenbang Wu &middot; Zhiyuan Fan &middot; Yiyang Zhou &middot; Kangyu Zhu &middot; Wenhao Zheng &middot; Zhaoyang Wang &middot; Xiao Wang &middot; Xuchao Zhang &middot; Chetan Bansal &middot; Marc Niethammer &middot; Junzhou Huang &middot; Hongtu Zhu &middot; Yun Li &middot; Jimeng Sun &middot; Zongyuan Ge &middot; Gang Li &middot; James Zou &middot; Huaxiu Yao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to comprehensively evaluate the Trustworthiness of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly release our benchmark and code in https://github.com/richard-peng-xia/CARES.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-29" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-29', event_id='97605', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5207</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97605">WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark</a></strong></h5>


                        <p class="text-muted">
                            Chunhui Zhang &middot; Li Liu &middot; Guanjie Huang &middot; Hao Wen &middot; XI ZHOU &middot; Yanfeng Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Underwater object tracking (UOT) is a foundational task for identifying and tracing submerged entities in underwater video sequences. However, current UOT datasets suffer from limitations in scale, diversity of target categories and scenarios covered, hindering the training and evaluation of modern tracking algorithms. To bridge this gap, we take the first step and introduce WebUOT-1M, \ie, the largest public UOT benchmark to date, sourced from complex and realistic underwater environments. It comprises 1.1 million frames across 1,500 video clips filtered from 408 target categories, largely surpassing previous UOT datasets, \eg, UVOT400. Through meticulous manual annotation and verification, we provide high-quality bounding boxes for underwater targets. Additionally, WebUOT-1M includes language prompts for video sequences, expanding its application areas, \eg, underwater vision-language tracking. Most existing trackers are tailored for open-air environments, leading to performance degradation when applied to UOT due to domain gaps. Retraining and fine-tuning these trackers are challenging due to sample imbalances and limited real-world underwater datasets. To tackle these challenges, we propose a novel omni-knowledge distillation framework based on WebUOT-1M, incorporating various strategies to guide the learning of the student Transformer. To the best of our knowledge, this framework is the first to effectively transfer open-air domain knowledge to the UOT model through knowledge distillation, as demonstrated by results on both existing UOT datasets and the newly proposed WebUOT-1M. Furthermore, we comprehensively evaluate WebUOT-1M using 30 deep trackers, showcasing its value as a benchmark for UOT research by presenting new challenges and opportunities for future studies. The complete dataset, codes and tracking results, will be made publicly available at \href{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}{\color{magenta}{here}}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-30" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-30', event_id='97596', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5208</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97596">PEACE: A Dataset of Pharmaceutical Care for Cancer Pain Analgesia Evaluation and Medication Decision</a></strong></h5>


                        <p class="text-muted">
                            Yutao Dou &middot; Huimin Yu &middot; Wei Li &middot; Jingyang Li &middot; Fei Xia &middot; Jian Xiao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Over half of cancer patients experience long-term pain management challenges. Recently, interest has grown in systems for cancer pain treatment effectiveness assessment (TEA) and medication recommendation (MR) to optimize pharmacological care. These systems aim to improve treatment effectiveness by recommending personalized medication plans based on comprehensive patient information. Despite progress, current systems lack multidisciplinary treatment (MDT) team assessments of treatment and the patient's perception of medication, crucial for effective cancer pain management. Moreover, managing cancer pain medication requires multiple adjustments to the treatment plan based on the patient's evolving condition, a detail often missing in existing datasets. To tackle these issues, we designed the PEACE dataset specifically for cancer pain medication research. It includes detailed pharmacological care records for over 38,000 patients, covering demographics, clinical examination, treatment outcomes, medication plans, and patient self-perceptions. Unlike existing datasets, PEACE records not only long-term and multiple follow-ups both inside and outside hospitals but also includes patients' self-assessments of medication effects and the impact on their lives. We conducted a proof-of-concept study with 11 machine learning algorithms on the PEACE dataset for the TEA (classification task) and MR (regression task). These experiments provide valuable insights into the potential of the PEACE dataset for advancing personalized cancer pain management. The dataset is accessible at: [https://github.com/YTYTYD/PEACE].</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-31" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-31', event_id='97590', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5209</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97590">EgoSim: An Egocentric Multi-view Simulator for Body-worn Cameras during Human Motion</a></strong></h5>


                        <p class="text-muted">
                            Dominik Hollidt &middot; Paul Streli &middot; Jiaxi Jiang &middot; Yasaman Haghighi &middot; Changlin Qian &middot; Xintong Liu &middot; Christian Holz
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Research on egocentric tasks in computer vision has mostly focused on head-mounted cameras, such as fisheye cameras or those integrated into immersive headsets.We argue that the increasing miniaturization of optical sensors will lead to the prolific integration of cameras into body-worn devices at various locations.This will bring fresh perspectives to established tasks in computer vision and benefit key areas such as human motion tracking, body pose estimation, or action recognition---particularly for the lower body, which is typically occluded.In this paper, we introduce EgoSim, a novel simulator of body-worn cameras that generates realistic egocentric renderings from multiple perspectives across a wearer's body.A key feature of EgoSim is its use of real motion capture data and a physical simulation of camera attachments to render motion artifacts, which especially affect arm- or leg-worn cameras.We also present MultiEgoView, a dataset of egocentric footage from six egocentric body-worn cameras and 3D body poses during several activities:77\,hours of data are based on AMASS motion sequences in two virtual environments and $\sim$5\,hours are from real-world motion data from 13 participants using six GoPro cameras together with an Xsens mo-cap suit.We show EgoSim's effectiveness by training an end-to-end video-only pose estimation network.Analyzing its domain gap showed that our dataset and simulator substantially aid training for inference on real-world data.EgoSim code and MultiEgoView dataset:</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-32" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-32', event_id='97584', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5210</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97584">Re-assembling the past: The RePAIR dataset and benchmark for real world 2D and 3D puzzle solving</a></strong></h5>


                        <p class="text-muted">
                            Theodore Tsesmelis &middot; Luca Palmieri &middot; Marina Khoroshiltseva &middot; Adeela Islam &middot; Gur Elkin &middot; Ofir I Shahar &middot; Gianluca Scarpellini &middot; Stefano Fiorini &middot; Yaniv Ohayon &middot; Nadav Alali &middot; Sinem Aslan &middot; Pietro Morerio &middot; Sebastiano Vascon &middot; Elena gravina &middot; Maria Napolitano &middot; Giuseppe Scarpati &middot; Gabriel zuchtriegel &middot; Alexandra Sphler &middot; Michel Fuchs &middot; Stuart James &middot; Ohad Ben-Shahar &middot; Marcello Pelillo &middot; Alessio Del Bue
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper proposes the RePAIR dataset that represents a challenging benchmark to test modern computational and data driven methods for puzzle-solving and reassembly tasks. Our dataset has unique properties that are uncommon to current benchmarks for 2D and 3D puzzle solving. The fragments and fractures are realistic, caused by a collapse of a fresco during a World War II bombing at the Pompeii archaeological park. The fragments are also eroded and have missing pieces with irregular shapes and different dimensions, challenging further the reassembly algorithms. The dataset is multi-modal providing  hi-res images with characteristic pictorial elements, detailed 3D scans of the fragments and meta-data annotated by the archaeologists. Ground truth has been generated through several years of unceasing fieldwork, including the excavation and cleaning of each fragment, followed by manual puzzle solving by archaeologists of a subset of 1,000 pieces among the 16,000 available. After digitizing all the fragments in 3D, a benchmark was prepared to challenge current reassembly and puzzle-solving methods that often solve more simplistic synthetic scenarios. The tested baselines show that there clearly exists a gap to fill in solving this computationally complex problem.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-33" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-33', event_id='97581', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5211</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97581">UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling</a></strong></h5>


                        <p class="text-muted">
                            Haider Al-Tahan &middot; Quentin Garrido &middot; Randall Balestriero &middot; Diane Bouchacourt &middot; Caner Hazirbas &middot; Mark Ibrahim
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks,researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress.To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. We showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. We find that while scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations.  Surprisingly, we also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, we find that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, we also offer guidance on selecting a suitable VLM for a given application. Finally, we release an easy-to-run UniBench code-base with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-34" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-34', event_id='97433', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5300</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97433">On the Effects of Data Scale on Computer Control Agents</a></strong></h5>


                        <p class="text-muted">
                            WEI Li &middot; William Bishop &middot; Alice Li &middot; Christopher Rawles &middot; Folawiyo Campbell-Ajala &middot; Divya Tyamagundlu &middot; Oriana Riva
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Autonomous agents that control computer interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless fine-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether fine-tuning alone is a viable approach for building real-world computer control agents. %In particularly, we investigate how performance measured on both high and low-level tasks in domain and out of domain scales as more training data is collected. To this end we collect and release a new dataset, AndroidControl, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each AndroidControl task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, AndroidControl is the most diverse computer control dataset to date, including 15,283 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we find that when tested in domain fine-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data.  Out of domain, performance scales significantly more slowly and suggests that in particular for high-level tasks, fine-tuning on more data alone may be insufficient for achieving robust out-of-domain performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-35" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-35', event_id='97446', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5301</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97446">LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment</a></strong></h5>


                        <p class="text-muted">
                            Jinyang Guo &middot; Ge Yang &middot; Changyi He &middot; Jianyu Wu &middot; Yifu Ding &middot; Aishan Liu &middot; Haotong Qin &middot; Pengliang Ji &middot; Xianglong Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Although large language models (LLMs) have demonstrated their strong intelligence ability, the high demand for computation and storage hinders their practical application. To this end, many model compression techniques are proposed to increase the efficiency of LLMs. However, current researches only validate their methods on limited models, datasets, metrics, etc, and still lack a comprehensive evaluation under more general scenarios. So it is still a question of which model compression approach we should use under a specific case. To mitigate this gap, we present the Large Language Model Compression Benchmark (LLMCBench), a rigorously designed benchmark with an in-depth analysis for LLM compression algorithms. We first analyze the actual model production requirements and carefully design evaluation tracks and metrics. Then, we conduct extensive experiments and comparison using multiple mainstream LLM compression approaches. Finally, we perform an in-depth analysis based on the evaluation and provide useful insight for LLM compression design. We hope our LLMCBench can contribute insightful suggestions for LLM compression algorithm design and serve as a foundation for future research.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-36" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-36', event_id='97461', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5302</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97461">Personalized Instance-based Navigation Toward User-Specific Objects in Realistic Environments</a></strong></h5>


                        <p class="text-muted">
                            Luca Barsellotti &middot; Roberto Bigazzi &middot; Marcella Cornia &middot; Lorenzo Baraldi &middot; Rita Cucchiara
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In the last years, the research interest in visual navigation towards objects in indoor environments has grown significantly. This growth can be attributed to the recent availability of large navigation datasets in photo-realistic simulated environments, like Gibson and Matterport3D. However, the navigation tasks supported by these datasets are often restricted to the objects present in the environment at acquisition time. Also, they fail to account for the realistic scenario in which the target object is a user-specific instance that can be easily confused with similar objects and may be found in multiple locations within the environment. To address these limitations, we propose a new task denominated Personalized Instance-based Navigation (PIN), in which an embodied agent is tasked with locating and reaching a specific personal object by distinguishing it among multiple instances of the same category. The task is accompanied by PInNED, a dedicated new dataset composed of photo-realistic scenes augmented with additional 3D objects. In each episode, the target object is presented to the agent using two modalities: a set of visual reference images on a neutral background and manually annotated textual descriptions. Through comprehensive evaluations and analyses, we showcase the challenges of the PIN task as well as the performance and shortcomings of currently available methods designed for object-driven navigation, considering modular and end-to-end agents.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-37" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-37', event_id='97480', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5303</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97480">MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs</a></strong></h5>


                        <p class="text-muted">
                            Ziyu Liu &middot; Tao Chu &middot; Yuhang Zang &middot; Xilin Wei &middot; Xiaoyi Dong &middot; Pan Zhang &middot; Zijian Liang &middot; Yuanjun Xiong &middot; Dahua Lin &middot; Yu Qiao &middot; Jiaqi Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Generating natural and meaningful responses to communicate with multi-modal human inputs is a fundamental capability of Large Vision-Language Models (LVLMs). While current open-source LVLMs demonstrate promising performance in simplified scenarios such as single-turn single-image input, they fall short in real-world conversation scenarios such as following instructions in a long context history with multi-turn and multi-images. Existing LVLM benchmarks primarily focus on single-choice questions or short-form responses, which do not adequately assess the capabilities of LVLMs in real-world human-AI interaction applications. Therefore, we introduce MMDU, a comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning dataset, designed to evaluate and improve LVLMs' abilities in multi-turn and multi-image conversations. We employ the clustering algorithm to find the relevant images and textual descriptions from the open-source Wikipedia and construct the question-answer pairs by human annotators with the assistance of the GPT-4o model.MMDU has a maximum of 18k image+text tokens, 20 images, and 27 turns, which is at least 5x longer than previous benchmarks and poses challenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs using MMDU reveals that open-source LVLMs lag behind closed-source counterparts due to limited conversational instruction tuning data.We demonstrate that fine-tuning open-source LVLMs on MMDU-45k significantly address this gap, generating longer and more accurate conversations, and improving scores on MMDU and existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA: +1.2%). Our contributions pave the way for bridging the gap between current LVLM models and real-world application demands. The links to MMDU, and MMDU-45k are available in the supplementary material.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-38" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-38', event_id='97481', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5304</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97481">ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction</a></strong></h5>


                        <p class="text-muted">
                            Juan Nathaniel &middot; Yongquan Qu &middot; Tung Nguyen &middot; Sungduk Yu &middot; Julius Busecke &middot; Aditya Grover &middot; Pierre Gentine
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster preparedness and robust decision making amidst climate change. Yet, forecasting beyond the weather timescale is challenging because it deals with problems other than initial conditions, including boundary interaction, butterfly effect, and our inherent lack of physical understanding. At present, existing benchmarks tend to have shorter forecasting range of up-to 15 days, do not include a wide range of operational baselines, and lack physics-based constraints for explainability. Thus, we propose ChaosBench, a challenging benchmark to extend the predictability range of data-driven weather emulators to S2S timescale. First, ChaosBench is comprised of variables beyond the typical surface-atmospheric ERA5 to also include ocean, ice, and land reanalysis products that span over 45 years to allow for full Earth system emulation that respects boundary conditions. We also propose physics-based, in addition to deterministic and probabilistic metrics, to ensure a physically-consistent ensemble that accounts for butterfly effect. Furthermore, we evaluate on a diverse set of physics-based forecasts from four national weather agencies as baselines to our data-driven counterpart such as ClimaX, PanguWeather, GraphCast, and FourCastNetV2. Overall, we find methods originally developed for weather-scale applications fail on S2S task: their performance simply collapse to an unskilled climatology. Nonetheless, we outline and demonstrate several strategies that can potentially extend the predictability range of existing weather emulators, including the use of ensembles and robust control of error propagation. Our benchmark, datasets, and instructions are available at https://leap-stc.github.io/ChaosBench.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-39" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-39', event_id='97509', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5305</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97509">Rethinking the Evaluation of Out-of-Distribution Detection: A Sorites Paradox</a></strong></h5>


                        <p class="text-muted">
                            Xingming Long &middot; Jie Zhang &middot; Shiguang Shan &middot; Xilin Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Most existing out-of-distribution (OOD) detection benchmarks classify samples with novel labels as the OOD data. However, some marginal OOD samples actually have close semantic contents to the in-distribution (ID) sample, which makes determining the OOD sample a Sorites Paradox. In this paper, we construct a benchmark named Incremental Shift OOD (IS-OOD) to address the issue, in which we divide the test samples into subsets with different semantic and covariate shift degrees relative to the ID dataset. The data division is achieved through a shift measuring method based on our proposed Language Aligned Image feature Decomposition (LAID). Moreover, we construct a Synthetic Incremental Shift (Syn-IS) dataset that contains high-quality generated images with more diverse covariate contents to complement the IS-OOD benchmark. We evaluate current OOD detection methods on our benchmark and find several important insights: (1) The performance of most OOD detection methods significantly improves as the semantic shift increases; (2) Some methods like GradNorm may have different OOD detection mechanisms as they rely less on semantic shifts to make decisions; (3) Excessive covariate shifts in the image are also likely to be considered as OOD for some methods. Our code and data are released in https://github.com/qqwsad5/IS-OOD.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-40" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-40', event_id='97531', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5306</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97531">EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations</a></strong></h5>


                        <p class="text-muted">
                            Jia Li &middot; Ge Li &middot; Xuanming Zhang &middot; YunFei Zhao &middot; Yihong Dong &middot; Zhi Jin &middot; Binhua Li &middot; Fei Huang &middot; Yongbin Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>How to evaluate Large Language Models (LLMs) in code generation remains an open question. Many benchmarks have been proposed, but they have two limitations, i.e., data leakage and lack of domain-specific evaluation.The former hurts the fairness of benchmarks, and the latter hinders practitioners from selecting superior LLMs for specific programming domains.To address these two limitations, we propose a new benchmark - EvoCodeBench, which has the following advances: (1) Evolving data. EvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid data leakage. This paper releases the first version - EvoCodeBench-2403, containing 275 samples from 25 repositories.(2) A domain taxonomy and domain labels. Based on the statistics of open-source communities, we design a programming domain taxonomy consisting of 10 popular domains. Based on the taxonomy, we annotate each sample in EvoCodeBench with a domain label. EvoCodeBench provides a broad platform for domain-specific evaluations.(3) Domain-specific evaluations. Besides the Pass@k, we compute the Domain-Specific Improvement (DSI) and define LLMs' comfort and strange domains. These evaluations help practitioners select superior LLMs in specific domains and discover the shortcomings of existing LLMs.Besides, EvoCodeBench is collected by a rigorous pipeline and aligns with real-world repositories in multiple aspects (e.g., code distributions).We evaluate 8 popular LLMs (e.g., gpt-4, DeepSeek Coder, StarCoder 2) on EvoCodeBench and summarize some insights. EvoCodeBench reveals the actual abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is only 20.74%. Besides, we evaluate LLMs in different domains and discover their comfort and strange domains. For example, gpt-4 performs best in most domains but falls behind others in the Internet domain. StarCoder 2-15B unexpectedly performs well in the Database domain and even outperforms 33B LLMs. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-41" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-41', event_id='97534', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5307</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97534">A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data</a></strong></h5>


                        <p class="text-muted">
                            Andrej Tschalzev &middot; Sascha Marton &middot; Stefan Ldtke &middot; Christian Bartelt &middot; Heiner Stuckenschmidt
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Tabular data is prevalent in real-world machine learning applications, and new models for supervised learning of tabular data are frequently proposed. Comparative studies assessing performance differences typically have model-centered evaluation setups with overly standardized data preprocessing. This limits the external validity of these studies, as in real-world modeling pipelines, models are typically applied after dataset-specific preprocessing and feature engineering. We address this gap by proposing a data-centric evaluation framework. We select 10 relevant datasets from Kaggle competitions and implement expert-level preprocessing pipelines for each dataset. We conduct experiments with different preprocessing pipelines and hyperparameter optimization (HPO) regimes to quantify the impact of model selection, HPO, feature engineering, and test-time adaptation. Our main findings reveal: 1) After dataset-specific feature engineering, model rankings change considerably, performance differences decrease, and the importance of model selection reduces. 2) Recent models, despite their measurable progress, still significantly benefit from manual feature engineering. This holds true for both tree-based models and neural networks. 3) While tabular data is typically considered static, samples are often collected over time, and adapting to distribution shifts can be important even in supposedly static data. These insights suggest that research efforts should be directed toward a data-centric perspective, acknowledging that tabular data requires feature engineering and often exhibits temporal characteristics.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-42" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-42', event_id='97541', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5308</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97541">A New Multi-Source Light Detection Benchmark and Semi-Supervised Focal Light Detection</a></strong></h5>


                        <p class="text-muted">
                            Jae-Yong Baek &middot; Yong-Sang Yoo &middot; Seung-Hwan Bae
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper addresses a multi-source light detection (LD) problem from vehicles, traffic signals, and streetlights under driving scenarios. Albeit it is crucial for autonomous driving and night vision, this problem has not been yet focused on as much as other object detection (OD). One of the main reasons is the absence of a public available LD benchmark dataset. Therefore, we construct a new large LD dataset consisting of different light sources via heavy annotation. YouTube Driving Light Detection dataset (YDLD). Compared to the existing LD datasets, our dataset has much more images and box annotations for multi-source lights. We also provide rigorous statistical analysis and transfer learning comparison of other well-known detection benchmark datasets to prove the generality of our YDLD. For the recent object detectors, we achieve the extensive comparison results on YDLD. However, they tend to yield the low mAP scores due to the intrinsic challenges of LD caused by very tiny size and similar appearance. To resolve those, we design a novel lightness focal loss which penalizes miss-classified samples more and a lightness spatial attention prior by reflecting a global scene context. In addition, we develop a semi-supervised focal loss detection (SS-FLD) by embedding our light focal loss into the SSOD. We prove that our methods can consistently boost mAP to the variety of types of recent detectors on YDLD. We will open both YDLD and SS-FLD code at https://github.com/YDLD-dataset/YDLD.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-43" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-43', event_id='96288', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5309</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96288">Global Convergence in Training Large-Scale Transformers</a></strong></h5>


                        <p class="text-muted">
                            Cheng Gao &middot; Yuan Cao &middot; Zihao Li &middot; Yihan He &middot; Mengdi Wang &middot; Han Liu &middot; Jason Klusowski &middot; Jianqing Fan
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Despite the widespread success of Transformers across various domains, their optimization guarantees in large-scale model settings are not well-understood. This paper rigorously analyzes the convergence properties of gradient flow in training Transformers with weight decay regularization. First, we construct the mean-field limit of large-scale Transformers, showing that as the model width and depth go to infinity, gradient flow converges to the Wasserstein gradient flow, which is represented by a partial differential equation. Then, we demonstrate that the gradient flow reaches a global minimum consistent with the PDE solution when the weight decay regularization parameter is sufficiently small. Our analysis is based on a series of novel mean-field techniques that adapt to Transformers. Compared with existing tools for deep networks (Lu et al., 2020) that demand homogeneity and global Lipschitz smoothness, we utilize a refined analysis assuming only $\textit{partial homogeneity}$ and $\textit{local Lipschitz smoothness}$. These new techniques may be of independent interest.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-44" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-44', event_id='97571', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5310</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97571">HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection</a></strong></h5>


                        <p class="text-muted">
                            Yuxin Wang &middot; Duanyu Feng &middot; Yongfu Dai &middot; Zhengyu Chen &middot; Jimin Huang &middot; Sophia Ananiadou &middot; Qianqian Xie &middot; Hao Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Data serves as the fundamental foundation for advancing deep learning, particularly tabular data presented in a structured format, which is highly conducive to modeling. However, even in the era of LLM, obtaining tabular data from sensitive domains remains a challenge due to privacy or copyright concerns. Hence, exploring how to effectively use models like LLMs to generate realistic and privacy-preserving synthetic tabular data is emergent. In this paper, we take a step forward to explore LLMs for tabular data synthesis and privacy protection, by introducing a new framework HARMONIC for tabular data generation and evaluation. In our tabular data generation framework, unlike previous small-scale LLM-based methods that rely on continued pre-training, we explore the larger-scale LLMs with fine-tuning to generate tabular data and enhance privacy. Based on idea of the k-nearest neighbors algorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to discover inter-row relationships. Then, with fine-tuning, LLMs are trained to remember the format and connections of the data rather than the data itself, which reduces the risk of privacy leakage. In our evaluation framework, we develop specific privacy risk metrics for LLM synthetic data generation, as well as performance evaluation metrics for downstream LLM tasks. Our experiments find that this tabular data generation framework achieves equivalent performance to existing methods with better privacy, which also demonstrates our evaluation framework for the effectiveness of synthetic data and privacy risks in LLM scenarios.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-45" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-45', event_id='97572', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5311</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97572">Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs</a></strong></h5>


                        <p class="text-muted">
                            Sukmin Yun &middot; haokun lin &middot; Rusiru Thushara &middot; Mohammad Bhat &middot; Yongxin Wang &middot; zutao jiang &middot; Mingkai Deng &middot; Jinhong Wang &middot; Tianhua Tao &middot; Junbo Li &middot; Haonan Li &middot; Preslav Nakov &middot; Timothy Baldwin &middot; Zhengzhong Liu &middot; Eric Xing &middot; Xiaodan Liang &middot; Zhiqiang Shen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multimodal large language models (MLLMs) have shown impressive success across modalities such as image, video, and audio in a variety of understanding and generation tasks. However, current MLLMs are surprisingly poor at understanding webpage screenshots and generating their corresponding HTML code. To address this problem, we propose Web2Code, a benchmark consisting of a new large-scale webpage-to-code dataset for instruction tuning and an evaluation framework for the webpage understanding and HTML code translation abilities of MLLMs. For dataset construction, we leveraging pretrained LLMs to enhance existing webpage-to-code datasets as well as generate a diverse pool of new webpages rendered into images. Specifically, the inputs are webpage images and instructions, while the responses are the webpages HTML code. We further include diverse natural language QA pairs about the webpage content in the responses to enable more comprehensive understanding of the web content. To evaluate model performance in these tasks, we develop an evaluation framework for testing MLLMs abilities in webpage understanding and web-to-code generation. Extensive experiments show that our proposed dataset is beneficial not only to our proposed tasks but also in the general visual domain, while previous datasets result in worse performance. We hope our work will contribute to the development of general MLLMs suitable for web-based content generation and task automation. Our data and code are available at https://github.com/MBZUAI-LLM/web2code</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-46" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-46', event_id='97430', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5400</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97430">Paloma: A Benchmark for Evaluating Language Model Fit</a></strong></h5>


                        <p class="text-muted">
                            Ian Magnusson &middot; Akshita Bhagia &middot; Valentin Hofmann &middot; Luca Soldaini &middot; Ananya Harsh Jha &middot; Oyvind Tafjord &middot; Dustin Schwenk &middot; Evan Walsh &middot; Yanai Elazar &middot; Kyle Lo &middot; Dirk Groeneveld &middot; Iz Beltagy &middot; Hannaneh Hajishirzi &middot; Noah Smith &middot; Kyle Richardson &middot; Jesse Dodge
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Evaluations of language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domainsvarying distributions of language. We introduce Perplexity Analysis for Language Model Assessment (Paloma), a benchmark to measure LM fit to 546 English and code domains, instead of assuming perplexity on one distribution extrapolates to others. We include two new datasets of the top 100 subreddits (e.g., r/depression on Reddit) and programming languages (e.g., Java on GitHub), both sources common in contemporary LMs. With our benchmark, we release 6 baseline 1B LMs carefully controlled to provide fair comparisons about which pretraining corpus is best and code for others to apply those controls to their own experiments. Our case studies demonstrate how the fine-grained results from Paloma surface findings such as that models pretrained without data beyond Common Crawl exhibit anomalous gaps in LM fit to many domains or that loss is dominated by the most frequently occurring strings in the vocabulary.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-47" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-47', event_id='96279', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5401</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96279">Efficient Lifelong Model Evaluation in an Era of Rapid Progress</a></strong></h5>


                        <p class="text-muted">
                            Ameya Prabhu &middot; Vishaal Udandarao &middot; Philip Torr &middot; Matthias Bethge &middot; Adel Bibi &middot; Samuel Albanie
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling \textit{ever-expanding} large-scale benchmarks called \textit{Lifelong Benchmarks}. As exemplars of our approach, we create \textit{Lifelong-CIFAR10} and \textit{Lifelong-ImageNet}, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: \textit{Sort \& Search (S\&S)}, which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking. Extensive empirical evaluations across $\sim$31,000 models demonstrate that \textit{S\&S} achieves highly-efficient approximate accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours ($\sim$1000x reduction) on a single A100 GPU, with low approximation error. As such, lifelong benchmarks offer a robust, practical solution to the ``benchmark exhaustion'' problem.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-48" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-48', event_id='96130', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5402</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96130">Fair Kernel K-Means: from Single Kernel to Multiple Kernel</a></strong></h5>


                        <p class="text-muted">
                            Peng Zhou &middot; Rongwen Li &middot; Liang Du
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Kernel k-means has been widely studied in machine learning. However, existing kernel k-means methods often ignore the \textit{fairness} issue, which may cause discrimination. To address this issue, in this paper, we propose a novel Fair Kernel K-Means (FKKM) framework. In this framework, we first propose a new fairness regularization term that can lead to a fair partition of data. The carefully designed fairness regularization term has a similar form to the kernel k-means which can be seamlessly integrated into the kernel k-means framework. Then, we extend this method to the multiple kernel setting, leading to a Fair Multiple Kernel K-Means (FMKKM) method. We also provide some theoretical analysis of the generalization error bound, and based on this bound we give a strategy to set the hyper-parameter, which makes the proposed methods easy to use. At last, we conduct extensive experiments on both the single kernel and multiple kernel settings to compare the proposed methods with state-of-the-art methods to demonstrate their effectiveness.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-49" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-49', event_id='95868', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5403</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95868">Achievable Fairness on Your Data With Utility Guarantees</a></strong></h5>


                        <p class="text-muted">
                            Muhammad Faaiz Taufiq &middot; Jean-Francois Ton &middot; Yang Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In machine learning fairness, training models that minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off inherently depends on dataset characteristics such as dataset imbalances or biases and therefore, using a uniform fairness requirement across diverse datasets remains questionable. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Crucially, we introduce a novel methodology for quantifying uncertainty in our estimates, thereby providing practitioners with a robust framework for auditing model fairness while avoiding false conclusions due to estimation errors. Our experiments spanning tabular (e.g., Adult), image (CelebA), and language (Jigsaw) datasets underscore that our approach not only reliably quantifies the optimum achievable trade-offs across various data modalities but also helps detect suboptimality in SOTA fairness methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-50" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-50', event_id='95133', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5404</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95133">A Closer Look at AUROC and AUPRC under Class Imbalance</a></strong></h5>


                        <p class="text-muted">
                            Matthew McDermott &middot; Haoran Zhang &middot; Lasse Hansen &middot; Giovanni Angelotti &middot; Jack Gallifant
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In machine learning (ML), a widespread claim is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for tasks with class imbalance. This paper refutes this notion on two fronts. First, we theoretically characterize the behavior of AUROC and AUPRC in the presence of model mistakes, establishing clearly that AUPRC is not generally superior in cases of class imbalance. We further show that AUPRC can be a harmful metric as it can unduly favor model improvements in subpopulations with more frequent positive labels, heightening algorithmic disparities. Next, we empirically support our theory using experiments on both semi-synthetic and real-world fairness datasets. Prompted by these insights, we conduct a review of over 1.5 million scientific papers to understand the origin of this invalid claim, finding that it is often made without citation, misattributed to papers that do not argue this point, and aggressively over-generalized from source arguments. Our findings represent a dual contribution: a significant technical advancement in understanding the relationship between AUROC and AUPRC and a stark warning about unchecked assumptions in the ML community.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-51" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-51', event_id='94521', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5405</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94521">Fairness in Social Influence Maximization via Optimal Transport</a></strong></h5>


                        <p class="text-muted">
                            Shubham Chowdhary &middot; Giulia De Pasquale &middot; Nicolas Lanzetti &middot; Ana-Andreea Stoica &middot; Florian Dorfler
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We study fairness in social influence maximization, whereby one seeks to selectseeds that spread a given information throughout a network, ensuring balancedoutreach among different communities (e.g. demographic groups). In the literature,fairness is often quantified in terms of the expected outreach within individualcommunities. In this paper, we demonstrate that such fairness metrics can bemisleading since they overlook the stochastic nature of information diffusionprocesses. When information diffusion occurs in a probabilistic manner, multipleoutreach scenarios can occur. As such, outcomes such as In 50% of the cases, noone in group 1 gets the information, while everyone in group 2 does, and in theother 50%, it is the opposite, which always results in largely unfair outcomes,are classified as fair by a variety of fairness metrics in the literature. We tacklethis problem by designing a new fairness metric, mutual fairness, that capturesvariability in outreach through optimal transport theory. We propose a new seed-selection algorithm that optimizes both outreach and mutual fairness, and we showits efficacy on several real datasets. We find that our algorithm increases fairnesswith only a minor decrease (and at times, even an increase) in efficiency.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-52" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-52', event_id='92995', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5406</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92995">Fair Wasserstein Coresets</a></strong></h5>


                        <p class="text-muted">
                            Zikai Xiong &middot; Niccolo Dalmasso &middot; Shubham Sharma &middot; Freddy Lecue &middot; Daniele Magazzeni &middot; Vamsi Potluru &middot; Tucker Balch &middot; Manuela Veloso
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored.  In this work, we present fair Wasserstein coresets ($\texttt{FWC}$), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. $\texttt{FWC}$ uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of $\texttt{FWC}$ is equivalent to Lloyd's algorithm for k-medians and k-means clustering. Experiments conducted on both synthetic and real datasets show that $\texttt{FWC}$:  (i) achieves a competitive fairness-performance tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (GPT-3.5 and GPT-4).</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-53" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-53', event_id='96526', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5407</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96526">Scale-invariant Optimal Sampling for Rare-events Data and Sparse Models</a></strong></h5>


                        <p class="text-muted">
                            Jing Wang &middot; HaiYing Wang &middot; Hao Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Subsampling is effective in tackling computational challenges for massive data with rare events.  Overly aggressive subsampling may adversely affect estimation efficiency, and optimal subsampling is essential to mitigate the information loss. However, existing optimal subsampling probabilities depends on data scales, and some scaling transformations may result in inefficient subsamples. This problem is more significant when there are inactive features, because their influence on the subsampling probabilities can be arbitrarily magnified by inappropriate scaling transformations. We tackle this challenge and introduce a scale-invariant optimal subsampling function in the context of sparse models, where inactive features are commonly assumed. Instead of focusing on estimating model parameters, we define an optimal subsampling function to minimize the prediction error,  using adaptive lasso as an example to outline the estimation procedure and study its theoretical guarantee.  We first introduce the adaptive lasso estimator for rare-events data and establish its oracle properties, thereby validating the use of subsampling. Then we derive a scale-invariant optimal subsampling function that minimizes the prediction error of the inverse probability weighted (IPW) adaptive lasso.  Finally, we present an estimator based on the maximum sampled conditional likelihood (MSCL) to further improve the estimation efficiency. We conduct numerical experiments using both simulated and real-world data sets to demonstrate the performance of the proposed methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-54" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-54', event_id='96512', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5408</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96512">Universal Rates of Empirical Risk Minimization</a></strong></h5>


                        <p class="text-muted">
                            Steve Hanneke &middot; Mingyue Xu
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The well-known $\textit{empirical risk minimization}$ (ERM) principle is the basis of many widely used machine learning algorithms, and plays an essential role in the classical PAC theory. A common description of a learning algorithm's performance is its so-called learning curve, that is, the decay of the expected error as a function of the input sample size. As the PAC model fails to explain the behavior of learning curves, recent research has explored an alternative universal learning model and has ultimately revealed a distinction between optimal universal and uniform learning rates (Bousquet et al., 2021). However, a basic understanding of such differences with a particular focus on the ERM principle has yet to be developed.     In this paper, we consider the problem of universal learning by ERM in the realizable case and study the possible universal rates. Our main result is a fundamental $\textit{tetrachotomy}$: there are only four possible universal learning rates by ERM, namely, the learning curves of any concept class learnable by ERM decay either at $e^{-n}$, $1/n$, $\log{(n)}/n$, or arbitrarily slow rates. Moreover, we provide a complete characterization of which concept classes fall into each of these categories, via new complexity structures. We also develop new combinatorial dimensions which supply sharp asymptotically-valid constant factors for these rates, whenever possible.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-55" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-55', event_id='96111', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5409</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96111">PAC-Bayes-Chernoff bounds for unbounded losses</a></strong></h5>


                        <p class="text-muted">
                            Ioar Casado Telletxea &middot; Luis Antonio Ortega Andrs &middot; Aritz Prez &middot; Andres Masegosa
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce a new PAC-Bayes oracle bound for unbounded losses that extends Cramr-Chernoff bounds to the PAC-Bayesian setting. The proof technique relies on controlling the tails of certain random variables involving the Cramr transform of the loss. Our approach naturally leverages properties of Cramr-Chernoff bounds, such as exact optimization of the free parameter in many PAC-Bayes bounds. We highlight several applications of the main theorem. Firstly, we show that our bound recovers and generalizes previous results. Additionally, our approach allows working with richer assumptions that result in more informative and potentially tighter bounds. In this direction, we provide a general bound under a new <em>model-dependent</em> assumption from which we obtain bounds based on parameter norms and log-Sobolev inequalities. Notably, many of these bounds can be minimized to obtain distributions beyond the Gibbs posterior and provide novel theoretical coverage to existing regularization techniques.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-56" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-56', event_id='95855', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5410</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95855">Learning the Infinitesimal Generator of Stochastic Diffusion Processes</a></strong></h5>


                        <p class="text-muted">
                            Vladimir Kostic &middot; Hlne Halconruy &middot; Timothe Devergne &middot; Karim Lounici &middot; Massimiliano Pontil
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We address data-driven learning of the infinitesimal generator of stochastic diffusion processes, essential for understanding numerical simulations of natural and physical systems. The unbounded nature of the generator poses significant challenges, rendering conventional analysis techniques for Hilbert-Schmidt operators ineffective. To overcome this, we introduce a novel framework based on the energy functional for these stochastic processes. Our approach integrates physical priors through an energy-based risk metric in both full and partial knowledge settings. We evaluate the statistical performance of a reduced-rank estimator in reproducing kernel Hilbert spaces (RKHS) in the partial knowledge setting. Notably, our approach provides learning bounds independent of the state space dimension and ensures non-spurious spectral estimation. Additionally, we elucidate how the distortion between the intrinsic energy-induced metric of the stochastic diffusion and the RKHS metric used for generator estimation impacts the spectral learning bounds.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-57" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-57', event_id='96734', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5500</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96734">Global Rewards in Restless Multi-Armed Bandits</a></strong></h5>


                        <p class="text-muted">
                            Naveen Raman &middot; Zheyuan Shi &middot; Fei Fang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Restless multi-armed bandits (RMAB) extend multi-armed bandits so arm pulls impact future arm states. Despite the success of RMABs, a key limiting assumption is the separability of rewards into a sum across arms. We address this deficiency by proposing restless-multi-armed bandit with global rewards (RMAB-G), a generalization of RMABs to global non-separable rewards. To solve RMAB-G, we develop the Linear-Whittle and Shapley-Whittle indices, which extend Whittle indices from RMABs to RMAB-Gs. We prove approximation bounds which demonstrate how Linear and Shapley-Whittle indices fail for non-linear rewards. To overcome this limitation, we propose two sets of adaptive policies: the first computes indices iteratively and the second combines indices with Monte-Carlo Tree Search (MCTS). Empirically, we demonstrate that adaptive policies outperform both pre-computed index policies and baselines in synthetic and real-world food rescue datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-58" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-58', event_id='93084', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5501</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93084">On $f$-Divergence Principled Domain Adaptation: An Improved Framework</a></strong></h5>


                        <p class="text-muted">
                            Ziqiao Wang &middot; Yongyi Mao
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Unsupervised domain adaptation (UDA) plays a crucial role in addressing distribution shifts in machine learning. In this work, we improve the theoretical foundations of UDA proposed in Acuna et al. (2021) by refining their $f$-divergence-based discrepancy and additionally introducing a new measure, $f$-domain discrepancy ($f$-DD). By removing the absolute value function and incorporating a scaling parameter, $f$-DD obtains novel target error and sample complexity bounds, allowing us to recover previous KL-based results and bridging the gap between algorithms and theory presented in Acuna et al. (2021). Using a localization technique, we also develop a fast-rate generalization bound. Empirical results demonstrate the superior performance of $f$-DD-based learning algorithms over previous works in popular UDA benchmarks.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-59" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-59', event_id='93113', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5502</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93113">A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness</a></strong></h5>


                        <p class="text-muted">
                            Yuri Kinoshita &middot; Taro Toyoizumi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While neural networks can enjoy an outstanding flexibility and exhibit unprecedented performance, the mechanism behind their behavior is still not well-understood. To tackle this fundamental challenge, researchers have tried to restrict and manipulate some of their properties in order to gain new insights and better control on them. Especially, throughout the past few years, the concept of <em>bi-Lipschitzness</em> has been proved as a beneficial inductive bias in many areas. However, due to its complexity, the design and control of bi-Lipschitz architectures are falling behind, and a model that is precisely designed for bi-Lipschitzness realizing a direct and simple control of the constants along with solid theoretical analysis is lacking. In this work, we investigate and propose a novel framework for bi-Lipschitzness that can achieve such a clear and tight control based on convex neural networks and the Legendre-Fenchel duality. Its desirable properties are illustrated with concrete experiments to illustrate its broad range of applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-60" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-60', event_id='93523', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5503</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93523">Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained Markov Decision Process</a></strong></h5>


                        <p class="text-muted">
                            Jiashuo Jiang &middot; Yinyu Ye
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\frac{1}{\Delta\cdot\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound, with $\Delta$ being a problem-dependent parameter, yet independent of $\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms of the dependency on $\epsilon$. To achieve this advance, we develop a new framework for analyzing CMDP problems. To be specific, our algorithm operates in the primal space and we resolve the primal LP for the CMDP problem at each period in an online manner, with \textit{adaptive} remaining resource capacities. The key elements of our algorithm are: i) a characterization of the instance hardness via LP basis, ii) an eliminating procedure that identifies one optimal basis of the primal LP, and; iii) a resolving procedure that is adaptive to the remaining resources and sticks to the characterized optimal basis.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-61" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-61', event_id='93790', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5504</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93790">Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound</a></strong></h5>


                        <p class="text-muted">
                            Reuben Adams &middot; John Shawe-Taylor &middot; Benjamin Guedj
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Current PAC-Bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis-classifications. We provide the first PAC-Bayes bound capable of providing such rich information by bounding the Kullback-Leibler divergence between the empirical and true probabilities of a set of $M$ error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. We transform our bound into a differentiable training objective. Our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing PAC-Bayes bounds can only bound a particular pre-decided weighting of the error types. In contrast our bound implicitly controls all uncountably many weightings simultaneously.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-62" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-62', event_id='94218', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5505</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94218">How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression</a></strong></h5>


                        <p class="text-muted">
                            Xingwu Chen &middot; Lei Zhao &middot; Difan Zou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite the remarkable success of transformer-based models in various real-world tasks, their underlying mechanisms remain poorly understood. Recent studies have suggested that transformers can implement gradient descent as an in-context learner for linear regression problems and have developed various theoretical analyses accordingly. However, these works mostly focus on the expressive power of transformers by designing specific parameter constructions, lacking a comprehensive understanding of their inherent working mechanisms post-training. In this study, we consider a sparse linear regression problem and investigate how a trained multi-head transformer performs in-context learning. We experimentally discover that the utilization of multi-heads exhibits different patterns across layers: multiple heads are utilized and essential in the first layer, while usually only a single head is sufficient for subsequent layers. We provide a theoretical explanation for this observation: the first layer preprocesses the context data, and the following layers execute simple optimization steps based on the preprocessed context. Moreover, we demonstrate that such a preprocess-then-optimize algorithm can significantly outperform naive gradient descent and ridge regression algorithms. Further experimental results support our explanations. Our findings offer insights into the benefits of multi-head attention and contribute to understanding the more intricate mechanisms hidden within trained transformers.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-63" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-63', event_id='94583', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5506</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94583">On the Sparsity of the Strong Lottery Ticket Hypothesis</a></strong></h5>


                        <p class="text-muted">
                            Emanuele Natale &middot; Davide Ferre &middot; Giordano Giambartolomei &middot; Frederic Giroire &middot; Frederik Mallmann-Trenn
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Considerable research efforts have recently been made to show that a random neural network $N$ contains subnetworks capable of accurately approximating any given neural network that is sufficiently smaller than $N$, without any training. This line of research, known as the Strong Lottery Ticket Hypothesis (SLTH), was originally motivated by the weaker Lottery Ticket Hypothesis, which states that a sufficiently large random neural network $N$ contains sparse subnetworks that can be trained efficiently to achieve performance comparable to that of training the entire network $N$.Despite its original motivation, results on the SLTH have so far not provided any guarantee on the size of subnetworks.Such limitation is due to the nature of the main technical tool leveraged by these results, the Random Subset Sum (RSS) Problem.Informally, the RSS Problem asks how large a random i.i.d. sample $\Omega$ should be so that we are able to approximate any number in $[-1,1]$, up to an error of $ \epsilon$, as the sum of a suitable subset of $\Omega$. We provide the first proof of the SLTH in classical settings, such as dense and equivariant networks, with guarantees on the sparsity of the subnetworks. Central to our results, is the proof of an essentially tight bound on the Random Fixed-Size Subset Sum Problem (RFSS), a variant of the RSS Problem in which we only ask for subsets of a given size, which is of independent interest.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-64" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-64', event_id='94640', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5507</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94640">Compositional PAC-Bayes: Generalization of GNNs with persistence and beyond</a></strong></h5>


                        <p class="text-muted">
                            Kirill Brilliantov &middot; Amauri Souza &middot; Vikas Garg
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Heterogeneity, e.g., due to different types of layers or multiple sub-models, poses key challenges in analyzing the generalization behavior of several modern architectures. For instance, descriptors based on Persistent Homology (PH) are being increasingly integrated into Graph Neural Networks (GNNs) to augment them with rich topological features; however, the generalization of such PH schemes remains unexplored. We introduce a novel <em>compositional</em> PAC-Bayes framework that provides a general recipe to analyze a broad spectrum of models including those with heterogeneous layers. Specifically, we provide the first data-dependent generalization bounds for a widely adopted PH vectorization scheme (that subsumes persistence landscapes, images, and silhouettes) as well as PH-augmented GNNs. Using our framework, we also obtain bounds for GNNs and neural nets with ease. Our bounds also inform the design of novel regularizers. Empirical evaluations on several standard real-world datasets demonstrate that our theoretical bounds highly correlate with empirical generalization performance, leading to improved classifier design via our regularizers. Overall, this work bridges a crucial gap in the theoretical understanding of PH methods and general heterogeneous models, paving the way for the design of better models for (graph) representation learning.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-65" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-65', event_id='94847', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5508</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94847">One-Layer Transformer Provably Learns One-Nearest Neighbor In Context</a></strong></h5>


                        <p class="text-muted">
                            Zihao Li &middot; Yuan Cao &middot; Cheng Gao &middot; Yihan He &middot; Han Liu &middot; Jason Klusowski &middot; Jianqing Fan &middot; Mengdi Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Transformers have achieved great success in recent years. Interestingly, transformers have shown particularly strong in-context learning capability -- even without fine-tuning, they are still able to solve unseen tasks well purely based on task-specific prompts. In this paper, we study the capability of one-layer transformers in learning the one-nearest neighbor prediction rule. Under a theoretical framework where the prompt contains a sequence of labeled training data and unlabeled test data, we show that, although the loss function is nonconvex, when trained with gradient descent, a single softmax attention layer can successfully learn to behave like a one-nearest neighbor classifier. Our result gives a concrete example on how transformers can be trained to implement nonparametric machine learning algorithms, and sheds light on the role of softmax attention in transformer models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-66" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-66', event_id='95011', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5509</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95011">High-dimensional (Group) Adversarial Training in Linear Regression</a></strong></h5>


                        <p class="text-muted">
                            Yiling Xie &middot; Xiaoming Huo
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Adversarial training can achieve robustness against adversarial perturbations and has been widely used in machine-learning models. This paper delivers a non-asymptotic consistency analysis of the adversarial training procedure under $\ell_\infty$-perturbation in high-dimensional linear regression. It will be shown that, under the restricted eigenvalue condition, the associated convergence rate of prediction error can achieve the minimax rate up to a logarithmic factor in the high-dimensional linear regression on the class of sparse parameters. Additionally, the group adversarial training procedure is analyzed. Compared with classic adversarial training, it will be proved that the group adversarial training procedure enjoys a better prediction error upper bound under certain group-sparsity patterns.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-67" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-67', event_id='95268', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5510</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95268">Nearly Minimax Optimal Regret for Multinomial Logistic Bandit</a></strong></h5>


                        <p class="text-muted">
                            Joongkyu Lee &middot; Min-hwan Oh
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In this paper, we study the contextual multinomial logit (MNL) bandit problem in which a learning agent sequentially selects an assortment based on contextual information, and user feedback follows an MNL choice model.There has been a significant discrepancy between lower and upper regret bounds, particularly regarding the maximum assortment size  $K$. Additionally, the variation in reward structures between these bounds complicates the quest for optimality. Under uniform rewards, where all items have the same expected reward, we establish a regret lower bound of $\Omega(d\sqrt{\smash[b]{T/K}})$ and propose a constant-time algorithm, OFU-MNL+, that achieves a matching upper bound of $\tilde{\mathcal{O}}(d\sqrt{\smash[b]{T/K}})$. We also provide instance-dependent minimax regret bounds under uniform rewards.Under non-uniform rewards, we prove a lower bound of $\Omega(d\sqrt{T})$ and an upper bound of $\tilde{\mathcal{O}}(d\sqrt{T})$, also achievable by OFU-MNL+. Our empirical studies support these theoretical findings. To the best of our knowledge, this is the first work in the contextual MNL bandit literature to prove minimax optimality --- for either uniform or non-uniform reward setting --- and to propose a computationally efficient algorithm that achieves this optimality up to logarithmic factors.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-68" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-68', event_id='96050', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5600</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96050">A Theory of Optimistically Universal Online Learnability for General Concept Classes</a></strong></h5>


                        <p class="text-muted">
                            Steve Hanneke &middot; Hongao Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We provide a full characterization of the concept classes that are optimistically universally online learnable with {0, 1} labels. The notion of optimistically universal online learning was defined in [Hanneke, 2021] in order to understand learnability under minimal assumptions. In this paper, following the philosophy behind that work, we investigate two questions, namely, for every concept class: (1) What are the minimal assumptions on the data process admitting online learnability? (2) Is there a learning algorithm which succeeds under every data process satisfying the minimal assumptions? Such an algorithm is said to be optimistically universal for the given concept class. We resolve both of these questions for all concept classes, and moreover, as part of our solution we design general learning algorithms for each case. Finally, we extend these algorithms and results to the agnostic case, showing an equivalence between the minimal assumptions on the data process for learnability in the agnostic and realizable cases, for every concept class, as well as the equivalence of optimistically universal learnability.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-69" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-69', event_id='95981', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5601</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95981">Stabilizing Linear Passive-Aggressive Online Learning with Weighted Reservoir Sampling</a></strong></h5>


                        <p class="text-muted">
                            Skyler Wu &middot; Fred Lu &middot; Edward Raff &middot; James Holt
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Online learning methods, like the seminal Passive-Aggressive (PA) classifier, are still highly effective for high-dimensional streaming data, out-of-core processing, and other throughput-sensitive applications. Many such algorithms rely on fast adaptation to individual errors as a key to their convergence. While such algorithms enjoy low theoretical regret, in real-world deployment they can be sensitive to individual outliers that cause the algorithm to over-correct. When such outliers occur at the end of the data stream, this can cause the final solution to have unexpectedly low accuracy. We design a weighted reservoir sampling (WRS) approach to obtain a stable ensemble model from the sequence of solutions without requiring additional passes over the data, hold-out sets, or a growing amount of memory. Our key insight is that good solutions tend to be error-free for more iterations than bad solutions, and thus, the number of passive rounds provides an estimate of a solution's relative quality. Our reservoir thus contains $K$ previous intermediate weight vectors with high survival times. We demonstrate our WRS approach on the Passive-Aggressive Classifier (PAC) and First-Order Sparse Online Learning (FSOL), where our method consistently and significantly outperforms the unmodified approach. We show that the risk of the ensemble classifier is bounded with respect to the regret of the underlying online learning method.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-70" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-70', event_id='95644', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5602</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95644">Learning to Price Homogeneous Data</a></strong></h5>


                        <p class="text-muted">
                            Keran Chen &middot; Joon Suk Huh &middot; Kirthevasan Kandasamy
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We study a data pricing problem, where a seller has access to $N$ homogeneous data points (e.g. drawn i.i.d. from some distribution).There are $m$ types of buyers in the market, where buyers of the same type $i$ have the same valuation curve $v_i:[N]\rightarrow [0,1]$, where $v_i(n)$ is the value for having $n$ data points.*A priori*, the seller is unaware of thedistribution of buyers, but can repeat the market for $T$ rounds so as to learn the revenue-optimal pricing curve $p:[N] \rightarrow [0, 1]$.To solve this online learning problem,we first develop novel discretization schemes to approximate any pricing curve.When compared to prior work,the size of our discretization schemes scales gracefully with the approximation parameter, which translates to better regret in online learning.Under assumptions like smoothness and diminishing returns which are satisfied by data, the discretization size can be reduced further.We then turn to the online learning problem, both in the stochastic and adversarial settings.On each round, the seller chooses an *anonymous* pricing curve $p_t$.A new buyer appears and may choose to purchase some amount of data.She then reveals her type *only if* she makes a purchase.Our online algorithms build on classical algorithms such as UCB and FTPL, but require novel ideas to account for the asymmetric nature of this feedback and to deal with the vastness of the space of pricing curves.Using the improved discretization schemes previously developed, we are able to achieve $\widetilde{O}(m\sqrt{T})$ regret in the stochastic setting and $\widetilde{\mathcal{O}}(m^{3/2}\sqrt{T})$ regret in the adversarial setting.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-71" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-71', event_id='95613', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5603</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95613">A Best-of-both-worlds Algorithm for Bandits with Delayed Feedback with Robustness to Excessive Delays</a></strong></h5>


                        <p class="text-muted">
                            Saeed Masoudian &middot; Julian Zimmert &middot; Yevgeny Seldin
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We propose a new best-of-both-worlds algorithm for bandits with variably delayed feedback. In contrast to prior work, which required prior knowledge of the maximal delay $d_{\max}$ and had a linear dependence of the regret on it, our algorithm can tolerate arbitrary excessive delays up to order $T$ (where $T$ is the time horizon). The algorithm is based on three technical innovations, which may all be of independent interest: (1) We introduce the first implicit exploration scheme that works in best-of-both-worlds setting. (2) We introduce the first control of distribution drift that does not rely on boundedness of delays. The control is based on the implicit exploration scheme and adaptive skipping of observations with excessive delays. (3) We introduce a procedure relating standard regret with drifted regret that does not rely on boundedness of delays. At the conceptual level, we demonstrate that complexity of best-of-both-worlds bandits with delayed feedback is characterized by the amount of information missing at the time of decision making (measured by the number of outstanding observations) rather than the time that the information is missing (measured by the delays).</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-72" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-72', event_id='95417', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5604</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95417">Provably Efficient Interactive-Grounded Learning with Personalized Reward</a></strong></h5>


                        <p class="text-muted">
                            Mengxiao Zhang &middot; Yuheng Zhang &middot; Haipeng Luo &middot; Paul Mineiro
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Interactive-Grounded Learning (IGL) [Xie et al., 2021] is a powerful framework in which a learner aims at maximizing unobservable rewards through interacting with an environment and observing reward-dependent feedback on the taken actions.To deal with personalized rewards that are ubiquitous in applications such as recommendation systems, Maghakian et al. [2022] study a version of IGL with context-dependent feedback, but their algorithm does not come with theoretical guarantees. In this work, we consider the same problem and provide the first provably efficient algorithms with sublinear regret under realizability. Our analysis reveals that the step-function estimator of prior work can deviate uncontrollably due to finite-sample effects. Our solution is a novel Lipschitz reward estimator which underestimates the true reward and enjoys favorable generalization performances. Building on this estimator, we propose two algorithms, one based on explore-then-exploit and the other based on inverse-gap weighting. We apply IGL to learning from image feedback and learning from text feedback, which are reward-free settings that arise in practice. Experimental results showcase the importance of using our Lipschitz reward estimator and the overall effectiveness of our algorithms.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-73" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-73', event_id='96948', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5605</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96948">RefDrop: Controllable Consistency in Image or Video Generation via Reference Feature Guidance</a></strong></h5>


                        <p class="text-muted">
                            Jiaojiao Fan &middot; Haotian Xue &middot; Qinsheng Zhang &middot; Yongxin Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>There is a rapidly growing interest in controlling consistency across multiple generated images using diffusion models. Among various methods, recent works have found that simply manipulating attention modules by concatenating features from multiple reference images provides an efficient approach to enhancing consistency without fine-tuning. Despite its popularity and success, few studies have elucidated the underlying mechanisms that contribute to its effectiveness. In this work, we reveal that the popular approach is a linear interpolation of image self-attention and cross-attention between synthesized content and reference features, with a constant rank-1 coefficient. Motivated by this observation, we find that a rank-1 coefficient is not necessary and simplifies the controllable generation mechanism. The resulting algorithm, which we coin as RefDrop, allows users to control the influence of reference context in a direct and precise manner. Besides further enhancing consistency in single-subject image generation, our method also enables more interesting applications, such as the consistent generation of multiple subjects, suppressing specific features to encourage more diverse content, and high-quality personalized video generation by boosting temporal consistency. Even compared with state-of-the-art image-prompt-based generators, such as IP-Adapter, RefDrop is competitive in terms of controllability and quality while avoiding the need to train a separate image encoder for feature injection from reference images, making it a versatile plug-and-play solution for any image or video diffusion model.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-74" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-74', event_id='94809', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5606</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94809">Strategic Multi-Armed Bandit Problems Under Debt-Free Reporting</a></strong></h5>


                        <p class="text-muted">
                            Ahmed Ben Yahmed &middot; Clment Calauznes &middot; Vianney Perchet
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We examine multi-armed bandit problems featuring strategic arms under debt-free reporting. In this context, each arm is characterized by a bounded support reward distribution and strategically aims to maximize its own utility by retaining a portion of the observed reward, potentially disclosing only a fraction of it to the player. This scenario unfolds as a game over $T$ rounds, leading to a competition of objectives between the player, aiming to minimize regret, and the arms, motivated by the desire to maximize their individual utilities. To address these dynamics, we propose an algorithm that establishes an equilibrium wherein each arm behaves truthfully and discloses as much of its rewards as possible. Utilizing this algorithm, the player can attain the second-highest average (true) reward among arms, with a cumulative regret bounded by $O(\log(T)/\Delta)$ (problem-dependent) or $O(\sqrt{T\log(T)})$ (worst-case).</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-75" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-75', event_id='94720', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5607</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94720">Fast Rates in Stochastic Online Convex Optimization by Exploiting the Curvature of Feasible Sets</a></strong></h5>


                        <p class="text-muted">
                            Taira Tsuchiya &middot; Shinji Ito
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In this work, we explore online convex optimization (OCO) and introduce a new condition and analysis that provides fast rates by exploiting the curvature of feasible sets. In online linear optimization, it is known that if the average gradient of loss functions exceeds a certain threshold, the curvature of feasible sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a logarithmic regret. This study reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets. In particular, we first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret bound of $O(\rho \log T)$ in stochastic environments. Here, $\rho > 0$ is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set. Our approach, unlike existing ones, can work directly with convex loss functions, exploiting the curvature of loss functions simultaneously, and can achieve the logarithmic regret only with a local property of feasible sets. Additionally, the algorithm achieves an $O(\sqrt{T})$ regret even in adversarial environments, in which FTL suffers an $\Omega(T)$ regret, and achieves an $O(\rho \log T + \sqrt{C \rho \log T})$ regret in corrupted stochastic environments with corruption level $C$. Furthermore, by extending our analysis, we establish a matching regret upper bound of $O\Big(T^{\frac{q-2}{2(q-1)}} (\log T)^{\frac{q}{2(q-1)}}\Big)$ for $q$-uniformly convex feasible sets, where uniformly convex sets include strongly convex sets and $\ell_p$-balls for $p \in [2,\infty)$. This bound bridges the gap between the $O(\log T)$ bound for strongly convex sets~($q=2$) and the $O(\sqrt{T})$ bound for non-curved sets~($q\to\infty$).</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-76" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-76', event_id='94616', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5608</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94616">Contextual Active Model Selection</a></strong></h5>


                        <p class="text-muted">
                            Xuefeng Liu &middot; Fangfang Xia &middot; Rick Stevens &middot; Yuxin Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While training models and labeling data are resource-intensive, a wealth of pre-trained models and unlabeled data exists. To effectively utilize these resources, we present an approach to actively select pre-trained models while minimizing labeling costs. We frame this as an online contextual active model selection problem: At each round, the learner receives an unlabeled data point as a context. The objective is to adaptively select the best model to make a prediction while limiting label requests. To tackle this problem, we propose CAMS, a contextual active model selection algorithm that relies on two novel components: (1) a contextual model selection mechanism, which leverages context information to make informed decisions about which model is likely to perform best for a given context, and (2)an active query component, which strategically chooses when to request labels for data points, minimizing the overall labeling cost. We provide rigorous theoretical analysis for the regret and query complexity under both adversarial and stochastic settings. Furthermore, we demonstrate the effectiveness of our algorithm on a diverse collection of benchmark classification tasks. Notably, CAMS requires substantially less labeling effort (less than 10%) compared to existing methods on CIFAR10 and DRIFT benchmarks, while achieving similar or better accuracy.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-77" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-77', event_id='98309', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5609</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/98309">A Continuous-time Stochastic Gradient Descent Method for Continuous Data</a></strong></h5>


                        <p class="text-muted">
                            Kexin Jin &middot; Jonas Latz &middot; Chenguang Liu &middot; Carola-Bibiane Schnlieb
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Optimization problems with continuous data appear in, e.g., robust machine learning, functional data analysis, and variational inference. Here, the target function is given as an integral over a family of (continuously) indexed target functions---integrated with respect to a probability measure. Such problems can often be solved by stochastic optimization methods:  performing optimization steps with respect to the indexed target function with randomly switched indices. In this work, we study a continuous-time variant of the stochastic gradient descent algorithm for optimization problems with continuous data. This so-called stochastic gradient process consists in a gradient flow minimizing an indexed target function that is coupled with a continuous-time index process determining the index. Index processes are, e.g., reflected diffusions, pure jump processes, or other Lvy processes on compact spaces. Thus, we study multiple sampling patterns for the continuous data space and allow for data simulated or streamed at runtime of the algorithm. We analyze the approximation properties of the stochastic gradient process and study its longtime behavior and ergodicity under constant and decreasing learning rates. We end with illustrating the applicability of the stochastic gradient process in a polynomial regression problem with noisy functional data, as well as in a physics-informed neural network.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-78" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-78', event_id='96363', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5610</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96363">S-STE: Continuous Pruning Function for Efficient 2:4 Sparse Pre-training</a></strong></h5>


                        <p class="text-muted">
                            Yuezhou Hu &middot; Jun Zhu &middot; Jianfei Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Training deep neural networks (DNNs) is costly. Fortunately, Nvidia Ampere and Hopper GPUs can accelerate matrix multiplications twice as fast as a dense equivalent by implementing 2:4 sparsity. However, previous STE-based 2:4 pre-training methods (\eg~STE with hard-thresholding, SR-STE) suffer from optimization difficulties because of discontinuous pruning function.In this study, we comprehensively analyse the bottleneck of traditional N:M sparse training and recognize three drawbacks with discontinuity: incorrect descending direction, inability to predict the amount of descent and sparse mask oscillation. In the light of this statement, we propose S-STE, a simple yet powerful 2:4 training method that contains two parts: to continuously project weights to be 2:4 sparse, and to rescale sparse weights with a per-tensor fixed scaling factor. Besides, we adopt minimum-variance unbiased estimation for activation gradient and FP8 quantization for whole process. Results show that our method surpass previous 2:4 pre-training recipes and is comparable even with full parameter models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-79" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-79', event_id='93783', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5700</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93783">GLinSAT: The General Linear Satisfiability Neural Network Layer By Accelerated Gradient Descent</a></strong></h5>


                        <p class="text-muted">
                            Hongtai Zeng &middot; Chao Yang &middot; Yanzhen Zhou &middot; Cheng Yang &middot; Qinglai Guo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Ensuring that the outputs of neural networks satisfy specific constraints is crucial for applying neural networks to real-life decision-making problems. In this paper, we consider making a batch of neural network outputs satisfy bounded and general linear constraints. We first reformulate the neural network output projection problem as an entropy-regularized linear programming problem. We show that such a problem can be equivalently transformed into an unconstrained convex optimization problem with Lipschitz continuous gradient according to the duality theorem. Then, based on an accelerated gradient descent algorithm with numerical performance enhancement, we present our architecture, GLinSAT, to solve the problem. To the best of our knowledge, this is the first general linear satisfiability layer in which all the operations are differentiable and matrix-factorization-free. Despite the fact that we can explicitly perform backpropagation based on automatic differentiation mechanism, we also provide an alternative approach in GLinSAT to calculate the derivatives based on implicit differentiation of the optimality condition. Experimental results on constrained traveling salesman problems, partial graph matching with outliers, predictive portfolio allocation and power system unit commitment demonstrate the advantages of GLinSAT over existing satisfiability layers. Our implementation is available at https://github.com/HunterTracer/GLinSAT.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-80" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-80', event_id='93835', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5701</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93835">Optimizing over Multiple Distributions under Generalized Quasar-Convexity Condition</a></strong></h5>


                        <p class="text-muted">
                            Ding Shihong &middot; Long Yang &middot; Luo Luo &middot; Cong Fang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We study a typical optimization model where the optimization variable is composed of multiple probability distributions. Though the model appears frequently in practice, such as for policy problems, it lacks specific analysis in the general setting. For this optimization problem,  we propose a new structural condition/landscape description named  generalized quasar-convexity (GQC) beyond the realms of convexity. In contrast to original quasar-convexity \citep{hinder2020near}, GQC allows an individual quasar-convex parameter $\gamma_i$ for each variable block $i$ and the smaller of $\gamma_i$ implies less block-convexity. To minimize the objective function, we consider a generalized oracle termed as the internal function that includes the standard gradient oracle as a special case. We provide optimistic mirror descent (OMD) for multiple distributions and prove that the algorithm can achieve an adaptive $\tilde{\mathcal{O}}((\sum_{i=1}^d1/\gamma_i)\epsilon^{-1})$ iteration complexity to find an $\varepsilon$-suboptimal global solution without pre-known the exact values of $\gamma_i$ when the objective admits ``polynomial-like'' structural. Notably, it achieves iteration complexity that does not explicitly depend on the number of distributions and strictly faster $(\sum_{i=1}^d 1/\gamma_i \text{ v.s. } d\max_{i\in[1:d]} 1/\gamma_i)$ than mirror decent methods. We also extend GQC to the minimax optimization problem proposing the generalized quasar-convexity-concavity (GQCC) condition and a decentralized variant of OMD with regularization. Finally, we show the applications of our algorithmic framework on discounted Markov Decision Processes problem and Markov games, which bring new insights on the landscape analysis of reinforcement learning.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-81" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-81', event_id='94081', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5702</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94081">Low-Rank Optimal Transport through Factor Relaxation with Latent Coupling</a></strong></h5>


                        <p class="text-muted">
                            Peter Halmos &middot; Xinhao Liu &middot; Julian Gold &middot; Benjamin Raphael
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Optimal transport (OT) is a general framework for finding a minimum-cost transport plan, or coupling, between probability distributions, and has many applications in machine learning. A key challenge in applying OT to massive datasets is the quadratic scaling of the coupling matrix with the size of the dataset. [Forrow et al. 2019] introduced a factored coupling for the k-Wasserstein barycenter problem, which [Scetbon et al. 2021] adapted to solve the primal low-rank OT problem. We derive an alternative parameterization of the low-rank problem based on the <em>latent coupling</em> (LC) factorization previously introduced by [Lin et al. 2021] generalizing [Forrow et al. 2019]. The LC factorization has multiple  advantages for low-rank OT including decoupling the problem into three OT problems and greater flexibility and interpretability. We leverage these advantages to derive a new algorithm <em>Factor Relaxation with Latent Coupling</em> (FRLC), which uses <em>coordinate</em> mirror descent to compute the LC factorization. FRLC handles multiple OT objectives (Wasserstein, Gromov-Wasserstein, Fused Gromov-Wasserstein), and marginal constraints (balanced, unbalanced, and semi-relaxed) with linear space complexity. We provide theoretical results on FRLC, and demonstrate superior performance on diverse applications -- including graph clustering and spatial transcriptomics --  while demonstrating its interpretability.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-82" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-82', event_id='94202', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5703</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94202">Pretrained Optimization Model for Zero-Shot Black Box Optimization</a></strong></h5>


                        <p class="text-muted">
                            Xiaobin Li &middot; Kai Wu &middot; yujian li &middot; Xiaoyu Zhang &middot; Handing Wang &middot; Jing Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Zero-shot optimization involves optimizing a target task that was not seen during training, aiming to provide the optimal solution without or with minimal adjustments to the optimizer. It is crucial to ensure reliable and robust performance in various applications. Current optimizers often struggle with zero-shot optimization and require intricate hyperparameter tuning to adapt to new tasks. To address this, we propose a Pretrained Optimization Model (POM) that leverages knowledge gained from optimizing diverse tasks, offering efficient solutions to zero-shot optimization through direct application or fine-tuning with few-shot samples. Evaluation on the BBOB benchmark and two robot control tasks demonstrates that POM outperforms state-of-the-art black-box optimization methods, especially for high-dimensional tasks. Fine-tuning POM with a small number of samples and budget yields significant performance improvements. Moreover, POM demonstrates robust generalization across diverse task distributions, dimensions, population sizes, and optimization horizons. For code implementation, see https://github.com/ninja-wm/POM/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-83" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-83', event_id='94579', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5704</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94579">Geometry-aware training of factorized layers in tensor Tucker format</a></strong></h5>


                        <p class="text-muted">
                            Emanuele Zangrando &middot; Steffen Schotthfer &middot; Gianluca Ceruti &middot; Jonas Kusch &middot; Francesco Tudisco
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Reducing parameter redundancies in neural network architectures is crucial for achieving feasible computational and memory requirements during train and inference of large networks. Given its easy implementation and flexibility, one promising approach is layer factorization, which reshapes weight tensors into a matrix format and parameterizes it as the product of two rank-r matrices. However, this family of approaches often requires an initial full-model warm-up phase, prior knowledge of a feasible rank, and it is sensitive to parameter initialization.In this work, we introduce a novel approach to train the factors of a Tucker decomposition of the weight tensors. Our training proposal proves to be optimal in locally approximating the original unfactorized dynamics and stable for the initialization. Furthermore, the rank of each mode is dynamically updated during training.We provide a theoretical analysis of the algorithm, showing convergence, approximation and local descent guarantees. The method's performance is further illustrated through a variety of experiments, showing remarkable training compression rates and comparable or even better performance than the full baseline and alternative layer factorization strategies.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-84" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-84', event_id='94865', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5705</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94865">Learning Generalized Linear Programming Value Functions</a></strong></h5>


                        <p class="text-muted">
                            Tu Anh-Nguyen &middot; Joey Huchette &middot; Christian Tjandraatmadja
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We develop a theoretically-grounded learning method for the Generalized Linear Programming Value Function (GVF), which models the optimal value of a linear programming (LP) problem as its objective and constraint bounds vary. This function plays a fundamental role in algorithmic techniques for large-scale optimization, particularly in decomposition for two-stage mixed-integer linear programs (MILPs). This paper establishes a structural characterization of the GVF that enables it to be modeled as a particular neural network architecture, which we then use to learn the GVF in a way that benefits from three notable properties. First, our method produces a true under-approximation of the value function with respect to the constraint bounds. Second, the model is input-convex in the constraint bounds, which not only matches the structure of the GVF but also enables the trained model to be efficiently optimized over using LP. Finally, our learning method is unsupervised, meaning that training data generation does not require computing LP optimal values, which can be prohibitively expensive at large scales.  We numerically show that our method can approximate the GVF well, even when compared to supervised methods that collect training data by solving an LP for each data point. Furthermore, as an application of our framework, we develop a fast heuristic method for large-scale two-stage MILPs with continuous second-stage variables, via a compact reformulation that can be solved faster than the full model linear relaxation at large scales and orders of magnitude faster than the original model.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-85" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-85', event_id='95432', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5707</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95432">Adaptive and Optimal Second-order Optimistic Methods for Minimax Optimization</a></strong></h5>


                        <p class="text-muted">
                            Ruichen Jiang &middot; Ali Kavis &middot; Qiujiang Jin &middot; Sujay Sanghavi &middot; Aryan Mokhtari
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose adaptive, line-search-free second-order methods with optimal rate of convergence for solving convex-concave min-max problems. By means of an adaptive step size, our algorithms feature a simple update rule that requires solving only one linear system per iteration, eliminating the need for line-search or backtracking mechanisms. Specifically, we base our algorithms on the optimistic method and appropriately combine it with second-order information. Moreover, distinct from common adaptive schemes, we define the step size recursively as a function of the gradient norm and the prediction error in the optimistic update. We first analyze a variant where the step size requires knowledge of the Lipschitz constant of the Hessian. Under the additional assumption of Lipschitz continuous gradients, we further design a parameter-free version by tracking the Hessian Lipschitz constant locally and ensuring the iterates remain bounded. We also evaluate the practical performance of our algorithm by comparing it to existing second-order algorithms for minimax optimization.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-86" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-86', event_id='95616', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5708</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95616">LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning</a></strong></h5>


                        <p class="text-muted">
                            Rui Pan &middot; Xiang Liu &middot; SHIZHE DIAO &middot; Renjie Pi &middot; Jipeng Zhang &middot; Chi Han &middot; Tong Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The machine learning community has witnessed impressive advancements since large language models (LLMs) first appeared. Yet, their massive memory consumption has become a significant roadblock to large-scale training. For instance, a 7B model typically requires at least 60 GB of GPU memory with full parameter training, which presents challenges for researchers without access to high-resource environments. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem. However, in most large-scale fine-tuning settings, their performance does not reach the level of full parameter training because they confine the parameter search to a low-rank subspace. Attempting to complement this deficiency, we investigate the layerwise properties of LoRA on fine-tuning tasks and observe an unexpected but consistent skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over 10%-35% in terms of MT-Bench score while achieving on-par or better performance in MMLU, AGIEval and WinoGrande. On large models, specifically LLaMA-2-70B, LISA surpasses LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-87" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-87', event_id='95816', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5709</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95816">SCAFFLSA: Taming Heterogeneity in Federated Linear Stochastic Approximation and TD Learning</a></strong></h5>


                        <p class="text-muted">
                            Paul Mangold &middot; Sergey Samsonov &middot; Safwan Labbi &middot; Ilya Levin &middot; REDA ALAMI &middot; Alexey Naumov &middot; Eric Moulines
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we analyze the sample and communication complexity of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the effects of local training with agent heterogeneity. We show that the communication complexity of FedLSA scales polynomially with the inverse of the desired accuracy . To overcome this, we propose SCAFFLSA a new variant of FedLSA that uses control variates to correct for client drift, and establish its sample and communication complexities. We show that for statistically heterogeneous agents, its communication complexity scales logarithmically with the desired accuracy, similar to Scaffnew. An important finding is that, compared to the existing results for Scaffnew, the sample complexity scales with the inverse of the number of agents, a property referred to as linear speed-up. Achieving this linear speed-up requires completely new theoretical arguments. We apply the proposed method to federated temporal difference learning with linear function approximation and analyze the corresponding complexity improvements.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-88" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-88', event_id='96047', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5710</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96047">Emergence of heavy tails in homogenized stochastic gradient descent</a></strong></h5>


                        <p class="text-muted">
                            Zhezhe Jiao &middot; Martin Keller-Ressel
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters. Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent (hSGD), and show in a regularized linear regression framework that it leads to an asymptotically heavy-tailed parameter distribution, even though local gradient noise is Gaussian. We give explicit upper and lower bounds on the tail-index of the resulting parameter distribution and validate these bounds in numerical experiments. Moreover, the explicit form of these bounds enables us to quantify the interplay between optimization hyperparameters and the tail-index. Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-89" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-89', event_id='98326', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5801</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/98326">TorchOpt: An Efficient Library for Differentiable Optimization</a></strong></h5>


                        <p class="text-muted">
                            Jie Ren &middot; Xidong Feng &middot; Bo Liu &middot; Xuehai Pan &middot; Yao Fu &middot; Luo Mai &middot; Yaodong Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Differentiable optimization algorithms often involve expensive computations of various meta-gradients. To address this, we design and implement TorchOpt, a new PyTorch-based differentiable optimization library. TorchOpt provides an expressive and unified programming interface that simplifies the implementation of explicit, implicit, and zero-order gradients. Moreover, TorchOpt has a distributed execution runtime capable of parallelizing diverse operations linked to differentiable optimization tasks across CPU and GPU devices. Experimental results demonstrate that TorchOpt achieves a 5.2 training time speedup in a cluster. TorchOpt is open-sourced at https://github.com/metaopt/torchopt and has become a PyTorch Ecosystem project.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-90" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-90', event_id='94279', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5802</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94279">First-Order Methods for Linearly Constrained Bilevel Optimization</a></strong></h5>


                        <p class="text-muted">
                            Guy Kornowski &middot; Swati Padmanabhan &middot; Kai Wang &middot; Zhe Zhang &middot; Suvrit Sra
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Algorithms for bilevel optimization often encounter Hessian computations, which are prohibitive in high dimensions. While recent works offer first-order methods for unconstrained bilevel problems, the constrained setting remains relatively underexplored. We present  first-order linearly constrained optimization methods with finite-time hypergradient stationarity guarantees. For linear equality constraints, we attain $\epsilon$-stationarity in $\widetilde{O}(\epsilon^{-2})$ gradient oracle calls, which is nearly-optimal. For linear inequality constraints, we attain $(\delta,\epsilon)$-Goldstein stationarity in $\widetilde{O}(d{\delta^{-1} \epsilon^{-3}})$ gradient oracle calls, where $d$ is the upper-level dimension. Finally, we obtain for the linear inequality setting dimension-free rates of $\widetilde{O}({\delta^{-1} \epsilon^{-4}})$ oracle complexity under the additional assumption of oracle access to the optimal dual variable. Along the way, we develop new nonsmooth nonconvex optimization methods with inexact oracles. Our numerical experiments verify these guarantees.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-91" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-91', event_id='93735', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5803</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93735">Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search</a></strong></h5>


                        <p class="text-muted">
                            Qiujiang Jin &middot; Ruichen Jiang &middot; Aryan Mokhtari
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In this paper, we present the first explicit and non-asymptotic global convergence rates of the BFGS method when implemented with an inexact line search scheme satisfying the Armijo-Wolfe conditions. We show that BFGS achieves a global linear convergence rate of $(1 - \frac{1}{\kappa})^t$ for $\mu$-strongly convex functions with $L$-Lipschitz gradients, where $\kappa = \frac{L}{\mu}$ represents the condition number. Additionally, if the objective function's Hessian is Lipschitz, BFGS with the Armijo-Wolfe line search achieves a linear convergence rate that depends solely on the line search parameters, independent of the condition number. We also establish a global superlinear convergence rate of $\mathcal{O}((\frac{1}{t})^t)$. These global bounds are all valid for any starting point $x_0$ and any symmetric positive definite initial Hessian approximation matrix $B_0$, though the choice of $B_0$ impacts the number of iterations needed to achieve these rates. By synthesizing these results, we outline the first global complexity characterization of BFGS with the Armijo-Wolfe line search. Additionally, we clearly define a mechanism for selecting the step size to satisfy the Armijo-Wolfe conditions and characterize its overall complexity.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-92" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-92', event_id='96911', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5804</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96911">ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Xiang Meng &middot; Kayhan Behdin &middot; Haoyue Wang &middot; Rahul Mazumder
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce ALPS, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. ALPS substantially outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the LLaMA3-8B model with 70\% sparsity, ALPS achieves a 29\% reduction in test perplexity on the WikiText dataset and a 8\% improvement in zero-shot benchmark performance compared to existing methods. Our code is available at https://github.com/mazumder-lab/ALPS.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-93" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-93', event_id='96690', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5805</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96690">Slack-Free Spiking Neural Network Formulation for Hypergraph Minimum Vertex Cover</a></strong></h5>


                        <p class="text-muted">
                            Tam Nguyen &middot; Anh-Dzung Doan &middot; zhipeng cai &middot; Tat-Jun Chin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Neuromorphic computers open up the potential of energy-efficient computation using spiking neural networks (SNN), which consist of neurons that exchange spike-based information asynchronously. In particular, SNNs have shown promise in solving combinatorial optimization. Underpinning the SNN methods is the concept of energy minimization of an Ising model, which is closely related to quadratic unconstrained binary optimization (QUBO). Thus, the starting point for many SNN methods is reformulating the target problem as QUBO, then executing an SNN-based QUBO solver. For many combinatorial problems, the reformulation entails introducing penalty terms, potentially with slack variables, that implement feasibility constraints in the QUBO objective. For more complex problems such as hypergraph minimum vertex cover (HMVC), numerous slack variables are introduced which drastically increase the search domain and reduce the effectiveness of the SNN solver. In this paper, we propose a novel SNN formulation for HMVC. Rather than using penalty terms with slack variables, our SNN architecture introduces additional spiking neurons with a constraint checking and correction mechanism that encourages convergence to feasible solutions. In effect, our method obviates the need for reformulating HMVC as QUBO. Experiments on neuromorphic hardware show that our method consistently yielded high quality solutions for HMVC on real and synthetic instances where the SNN-based QUBO solver often failed, while consuming measurably less energy than global solvers on CPU.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-94" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-94', event_id='93853', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5806</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93853">Practical $0.385$-Approximation for Submodular Maximization Subject to a Cardinality Constraint</a></strong></h5>


                        <p class="text-muted">
                            Morad Tukan &middot; Loay Mualem &middot; Moran Feldman
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Non-monotone constrained submodular maximization plays a crucial role in various machine learning applications. However, existing algorithms often struggle with a trade-off between approximation guarantees and practical efficiency. The current state-of-the-art is a recent $0.401$-approximation algorithm, but its computational complexity makes it highly impractical. The best practical algorithms for the problem only guarantee $1/e$-approximation. In this work, we present a novel algorithm for submodular maximization subject to a cardinality constraint that combines a guarantee of $0.385$-approximation with a low and practical query complexity of $O(n+k^2)$. Furthermore, we evaluate our algorithm's performance through extensive machine learning applications, including Movie Recommendation, Image Summarization, and more. These evaluations demonstrate the efficacy of our approach.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-95" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-95', event_id='95924', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5807</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95924">Byzantine Robustness and Partial Participation Can Be Achieved at Once: Just Clip Gradient Differences</a></strong></h5>


                        <p class="text-muted">
                            Grigory Malinovsky &middot; Peter Richtarik &middot; Samuel Horvth &middot; Eduard Gorbunov
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Distributed learning has emerged as a leading paradigm for training large machine learning models. However, in real-world scenarios, participants may be unreliable or malicious, posing a significant challenge to the integrity and accuracy of the trained models. Byzantine fault tolerance mechanisms have been proposed to address these issues, but they often assume full participation from all clients, which is not always practical due to the unavailability of some clients or communication constraints. In our work, we propose the first distributed method with client sampling and provable tolerance to Byzantine workers. The key idea behind the developed method is the use of gradient clipping to control stochastic gradient differences in recursive variance reduction. This allows us to bound the potential harm caused by Byzantine workers, even during iterations when all sampled clients are Byzantine. Furthermore, we incorporate communication compression into the method to enhance communication efficiency. Under general assumptions, we prove convergence rates for the proposed method that match the existing state-of-the-art (SOTA) theoretical results. We also propose a heuristic on how to adjust any Byzantine-robust method to a partial participation scenario via clipping.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-96" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-96', event_id='95766', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5808</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95766">Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks</a></strong></h5>


                        <p class="text-muted">
                            Dmitry Kovalev &middot; Ekaterina Borodich &middot; Alexander Gasnikov &middot; Dmitrii Feoktistov
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We consider the task of minimizing the sum of convex functions stored in a decentralized manner across the nodes of a communication network. This problem is relatively well-studied in the scenario when the objective functions are smooth, or the links of the network are fixed in time, or both. In particular, lower bounds on the number of decentralized communications and (sub)gradient computations required to solve the problem have been established, along with matching optimal algorithms. However, the remaining and most challenging setting of non-smooth decentralized optimization over time-varying networks is largely underexplored, as neither lower bounds nor optimal algorithms are known in the literature. We resolve this fundamental gap with the following contributions: (i) we establish the first lower bounds on the communication and subgradient computation complexities of solving non-smooth convex decentralized optimization problems over time-varying networks; (ii) we develop the first optimal algorithm that matches these lower bounds and offers substantially improved theoretical performance compared to the existing state of the art.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-97" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-97', event_id='95763', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5809</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95763">On the Optimal Time Complexities in Decentralized Stochastic Asynchronous Optimization</a></strong></h5>


                        <p class="text-muted">
                            Alexander Tyurin &middot; Peter Richtarik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We consider the decentralized stochastic asynchronous optimization setup, where many workers asynchronously calculate stochastic gradients and asynchronously communicate with each other using edges in a multigraph. For both homogeneous and heterogeneous setups, we prove new time complexity lower bounds under the assumption that computation and communication speeds are bounded by constants. After that, we developed a new nearly optimal method, Fragile SGD, and a new optimal method, Amelie SGD, that converge with arbitrary heterogeneous computation and communication speeds and match our lower bounds (up to a logarithmic factor in the homogeneous setting). Our time complexities are new, nearly optimal, and provably improve all previous asynchronous/synchronous stochastic methods in the decentralized setup.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-98" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-98', event_id='94577', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5810</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94577">Hierarchical Federated Learning with Multi-Timescale Gradient Correction</a></strong></h5>


                        <p class="text-muted">
                            Wenzhi Fang &middot; Dong-Jun Han &middot; Evan Chen &middot; Shiqiang Wang &middot; Christopher Brinton
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While traditional federated learning (FL) typically focuses on a star topology where clients are directly connected to a central server, real-world distributed systems often exhibit hierarchical architectures. Hierarchical FL (HFL) has emerged as a promising solution to bridge this gap, leveraging aggregation points at multiple levels of the system. However, existing algorithms for HFL encounter challenges in dealing with multi-timescale model drift, i.e., model drift occurring across hierarchical levels of data heterogeneity. In this paper, we propose a multi-timescale gradient correction (MTGC) methodology to resolve this issue. Our key idea is to introduce distinct control variables to (i) correct the client gradient towards the group gradient, i.e., to reduce client model drift caused by local updates based on individual datasets, and (ii) correct the group gradient towards the global gradient, i.e., to reduce group model drift caused by FL over clients within the group. We analytically characterize the convergence behavior of MTGC under general non-convex settings, overcoming challenges associated with couplings between correction terms. We show that our convergence bound is immune to the extent of data heterogeneity, confirming the stability of the proposed algorithm against multi-level non-i.i.d. data. Through extensive experiments on various datasets and models, we validate the effectiveness of MTGC in diverse HFL settings. The code for this project is available at https://github.com/wenzhifang/MTGC.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-99" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-99', event_id='96236', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5900</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96236">Auditing Privacy Mechanisms via Label Inference Attacks</a></strong></h5>


                        <p class="text-muted">
                            Rbert Busa-Fekete &middot; Travis Dick &middot; Claudio Gentile &middot; Andres Munoz Medina &middot; Adam Smith &middot; Marika Swanberg
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose reconstruction advantage measures to audit label privatization mechanisms. A reconstruction advantage measure quantifies the increase in an attacker's ability to infer the true label of an unlabeled example when provided with a private version of the labels in a dataset (e.g., aggregate of labels from different users or noisy labels output by randomized response), compared to an attacker that only observes the feature vectors, but may have prior knowledge of the correlation between features and labels. We consider two such auditing measures: one additive, and on multiplicative. These cover previous approaches taken in the literature on empirical auditing and differential privacy. These measures allow us to place a variety of proposed privatization schemes---some differentially private, some not---on the same footing. We analyze these measures theoretically under a distributional model which, we claim, encapsulates reasonable adversarial settings. We also quantify their behavior empirically on real and simulated  prediction tasks. Across a range of experimental settings, we find that differentially private schemes dominate or match the privacy-utility tradeoff of more heuristic approaches.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-100" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-100', event_id='96602', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5901</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96602">Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning</a></strong></h5>


                        <p class="text-muted">
                            Dario Fenoglio &middot; Gabriele Dominici &middot; Pietro Barbiero &middot; Alberto Tonda &middot; Martin Gjoreski &middot; Marc Langheinrich
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Federated Learning (FL), a privacy-aware approach in distributed deep learning environments, enables many clients to collaboratively train a model without sharing sensitive data, thereby reducing privacy risks. However, enabling human trust and control over FL systems requires understanding the evolving behaviour of clients, whether beneficial or detrimental for the training, which still represents a key challenge in the current literature. To address this challenge, we introduce Federated Behavioural Planes (FBPs), a novel method to analyse, visualise, and explain the dynamics of FL systems, showing how clients behave under two different lenses: predictive performance (error behavioural space) and decision-making processes (counterfactual behavioural space). Our experiments demonstrate that FBPs provide informative trajectories describing the evolving states of clients and their contributions to the global model, thereby enabling the identification of clusters of clients with similar behaviours. Leveraging the patterns identified by FBPs, we propose a robust aggregation technique named Federated Behavioural Shields to detect malicious or noisy client models, thereby enhancing security and surpassing the efficacy of existing state-of-the-art FL defense mechanisms. Our code is publicly available on GitHub.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-101" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-101', event_id='96679', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5902</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96679">Instance-Specific Asymmetric Sensitivity in Differential Privacy</a></strong></h5>


                        <p class="text-muted">
                            David Durfee
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We provide a new algorithmic framework for differentially private estimation of general functions that adapts to the hardness of the underlying dataset. We build upon previous work that gives a paradigm for selecting an output through the exponential mechanism based upon closeness of the inverse to the underlying dataset, termed the inverse sensitivity mechanism. Our framework will slightly modify the closeness metric and instead give a simple and efficient application of the sparse vector technique. While the inverse sensitivity mechanism was shown to be instance optimal, it was only with respect to a class of unbiased mechanisms such that the most likely outcome matches the underlying data. We break this assumption in order to more naturally navigate the bias-variance tradeoff, which will also critically allow for extending our method to unbounded data. In consideration of this tradeoff, we provide theoretical guarantees and empirical validation that our technique will be particularly effective when the distances to the underlying dataset are asymmetric. This asymmetry is inherent to a range of important problems including fundamental statistics such as variance, as well as commonly used machine learning performance metrics for both classification and regression tasks. We efficiently instantiate our method in $O(n)$ time for these problems and empirically show that our techniques will give substantially improved differentially private estimations.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-102" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-102', event_id='93250', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5903</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93250">Drago: Primal-Dual Coupled Variance Reduction for Faster Distributionally Robust Optimization</a></strong></h5>


                        <p class="text-muted">
                            Ronak Mehta &middot; Jelena Diakonikolas &middot; Zaid Harchaoui
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We consider the penalized distributionally robust optimization (DRO) problem with a closed, convex uncertainty set, a setting that encompasses learning using $f$-DRO and spectral/$L$-risk minimization. We present Drago, a stochastic primal-dual algorithm which combines cyclic and randomized components with a carefully regularized primal update to achieve dual variance reduction. Owing to its design, Drago enjoys a state-of-the-art linear convergence rate on strongly convex-strongly concave DRO problems witha fine-grained dependency on primal and dual condition numbers. The theoretical results are supported with numerical benchmarks on regression and classification tasks.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-103" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-103', event_id='94401', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5904</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94401">Lookback Prophet Inequalities</a></strong></h5>


                        <p class="text-muted">
                            Ziyad Benomar &middot; Dorian Baudry &middot; Vianney Perchet
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Prophet inequalities are fundamental optimal stopping problems, where a decision-maker observes sequentially items with values sampled independently from known distributions, and must decide at each new observation to either stop and gain the current value or reject it irrevocably and move to the next step. This model is often too pessimistic and does not adequately represent real-world online selection processes. Potentially, rejectesd items can be revisited and a fraction of their value can be recovered. To analyze this problem, we consider general decay functions $D_1,D_2,\ldots$, quantifying the value to be recovered from a rejected item, depending on how far it has been observed in the past. We analyze how lookback improves, or not, the competitive ratio in prophet inequalities in different order models. We show that, under mild monotonicity assumptions on the decay functions, the problem can be reduced to the case where all the decay functions are equal to the same function $x \mapsto \gamma x$, where $\gamma = \inf_{x>0} \inf_{j \geq 1} D_j(x)/x$. Consequently, we focus on this setting and refine the analyses of the competitive ratios, with upper and lower bounds expressed as increasing functions of $\gamma$.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-104" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-104', event_id='93314', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5905</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93314">Adaptive Variance Reduction for Stochastic Optimization under Weaker Assumptions</a></strong></h5>


                        <p class="text-muted">
                            Wei Jiang &middot; Sifan Yang &middot; Yibo Wang &middot; Lijun Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>This paper explores adaptive variance reduction methods for stochastic optimization based on the STORM technique. Existing adaptive extensions of STORM rely on strong assumptions like bounded gradients and bounded function values, or suffer an additional $\mathcal{O}(\log T)$ term in the convergence rate. To address these limitations, we introduce a novel adaptive STORM method that achieves an optimal convergence rate of $\mathcal{O}(T^{-1/3})$ for non-convex functions with our newly designed learning rate strategy. Compared with existing approaches, our method requires weaker assumptions and attains the optimal convergence rate without the additional $\mathcal{O}(\log T)$ term. We also extend the proposed technique to stochastic compositional optimization, obtaining the same optimal rate of $\mathcal{O}(T^{-1/3})$. Furthermore, we investigate the non-convex finite-sum problem and develop another innovative adaptive variance reduction method that achieves an optimal convergence rate of $\mathcal{O}(n^{1/4} T^{-1/2} )$, where $n$ represents the number of component functions. Numerical experiments across various tasks validate the effectiveness of our method.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-105" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-105', event_id='95610', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5906</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95610">Non-geodesically-convex optimization in the Wasserstein space</a></strong></h5>


                        <p class="text-muted">
                            Hoang Phuc Hau Luu &middot; Hanlin Yu &middot; Bernardo Williams &middot; Petrus Mikkola &middot; Marcelo Hartmann &middot; Kai Puolamki &middot; Arto Klami
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We study a class of optimization problems in the Wasserstein space (the space of probability measures) where the objective function is nonconvex along generalized geodesics. Specifically, the objective exhibits some difference-of-convex structure along these geodesics. The setting also encompasses sampling problems where the logarithm of the target distribution is difference-of-convex. We derive multiple convergence insights for a novel semi Forward-Backward Euler scheme under several nonconvex (and possibly nonsmooth) regimes. Notably, the semi Forward-Backward Euler is just a slight modification of the Forward-Backward Euler whose convergence is---to our knowledge---still unknown in our very general non-geodesically-convex setting.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-106" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-106', event_id='93670', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5907</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93670">DistrictNet: Decision-aware learning for geographical districting</a></strong></h5>


                        <p class="text-muted">
                            Cheikh Ahmed &middot; Alexandre Forel &middot; Axel Parmentier &middot; Thibaut Vidal
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Districting is a complex combinatorial problem that consists in partitioning a geographical area into small districts. In logistics, it is a major strategic decision determining operating costs for several years. Solving districting problems using traditional methods is intractable even for small geographical areas and existing heuristics often provide sub-optimal results. We present a structured learning approach to find high-quality solutions to real-world districting problems in a few minutes. It is based on integrating a combinatorial optimization layer, the capacitated minimum spanning tree problem, into a graph neural network architecture. To train this pipeline in a decision-aware fashion, we show how to construct target solutions embedded in a suitable space and learn from target solutions. Experiments show that our approach outperforms existing methods as it can significantly reduce costs on real-world cities.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-107" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-107', event_id='96423', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5908</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96423">Performative Control for Linear Dynamical Systems</a></strong></h5>


                        <p class="text-muted">
                            Songfu Cai &middot; Fei Han &middot; Xuanyu Cao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce the framework of performative control, where the policy chosen by the controller affects the underlying dynamics of the control system. This results in a sequence of policy-dependent system state data with policy-dependent temporal correlations. Following the recent literature on performative prediction \cite{perdomo2020performative}, we introduce the concept of a performatively stable control (PSC) solution. We first propose a sufficient condition for the performative control problem to admit a unique PSC solution with a problem-specific structure of distributional sensitivity propagation and aggregation. We further analyze the impacts of system stability on the existence of the PSC solution. Specifically, for almost surely stable policy-dependent dynamics, the PSC solution exists if the sum of the distributional sensitivities is small enough. However, for almost surely unstable policy-dependent dynamics, the existence of the PSC solution will necessitate a temporally backward decaying of the distributional sensitivities. We finally provide a repeated stochastic gradient descent scheme that converges to the PSC solution and analyze its non-asymptotic convergence rate. Numerical results validate our theoretical analysis.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-108" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-108', event_id='96724', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5909</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96724">HyperPrism: An Adaptive Non-linear Aggregation Framework for Distributed Machine Learning over Non-IID Data and Time-varying Communication Links</a></strong></h5>


                        <p class="text-muted">
                            Haizhou Du &middot; Yijian Chen &middot; Ryan Yang &middot; Yuchen Li &middot; Linghe Kong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While Distributed Machine Learning (DML) has been widely used to achieve decent performance, it is still challenging to take full advantage of data and devices distributed at multiple vantage points to adapt and learn, especially it is non-trivial to address dynamic and divergence challenges based on the linear aggregation framework as follows: (1) heterogeneous learning data at different devices (i.e., non-IID data) resulting in model divergence and (2) in the case of time-varying communication links, the limited ability for devices to reconcile model divergence. In this paper, we contribute a non-linear class aggregation framework HyperPrism that leverages distributed mirror descent with averaging done in the mirror descent dual space and adapts the degree of Weighted Power Mean (WPM) used in each round. Moreover, HyperPrism could adaptively choose different mapping for different layers of the local model with a dedicated hypernetwork per device, achieving automatic optimization of DML in high divergence settings. We perform rigorous analysis and experimental evaluations to demonstrate the effectiveness of adaptive, mirror-mapping DML. In particular, we extend the generalizability of existing related works and position them as special cases within HyperPrism. Our experimental results show that HyperPrism can improve the convergence speed up to 98.63% and scale well to more devices compared with the state-of-the-art, all with little additional computation overhead compared to traditional linear aggregation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-109" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-109', event_id='93167', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5910</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93167">Communication Efficient Distributed Training with Distributed Lion</a></strong></h5>


                        <p class="text-muted">
                            Bo Liu &middot; Lemeng Wu &middot; Lizhang Chen &middot; Kaizhao Liang &middot; Jiaxu Zhu &middot; Chen Liang &middot; Raghuraman Krishnamoorthi &middot; Qiang Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages in memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires to communicate binary or lower-precision vectorsbetween workers to the center server, significantly reducing the communication cost.  Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large models. In addition, we also demonstrate that \mavolion{} presents a more favorable performance-bandwidth balance compared to existing efficient distributed methods such as deep gradient compression and ternary gradients.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-110" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-110', event_id='93233', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#5911</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93233">Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML</a></strong></h5>


                        <p class="text-muted">
                            Tehila Dahan &middot; Kfir Y. Levy
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We address the challenges of Byzantine-robust training in asynchronous distributed machine learning systems, aiming to enhance efficiency amid massive parallelization and heterogeneous compute resources. Asynchronous systems, marked by independently operating workers and intermittent updates, uniquely struggle with maintaining integrity against Byzantine failures, which encompass malicious or erroneous actions that disrupt learning. The inherent delays in such settings not only introduce additional bias to the system but also obscure the disruptions caused by Byzantine faults. To tackle these issues, we adapt the Byzantine framework to asynchronous dynamics by introducing a novel weighted robust aggregation framework. This allows for the extension of robust aggregators and a recent meta-aggregator to their weighted versions, mitigating the effects of delayed updates. By further incorporating a recent variance-reduction technique, we achieve an optimal convergence rate for the first time in an asynchronous Byzantine environment. Our methodology is rigorously validated through empirical and theoretical analysis, demonstrating its effectiveness in enhancing fault tolerance and optimizing performance in asynchronous ML systems.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-111" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-111', event_id='96120', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6000</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96120">The Limits of Differential Privacy in Online Learning</a></strong></h5>


                        <p class="text-muted">
                            Bo Li &middot; Wei Wang &middot; Peng Ye
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Differential privacy (DP) is a formal notion that restricts the privacy leakage of an algorithm when running on sensitive data, in which privacy-utility trade-off is one of the central problems in private data analysis. In this work, we investigate the fundamental limits of differential privacy in online learning algorithms and present evidence that separates three types of constraints: no DP, pure DP, and approximate DP. We first describe a hypothesis class that is online learnable under approximate DP but not online learnable under pure DP under the adaptive adversarial setting. This indicates that approximate DP must be adopted when dealing with adaptive adversaries. We then prove that any private online learner must make an infinite number of mistakes for almost all hypothesis classes. This essentially generalizes previous results and shows a strong separation between private and non-private settings since a finite mistake bound is always attainable (as long as the class is online learnable) when there is no privacy requirement.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-112" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-112', event_id='95798', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6001</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95798">Mitigating Biases in Blackbox Feature Extractors for Image Classification Tasks</a></strong></h5>


                        <p class="text-muted">
                            Abhipsa Basu &middot; Saswat Subhajyoti Mallick &middot; Venkatesh Babu R
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In image classification, it is common to utilize a pretrained model to extract meaningful features of the input images, and then to train a classifier on top of it to make predictions for any downstream task. Trained on enormous amounts of data, these models have been shown to contain harmful biases which can hurt their performance when adapted for a downstream classification task. Further, very often they may be blackbox, either due to scale, or because of unavailability of model weights or architecture. Thus, during a downstream task, we cannot debias such models by updating the weights of the feature encoder, as only the classifier can be finetuned. In this regard, we investigate the suitability of some existing debiasing techniques and thereby motivate the need for more focused research towards this problem setting. Furthermore, we propose a simple method consisting of a clustering-based adaptive margin loss with a blackbox feature encoder, with no knowledge of the bias attribute. Our experiments demonstrate the effectiveness of our method across multiple benchmarks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-113" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-113', event_id='95064', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6002</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95064">Differentially Private Reinforcement Learning with Self-Play</a></strong></h5>


                        <p class="text-muted">
                            Dan Qiao &middot; Yu-Xiang Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We study the problem of multi-agent reinforcement learning (multi-agent RL) with differential privacy (DP) constraints. This is well-motivated by various real-world applications involving sensitive data, where it is critical to protect users' private information. We first extend the definitions of Joint DP (JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where both definitions ensure trajectory-wise privacy protection. Then we design a provably efficient algorithm based on optimistic Nash value iteration and privatization of Bernstein-type bonuses.  The algorithm is able to satisfy JDP and LDP requirements when instantiated with appropriate privacy mechanisms. Furthermore, for both notions of DP, our regret bound generalizes the best known result under the single-agent RL case, while our regret could also reduce to the best known result for multi-agent RL without privacy constraints. To the best of our knowledge, these are the first results towards understanding trajectory-wise privacy protection in multi-agent RL.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-114" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-114', event_id='94531', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6003</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94531">Differentially Private Graph Diffusion with Applications in Personalized PageRanks</a></strong></h5>


                        <p class="text-muted">
                            Rongzhe Wei &middot; Eli Chien &middot; Pan Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Graph diffusion, which iteratively propagates real-valued substances among the graph, is used in numerous graph/network-involved applications. However, releasing diffusion vectors may reveal sensitive linking information in the data such as transaction information in financial network data. However, protecting the privacy of graph data is challenging due to its interconnected nature.    This work proposes a novel graph diffusion framework with edge-level different privacy guarantees by using noisy diffusion iterates.    The algorithm injects Laplace noise per diffusion iteration and adopts a degree-based thresholding function to mitigate the high sensitivity induced by low-degree nodes. Our privacy loss analysis is based on Privacy Amplification by Iteration (PABI), which to our best knowledge, is the first effort that analyzes PABI with Laplace noise and provides relevant applications.    We also introduce a novel $\infty$-Wasserstein distance tracking method, which tightens the analysis of privacy leakage and makes PABI more applicable in practice.     We evaluate this framework by applying it to Personalized Pagerank computation for ranking tasks. Experiments on real-world network data demonstrate the superiority of our method under stringent privacy conditions.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-115" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-115', event_id='94074', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6004</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94074">Faster Algorithms for User-Level Private Stochastic Convex Optimization</a></strong></h5>


                        <p class="text-muted">
                            Andrew Lowy &middot; Daogao Liu &middot; Hilal Asi
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We study private stochastic convex optimization (SCO) under user-level differential privacy (DP) constraints. In this setting, there are $n$ users (e.g., cell phones), each possessing $m$ data items (e.g., text messages), and we need to protect the privacy of each user's entire collection of data items. Existing algorithms for user-level DP SCO are impractical in many large-scale machine learning scenarios because: (i) they make restrictive assumptions on the smoothness parameter of the loss function and require the number of users to grow polynomially with the dimension of the parameter space; or (ii) they are prohibitively slow, requiring at least $(mn)^{3/2}$ gradient computations for smooth losses and $(mn)^3$ computations for non-smooth losses. To address these limitations, we provide novel user-level DP algorithms with state-of-the-art excess risk and runtime guarantees, without stringent assumptions. First, we develop a linear-time algorithm with state-of-the-art excess risk (for a non-trivial linear-time algorithm) under a mild smoothness assumption. Our second algorithm applies to arbitrary smooth losses and achieves optimal excess risk in $\approx (mn)^{9/8}$ gradient computations. Third, for non-smooth loss functions, we obtain optimal excess risk in $n^{11/8} m^{5/4}$ gradient computations. Moreover, our algorithms do not require the number of users to grow polynomially with the dimension.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-116" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-116', event_id='93891', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6005</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93891">Dimension-free Private Mean Estimation for Anisotropic Distributions</a></strong></h5>


                        <p class="text-muted">
                            Yuval Dagan &middot; Michael Jordan &middot; Xuelin Yang &middot; Lydia Zakynthinou &middot; Nikita Zhivotovskiy
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We present differentially private algorithms for high-dimensional mean estimation. Previous private estimators on distributions over $\mathbb{R}^d$ suffer from a curse of dimensionality, as they require $\Omega(d^{1/2})$ samples to achieve non-trivial error, even in cases where $O(1)$ samples suffice without privacy. This rate is  unavoidable when the distribution is isotropic, namely, when the covariance is a multiple of the identity matrix. Yet, real-world data is often highly anisotropic, with signals concentrated on a small number of principal components. We develop estimators that are appropriate for such signals---our estimators are $(\varepsilon,\delta)$-differentially private and have sample complexity that is dimension-independent for anisotropic subgaussian distributions.  Given $n$ samples from a distribution with known covariance-proxy $\Sigma$ and unknown mean $\mu$, we present an estimator $\hat{\mu}$ that achieves error, $\|\hat{\mu}-\mu\|_2\leq \alpha$, as long as $n\gtrsim \text{tr}(\Sigma)/\alpha^2+ \text{tr}(\Sigma^{1/2})/(\alpha\varepsilon)$. We show that this is the optimal sample complexity for this task up to logarithmic factors. Moreover, for the case of unknown covariance, we present an algorithm whose sample complexity has improved dependence on the dimension, from $d^{1/2}$ to $d^{1/4}$.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-117" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-117', event_id='96709', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6006</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96709">OASIS: Conditional Distribution Shaping for Offline Safe Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Yihang Yao &middot; Zhepeng Cen &middot; Wenhao Ding &middot; Haohong Lin &middot; Shiqi Liu &middot; Tingnan Zhang &middot; Wenhao Yu &middot; DING ZHAO
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Offline safe reinforcement learning (RL) aims to train a policy that satisfies con- straints using a pre-collected dataset. Most current methods struggle with the mismatch between imperfect demonstrations and the desired safe and rewarding performance. In this paper, we mitigate this issue from a data-centric perspective and introduce OASIS (cOnditionAl diStributIon Shaping), a new paradigm in offline safe RL designed to overcome these critical limitations. OASIS utilizes a conditional diffusion model to synthesize offline datasets, thus shaping the data dis- tribution toward a beneficial target domain. Our approach makes compliance with safety constraints through effective data utilization and regularization techniques to benefit offline safe RL training. Comprehensive evaluations on public benchmarks and varying datasets showcase OASISs superiority in benefiting offline safe RL agents to achieve high-reward behavior while satisfying the safety constraints, out- performing established baselines. Furthermore, OASIS exhibits high data efficiency and robustness, making it suitable for real-world applications, particularly in tasks where safety is imperative and high-quality demonstrations are scarce. More details are available at the website https://sites.google.com/view/saferl-oasis/home.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-118" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-118', event_id='96479', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6008</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96479">Zero-Shot Reinforcement Learning from Low Quality Data</a></strong></h5>


                        <p class="text-muted">
                            Scott Jeen &middot; Tom Bewley &middot; Jonathan Cullen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Zero-shot reinforcement learning (RL) promises to provide agents that can perform <em>any</em> task in an environment after an offline, reward-free pre-training phase. Methods leveraging successor measures and successor features have shown strong performance in this setting, but require access to large heterogenous datasets for pre-training which cannot be expected for most real problems. Here, we explore how the performance of zero-shot RL methods degrades when trained on small homogeneous datasets, and propose fixes inspired by <em>conservatism</em>, a well-established feature of performant single-task offline RL algorithms. We evaluate our proposals across various datasets, domains and tasks, and show that conservative zero-shot RL algorithms outperform their non-conservative counterparts on low quality datasets, and perform no worse on high quality datasets. Somewhat surprisingly, our proposals also outperform baselines that get to see the task during training. Our code is available via the project page https://enjeeneer.io/projects/zero-shot-rl/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-119" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-119', event_id='96114', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6009</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96114">Simplifying Latent Dynamics with Softly State-Invariant World Models</a></strong></h5>


                        <p class="text-muted">
                            Tankred Saanum &middot; Peter Dayan &middot; Eric Schulz
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>To solve control problems via model-based reasoning or planning, an agent needs to know how its actions affect the state of the world. The actions an agent has at its disposal often change the state of the environment in systematic ways. However, existing techniques for world modelling do not guarantee that the effect of actions are represented in such systematic ways. We introduce the Parsimonious Latent Space Model (PLSM), a world model that regularizes the latent dynamics to make the effect of the agent's actions more predictable. Our approach minimizes the mutual information between latent states and the change that an action produces in the agent's latent state, in turn minimizing the dependence the state has on the dynamics. This makes the world model softly state-invariant. We combine PLSM with different model classes used for i) future latent state prediction, ii) planning, and iii) model-free reinforcement learning. We find that our regularization improves accuracy, generalization, and performance in downstream tasks, highlighting the importance of systematic treatment of actions in world models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-120" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-120', event_id='94884', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6010</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94884">AlphaMath Almost Zero: Process Supervision without Process</a></strong></h5>


                        <p class="text-muted">
                            Guoxin Chen &middot; Minpeng Liao &middot; Chengxi Li &middot; Kai Fan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Although recent advancements in large language models (LLMs) have significantly improved their performance on various tasks, they still face challenges with complex and symbolic multi-step reasoning, particularly in mathematical reasoning. To bolster the mathematical reasoning capabilities of LLMs, most existing efforts concentrate on seeking assistance from either domain experts or GPT-4 for high-quality process-supervised data, which is not only expensive but also labor-intensive. In our study, we propose an innovative framework, AlphaMath, that bypasses the need for process annotations (from humans or GPTs) by leveraging Monte Carlo Tree Search (MCTS). This framework focuses on unleashing the potential of a well-pretrained LLM to autonomously enhance its mathematical reasoning. Specifically, we integrate a value model with the LLM, automatically generating both process supervision and step-level evaluation signals in MCTS. Furthermore, we propose an efficient inference strategystep-level beam search, where the value model is crafted to assist the policy model (i.e., LLM) in navigating more effective reasoning paths, rather than solely relying on prior probabilities. The experimental results on both in-domain and out-of-domain datasets demonstrate that even without GPT-4 or human-annotated process supervision, our AlphaMath framework achieves comparable or superior results to previous state-of-the-art methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-121" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-121', event_id='94254', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6011</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94254">Adversarial Environment Design via Regret-Guided Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Hojun Chung &middot; Junseo Lee &middot; Minsoo Kim &middot; Dohyeong Kim &middot; Songhwai Oh
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Training agents that are robust to environmental changes remains a significant challenge in deep reinforcement learning (RL). Unsupervised environment design (UED) has recently emerged to address this issue by generating a set of training environments tailored to the agent's capabilities. While prior works demonstrate that UED has the potential to learn a robust policy, their performance is constrained by the capabilities of the environment generation. To this end, we propose a novel UED algorithm, adversarial environment design via regret-guided diffusion models (ADD). The proposed method guides the diffusion-based environment generator with the regret of the agent to produce environments that the agent finds challenging but conducive to further improvement. By exploiting the representation power of diffusion models, ADD can directly generate adversarial environments while maintaining the diversity of training environments, enabling the agent to effectively learn a robust policy. Our experimental results demonstrate that the proposed method successfully generates an instructive curriculum of environments, outperforming UED baselines in zero-shot generalization across novel, out-of-distribution environments.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-122" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-122', event_id='93924', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6012</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93924">Solving Minimum-Cost Reach Avoid using Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Oswin So &middot; Cheng Ge &middot; Chuchu Fan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Current reinforcement-learning methods are unable to directly learn policies that solve the minimum cost reach-avoid problem to minimize cumulative costs subject to the constraints of reaching the goal and avoiding unsafe states, as the structure of this new optimization problem is incompatible with current methods. Instead, a surrogate problem is solved where all objectives are combined with a weighted sum. However, this surrogate objective results in suboptimal policies that do not directly minimize the cumulative cost. In this work, we propose RC-PPO, a reinforcement-learning-based method for solving the minimum-cost reach-avoid problem by using connections to Hamilton-Jacobi reachability. Empirical results demonstrate that RC-PPO learns policies with comparable goal-reaching rates to while achieving up to 57% lower cumulative costs compared to existing methods on a suite of minimum-cost reach-avoid benchmarks on the Mujoco simulator. The project page can be found at https://oswinso.xyz/rcppo.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-123" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-123', event_id='95215', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6101</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95215">Multi-Agent Imitation Learning: Value is Easy, Regret is Hard</a></strong></h5>


                        <p class="text-muted">
                            Jingwu Tang &middot; Gokul Swamy &middot; Fei Fang &middot; Steven Wu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We study a multi-agent imitation learning (MAIL) problem where we take the perspective of a learner attempting to <em>coordinate</em> a group of agents based on demonstrations of an expert doing so. Most prior work in MAIL essentially reduces the problem to matching the behavior of the expert <em>within</em> the support of the demonstrations. While doing so is sufficient to drive the <em>value gap</em> between the learner and the expert to zero under the assumption that agents are non-strategic, it does not guarantee robustness to deviations by strategic agents. Intuitively, this is because strategic deviations can depend on a counterfactual quantity: the coordinator's recommendations outside of the state distribution their recommendations induce. In response, we initiate the study of an alternative objective for MAIL in Markov Games we term the <em>regret gap</em> that explicitly accounts for potential deviations by agents in the group. We first perform an in-depth exploration of the relationship between the value and regret gaps. First, we show that while the value gap can be efficiently minimized via a direct extension of single-agent IL algorithms, even <em>value equivalence</em> can lead to an arbitrarily large regret gap. This implies that achieving regret equivalence is harder than achieving value equivalence in MAIL. We then provide a pair of efficient reductions to no-regret online convex optimization that are capable of minimizing the regret gap <em>(a)</em> under a coverage assumption on the expert (MALICE) or <em>(b)</em> with access to a queryable expert (BLADES).</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-124" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-124', event_id='95347', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6102</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95347">Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Hao Ma &middot; Tianyi Hu &middot; Zhiqiang Pu &middot; Liu Boyin &middot; Xiaolin Ai &middot; Yanyan Liang &middot; Min Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Reinforcement learning (RL) has emerged as a pivotal technique for fine-tuning large language models (LLMs) on specific tasks. However, prevailing RL fine-tuning methods predominantly rely on PPO and its variants. Though these algorithms are effective in general RL settings, they often exhibit suboptimal performance and vulnerability to distribution collapse when applied to the fine-tuning of LLMs. In this paper, we propose CORY, extending the RL fine-tuning of LLMs to a sequential cooperative multi-agent reinforcement learning framework, to leverage the inherent coevolution and emergent capabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is initially duplicated into two autonomous agents: a pioneer and an observer. The pioneer generates responses based on queries, while the observer generates responses using both the queries and the pioneers responses. The two agents are trained together. During training, the agents exchange roles periodically, fostering cooperation and coevolution between them. Experiments evaluate CORY's performance by fine-tuning GPT-2 and Llama-2 under subjective and objective reward functions on the IMDB Review and GSM8K datasets, respectively. Results show that CORY outperforms PPO in terms of policy optimality, resistance to distribution collapse, and training robustness, thereby underscoring its potential as a superior methodology for refining LLMs in real-world applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-125" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-125', event_id='95522', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6103</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95522">Boosting Sample Efficiency and Generalization in Multi-agent Reinforcement Learning via Equivariance</a></strong></h5>


                        <p class="text-muted">
                            Josh McClellan &middot; Naveed Haghani &middot; John Winder &middot; Furong Huang &middot; Pratap Tokekar
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multi-Agent Reinforcement Learning (MARL) struggles with sample inefficiency and poor generalization [1]. These challenges are partially due to a lack of structure or inductive bias in the neural networks typically used in learning the policy. One such form of structure that is commonly observed in multi-agent scenarios is symmetry. The field of Geometric Deep Learning has developed Equivariant Graph Neural Networks (EGNN) that are equivariant (or symmetric) to rotations, translations, and reflections of nodes. Incorporating equivariance has been shown to improve learning efficiency and decrease error [ 2 ]. In this paper, we demonstrate that EGNNs improve the sample efficiency and generalization in MARL. However, we also show that a naive application of EGNNs to MARL results in poor early exploration due to a bias in the EGNN structure. To mitigate this bias, we present Exploration-enhanced Equivariant Graph Neural Networks or E2GN2. We compare E2GN2 to other common function approximators using common MARL benchmarks MPE and SMACv2. E2GN2 demonstrates a significant improvement in sample efficiency, greater final reward convergence, and a 2x-5x gain in over standard GNNs in our generalization tests. These results pave the way for more reliable and effective solutions in complex multi-agent systems.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-126" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-126', event_id='95865', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6104</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95865">Value-Based Deep Multi-Agent Reinforcement Learning with Dynamic Sparse Training</a></strong></h5>


                        <p class="text-muted">
                            Pihe Hu &middot; Shaolong Li &middot; Zhuoran Li &middot; Ling Pan &middot; Longbo Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Deep Multi-agent Reinforcement Learning (MARL) relies on neural networks with numerous parameters in multi-agent scenarios, often incurring substantial computational overhead. Consequently, there is an urgent need to expedite training and enable model compression in MARL. This paper proposes the utilization of dynamic sparse training (DST), a technique proven effective in deep supervised learning tasks, to alleviate the computational burdens in MARL training. However, a direct adoption of DST fails to yield satisfactory MARL agents, leading to breakdowns in value learning within deep sparse value-based MARL models. Motivated by this challenge, we introduce an innovative Multi-Agent Sparse Training (MAST) framework aimed at simultaneously enhancing the reliability of learning targets and the rationality of sample distribution to improve value learning in sparse models. Specifically, MAST incorporates the Soft Mellowmax Operator with a hybrid TD-($\lambda$) schema to establish dependable learning targets. Additionally, it employs a dual replay buffer mechanism to enhance the distribution of training samples. Building upon these aspects, MAST utilizes gradient-based topology evolution to exclusively train multiple MARL agents using sparse networks. Our comprehensive experimental investigation across various value-based MARL algorithms on multiple benchmarks demonstrates, for the first time, significant reductions in redundancy of up to $20\times$ in Floating Point Operations (FLOPs) for both training and inference, with less than 3% performance degradation.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-127" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-127', event_id='96274', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6105</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96274">Feint Behaviors and Strategies: Formalization, Implementation and Evaluation</a></strong></h5>


                        <p class="text-muted">
                            Junyu Liu &middot; Xiangjun Peng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Feint behaviors refer to a set of deceptive behaviors in a nuanced manner, which enable players to obtain temporal and spatial advantages over opponents in competitive games. Such behaviors are crucial tactics in most competitive multi-player games (e.g., boxing, fencing, basketball, motor racing, etc.). However, existing literature does not provide a comprehensive (and/or concrete) formalization for Feint behaviors, and their implications on game strategies. In this work, we introduce the first comprehensive formalization of Feint behaviors at both action-level and strategy-level, and provide concrete implementation and quantitative evaluation of them in multi-player games. The key idea of our work is to (1) allow automatic generation of Feint behaviors via Palindrome-directed templates, combine them into meaningful behavior sequences via a Dual-Behavior Model; (2) concertize the implications from our formalization of Feint on game strategies, in terms of temporal, spatial, and their collective impacts respectively; and (3) provide a unified implementation scheme of Feint behaviors in existing MARL frameworks. The experimental results show that our design of Feint behaviors can (1) greatly improve the game reward gains; (2) significantly improve the diversity of Multi-Player Games; and (3) only incur negligible overheads in terms of time consumption.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-128" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-128', event_id='96673', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6106</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96673">The Dormant Neuron Phenomenon in Multi-Agent Reinforcement Learning Value Factorization</a></strong></h5>


                        <p class="text-muted">
                            Haoyuan Qin &middot; Chennan Ma &middot; Deng &middot; Zhengzhu Liu &middot; Songzhu Mei &middot; Xinwang Liu &middot; Cheng Wang &middot; Siqi Shen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this work, we study the dormant neuron phenomenon in multi-agent reinforcement learning value factorization, where the mixing network suffers from reduced network expressivity caused by an increasing number of inactive neurons. We demonstrate the presence of the dormant neuron phenomenon across multiple environments and algorithms, and show that this phenomenon negatively affects the learning process. We show that dormant neurons correlates with the existence of over-active neurons, which have large activation scores. To address the dormant neuron issue, we propose ReBorn, a simple but effective method that transfers the weights from over-active neurons to dormant neurons. We theoretically show that this method can ensure the learned action preferences are not forgotten after the weight-transferring procedure, which increases learning effectiveness. Our extensive experiments reveal that ReBorn achieves promising results across various environments and improves the performance of multiple popular value factorization approaches. The source code of ReBorn is available in \url{https://github.com/xmu-rl-3dv/ReBorn}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-129" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-129', event_id='96810', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6107</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96810">Aligning Individual and Collective Objectives in Multi-Agent Cooperation</a></strong></h5>


                        <p class="text-muted">
                            Yang Li &middot; Wenhao Zhang &middot; Jianhong Wang &middot; Shao Zhang &middot; Yali Du &middot; Ying Wen &middot; Wei Pan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Among the research topics in multi-agent learning, mixed-motive cooperation is one of the most prominent challenges, primarily due to the mismatch between individual and collective goals. The cutting-edge research is focused on incorporating domain knowledge into rewards and introducing additional mechanisms to incentivize cooperation. However, these approaches often face shortcomings such as the effort on manual design and the absence of theoretical groundings. To close this gap, we model the mixed-motive game as a differentiable game for the ease of illuminating the learning dynamics towards cooperation. More detailed, we introduce a novel optimization method named \textbf{\textit{A}}ltruistic \textbf{\textit{G}}radient \textbf{\textit{A}}djustment (\textbf{\textit{AgA}}) that employs gradient adjustments to progressively align individual and collective objectives. Furthermore, we theoretically prove that AgA effectively attracts gradients to stable fixed points of the collective objective while considering individual interests, and we validate these claims with empirical evidence. We evaluate the effectiveness of our algorithm AgA through benchmark environments for testing mixed-motive collaboration with small-scale agents such as the two-player public good game and the sequential social dilemma games, Cleanup and Harvest, as well as our self-developed large-scale environment in the game StarCraft II.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-130" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-130', event_id='97511', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6108</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97511">AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making</a></strong></h5>


                        <p class="text-muted">
                            Yizhe Huang &middot; Xingbo Wang &middot; Hao Liu &middot; Fanqi Kong &middot; Aoyang Qin &middot; Min Tang &middot; Xiaoxi Wang &middot; Song-Chun Zhu &middot; Mingjie Bi &middot; Siyuan Qi &middot; Xue Feng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors.To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake.In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://anonymous.4open.science/r/AdaSociety-447A.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-131" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-131', event_id='95594', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6109</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95594">Near-Optimal Dynamic Regret for Adversarial Linear Mixture MDPs</a></strong></h5>


                        <p class="text-muted">
                            Long-Fei Li &middot; Peng Zhao &middot; Zhi-Hua Zhou
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We study episodic linear mixture MDPs with the unknown transition and adversarial rewards under full-information feedback, employing *dynamic regret* as the performance measure. We start with in-depth analyses of the strengths and limitations of the two most popular methods: occupancy-measure-based and policy-based methods. We observe that while the occupancy-measure-based method is effective in addressing non-stationary environments, it encounters difficulties with the unknown transition. In contrast, the policy-based method can deal with the unknown transition effectively but faces challenges in handling non-stationary environments. Building on this, we propose a novel algorithm that combines the benefits of both methods. Specifically, it employs (i) an *occupancy-measure-based global optimization* with a two-layer structure to handle non-stationary environments; and (ii) a *policy-based variance-aware value-targeted regression* to tackle the unknown transition. We bridge these two parts by a novel conversion. Our algorithm enjoys an $\widetilde{\mathcal{O}}(d \sqrt{H^3 K} + \sqrt{HK(H + \bar{P}_K)})$ dynamic regret, where $d$ is the feature mapping dimension, $H$ is the episode length, $K$ is the number of episodes, $\bar{P}_K$ is the non-stationarity measure. We show it is minimax optimal up to logarithmic factors by establishing a matching lower bound. To the best of our knowledge, this is the **first** work that achieves **near-optimal** dynamic regret for adversarial linear mixture MDPs with the unknown transition without prior knowledge of the non-stationarity measure.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-132" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-132', event_id='92947', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6110</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92947">GenRL: Multimodal-foundation world models for generalization in embodied agents</a></strong></h5>


                        <p class="text-muted">
                            Pietro Mazzaglia &middot; Tim Verbelen &middot; Bart Dhoedt &middot; Aaron Courville &middot; Sai Rajeswar Mudumba
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Learning generalist embodied agents, able to solve multitudes of tasks in different domains is a long-standing problem. Reinforcement learning (RL) is hard to scale up as it requires a complex reward design for each task. In contrast, language can specify tasks in a more natural way. Current foundation vision-language models (VLMs) generally require fine-tuning or other adaptations to be adopted in embodied contexts, due to the significant domain gap. However, the lack of multimodal data in such domains represents an obstacle to developing foundation models for embodied applications. In this work, we overcome these problems by presenting multimodal-foundation world models, able to connect and align the representation of foundation VLMs with the latent space of generative world models for RL, without any language annotations. The resulting agent learning framework, GenRL, allows one to specify tasks through vision and/or language prompts, ground them in the embodied domains dynamics, and learn the corresponding behaviors in imagination.As assessed through large-scale multi-task benchmarking in locomotion and manipulation domains, GenRL enables multi-task generalization from language and visual prompts. Furthermore, by introducing a data-free policy learning strategy, our approach lays the groundwork for foundational policy learning using generative world models. Website, code and data: https://mazpie.github.io/genrl/</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-133" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-133', event_id='95030', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6200</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95030">What type of inference is planning?</a></strong></h5>


                        <p class="text-muted">
                            Miguel Lazaro-Gredilla &middot; Li Ku &middot; Kevin Murphy &middot; Dileep George
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multiple types of inference are available for probabilistic graphical models, e.g., marginal, maximum-a-posteriori, and even marginal maximum-a-posteriori. Which one do researchers mean when they talk about ``planning as inference''? There is no consistency in the literature, different types are used, and their ability to do planning is further entangled with specific approximations or additional constraints. In this work we use the variational framework to show that, just like all commonly used types of inference correspond to different weightings of the entropy terms in the variational problem, planning corresponds <em>exactly</em> to a <em>different</em> set of weights. This means that all the tricks of variational inference are readily applicable to planning. We develop an analogue of loopy belief propagation that allows us to perform approximate planning in factored-state Markov decisions processes without incurring intractability due to the exponentially large state space. The variational perspective shows that the previous types of inference for planning are only adequate in environments with low stochasticity, and allows us to characterize each type by its own merits, disentangling the type of inference from the additional approximations that its practical use requires. We validate these results empirically on synthetic MDPs and tasks posed in the International Planning Competition.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-134" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-134', event_id='94732', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6201</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94732">Goal Reduction with Loop-Removal Accelerates RL and Models Human Brain Activity in Goal-Directed Learning</a></strong></h5>


                        <p class="text-muted">
                            Huzi Cheng &middot; Joshua Brown
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Goal-directed planning presents a challenge for classical RL algorithms due to the vastness of the combinatorial state and goal spaces, while humans and animals adapt to complex environments, especially with diverse, non-stationary objectives, often employing intermediate goals for long-horizon tasks.Here, we propose a goal reduction mechanism for effectively deriving subgoals from arbitrary and distant original goals, using a novel loop-removal technique.The product of the method, called goal-reducer, distills high-quality subgoals from a replay buffer, all without the need for prior global environmental knowledge.Simulations show that the goal-reducer can be integrated into RL frameworks like Deep Q-learning and Soft Actor-Critic.It accelerates performance in both discrete and continuous action space tasks, such as grid world navigation and robotic arm manipulation, relative to the corresponding standard RL models.Moreover, the goal-reducer, when combined with a local policy, without iterative training, outperforms its integrated deep RL counterparts in solving a navigation task.This goal reduction mechanism also models human problem-solving.Comparing the model's performance and activation with human behavior and fMRI data in a treasure hunting task, we found matching representational patterns between an goal-reducer agent's components and corresponding human brain areas, particularly the vmPFC and basal ganglia. The results suggest that humans may use a similar computational framework for goal-directed behaviors.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-135" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-135', event_id='93892', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6202</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93892">Identifying Latent State-Transition Processes for Individualized Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Yuewen Sun &middot; Biwei Huang &middot; Yu Yao &middot; Donghuo Zeng &middot; Xinshuai Dong &middot; Songyao Jin &middot; Boyang Sun &middot; Roberto Legaspi &middot; Kazushi Ikeda &middot; Peter Spirtes &middot; Kun Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In recent years, the application of reinforcement learning (RL) involving interactions with individuals has seen significant growth. These interactions, influenced by individual-specific factors ranging from personal preferences to physiological differences, can causally affect state transitions, such as the health conditions in healthcare or learning progress in education. Consequently, different individuals may exhibit different state-transition processes. Understanding these individualized state-transition processes is crucial for optimizing individualized policies. In practice, however, identifying these state-transition processes is challenging, especially since individual-specific factors often remain latent. In this paper, we establish the identifiability of these latent factors and present a practical method that effectively learns these processes from observed state-action trajectories. Our experiments on various datasets show that our method can effectively identify the latent state-transition processes and help learn individualized RL policies.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-136" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-136', event_id='96099', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6204</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96099">Variational Delayed Policy Optimization</a></strong></h5>


                        <p class="text-muted">
                            Qingyuan Wu &middot; Simon Zhan &middot; Yixuan Wang &middot; Yuhui Wang &middot; Chung-Wei Lin &middot; Chen Lv &middot; Qi Zhu &middot; Chao Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In environments with delayed observation, state augmentation by including actions within the delay window is adopted to retrieve Markovian property to enable reinforcement learning (RL). Whereas, state-of-the-art (SOTA) RL techniques with Temporal-Difference (TD) learning frameworks commonly suffer from learning inefficiency, due to the significant expansion of the augmented state space with the delay. To improve the learning efficiency without sacrificing performance, this work novelly introduces Variational Delayed Policy Optimization (VDPO), reforming delayed RL as a variational inference problem. This problem is further modelled as a two-step iterative optimization problem, where the first step is TD learning in the delay-free environment with a small state space, and the second step is behaviour cloning which can be addressed much more efficiently than TD learning. We not only provide a theoretical analysis of VDPO in terms of sample complexity and performance, but also empirically demonstrate that VDPO can achieve consistent performance with SOTA methods, with a significant enhancement of sample efficiency (approximately 50\% less amount of samples) in the MuJoCo benchmark.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-137" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-137', event_id='95220', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6205</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95220">On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games</a></strong></h5>


                        <p class="text-muted">
                            Awni Altabaa &middot; Zhuoran Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In sequential decision-making problems, the <em>information structure</em> describes the causal dependencies between system variables, encompassing the dynamics of the environment and the agents' actions. Classical models of reinforcement learning (e.g., MDPs, POMDPs) assume a restricted and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decision-making problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure. In this paper, we formalize a novel reinforcement learning model which explicitly represents the information structure.We then use this model to carry out an information-structural analysis of the statistical complexity of general sequential decision-making problems, obtaining a characterization via a graph-theoretic quantity of the DAG representation of the information structure. We prove an upper bound on the sample complexity of learning a general sequential decision-making problem in terms of its information structure by exhibiting an algorithm achieving the upper bound. This recovers known tractability results and gives a novel perspective on reinforcement learning in general sequential decision-making problems, providing a systematic way of identifying new tractable classes of problems.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-138" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-138', event_id='95159', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6206</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95159">Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation</a></strong></h5>


                        <p class="text-muted">
                            Daehee Lee &middot; Minjong Yoo &middot; Woo Kyung Kim &middot; Wonje Choi &middot; Honguk Woo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Continual Imitation Learning (CiL) involves extracting and accumulating task knowledge from demonstrations across multiple stages and tasks to achieve a multi-task policy. With recent advancements in foundation models, there has been a growing interest in adapter-based CiL approaches, where adapters are established parameter-efficiently for tasks newly demonstrated. While these approaches isolate parameters for specific tasks and tend to mitigate catastrophic forgetting, they limit knowledge sharing among different demonstrations. We introduce IsCiL, an adapter-based CiL framework that addresses this limitation of knowledge sharing by incrementally learning shareable skills from different demonstrations, thus enabling sample-efficient task adaptation using the skills particularly in non-stationary CiL environments. In IsCiL, demonstrations are mapped into the state embedding space, where proper skills can be retrieved upon input states through prototype-based memory. These retrievable skills are incrementally learned on their corresponding adapters. Our CiL experiments with complex tasks in the Franka-Kitchen and Meta-World demonstrate the robust performance of IsCiL in both task adaptation and sample-efficiency. We also show a simple extension of IsCiL for task unlearning scenarios.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-139" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-139', event_id='92983', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6207</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92983">REBEL: Reinforcement Learning via Regressing Relative Rewards</a></strong></h5>


                        <p class="text-muted">
                            Zhaolin Gao &middot; Jonathan Chang &middot; Wenhao Zhan &middot; Owen Oertell &middot; Gokul Swamy &middot; Kiant Brantley &middot; Thorsten Joachims &middot; Drew Bagnell &middot; Jason Lee &middot; Wen Sun
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping), and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a <em>minimalist</em> RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the <em>relative reward</em> between two completions to a prompt in terms of the policy, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and be extended to handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally efficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong performance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard. Implementation of REBEL can be found at <a href="https://github.com/ZhaolinGao/REBEL">https://github.com/ZhaolinGao/REBEL</a>, and models trained by REBEL can be found at <a href="https://huggingface.co/Cornell-AGI">https://huggingface.co/Cornell-AGI</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-140" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-140', event_id='98307', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6208</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/98307">Nonparametric Regression for 3D Point Cloud Learning</a></strong></h5>


                        <p class="text-muted">
                            Xinyi Li &middot; Shan Yu &middot; Yueying Wang &middot; Guannan Wang &middot; Li Wang &middot; Ming-Jun Lai
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In recent years, there has been an exponentially increased amount of point clouds collected with irregular shapes in various areas. Motivated by the importance of solid modeling for point clouds, we develop a novel and efficient smoothing tool based on multivariate splines over the triangulation to extract the underlying signal and build up a 3D solid model from the point cloud. The proposed method can denoise or deblur the point cloud effectively, provide a multi-resolution reconstruction of the actual signal, and handle sparse and irregularly distributed point clouds to recover the underlying trajectory. In addition, our method provides a natural way of numerosity data reduction. We establish the theoretical guarantees of the proposed method, including the convergence rate and asymptotic normality of the estimator, and show that the convergence rate achieves optimal nonparametric convergence. We also introduce a bootstrap method to quantify the uncertainty of the estimators. Through extensive simulation studies and a real data example, we demonstrate the superiority of the proposed method over traditional smoothing methods in terms of estimation accuracy and efficiency of data reduction.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-141" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-141', event_id='96713', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6209</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96713">Conformal Classification with Equalized Coverage for Adaptively Selected Groups</a></strong></h5>


                        <p class="text-muted">
                            Yanfei Zhou &middot; Matteo Sesia
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper introduces a conformal inference method to evaluate uncertainty in classification by generating prediction sets with valid coverage conditional on adaptively chosen features. These features are carefully selected to reflect potential model limitations or biases. This can be useful to find a practical compromise between efficiency---by providing informative predictions---and algorithmic fairness---by ensuring equalized coverage for the most sensitive groups. We demonstrate the validity and effectiveness of this method on simulated and real data sets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-142" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-142', event_id='96567', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6210</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96567">Verifiably Robust Conformal Prediction</a></strong></h5>


                        <p class="text-muted">
                            Linus Jeary &middot; Tom Kuipers &middot; Mehran Hosseini &middot; Nicola Paoletti
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Conformal Prediction (CP) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable. In such a case, CP's prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability. Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage. Recently, several approaches have been put forward to recover CP guarantees in this setting. These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations. They are, however, limited in that they only support $\ell_2$-bounded perturbations and classification tasks. This paper introduces VRCP (Verifiably Robust Conformal Prediction), a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks. Our VRCP method is the first to support perturbations bounded by arbitrary norms including $\ell_1$, $\ell_2$, and $\ell_\infty$, as well as regression tasks. We evaluate and compare our approach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet) and regression tasks for deep reinforcement learning environments. In every case, VRCP achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the SotA.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-143" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-143', event_id='93610', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6300</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93610">Carrot and Stick: Eliciting Comparison Data and Beyond</a></strong></h5>


                        <p class="text-muted">
                            Yiling Chen &middot; Shi Feng &middot; Fang-Yi Yu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Comparison data elicited from people are fundamental to many machine learning tasks, including reinforcement learning from human feedback for large language models and estimating ranking models. They are typically subjective and not directly verifiable. How to truthfully elicit such comparison data from rational individuals? We design peer prediction mechanisms for eliciting comparison data using a bonus-penalty payment. Our design leverages on the strong stochastic transitivity for comparison data to create symmetrically strongly truthful mechanisms such that truth-telling 1) forms a strict Bayesian Nash equilibrium, and 2) yields the highest payment among all symmetric equilibria. Each individual only needs to evaluate one pair of items and report her comparison in our mechanism.We further extend the bonus-penalty payment concept to eliciting networked data, designing a symmetrically strongly truthful mechanism when agents private signals are sampled according to the Ising models. We provide the necessary and sufficient conditions for our bonus-penalty payment to have truth-telling as a strict Bayesian Nash equilibrium. Experiments on two real-world datasets further support our theoretical discoveries.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-144" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-144', event_id='94637', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6301</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94637">Learning the Expected Core of Strictly Convex Stochastic Cooperative Games</a></strong></h5>


                        <p class="text-muted">
                            Phuong Nam Tran &middot; The Anh Ta &middot; shuqing shi &middot; Debmalya Mandal &middot; Yali Du &middot; Long Tran-Thanh
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Reward allocation, also known as the credit assignment problem, has been an important topic in economics, engineering, and machine learning. An important concept in reward allocation is the core, which is the set of stable allocations where no agent has the motivation to deviate from the grand coalition. In previous works, computing the core requires either knowledge of the reward function in deterministic games or the reward distribution in stochastic games. However, this is unrealistic, as the reward function or distribution is often only partially known and may be subject to uncertainty. In this paper, we consider the core learning problem in stochastic cooperative games, where the reward distribution is unknown. Our goal is to learn the expected core, that is, the set of allocations that are stable in expectation, given an oracle that returns a stochastic reward for an enquired coalition each round. Within the class of strictly convex games, we present an algorithm named \texttt{Common-Points-Picking} that returns a point in the expected core given a polynomial number of samples, with high probability. To analyse the algorithm, we develop a new extension of the separation hyperplane theorem for multiple convex sets.t.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-145" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-145', event_id='95094', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6302</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95094">Bayesian Strategic Classification</a></strong></h5>


                        <p class="text-muted">
                            Lee Cohen &middot; Saeed Sharifi-Malvajerdi &middot; Kevin Stangl &middot; Ali Vakilian &middot; Juba Ziani
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In strategic classification, agents modify their features, at a cost, to obtain a positive classification outcome from the learners classifier, typically assuming agents have full knowledge of the deployed classifier. In contrast, we consider a Bayesian setting where agents have a common distributional prior on the classifier being used and agents manipulate their features to maximize their expected utility according to this prior.The learner can reveal truthful, yet not necessarily complete, information about the classifier to the agents, aiming to release just enough information to shape the agents' behavior and thus maximize accuracy. We show that partial information release can counter-intuitively benefit the learners accuracy, allowing qualified agents to pass the classifier while preventing unqualified agents from doing so. Despite the intractability of computing the best response of an agent in the general case, we provide oracle-efficient algorithms for scenarios where the learners hypothesis class consists of low-dimensional linear classifiers or when the agents cost function satisfies a sub-modularity condition. Additionally, we address the learners optimization problem, offering both positive and negative results on determining the optimal information release to maximize expected accuracy, particularly in settings where an agents qualification can be represented by a real-valued number.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-146" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-146', event_id='96155', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6303</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96155">Convergence of No-Swap-Regret Dynamics in Self-Play</a></strong></h5>


                        <p class="text-muted">
                            Renato Leme &middot; Georgios Piliouras &middot; Jon Schneider
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we investigate the question of whether no-swap-regret dynamics have stronger convergence properties in repeated games than regular no-external-regret dynamics. We prove that in almost all symmetric zero-sum games under symmetric initializations of the agents, no-swap-regret dynamics in self-play are guaranteed to converge in a strong ``frequent-iterate'' sense to the Nash equilibrium: in all but a vanishing fraction of the rounds, the players must play a strategy profile close to a symmetric Nash equilibrium.  Remarkably, relaxing any of these three constraints, i.e. by allowing either i) asymmetric initial conditions, or ii) an asymmetric game or iii) no-external regret dynamics suffices to destroy this result and lead to complex non-equilibrating or even chaotic behavior. In a dual type of result, we show that the power of no-swap-regret dynamics comes at a cost of imposing a time-asymmetry on its inputs. While no-external-regret dynamics can be completely determined by the cumulative reward vector received by each player, we show there does not exist any general no-swap-regret dynamics defined on the same state space. In fact, we prove that any no-swap-regret learning algorithm must play a time-asymmetric function over the set of previously observed rewards, ruling out any dynamics based on a symmetric function of the current set of rewards.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-147" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-147', event_id='96322', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6304</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96322">Computational Aspects of Bayesian Persuasion under Approximate Best Response</a></strong></h5>


                        <p class="text-muted">
                            Kunhe Yang &middot; Hanrui Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We study Bayesian persuasion under approximate best response, where the receiver may choose any action that is not too much suboptimal, given their posterior belief upon receiving the signal.  We focus on the computational aspects of the problem, aiming to design algorithms that efficiently compute (almost) optimal strategies for the sender.  Despite the absence of the revelation principle --- which has been one of the most powerful tools in Bayesian persuasion --- we design polynomial-time exact algorithms for the problem when either the state space or the action space is small, as well as a quasi-polynomial-time approximation scheme (QPTAS) for the general problem.  On the negative side, we show there is no polynomial-time exact algorithm for the general problem unless $\mathsf{P} = \mathsf{NP}$.  Our results build on several new algorithmic ideas, which might be useful in other principal-agent problems where robustness is desired.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-148" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-148', event_id='95049', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6305</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95049">LaSCal: Label-Shift Calibration without target labels</a></strong></h5>


                        <p class="text-muted">
                            Teodora Popordanoska &middot; Gorjan Radevski &middot; Tinne Tuytelaars &middot; Matthew Blaschko
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>When machine learning systems face dataset shift, model calibration plays a pivotal role in ensuring their reliability.Calibration error (CE) provides insights into the alignment between the predicted confidence scores and the classifier accuracy.While prior works have delved into the implications of dataset shift on calibration, existing CE estimators either (i) assume access to labeled data from the target domain, often unavailable in practice, or (ii) are derived under a covariate shift assumption.In this work we propose a novel, label-free, consistent CE estimator under label shift. Label shift is characterized by changes in the marginal label distribution p(Y), with a constant conditional p(X|Y) distribution between the source and target. We introduce a novel calibration method, called LaSCal, which uses the estimator in conjunction with a post-hoc calibration strategy, to perform unsupervised calibration on the target distribution. Our thorough empirical analysis demonstrates the effectiveness and reliability of the proposed approach across different modalities, model architectures and label shift intensities.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-149" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-149', event_id='95095', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6306</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95095">LFME: A Simple Framework for Learning from Multiple Experts in Domain Generalization</a></strong></h5>


                        <p class="text-muted">
                            Liang Chen &middot; Yong Zhang &middot; Yibing Song &middot; Zhiqiang Shen &middot; Lingqiao Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Domain generalization (DG) methods aim to maintain good performance in an unseen target domain by using training data from multiple source domains. While success on certain occasions are observed, enhancing the baseline across most scenarios remains challenging. This work introduces a simple yet effective framework, dubbed learning from multiple experts (LFME), that aims to make the target model an expert in all source domains to improve DG. Specifically, besides learning the target model used in inference, LFME will also train multiple experts specialized in different domains, whose output probabilities provide professional guidance by simply regularizing the logit of the target model. Delving deep into the framework, we reveal that the introduced logit regularization term implicitly provides effects of enabling the target model to harness more information, and mining hard samples from the experts during training. Extensive experiments on benchmarks from different DG tasks demonstrate that LFME is consistently beneficial to the baseline and can achieve comparable performance to existing arts.  Code is available at https://github.com/liangchen527/LFME.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-150" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-150', event_id='95295', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6307</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95295">DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices</a></strong></h5>


                        <p class="text-muted">
                            Yongzhe Jia &middot; Xuyun Zhang &middot; Hongsheng Hu &middot; Kim-Kwang Raymond Choo &middot; Lianyong Qi &middot; Xiaolong Xu &middot; Amin Beheshti &middot; Wanchun Dou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Federated learning (FL) has emerged as a prominent machine learning paradigm in edge computing environments, enabling edge devices to collaboratively optimize a global model without sharing their private data. However, existing FL frameworks suffer from efficacy deterioration due to the system heterogeneity inherent in edge computing, especially in the presence of domain shifts across local data. In this paper, we propose a heterogeneous FL framework DapperFL, to enhance model performance across multiple domains. In DapperFL, we introduce a dedicated Model Fusion Pruning (MFP) module to produce personalized compact local models for clients to address the system heterogeneity challenges. The MFP module prunes local models with fused knowledge obtained from both local and remaining domains, ensuring robustness to domain shifts. Additionally, we design a Domain Adaptive Regularization (DAR) module to further improve the overall performance of DapperFL. The DAR module employs regularization generated by the pruned model, aiming to learn robust representations across domains. Furthermore, we introduce a specific aggregation algorithm for aggregating heterogeneous local models with tailored architectures and weights. We implement DapperFL on a real-world FL platform with heterogeneous clients. Experimental results on benchmark datasets with multiple domains demonstrate that DapperFL outperforms several state-of-the-art FL frameworks by up to 2.28%, while significantly achieving model volume reductions ranging from 20% to 80%. Our code is available at: https://github.com/jyzgh/DapperFL.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-151" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-151', event_id='96844', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6308</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96844">Generalized Tensor Decomposition for Understanding Multi-Output Regression under Combinatorial Shifts</a></strong></h5>


                        <p class="text-muted">
                            Andong Wang &middot; Yuning Qiu &middot; Mingyuan Bai &middot; Zhong Jin &middot; Guoxu Zhou &middot; Qibin Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In multi-output regression, we identify a previously neglected challenge that arises from the inability of training distribution to cover all combinations of input features, leading to combinatorial distribution shift (CDS). To the best of our knowledge, this is the first work to formally define and address this problem. We tackle it through a novel tensor decomposition perspective, proposing the Functional t-Singular Value Decomposition (Ft-SVD) theorem which extends the classical tensor SVD to infinite and continuous feature domains, providing a natural tool for representing and analyzing multi-output functions. Within the Ft-SVD framework, we formulate the multi-output regression problem under CDS as a low-rank tensor estimation problem under the missing not at random (MNAR) setting, and introduce a series of assumptions about the true functions, training and testing distributions, and spectral properties of the ground-truth embeddings, making the problem more tractable.To address the challenges posed by CDS in multi-output regression, we develop a tailored Double-Stage Empirical Risk Minimization (ERM-DS) algorithm that leverages the spectral properties of the embeddings and uses specific hypothesis classes in each frequency component to better capture the varying spectral decay patterns. We provide rigorous theoretical analyses that establish performance guarantees for the ERM-DS algorithm. This work lays a preliminary theoretical foundation for multi-output regression under CDS.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-152" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-152', event_id='95978', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6309</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95978">Using Surrogates in Covariate-adjusted Response-adaptive Randomization Experiments with Delayed Outcomes</a></strong></h5>


                        <p class="text-muted">
                            Lei Shi &middot; Waverly Wei &middot; Jingshen Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Covariate-adjusted response-adaptive randomization (CARA) designs are gaining increasing attention. These designs combine the advantages of randomized experiments with the ability to adaptively revise treatment allocations based on data collected across multiple stages, enhancing estimation efficiency. Yet, CARA designs often assume that primary outcomes are immediately observable, which is not the case in many clinical scenarios where there is a delay in observing primary outcomes. This assumption can lead to significant missingness and inefficient estimation of treatment effects. To tackle this practical challenge, we propose a CARA experimental strategy integrating delayed primary outcomes with immediately observed surrogate outcomes. Surrogate outcomes are intermediate clinical outcomes that are predictive or correlated with the primary outcome of interest. Our design goal is to improve the estimation efficiency of the average treatment effect (ATE) of the primary outcome utilizing surrogate outcomes. From a methodological perspective, our approach offers two benefits: First, we accommodate arm and covariates-dependent delay mechanisms without imposing any parametric modeling assumptions on the distribution of outcomes. Second, when primary outcomes are not fully observed, surrogate outcomes can guide the adaptive treatment allocation rule. From a theoretical standpoint, we prove the semiparametric efficiency bound of estimating ATE under delayed primary outcomes while incorporating surrogate outcomes. We show that the ATE estimator under our proposed design strategy attains this semiparametric efficiency bound and achieves asymptotic normality. Through theoretical investigations and a synthetic HIV study, we show that our design is more efficient than the design without incorporating any surrogate information.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-153" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-153', event_id='96287', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6310</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96287">Attention boosted Individualized Regression</a></strong></h5>


                        <p class="text-muted">
                            Guang Yang &middot; Yuan Cao &middot; Long Feng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Different from classical one-model-fits-all strategy, individualized models allow parameters to vary across samples and are gaining popularity in various fields, particularly in personalized medicine. Motivated by medical imaging analysis, this paper introduces a novel individualized modeling framework for matrix-valued data that does not require additional information on sample similarity for the individualized coefficients. Under our framework, the model individualization stems from an optimal internal relation map within the samples themselves. We refer to the proposed method as Attention boosted Individualized Regression, due to its close connections with the self-attention mechanism. Therefore, our approach provides a new interpretation for  attention from the perspective of individualized modeling. Comprehensive numerical experiments and real brain MRI analysis using an ADNI dataset demonstrated the superior performance of our model.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-154" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-154', event_id='93602', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6400</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93602">Learning to Mitigate Externalities: the Coase Theorem with Hindsight Rationality</a></strong></h5>


                        <p class="text-muted">
                            Antoine Scheid &middot; Aymeric Capitaine &middot; Etienne Boursier &middot; Eric Moulines &middot; Michael Jordan &middot; Alain Durmus
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In Economics, the concept of externality refers to any indirect effect resulting from an interaction between players and affecting a third party without compensation. Most of the models within which externality has been studied assume that agents have perfect knowledge of their environment and preferences. This is a major hindrance to the practical implementation of many proposed solutions. To adress this issue, we consider a two-players bandit game setting where the actions of one of the player affect the other one. Building upon this setup, we extend the Coase theorem [Coase, 2013], which suggests that the optimal approach for maximizing the social welfare in the presence of externality is to establish property rights, i.e., enabling transfers and bargaining between the players. Nonetheless, this fundamental result relies on the assumption that bargainers possess perfect knowledge of the underlying game. We first demonstrate that in the absence of property rights in the considered online scenario, the social welfare breaks down. We then provide a policy for the players, which allows them to learn a bargaining strategy which maximizes the total welfare, recovering the Coase theorem under uncertainty.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-155" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-155', event_id='95619', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6401</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95619">Symmetries in Overparametrized Neural Networks: A Mean Field View</a></strong></h5>


                        <p class="text-muted">
                            Javier Maass &middot; Joaquin Fontbona
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We develop a Mean-Field (MF) view of the learning dynamics of overparametrized Artificial Neural Networks (NN) under distributional symmetries of the data w.r.t. the action of a general compact group $G$. We consider for this a class of  generalized shallow NNs given by an ensemble of  $N$ multi-layer units, jointly trained  using stochastic gradient descent (SGD) and possibly symmetry-leveraging (SL) techniques, such as Data Augmentation (DA), Feature Averaging  (FA) or Equivariant Architectures (EA).  We introduce the notions of weakly and strongly invariant  laws (WI and SI) on the parameter space of each single unit, corresponding, respectively, to $G$-invariant distributions, and to distributions supported on parameters fixed by the group action (which encode EA). This allows us to define symmetric models compatible with taking $N\to\infty$ and  give an interpretation of the asymptotic dynamics of DA, FA and EA in terms of Wasserstein Gradient Flows describing their MF limits. When activations respect the group action, we show that, for  symmetric data, DA, FA and freely-trained models obey the exact same MF  dynamic, which stays in the space of WI parameter laws and attains therein the population risk's minimizer. We also provide a counterexample to the general attainability of such an optimum over SI laws.Despite this,  and quite remarkably, we show that the space of SI laws  is also preserved by these MF distributional dynamics even when freely trained. This sharply contrasts the finite-$N$ setting, in which EAs are generally not preserved by unconstrained SGD. We illustrate the validity of our findings as $N$ gets larger,  in a teacher-student experimental setting, training a student NN to learn from a WI, SI  or arbitrary teacher model through various SL schemes. We lastly deduce a data-driven heuristic to discover the largest subspace of parameters supporting SI distributions for a problem, that could be used for designing EA with minimal generalization error.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-156" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-156', event_id='96718', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6402</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96718">Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm</a></strong></h5>


                        <p class="text-muted">
                            Qinbo Bai &middot; Washim Mondal &middot; Vaneet Aggarwal
                        </p>

                    </div>
                    <div class="abstract">
                        <p>This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDPs). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual-based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, our proposed algorithm achieves $\tilde{\mathcal{O}}({T}^{4/5})$ objective regret and $\tilde{\mathcal{O}}({T}^{4/5})$ constraint violation bounds.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-157" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-157', event_id='96414', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6403</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96414">Randomized Exploration for Reinforcement Learning with Multinomial Logistic Function Approximation</a></strong></h5>


                        <p class="text-muted">
                            Wooseong Cho &middot; Taehyun Hwang &middot; Joongkyu Lee &middot; Min-hwan Oh
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We study reinforcement learning with _multinomial logistic_ (MNL) function approximation where the underlying transition probability kernel of the _Markov decision processes_ (MDPs) is parametrized by an unknown transition core with features of state and action. For the finite horizon episodic setting with inhomogeneous state transitions, we propose provably efficient algorithms with randomized exploration having frequentist regret guarantees. For our first algorithm, $\texttt{RRL-MNL}$, we adapt optimistic sampling to ensure the optimism of the estimated value function with sufficient frequency and establish that $\texttt{RRL-MNL}$ is both _statistically_ and _computationally_ efficient, achieving a $\tilde{\mathcal{O}}(\kappa^{-1} d^{\frac{3}{2}} H^{\frac{3}{2}} \sqrt{T})$ frequentist regret bound with constant-time computational cost per episode. Here, $d$ is the dimension of the transition core, $H$ is the horizon length, $T$ is the total number of steps, and $\kappa$ is a problem-dependent constant. Despite the simplicity and practicality of $\texttt{RRL-MNL}$, its regret bound scales with $\kappa^{-1}$, which is potentially large in the worst case. To improve the dependence on $\kappa^{-1}$, we propose $\texttt{ORRL-MNL}$, which estimates the value function using local gradient information of the MNL transition model. We show that its frequentist regret bound is $\tilde{\mathcal{O}}(d^{\frac{3}{2}} H^{\frac{3}{2}} \sqrt{T} + \kappa^{-1} d^2 H^2)$. To the best of our knowledge, these are the first randomized RL algorithms for the MNL transition model that achieve both computational and statistical efficiency. Numerical experiments demonstrate the superior performance of the proposed algorithms.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-158" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-158', event_id='96396', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6404</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96396">Offline Oracle-Efficient Learning for Contextual MDPs via Layerwise Exploration-Exploitation Tradeoff</a></strong></h5>


                        <p class="text-muted">
                            Jian Qian &middot; Haichen Hu &middot; David Simchi-Levi
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Motivated by the recent discovery of a statistical and computational reduction from contextual bandits to offline regression \citep{simchi2020bypassing}, we address the general (stochastic) Contextual Markov Decision Process (CMDP) problem with horizon $H$ (as known as CMDP with $H$ layers). In this paper, we introduce a reduction from CMDPs to offline density estimation under the realizability assumption, i.e., a model class $\mathcal{M}$ containing the true underlying CMDP is provided in advance. We develop an efficient, statistically near-optimal algorithm requiring only $O(H \log T)$ calls to an offline density estimation algorithm (or oracle) across all $T$ rounds. This number can be further reduced to $O(H \log \log T)$ if $T$ is known in advance. Our results mark the first efficient and near-optimal reduction from CMDPs to offline density estimation without imposing any structural assumptions on the model class. A notable feature of our algorithm is the design of a layerwise exploration-exploitation tradeoff tailored to address the layerwise structure of CMDPs. Additionally, our algorithm is versatile and applicable to pure exploration tasks in reward-free reinforcement learning.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-159" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-159', event_id='96181', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6405</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96181">Truncated Variance Reduced Value Iteration</a></strong></h5>


                        <p class="text-muted">
                            Yujia Jin &middot; Ishani Karmarkar &middot; Aaron Sidford &middot; Jiayi Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We provide faster randomized algorithms for computing an $\epsilon$-optimal policy in a discounted Markov decision process with $A_{\text{tot}}$-state-action pairs, bounded rewards, and discount factor $\gamma$. We provide an $\tilde{O}(A_{\text{tot}}[(1 - \gamma)^{-3}\epsilon^{-2} + (1 - \gamma)^{-2}])$-time algorithm in the sampling setting, where the probability transition matrix is unknown but accessible through a generative model which can be queried in $\tilde{O}(1)$-time, and an $\tilde{O}(s +  (1-\gamma)^{-2})$-time algorithm in the offline setting where the probability transition matrix is known and $s$-sparse. These results improve upon the prior state-of-the-art which either ran in $\tilde{O}(A_{\text{tot}}[(1 - \gamma)^{-3}\epsilon^{-2} + (1 - \gamma)^{-3}])$ time [Sidford, Wang, Wu, Ye 2018] in the sampling setting, $\tilde{O}(s + A_{\text{tot}} (1-\gamma)^{-3})$ time [Sidford, Wang, Wu, Yang, Ye 2018] in the offline setting, or time at least quadratic in the number of states using interior point methods for linear programming. We achieve our results by building upon prior stochastic variance-reduced value iteration methods [Sidford, Wang, Wu, Yang, Ye 2018]. We provide a variant that carefully truncates the progress of its iterates to improve the variance of new variance-reduced sampling procedures that we introduce to implement the steps. Our method is essentially model-free and can be implemented in $\tilde{O}(A_{\text{tot}})$-space when given generative model access. Consequently, our results take a step in closing the sample-complexity gap between model-free and model-based methods.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-160" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-160', event_id='95864', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6406</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95864">Preference-based Pure Exploration</a></strong></h5>


                        <p class="text-muted">
                            Apurv Shukla &middot; Debabrota Basu
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We study the preference-based pure exploration problem for bandits with vector-valued rewards and a set of preferences imposed over them. Specifically, we aim to identify the most preferred policy over a set of arms according to the preferences induced on the reward vectors by an ordering cone $C$. First, to quantify the impact of preferences, we derive a novel lower bound on the sample complexity for identifying the most preferred arm with confidence level $1-\delta$. Our lower bound shows that how the geometry of the preferences and reward vectors changes the hardness of this problem. We further explicate this geometry for Gaussian distributions of rewards, and provide a convex reformulation of the lower bound solvable with linear programming. Then, we leverage this convex reformulation of the lower bound to design the Track and Stop with Preferences (TSwP) algorithm that identifies the most preferred policy. Finally, we derive a new concentration result for vector-valued rewards, and show that TSwP achieves a matching sample complexity upper bound.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-161" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-161', event_id='95322', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6407</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95322">Thompson Sampling For Combinatorial Bandits: Polynomial Regret and Mismatched Sampling Paradox</a></strong></h5>


                        <p class="text-muted">
                            Raymond Zhang &middot; Richard Combes
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We consider Thompson Sampling (TS) for linear combinatorial semi-bandits and subgaussian rewards. We propose the first known TS whose finite-time regret does not scale exponentially with the dimension of the problem. We further show the mismatched sampling paradox: A learner who knows the rewards distributions and samples from the correct posterior distribution can perform exponentially worse than a learner who does not know the rewards and simply samples from a well-chosen Gaussian posterior. The code used to generate the experiments is available at https://github.com/RaymZhang/CTS-Mismatched-Paradox</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-162" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-162', event_id='94654', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6408</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94654">Online Control in Population Dynamics</a></strong></h5>


                        <p class="text-muted">
                            Noah Golowich &middot; Elad Hazan &middot; Zhou Lu &middot; Dhruv Rohatgi &middot; Y. Jennifer Sun
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The study of population dynamics originated with early sociological works but has since extended into many fields, including biology, epidemiology, evolutionary game theory, and economics. Most studies on population dynamics focus on the problem of prediction rather than control. Existing mathematical models for population control are often restricted to specific, noise-free dynamics, while real-world population changes can be complex and adversarial. To address this gap, we propose a new framework based on the paradigm of online control. We first characterize a set of linear dynamical systems that can naturally model evolving populations. We then give an efficient gradient-based controller for these systems, with near-optimal regret bounds with respect to a broad class of linear policies. Our empirical evaluations demonstrate the effectiveness of the proposed algorithm for population control even in non-linear models such as SIR and replicator dynamics.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-163" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-163', event_id='94483', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6409</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94483">Hybrid Reinforcement Learning Breaks Sample Size Barriers In Linear MDPs</a></strong></h5>


                        <p class="text-muted">
                            Kevin Tan &middot; Wei Fan &middot; Yuting Wei
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Hybrid Reinforcement Learning (RL), where an agent learns from both an offline dataset and online explorations in an unknown environment, has garnered significant recent interest. A crucial question posed by Xie et al. (2022) is whether hybrid RL can improve upon the existing lower bounds established in purely offline and purely online RL without relying on the single-policy concentrability assumption. While Li et al. (2023) provided an affirmative answer to this question in the tabular PAC RL case, the question remains unsettled for both the regret-minimizing RL case and the non-tabular case. In this work, building upon recent advancements in offline RL and reward-agnostic exploration, we develop computationally efficient algorithms for both PAC and regret-minimizing RL with linear function approximation, without requiring concentrability on the entire state-action space. We demonstrate that these algorithms achieve sharper error or regret bounds that are no worse than, and can improve on, the optimal sample complexity in offline RL (the first algorithm, for PAC RL) and online RL (the second algorithm, for regret-minimizing RL) in linear Markov decision processes (MDPs), regardless of the quality of the behavior policy. To our knowledge, this work establishes the tightest theoretical guarantees currently available for hybrid RL in linear MDPs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-164" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-164', event_id='93854', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6410</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93854">Towards the Transferability of Rewards Recovered via Regularized Inverse Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Andreas Schlaginhaufen &middot; Maryam Kamgarpour
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Inverse reinforcement learning (IRL) aims to infer a reward from expert demonstrations, motivated by the idea that the reward, rather than the policy, is the most succinct and transferable description of a task [Ng et al., 2000]. However, the reward corresponding to an optimal policy is not unique, making it unclear if an IRL-learned reward is transferable to new transition laws in the sense that its optimal policy aligns with the optimal policy corresponding to the expert's true reward. Past work has addressed this problem only under the assumption of full access to the expert's policy, guaranteeing transferability when learning from two experts with the same reward but different transition laws that satisfy a specific rank condition [Rolland et al., 2022]. In this work, we show that the conditions developed under full access to the expert's policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert. Instead of a binary rank condition, we propose principal angles as a more refined measure of similarity and dissimilarity between transition laws. Based on this, we then establish two key results: 1) a sufficient condition for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws, and 2) a sufficient condition for transferability to local changes in the transition law when learning from a single expert. Furthermore, we also provide a probably approximately correct (PAC) algorithm and an end-to-end analysis for learning transferable rewards from demonstrations of multiple experts.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-165" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-165', event_id='97564', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6500</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97564">DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception</a></strong></h5>


                        <p class="text-muted">
                            Xiaotong Li &middot; Fan Zhang &middot; Haiwen Diao &middot; Yueze Wang &middot; Xinlong Wang &middot; LINGYU DUAN
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Existing Multimodal Large Language Models (MLLMs) increasingly emphasize complex understanding of various visual elements, including multiple objects, text information, spatial relations. Their development for comprehensive visual perception hinges on the availability of high-quality image-text datasets that offer diverse visual elements and throughout image descriptions. However, the scarcity of such hyper-detailed datasets currently hinders progress within the MLLM community. The bottleneck stems from the limited perceptual capabilities of current caption engines, which fall short in providing complete and accurate annotations. To facilitate the cutting-edge research of MLLMs on comprehensive vision perception, we thereby propose Perceptual Fusion, using a low-budget but highly effective caption engine for complete and accurate image descriptions.  Specifically, Perceptual Fusion integrates diverse perception experts as image priors to provide explicit information on visual elements and adopts an efficient MLLM as a centric pivot to mimic advanced MLLMs' perception abilities. We carefully select 1M highly representative images from uncurated LAION dataset and generate dense descriptions using our engine, dubbed DenseFusion-1M. Extensive experiments validate that our engine outperforms its counterparts, where the resulting dataset significantly improves the perception and cognition abilities of existing MLLMs across diverse vision-language benchmarks, especially with high-resolution images as inputs. The dataset will be available at https://huggingface.co/datasets/BAAI/DenseFusion-1M.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-166" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-166', event_id='97602', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6501</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97602">There is No Silver Bullet: Benchmarking Methods in Predictive Combinatorial Optimization</a></strong></h5>


                        <p class="text-muted">
                            Haoyu Geng &middot; Hang Ruan &middot; Runzhong Wang &middot; Yang Li &middot; YANG WANG &middot; Lei Chen &middot; Junchi Yan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Predictive combinatorial optimization, where the parameters of combinatorial optimization (CO) are unknown at the decision-making time, is the precise modeling of many real-world applications, including energy cost-aware scheduling and budget allocation on advertising. Tackling such a problem usually involves a prediction model and a CO solver. These two modules are integrated into the predictive CO pipeline following two design principles: ''Predict-then-Optimize (PtO)'', which learns predictions by supervised training and subsequently solves CO using predicted coefficients, while the other, named ''Predict-and-Optimize (PnO)'', directly optimizes towards the ultimate decision quality and claims to yield better decisions than traditional PtO approaches. However, there lacks a systematic benchmark of both approaches, including the specific design choices at the module level, as well as an evaluation dataset that covers representative real-world scenarios. To this end, we develop a modular framework to benchmark 11 existing PtO/PnO methods on 8 problems, including a new industrial dataset for combinatorial advertising that will be released. Our study shows that PnO approaches are better than PtO on 7 out of 8 benchmarks, but there is no silver bullet found for the specific design choices of PnO. A comprehensive categorization of current approaches and integration of typical scenarios are provided under a unified benchmark. Therefore, this paper could serve as a comprehensive benchmark for future PnO approach development and also offer fast prototyping for application-focused development.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-167" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-167', event_id='97698', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6502</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97698">STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases</a></strong></h5>


                        <p class="text-muted">
                            Shirley Wu &middot; Shiyu Zhao &middot; Michihiro Yasunaga &middot; Kexin Huang &middot; Kaidi Cao &middot; Qian Huang &middot; Vassilis Ioannidis &middot; Karthik Subbian &middot; James Zou &middot; Jure Leskovec
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Answering real-world complex queries, such as complex product search, often requires accurate retrieval from semi-structured knowledge bases that involve blend of unstructured (e.g., textual descriptions of products) and structured (e.g., entity relations of products) information. However, many previous works studied textual and relational retrieval tasks as separate topics. To address the gap, we develop STaRK, a large-scale Semi-structure retrieval benchmark on Textual and Relational Knowledge Bases. Our benchmark covers three domains: product search, academic paper search, and queries in precision medicine. We design a novel pipeline to synthesize realistic user queries that integrate diverse relational information and complex textual properties, together with their ground-truth answers (items). We conduct rigorous human evaluation to validate the quality of our synthesized queries. We further enhance the benchmark with high-quality human-generated queries to provide an authentic reference. STaRK serves as a comprehensive testbed for evaluating the performance of retrieval systems driven by large language models (LLMs). Our experiments suggest that STaRK presents significant challenges to the current retrieval and LLM systems, highlighting the need for more capable semi-structured retrieval systems.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-168" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-168', event_id='97717', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6503</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97717">A Novel Benchmark for Decision-Making in Uncertain and Competitive Games</a></strong></h5>


                        <p class="text-muted">
                            Kefan Su &middot; Yusen Huo &middot; ZHILIN ZHANG &middot; Shuai Dou &middot; Chuan Yu &middot; Jian Xu &middot; Zongqing Lu &middot; Bo Zheng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The study of decision-making in large-scale game environments is a crucial domain within artificial intelligence, possessing substantial implications for practical applications. Nevertheless, the lack of comprehensive, realistic game environments and associated datasets has limited progress in this field. To address this and promote research on this important problem, we introduce the Large-Scale Auction (LSA) Benchmark derived from online advertising, a rapidly expanding industry worth $626.8 billion in 2023. The LSA Benchmark consists of an environment and the corresponding dataset. The LSA Environment is augmented with the deep generative model to reduce the gap between the simulation environment and reality while avoiding the risks of sensitive data exposure. The LSA Dataset comprises over 500 million records, totaling 40 GB in size, which contains trajectories with 50 diverse agents competing with each other, for effective offline training. We evaluate different types of existing algorithms in the LSA Environment. We hope the LSA benchmark can promote the development of decision-making in large-scale games.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-169" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-169', event_id='97741', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6504</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97741">NeuralPlane: An Efficiently Parallelizable Platform for Fixed-wing Aircraft Control with Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Chuanyi Xue &middot; Qihan Liu &middot; Xiaoteng Ma &middot; Xinyao Qin &middot; Gui Ning &middot; Yang Qi &middot; Jinsheng Ren &middot; Bin Liang &middot; Jun Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Reinforcement learning (RL) demonstrates superior potential over traditional flight control methods for fixed-wing aircraft, particularly under extreme operational conditions. However, the high demand for training samples and the lack of efficient computation in existing simulators hinder its further application. In this paper, we introduce NeuralPlane, the first benchmark platform for large-scale parallel simulations of fixed-wing aircraft. NeuralPlane significantly boosts high-fidelity simulation via GPU-accelerated Flight Dynamics Model (FDM) computation, achieving a single-step simulation time of just 0.2 seconds at a parallel scale of $10^{6}$, far exceeding current platforms. We also provide clear code templates, comprehensive evaluation/visualization tools and hierarchical frameworks for integrating RL and traditional control methods. We believe that NeuralPlane can accelerate the development of RL-based fixed-wing flight control and serve as a new challenging benchmark for the RL community.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-170" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-170', event_id='97834', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6505</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97834">CleanDiffuser: An Easy-to-use Modularized Library for Diffusion Models in Decision Making</a></strong></h5>


                        <p class="text-muted">
                            Zibin Dong &middot; Yifu Yuan &middot; Jianye Hao &middot; Fei Ni &middot; Yi Ma &middot; Pengyi Li &middot; YAN ZHENG
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Leveraging the powerful generative capability of diffusion models (DMs) to build decision-making agents has achieved extensive success. However, there is still a demand for an easy-to-use and modularized open-source library that offers customized and efficient development for DM-based decision-making algorithms. In this work, we introduce <strong>CleanDiffuser</strong>, the first DM library specifically designed for decision-making algorithms. By revisiting the roles of DMs in the decision-making domain, we identify a set of essential sub-modules that constitute the core of CleanDiffuser, allowing for the implementation of various DM algorithms with simple and flexible building blocks. To demonstrate the reliability and flexibility of CleanDiffuser, we conduct comprehensive evaluations of various DM algorithms implemented with CleanDiffuser across an extensive range of tasks. The analytical experiments provide a wealth of valuable design choices and insights, reveal opportunities and challenges, and lay a solid groundwork for future research. CleanDiffuser will provide long-term support to the decision-making community, enhancing reproducibility and fostering the development of more robust solutions.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-171" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-171', event_id='97866', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6506</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97866">MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions</a></strong></h5>


                        <p class="text-muted">
                            Xuan Ju &middot; Yiming Gao &middot; Zhaoyang Zhang &middot; Ziyang Yuan &middot; Xintao Wang &middot; AILING ZENG &middot; Yu Xiong &middot; Qiang Xu &middot; Ying Shan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Sora's high-motion intensity and long consistent videos have significantly impacted the field of video generation, attracting unprecedented attention. However, existing publicly available datasets are inadequate for generating Sora-like videos, as they mainly contain short videos with low motion intensity and brief captions. To address these issues, we propose MiraData, a high-quality video dataset that surpasses previous ones in video duration, caption detail, motion strength, and visual quality. We curate MiraData from diverse, manually selected sources and meticulously process the data to obtain semantically consistent clips. GPT-4V is employed to annotate structured captions, providing detailed descriptions from four different perspectives along with a summarized dense caption. To better assess temporal consistency and motion intensity in video generation, we introduce MiraBench, which enhances existing benchmarks by adding 3D consistency and tracking-based motion strength metrics. MiraBench includes 150 evaluation prompts and 17 metrics covering temporal consistency, motion strength, 3D consistency, visual quality, text-video alignment, and distribution similarity. To demonstrate the utility and effectiveness of MiraData, we conduct experiments using our DiT-based video generation model, MiraDiT. The experimental results on MiraBench demonstrate the superiority of MiraData, especially in motion strength.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-172" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-172', event_id='93281', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6507</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93281">Pretrained Transformer Efficiently Learns Low-Dimensional Target Functions In-Context</a></strong></h5>


                        <p class="text-muted">
                            Kazusato Oko &middot; Yujin Song &middot; Taiji Suzuki &middot; Denny Wu
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Transformers can efficiently learn in-context from example demonstrations. Most existing theoretical analyses studied the in-context learning (ICL) ability of transformers for linear function classes, where it is typically shown that the minimizer of the pretraining loss implements one gradient descent step on the least squares objective. However, this simplified linear setting arguably does not demonstrate the statistical efficiency of ICL, since the pretrained transformer does not outperform directly solving linear regression on the test prompt. In this paper, we study ICL of a nonlinear function class via transformer with nonlinear MLP layer: given a class of \textit{single-index} target functions $f_*(\boldsymbol{x}) = \sigma_*(\langle\boldsymbol{x},\boldsymbol{\beta}\rangle)$, where the index features $\boldsymbol{\beta}\in\mathbb{R}^d$ are drawn from a $r$-dimensional subspace, we show that a nonlinear transformer optimized by gradient descent (with a pretraining sample complexity that depends on the \textit{information exponent} of the link functions $\sigma_*$) learns $f_*$ in-context with a prompt length that only depends on the dimension of the distribution of target functions $r$; in contrast, any algorithm that directly learns $f_*$ on test prompt yields a statistical complexity that scales with the ambient dimension $d$.  Our result highlights the adaptivity of the pretrained transformer to low-dimensional structures of the function class, which enables sample-efficient ICL that outperforms estimators that only have access to the in-context data.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-173" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-173', event_id='94386', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6508</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94386">Axioms for AI Alignment from Human Feedback</a></strong></h5>


                        <p class="text-muted">
                            Luise Ge &middot; Daniel Halpern &middot; Evi Micha &middot; Ariel Procaccia &middot; Itai Shapira &middot; Yevgeniy Vorobeychik &middot; Junlin Wu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In the context of reinforcement learning from human feedback (RLHF), the reward function is generally derived from maximum likelihood estimation of a random utility model based on pairwise comparisons made by humans. The problem of learning a reward function is one of preference aggregation that, we argue, largely falls within the scope of social choice theory. From this perspective, we can evaluate different aggregation methods via established axioms, examining whether these methods meet or fail well-known standards. We demonstrate that both the Bradley-Terry-Luce Model and its broad generalizations fail to meet basic axioms. In response, we develop novel rules for learning reward functions with strong axiomatic guarantees. A key innovation from the standpoint of social choice is that our problem has a <em>linear</em> structure, which greatly restricts the space of feasible rules and leads to a new paradigm that we call <em>linear social choice</em>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-174" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-174', event_id='94932', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6509</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94932">Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers</a></strong></h5>


                        <p class="text-muted">
                            Lorenzo Tiberi &middot; Francesca Mignacco &middot; Kazuki Irie &middot; Haim Sompolinsky
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Despite the remarkable empirical performance of Transformers, their theoretical understanding remains elusive. Here, we consider a deep multi-head self-attention network, that is closely related to Transformers yet analytically tractable. We develop a statistical mechanics theory of Bayesian learning in this model, deriving exact equations for the network's predictor statistics under the finite-width thermodynamic limit, i.e., $N,P\rightarrow\infty$, $P/N=\mathcal{O}(1)$, where $N$ is the network width and $P$ is the number of training examples. Our theory shows that the predictor statistics are expressed as a sum of independent kernels, each one pairing different "attention paths", defined as information pathways through different attention heads across layers. The kernels are weighted according to a "task-relevant kernel combination" mechanism that aligns the total kernel with the task labels. As a consequence, this interplay between attention paths enhances generalization performance. Experiments confirm our findings on both synthetic and real-world sequence classification tasks. Finally, our theory explicitly relates the kernel combination mechanism to properties of the learned weights, allowing for a qualitative transfer of its insights to models trained via gradient descent. As an illustration, we demonstrate an efficient size reduction of the network, by pruning those attention heads that are deemed less relevant by our theory.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-175" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-175', event_id='95799', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6510</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95799">Variance estimation in compound decision theory under boundedness</a></strong></h5>


                        <p class="text-muted">
                            Subhodh Kotekal
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The normal means model is often studied under the assumption of a known variance. However, ignorance of the variance is a frequent issue in applications and basic theoretical questions still remain open in this setting. This article establishes that the sharp minimax rate of variance estimation in square error is $(\frac{\log\log n}{\log n})^2$ under arguably the most mild assumption imposed for identifiability: bounded means. The rate-optimal estimator proposed in this article achieves the optimal rate by estimating $O\left(\frac{\log n}{\log\log n}\right)$ cumulants and leveraging a variational representation of the noise variance in terms of the cumulants of the data distribution. The minimax lower bound involves a moment matching construction.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-176" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-176', event_id='97527', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6600</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97527">ProbTS: Benchmarking Point and Distributional Forecasting across Diverse Prediction Horizons</a></strong></h5>


                        <p class="text-muted">
                            Jiawen Zhang &middot; Xumeng Wen &middot; Zhenwei Zhang &middot; Shun Zheng &middot; Jia Li &middot; Jiang Bian
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Delivering precise point and distributional forecasts across a spectrum of prediction horizons represents a significant and enduring challenge in the application of time-series forecasting within various industries.Prior research on developing deep learning models for time-series forecasting has often concentrated on isolated aspects, such as long-term point forecasting or short-term probabilistic estimations. This narrow focus may result in skewed methodological choices and hinder the adaptability of these models to uncharted scenarios.While there is a rising trend in developing universal forecasting models, a thorough understanding of their advantages and drawbacks, especially regarding essential forecasting needs like point and distributional forecasts across short and long horizons, is still lacking.In this paper, we present ProbTS, a benchmark tool designed as a unified platform to evaluate these fundamental forecasting needs and to conduct a rigorous comparative analysis of numerous cutting-edge studies from recent years.We dissect the distinctive data characteristics arising from disparate forecasting requirements and elucidate how these characteristics can skew methodological preferences in typical research trajectories, which often fail to fully accommodate essential forecasting needs.Building on this, we examine the latest models for universal time-series forecasting and discover that our analyses of methodological strengths and weaknesses are also applicable to these universal models.Finally, we outline the limitations inherent in current research and underscore several avenues for future exploration.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-177" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-177', event_id='97438', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6601</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97438">CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs</a></strong></h5>


                        <p class="text-muted">
                            Jihyung Kil &middot; Zheda Mai &middot; Justin Lee &middot; Zihe Wang &middot; Kerrie Cheng &middot; Lemeng Wang &middot; Ye Liu &middot; Arpita Chowdhury &middot; Wei-Lun (Harry) Chao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The ability to compare objects, scenes, or situations is crucial for effective decision-making and problem-solving in everyday life. For instance, comparing the freshness of apples enables better choices during grocery shopping, while comparing sofa designs helps optimize the aesthetics of our living space. Despite its significance, the comparative capability is largely unexplored in artificial general intelligence (AGI). In this paper, we introduce CompBench, a benchmark designed to evaluate the comparative reasoning capability of multimodal large language models (MLLMs). CompBench mines and pairs images through visually oriented questions covering eight dimensions of relative comparison: visual attribute, existence, state, emotion, temporality, spatiality, quantity, and quality. We curate a collection of around 40K image pairs using metadata from diverse vision datasets and CLIP similarity scores. These image pairs span a broad array of visual domains, including animals, fashion, sports, and both outdoor and indoor scenes. The questions are carefully crafted to discern relative characteristics between two images and are labeled by human annotators for accuracy and relevance. We use CompBench to evaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable shortcomings in their comparative abilities. We believe CompBench not only sheds light on these limitations but also establishes a solid foundation for future enhancements in the comparative capability of MLLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-178" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-178', event_id='97427', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6602</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97427">$E^3$: Exploring Embodied Emotion Through A Large-Scale Egocentric Video Dataset</a></strong></h5>


                        <p class="text-muted">
                            wang lin &middot; Yueying Feng &middot; WenKang Han &middot; Tao Jin &middot; Zhou Zhao &middot; Fei Wu &middot; Chang Yao &middot; Jingyuan Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Understanding human emotions is fundamental to enhancing human-computer interaction, especially for embodied agents that mimic human behavior.  Traditional emotion analysis often takes a third-person perspective, limiting the ability of agents to interact naturally and empathetically.  To address this gap, this paper presents $E^3$ for Exploring Embodied Emotion, the first massive first-person view video dataset. $E^3$ contains more than $50$ hours of video, capturing $8$ different emotion types in diverse scenarios and languages. The dataset features videos recorded by individuals in their daily lives, capturing a wide range of real-world emotions conveyed through visual, acoustic, and textual modalities. By leveraging this dataset, we define $4$ core benchmark tasks - emotion recognition, emotion classification, emotion localization, and emotion reasoning - supported by more than $80$k manually crafted annotations, providing a comprehensive resource for training and evaluating emotion analysis models. We further present Emotion-LlaMa, which complements visual modality with acoustic modality to enhance the understanding of emotion in first-person videos. The results of comparison experiments with a large number of baselines demonstrate the superiority of Emotion-LlaMa and set a new benchmark for embodied emotion analysis. We expect that $E^3$ can promote advances in multimodal understanding, robotics, and augmented reality, and provide a solid foundation for the development of more empathetic and context-aware embodied agents.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-179" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-179', event_id='96954', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6603</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96954">Mimicking To Dominate: Imitation Learning Strategies for Success in Multiagent Games</a></strong></h5>


                        <p class="text-muted">
                            The Viet Bui &middot; Tien Mai &middot; Thanh Nguyen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Training agents in multi-agent games presents significant challenges due to their intricate nature. These challenges are exacerbated by dynamics influenced not only by the environment but also by strategies of opponents. Existing methods often struggle with slow convergence and instability.To address these challenges, we harness the potential of imitation learning (IL) to comprehend and anticipate actions of the opponents, aiming to mitigate uncertainties with respect to the game dynamics.Our key contributions include:(i) a new multi-agent IL model for predicting next moves of the opponents - our model works with hidden actions of opponents and local observations;(ii) a new multi-agent reinforcement learning (MARL) algorithm that combines our IL model and policy training into one single training process;and (iii) extensive experiments in three challenging game environments, including an advanced version of the Star-Craft multi-agent challenge (i.e., SMACv2).Experimental results show that our approach achieves superior performance compared to state-of-the-art MARL algorithms.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-180" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-180', event_id='96939', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6604</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96939">When to Sense and Control? A Time-adaptive Approach for Continuous-Time RL</a></strong></h5>


                        <p class="text-muted">
                            Lenart Treven &middot; Bhavya &middot; Yarden As &middot; Florian Dorfler &middot; Andreas Krause
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Reinforcement learning (RL) excels in optimizing policies for discrete-time Markov decision processes (MDP). However, various systems are inherently continuous in time, making discrete-time MDPs an inexact modeling choice. In many applications, such as greenhouse control or medical treatments, each interaction (measurement or switching of action) involves manual intervention and thus is inherently costly. Therefore, we generally prefer a time-adaptive approach with fewer interactions with the system.In this work, we formalize an RL framework, <strong>T</strong>ime-<strong>a</strong>daptive <strong>Co</strong>ntrol \&amp; <strong>S</strong>ensing (<strong>TaCoS</strong>), that tackles this challenge by optimizing over policies that besides control predict the duration of its application. Our formulation results in an extended MDP that any standard RL algorithm can solve.We demonstrate that state-of-the-art RL algorithms trained on TaCoS drastically reduce the interaction amount over their discrete-time counterpart while retaining the same or improved performance, and exhibiting robustness over discretization frequency.Finally, we propose OTaCoS, an efficient model-based algorithm for our setting. We show that OTaCoS enjoys sublinear regret for systems with sufficiently smooth dynamics and empirically results in further sample-efficiency gains.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-181" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-181', event_id='96906', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6606</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96906">Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling</a></strong></h5>


                        <p class="text-muted">
                            Mingze Wang &middot; Weinan E
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates.Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads.These theoretical insights are validated experimentally and offer natural suggestions for alternative architectures.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-182" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-182', event_id='96875', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6608</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96875">A Global Depth-Range-Free Multi-View Stereo Transformer Network with Pose Embedding</a></strong></h5>


                        <p class="text-muted">
                            Yitong Dong &middot; Yijin Li &middot; Zhaoyang Huang &middot; Weikang Bian &middot; Jingbo Liu &middot; Hujun Bao &middot; Zhaopeng Cui &middot; Hongsheng Li &middot; Guofeng Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we propose a novel multi-view stereo (MVS) framework that gets rid of the depth range prior. Unlike recent prior-free MVS methods that work in a pair-wise manner, our method simultaneously considers all the source images. Specifically, we introduce a Multi-view Disparity Attention (MDA) module to aggregate long-range context information within and across multi-view images.  Considering the asymmetry of the epipolar disparity flow, the key to our method lies in accurately modeling multi-view geometric constraints. We integrate pose embedding to encapsulate information such as multi-view camera poses, providing implicit geometric constraints for multi-view disparity feature fusion dominated by attention. Additionally, we construct corresponding hidden states for each source image due to significant differences in the observation quality of the same pixel in the reference frame across multiple source frames. We explicitly estimate the quality of the current pixel corresponding to sampled points on the epipolar line of the source image and dynamically update hidden states through the uncertainty estimation module. Extensive results on the DTU dataset and Tanks\&amp;Temple benchmark demonstrate the effectiveness of our method.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-183" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-183', event_id='96703', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6609</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96703">ETO:Efficient Transformer-based Local Feature Matching by Organizing Multiple Homography Hypotheses</a></strong></h5>


                        <p class="text-muted">
                            Junjie Ni &middot; Guofeng Zhang &middot; Guanglin Li &middot; Yijin Li &middot; Xinyang Liu &middot; Zhaoyang Huang &middot; Hujun Bao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We tackle the efficiency problem of learning local feature matching.Recent advancements have given rise to purely CNN-based and transformer-based approaches, each augmented with deep learning techniques. While CNN-based methods often excel in matching speed, transformer-based methods tend to provide more accurate matches. We propose an efficient transformer-based network architecture for local feature matching.This technique is built on constructing multiple homography hypotheses to approximate the continuous correspondence in the real world and uni-directional cross-attention to accelerate the refinement. On the YFCC100M dataset, our matching accuracy is competitive with LoFTR, a state-of-the-art transformer-based architecture, while the inference speed is boosted to 4 times, even outperforming the CNN-based methods.Comprehensive evaluations on other open datasets such as Megadepth, ScanNet, and HPatches demonstrate our method's efficacy, highlighting its potential to significantly enhance a wide array of downstream applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-184" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-184', event_id='96689', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6610</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96689">FlexSBDD: Structure-Based Drug Design with Flexible Protein Modeling</a></strong></h5>


                        <p class="text-muted">
                            ZAIXI ZHANG &middot; Mengdi Wang &middot; Qi Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Structure-based drug design (SBDD), which aims to generate 3D ligand molecules binding to target proteins, is a fundamental task in drug discovery. Existing SBDD methods typically treat protein as rigid and neglect protein structural change when binding with ligand molecules, leading to a big gap with real-world scenarios and inferior generation qualities (e.g., many steric clashes). To bridge the gap, we propose FlexSBDD, a deep generative model capable of accurately modeling the flexible protein-ligand complex structure for ligand molecule generation. FlexSBDD adopts an efficient flow matching framework and leverages E(3)-equivariant network with scalar-vector dual representation to model dynamic structural changes. Moreover, novel data augmentation schemes based on structure relaxation/sidechain repacking are adopted to boost performance. Extensive experiments demonstrate that FlexSBDD achieves state-of-the-art performance in generating high-affinity molecules and effectively modeling the protein's conformation change to increase favorable protein-ligand interactions (e.g., Hydrogen bonds) and decrease steric clashes.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-185" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-185', event_id='96439', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6612</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96439">Provably and Practically Efficient Adversarial Imitation Learning with General Function Approximation</a></strong></h5>


                        <p class="text-muted">
                            Tian Xu &middot; Zhilong Zhang &middot; Ruishuo Chen &middot; Yihao Sun &middot; Yang Yu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>As a prominent category of imitation learning methods, adversarial imitation learning (AIL) has garnered significant practical success powered by neural network approximation. However, existing theoretical studies on AIL are primarily limited to simplified scenarios such as tabular and linear function approximation and involve complex algorithmic designs that hinder practical implementation, highlighting a gap between theory and practice. In this paper, we explore the theoretical underpinnings of online AIL with general function approximation. We introduce a new method called optimization-based AIL (OPT-AIL), which centers on performing online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions. Theoretically, we prove that OPT-AIL achieves polynomial expert sample complexity and interaction complexity for learning near-expert policies. To our best knowledge, OPT-AIL is the first provably efficient AIL method with general function approximation. Practically, OPT-AIL only requires the approximate optimization of two objectives, thereby facilitating practical implementation. Empirical studies demonstrate that OPT-AIL outperforms previous state-of-the-art deep AIL methods in several challenging tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-186" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-186', event_id='95148', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6700</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95148">OTTER: Effortless Label Distribution Adaptation of Zero-shot Models</a></strong></h5>


                        <p class="text-muted">
                            Changho Shin &middot; Jitian Zhao &middot; Sonia Cromp &middot; Harit Vishwakarma &middot; Frederic Sala
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Popular zero-shot models suffer due to artifacts inherited from pretraining. One particularly detrimental issue, caused by unbalanced web-scale pretraining data, is mismatched label distribution. Existing approaches that seek to repair the label distribution are not suitable in zero-shot settings, as they have mismatching  requirements, such as needing access to labeled downstream task data or knowledge of the true label balance in the pretraining distribution. We sidestep these challenges and introduce a simple and lightweight approach to adjust pretrained model predictions via optimal transport. Our technique requires only an estimate of the label distribution of a downstream task.  Theoretically, we characterize the improvement produced by our procedure under certain mild conditions and provide bounds on the error caused by misspecification. Empirically, we validate our method in a wide array of zero-shot image and text classification tasks, improving accuracy by 4.8% and 15.9% on average, and beating baselines like prior matching---often by significant margins---in 17 out of 21 datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-187" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-187', event_id='95355', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6701</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95355">Decomposable Transformer Point Processes</a></strong></h5>


                        <p class="text-muted">
                            Aristeidis Panos
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The standard paradigm of modeling marked point processes is by parameterizing the intensity function using an attention-based (Transformer-style) architecture. Despite the flexibility of these methods, their inference is based on the computationally intensive thinning algorithm. In this work, we propose a framework where the advantages of the attention-based architecture are maintained and the limitation of the thinning algorithm is circumvented. The framework depends on modeling the conditional distribution of inter-event times with a mixture of log-normals satisfying a Markov property and the conditional probability mass function for the marks with a Transformer-based architecture. The proposed method attains state-of-the-art performance in predicting the next event of a sequence given its history. The experiments also reveal the efficacy of the methods that do not rely on the thinning algorithm during inference over the ones they do. Finally, we test our method on the challenging long-horizon prediction task and find that it outperforms a baseline developed specifically for tackling this task; importantly, inference requires just a fraction of time compared to the thinning-based baseline.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-188" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-188', event_id='95453', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6702</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95453">Enhancing LLM Reasoning via Vision-Augmented Prompting</a></strong></h5>


                        <p class="text-muted">
                            Ziyang Xiao &middot; Dongxiang Zhang &middot; Xiongwei Han &middot; Xiaojin Fu &middot; Wing Yin YU &middot; Tao Zhong &middot; Sai Wu &middot; Yuan Wang &middot; Jianwei Yin &middot; Gang Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Verbal and visual-spatial information processing are two critical subsystems that activate different brain regions and often collaborate together for cognitive reasoning. Despite the rapid advancement of LLM-based reasoning, the mainstream frameworks, such as Chain-of-Thought (CoT) and its variants, primarily focus on the verbal dimension, resulting in limitations in tackling reasoning problems with visual and spatial clues. To bridge the gap, we propose a novel dual-modality reasoning framework called Vision-Augmented Prompting (VAP). Upon receiving a textual problem description, VAP automatically synthesizes an image from the visual and spatial clues by utilizing external drawing tools. Subsequently, VAP formulates a chain of thought in both modalities and iteratively refines the synthesized image. Finally, a conclusive reasoning scheme based on self-alignment is proposed for final result generation. Extensive experiments are conducted across four versatile tasks, including solving geometry problems, Sudoku, time series prediction, and travelling salesman problem. The results validated the superiority of VAP over existing LLMs-based reasoning frameworks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-189" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-189', event_id='95538', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6703</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95538">From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Pusen Dong &middot; Tianchen Zhu &middot; yue qiu &middot; Haoyi Zhou &middot; Jianxin Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Safe reinforcement learning (RL) requires the agent to finish a given task while obeying specific constraints. Giving constraints in natural language form has great potential for practical scenarios due to its flexible transfer capability and accessibility. Previous safe RL methods with natural language constraints typically need to design cost functions manually for each constraint, which requires domain expertise and lacks flexibility. In this paper, we harness the dual role of text in this task, using it not only to provide constraint but also as a training signal. We introduce the Trajectory-level Textual Constraints Translator (TTCT) to replace the manually designed cost function. Our empirical results demonstrate that TTCT effectively comprehends textual constraint and trajectory, and the policies trained by TTCT can achieve a lower violation rate than the standard cost function. Extra studies are conducted to demonstrate that the TTCT has zero-shot transfer capability to adapt to constraint-shift environments.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-190" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-190', event_id='95629', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6704</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95629">Injecting Undetectable Backdoors in Obfuscated Neural Networks and Language Models</a></strong></h5>


                        <p class="text-muted">
                            Alkis Kalavasis &middot; Amin Karbasi &middot; Argyris Oikonomou &middot; Katerina Sotiraki &middot; Grigoris Velegkas &middot; Manolis Zampetakis
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>As ML models become increasingly complex and integral to high-stakes domains such as finance and healthcare, they also become more susceptible to sophisticated adversarial attacks. We investigate the threat posed by undetectable backdoors, as defined in Goldwasser et al. [2022], in models developed by insidious external expert firms. When such backdoors exist, they allow the designer of the model to sell information on how to slightly perturb their input to change the outcome of the model. We develop a general strategy to plant backdoors to obfuscated neural networks, that satisfy the security properties of the celebrated notion of indistinguishability obfuscation. Applying obfuscation before releasing neural networks is a strategy that is well motivated to protect sensitive information of the external expert firm. Our method to plant backdoors ensures that even if the weights and architecture of the obfuscated model are accessible, the existence ofthe backdoor is still undetectable. Finally, we introduce the notion of undetectable backdoors to language models and extend our neural network backdoor attacks to such models based on the existence of steganographic functions.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-191" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-191', event_id='95803', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6705</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95803">SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation</a></strong></h5>


                        <p class="text-muted">
                            Hang Yin &middot; Xiuwei Xu &middot; Zhenyu Wu &middot; Jie Zhou &middot; Jiwen Lu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we propose a new framework for zero-shot object navigation.Existing zero-shot object navigation methods prompt LLM with the text of spatially closed objects, which lacks enough scene context for in-depth reasoning.To better preserve the information of environment and fully exploit the reasoning ability of LLM, we propose to represent the observed scene with 3D scene graph. The scene graph encodes the relationships between objects, groups and rooms with a LLM-friendly structure, for which we design a hierarchical chain-of-thought prompt to help LLM reason the goal location according to scene context by traversing the nodes and edges.Moreover, benefit from the scene graph representation, we further design a re-perception mechanism to empower the object navigation framework with the ability to correct perception error.We conduct extensive experiments on MP3D, HM3D and RoboTHOR environments, where SG-Nav surpasses previous state-of-the-art zero-shot methods by more than \textbf{10\%} SR on all benchmarks, while the decision process is explainable. To the best of our knowledge, SG-Nav is the first zero-shot method that achieves even higher performance than supervised object navigation methods on the challenging MP3D benchmark.Code of this project will be released in the final version.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-192" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-192', event_id='95901', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6706</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95901">CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training</a></strong></h5>


                        <p class="text-muted">
                            David Brandfonbrener &middot; Hanlin Zhang &middot; Andreas Kirsch &middot; Jonathan Richard Schwarz &middot; Sham Kakade
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models. A major challenge lies in identifying this optimal subset, a problem generally considered intractable, thus necessitating scalable and effective heuristics. In this work, we propose a data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering), which leverages an empirical Bayes-inspired approach to derive a simple and computationally efficient selection criterion based on the relative loss values of two auxiliary models.In addition to the modeling rationale, we evaluate CoLoR-Filter empirically on two language modeling tasks: (1) selecting data from C4 for domain adaptation to evaluation on Books and (2) selecting data from C4 for a suite of downstream multiple-choice question answering tasks. We demonstrate favorable scaling both as we subselect more aggressively and using small auxiliary models to select data for large target models. As one headline result, CoLoR-Filter data selected using a pair of 150m parameter auxiliary models can train a 1.2b parameter target model to match a 1.2b parameter model trained on 25b randomly selected tokens with 25x less data for Books and 11x less data for the downstream tasks. Code: https://github.com/davidbrandfonbrener/color-filter-olmoFiltered data: https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-193" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-193', event_id='96032', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6707</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96032">On the Parameter Identifiability of Partially Observed Linear Causal Models</a></strong></h5>


                        <p class="text-muted">
                            Xinshuai Dong &middot; Ignavier Ng &middot; Biwei Huang &middot; Yuewen Sun &middot; Songyao Jin &middot; Roberto Legaspi &middot; Peter Spirtes &middot; Kun Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Linear causal models are important tools for modeling causal dependencies and yet in practice, only a subset of the variables can be observed. In this paper,  we examine the parameter identifiability of these models by investigating whether the edge coefficients can be recovered given the causal structure and partially observed data. Our setting is more general than that of prior researchwe allow all variables, including both observed and latent ones, to be flexibly related, and we consider the coefficients of all edges, whereas most existing works focus only on the edges between observed variables. Theoretically, we identify three types of indeterminacy for the parameters in partially observed linear causal models. We then provide graphical conditions that are sufficient for all parameters to be identifiable and show that some of them are provably necessary. Methodologically, we propose a novel likelihood-based parameter estimation method that addresses the variance indeterminacy of latent variables in a specific way and can asymptotically recover the underlying parameters up to trivial indeterminacy. Empirical studies on both synthetic and real-world datasets validate our identifiability theory and the effectiveness of the proposed method in the finite-sample regime.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-194" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-194', event_id='96168', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6708</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96168">Out-Of-Distribution Detection with Diversification (Provably)</a></strong></h5>


                        <p class="text-muted">
                            Haiyun Yao &middot; Zongbo Han &middot; Huazhu Fu &middot; Xi Peng &middot; Qinghua Hu &middot; Changqing Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Out-of-distribution (OOD) detection is crucial for ensuring reliable deployment of machine learning models. Recent advancements focus on utilizing easily accessible auxiliary outliers (e.g., data from the web or other datasets) in training. However, we experimentally reveal that these methods still struggle to generalize their detection capabilities to unknown OOD data, due to the limited diversity of the auxiliary outliers collected. Therefore, we thoroughly examine this problem from the generalization perspective and demonstrate that a more diverse set of auxiliary outliers is essential for enhancing the detection capabilities. However, in practice, it is difficult and costly to collect sufficiently diverse auxiliary outlier data. Therefore, we propose a simple yet practical approach with a theoretical guarantee, termed Diversity-induced Mixup for OOD detection (diverseMix), which enhances the diversity of auxiliary outlier set for training in an efficient way. Extensive experiments show that diverseMix achieves superior performance on commonly used and recent challenging large-scale benchmarks, which further confirm the importance of the diversity of auxiliary outliers.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-195" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-195', event_id='96295', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6709</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96295">RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold</a></strong></h5>


                        <p class="text-muted">
                            Amrith Setlur &middot; Saurabh Garg &middot; Xinyang Geng &middot; Naman Garg &middot; Virginia Smith &middot; Aviral Kumar
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this question for math reasoning via an empirical study, followed by building a conceptual understanding of our observations. First, we find that while the typical approach of finetuning a model on synthetic correct or positive problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner itself followed by subsequent fine-tuning on this self-generated data doubles the efficiency of the same synthetic problems. At the same time, training on model-generated positives can amplify various spurious  correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize negative responses, i.e., model-generated responses that are deemed incorrect by a final answer verifier. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or advantage of each intermediate step in the negative response. With this per-step scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by $\mathbf{8 \times}$. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits robustness benefits of RL over imitating positive data alone.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-196" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-196', event_id='96311', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6710</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96311">MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution</a></strong></h5>


                        <p class="text-muted">
                            Wenzhuo Liu &middot; Fei Zhu &middot; Shijie Ma &middot; Cheng-lin Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Although Vision Transformers (ViTs) have recently advanced computer vision tasks significantly, an important real-world problem was overlooked: adapting to variable input resolutions. Typically, images are resized to a fixed resolution, such as 224x224, for efficiency during training and inference. However, uniform input size conflicts with real-world scenarios where images naturally vary in resolution. Modifying the preset resolution of a model may severely degrade the performance. In this work, we propose to enhance the model adaptability to resolution variation by optimizing the patch embedding. The proposed method, called Multi-Scale Patch Embedding (MSPE), substitutes the standard patch embedding with multiple variable-sized patch kernels and selects the best parameters for different resolutions, eliminating the need to resize the original image. Our method does not require high-cost training or modifications to other parts, making it easy to apply to most ViT models. Experiments in image classification, segmentation, and detection tasks demonstrate the effectiveness of MSPE, yielding superior performance on low-resolution inputs and performing comparably on high-resolution inputs with existing methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-197" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-197', event_id='95104', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6800</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95104">Ensemble sampling for linear bandits: small ensembles suffice</a></strong></h5>


                        <p class="text-muted">
                            David Janz &middot; Alexander Litvak &middot; Csaba Szepesvari
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We provide the first useful and rigorous analysis of ensemble sampling for the stochastic linear bandit setting. In particular, we show that, under standard assumptions, for a $d$-dimensional stochastic linear bandit with an interaction horizon $T$, ensemble sampling with an ensemble of size of order $\smash{d \log T}$ incurs regret at most of the order $\smash{(d \log T)^{5/2} \sqrt{T}}$. Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly with $T$---which defeats the purpose of ensemble sampling---while obtaining  near $\smash{\sqrt{T}}$ order regret. Ours is also the first result that allows infinite action sets.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-198" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-198', event_id='94993', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6801</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94993">Interfacing Foundation Models&#x27; Embeddings</a></strong></h5>


                        <p class="text-muted">
                            Xueyan Zou &middot; Linjie Li &middot; Jianfeng Wang &middot; Jianwei Yang &middot; Mingyu Ding &middot; Junyi Wei &middot; Zhengyuan Yang &middot; Feng Li &middot; Hao Zhang &middot; Shilong Liu &middot; Arul Aravinthan &middot; Yong Jae Lee &middot; Lijuan Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Foundation models possess strong capabilities in reasoning and memorizing across modalities. To further unleash the power of foundation models, we present FIND, a generalized interface for aligning foundation models' embeddings with unified image and dataset-level understanding spanning modality and granularity. As shown in Fig.1, a lightweight transformer interface without tuning any foundation model weights is enough for segmentation, grounding, and retrieval in an interleaved manner. The proposed interface has the following favorable attributes: (1) Generalizable. It applies to various tasks spanning retrieval, segmentation, etc., under the same architecture and weights.  (2) Interleavable. With the benefit of multi-task multi-modal training, the proposed interface creates an interleaved shared embedding space. (3) Extendable. The proposed interface is adaptive to new tasks, and new models. In light of the interleaved embedding space, we introduce FIND-Bench, which introduces new training and evaluation annotations to the COCO dataset for interleaved segmentation and retrieval. We are the first work aligning foundations models' embeddings for interleave understanding. Meanwhile, our approach achieves state-of-the-art performance on FIND-Bench and competitive performance on standard retrieval and segmentation settings.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-199" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-199', event_id='94988', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6802</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94988">Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement</a></strong></h5>


                        <p class="text-muted">
                            Zhi Wang &middot; Li Zhang &middot; Wenhao Wu &middot; Yuanheng Zhu &middot; Dongbin Zhao &middot; Chunlin Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>A longstanding goal of artificial general intelligence is highly capable generalists that can learn from diverse experiences and generalize to unseen tasks. The language and vision communities have seen remarkable progress toward this trend by scaling up transformer-based models trained on massive datasets, while reinforcement learning (RL) agents still suffer from poor generalization capacity under such paradigms. To tackle this challenge, we propose Meta Decision Transformer (Meta-DT), which leverages the sequential modeling ability of the transformer architecture and robust task representation learning via world model disentanglement to achieve efficient generalization in offline meta-RL. We pretrain a context-aware world model to learn a compact task representation, and inject it as a contextual condition to the causal transformer to guide task-oriented sequence generation. Then, we subtly utilize history trajectories generated by the meta-policy as a self-guided prompt to exploit the architectural inductive bias. We select the trajectory segment that yields the largest prediction error on the pretrained world model to construct the prompt, aiming to encode task-specific information complementary to the world model maximally. Notably, the proposed framework eliminates the requirement of any expert demonstration or domain knowledge at test time. Experimental results on MuJoCo and Meta-World benchmarks across various dataset types show that Meta-DT exhibits superior few and zero-shot generalization capacity compared to strong baselines while being more practical with fewer prerequisites. Our code is available at https://github.com/NJU-RL/Meta-DT.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-200" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-200', event_id='94913', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6803</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94913">Connectivity-Driven Pseudo-Labeling Makes Stronger Cross-Domain Segmenters</a></strong></h5>


                        <p class="text-muted">
                            Dong Zhao &middot; Qi Zang &middot; Shuang Wang &middot; Nicu Sebe &middot; Zhun Zhong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Presently, pseudo-labeling stands as a prevailing approach in cross-domain semantic segmentation, enhancing model efficacy by training with pixels assigned with reliable pseudo-labels.  However, we identify two key limitations within this paradigm: (1) under relatively severe domain shifts, most selected reliable pixels appear speckled and remain noisy. (2) when dealing with wild data, some pixels belonging to the open-set class may exhibit high confidence and also appear speckled. These two points make it difficult for the pixel-level selection mechanism to identify and correct these speckled close- and open-set noises. As a result, error accumulation is continuously introduced into subsequent self-training, leading to inefficiencies in pseudo-labeling. To address these limitations, we propose a novel method called Semantic Connectivity-driven Pseudo-labeling (SeCo).  SeCo formulates pseudo-labels at the connectivity level, which makes it easier to locate and correct closed and open set noise. Specifically, SeCo comprises two key components: Pixel Semantic Aggregation (PSA) and Semantic Connectivity Correction (SCC). Initially, PSA categorizes semantics into <code>stuff'' and</code>things'' categories and aggregates speckled pseudo-labels into semantic connectivity through efficient interaction with the Segment Anything Model (SAM).  This enables us not only to obtain accurate boundaries but also simplifies noise localization. Subsequently, SCC introduces a simple connectivity classification task, which enables us to locate and correct connectivity noise with the guidance of loss distribution. Extensive experiments demonstrate that SeCo can be flexibly applied to various cross-domain semantic segmentation tasks, \textit{i.e.} domain generalization and domain adaptation,  even including source-free, and black-box domain adaptation, significantly improving the performance of existing state-of-the-art methods. The code is provided in the appendix and will be open-source.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-201" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-201', event_id='94897', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6804</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94897">PageRank Bandits for Link Prediction</a></strong></h5>


                        <p class="text-muted">
                            Yikun Ban &middot; Jiaru Zou &middot; Zihao Li &middot; Yunzhe Qi &middot; Dongqi Fu &middot; Jian Kang &middot; Hanghang Tong &middot; Jingrui He
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Link prediction is a critical problem in graph learning with broad applications such as recommender systems and knowledge graph completion. Numerous research efforts have been directed at solving this problem, including approaches based on similarity metrics and Graph Neural Networks (GNN). However, most existing solutions are still rooted in conventional supervised learning, which makes it challenging to adapt over time to changing customer interests and to address the inherent dilemma of exploitation versus exploration in link prediction.To tackle these challenges, this paper reformulates link prediction as a sequential decision-making process, where each link prediction interaction occurs sequentially. We propose a novel fusion algorithm, PRB (PageRank Bandits), which is the first to combine contextual bandits with PageRank for collaborative exploitation and exploration. We also introduce a new reward formulation and provide a theoretical performance guarantee for PRB. Finally, we extensively evaluate PRB in both online and offline settings, comparing it with bandit-based and graph-based methods. The empirical success of PRB demonstrates the value of the proposed fusion approach. Our code is released at https://github.com/jiaruzouu/PRB.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-202" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-202', event_id='94845', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6805</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94845">Efficient Reinforcement Learning by Discovering Neural Pathways</a></strong></h5>


                        <p class="text-muted">
                            Samin Yeasar Arnob &middot; Riyasat Ohib &middot; Sergey Plis &middot; Amy Zhang &middot; Alessandro Sordoni &middot; Doina Precup
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Reinforcement learning (RL) algorithms have been very successful at tackling complex control problems, such as AlphaGo or fusion control. However, current research mainly emphasizes solution quality, often achieved by using large models trained on large amounts of data, and does not account for the financial, environmental, and societal costs associated with developing and deploying such models. Modern neural networks are often overparameterized and a significant number of parameters can be pruned without meaningful loss in performance, resulting in more efficient use of the model's capacity lottery ticket. We present a methodology for identifying sub-networks within a larger network in reinforcement learning (RL). We call such sub-networks, neural pathways. We show empirically that even very small learned sub-networks, using less than 5%  of the large network's parameters, can provide very good quality solutions. We also demonstrate the training of multiple pathways within the same networks in a multitask setup, where each pathway is encouraged to tackle a separate task. We evaluate empirically our approach on several continuous control tasks, in both online and offline training</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-203" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-203', event_id='94843', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6806</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94843">ALPINE: Unveiling The Planning Capability of Autoregressive Learning in Language Models</a></strong></h5>


                        <p class="text-muted">
                            Siwei Wang &middot; Yifei Shen &middot; Shi Feng &middot; Haoran Sun &middot; Shang-Hua Teng &middot; Wei Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Planning is a crucial element of both human intelligence and contemporary large language models (LLMs). In this paper, we initiate a theoretical investigation into the emergence of planning capabilities in Transformer-based LLMs via their next-word prediction mechanisms. We model planning as a network path-finding task, where the objective is to generate a valid path from a specified source node to a designated target node. Our mathematical characterization  shows that Transformer architectures can execute path-finding by embedding the adjacency and reachability matrices within their weights. Furthermore, our theoretical analysis of gradient-based learning dynamics reveals that LLMs can learn both the adjacency  and a limited form of the reachability matrices. These theoretical insights are then validated through experiments, which demonstrate that Transformer architectures indeed learn the adjacency and an incomplete reachability matrices, consistent with our theoretical predictions. When applying our methodology to the real-world planning benchmark Blocksworld, our observations remain consistent. Additionally, our analyses uncover a fundamental limitation of current Transformer architectures in path-finding: these architectures cannot identify reachability relationships through transitivity, which leads to failures in generating paths when concatenation is required. These findings provide new insights into how the internal mechanisms of autoregressive learning facilitate intelligent planning and deepen our understanding of how future LLMs might achieve more advanced and general planning-and-reasoning capabilities across diverse applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-204" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-204', event_id='94814', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6807</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94814">Gliding over the Pareto Front with Uniform Designs</a></strong></h5>


                        <p class="text-muted">
                            Xiaoyuan Zhang &middot; Genghui Li &middot; Xi Lin &middot; Yichi Zhang &middot; Yifan Chen &middot; Qingfu Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Multiobjective optimization (MOO) plays a critical role in various real-world domains. A major challenge therein is generating $K$ uniform Pareto-optimal solutions that represent the entire Pareto front. To address the very challenge, this study first introduces \emph{fill distance} to evaluate the $K$ design points, which provides a quantitative metric for the representativeness of the design. However, the direct specification of the optimal design that minimizes fill distance is almost intractable considering the nested $\min-\max-\min$ optimization problem. We further propose a surrogate to the fill distance, which is easier to optimize and induce a rate-optimal design whose fill distance proves at most $4\times$ the minimum one. Rigorous derivation also shows that asymptotically this induced design will converge to the uniform measure over the Pareto front. Extensive experiments on synthetic and real-world benchmarks demonstrate that our proposed paradigm efficiently produces high-quality, representative solutions and outperforms baseline methods.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-205" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-205', event_id='94760', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6808</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94760">Gradient Rewiring for Editable Graph Neural Network Training</a></strong></h5>


                        <p class="text-muted">
                            Zhimeng Jiang &middot; Zirui Liu &middot; Xiaotian Han &middot; Qizhang Feng &middot; Hongye Jin &middot; Qiaoyu Tan &middot; Kaixiong Zhou &middot; Na Zou &middot; Xia Hu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deep neural networks are ubiquitously adopted in many applications, such as computer vision, natural language processing, and graph analytics. However, well-trained neural networks can make prediction errors after deployment as the world changes. \textit{Model editing} involves updating the base model to correct prediction errors with less accessible training data and computational resources.Despite recent advances in model editors in computer vision and natural language processing, editable training in graph neural networks (GNNs) is rarely explored. The challenge with editable GNN training lies in the inherent information aggregation across neighbors, which can lead model editors to affect the predictions of other nodes unintentionally. In this paper, we first observe the gradient of cross-entropy loss for the target node and training nodes with significant inconsistency, which indicates that directly fine-tuning the base model using the loss on the target node deteriorates the performance on training nodes. Motivated by the gradient inconsistency observation, we propose a simple yet effective \underline{G}radient \underline{R}ewiring method for \underline{E}ditable graph neural network training, named \textbf{GRE}. Specifically, we first store the anchor gradient of the loss on training nodes to preserve the locality. Subsequently, we rewire the gradient of the loss on the target node to preserve performance on the training node using anchor gradient. Experiments demonstrate the effectiveness of GRE on various model architectures and graph datasets in terms of multiple editing situations. The source code is available at \url{https://github.com/zhimengj0326/Gradient<em>rewiring</em>editing}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-206" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-206', event_id='94656', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6809</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94656">Neural Flow Diffusion Models: Learnable Forward Process for Improved Diffusion Modelling</a></strong></h5>


                        <p class="text-muted">
                            Grigory Bartosh &middot; Dmitry Vetrov &middot; Christian Andersson Naesseth
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Conventional diffusion models typically relies on a fixed forward process, which implicitly defines complex marginal distributions over latent variables. This can often complicate the reverse process task in learning generative trajectories, and results in costly inference for diffusion models. To address these limitations, we introduce Neural Flow Diffusion Models (NFDM), a novel framework that enhances diffusion models by supporting a broader range of forward processes beyond the standard Gaussian. We also propose a novel parameterization technique for learning the forward process. Our framework provides an end-to-end, simulation-free optimization objective, effectively minimizing a variational upper bound on the negative log-likelihood. Experimental results demonstrate NFDMs strong performance, evidenced by state-of-the-art likelihood estimation. Furthermore, we investigate NFDMs capacity for learning generative dynamics with specific characteristics, such as deterministic straight lines trajectories, and demonstrate how the framework may be adopted for learning bridges between two distributions. The results underscores NFDMs versatility and its potential for a wide range of applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-207" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-207', event_id='94644', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6810</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94644">DeTrack: In-model Latent Denoising Learning for Visual Object Tracking</a></strong></h5>


                        <p class="text-muted">
                            Xinyu Zhou &middot; Jinglun Li &middot; Lingyi Hong &middot; Kaixun Jiang &middot; Pinxue Guo &middot; Weifeng Ge &middot; Wenqiang Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Previous visual object tracking methods employ image-feature regression models or coordinate autoregression models for bounding box prediction. Image-feature regression methods heavily depend on matching results and do not utilize positional prior, while the autoregressive approach can only be trained using bounding boxes available in the training set, potentially resulting in suboptimal performance during testing with unseen data. Inspired by the diffusion model, denoising learning enhances the models robustness to unseen data. Therefore, We introduce noise to bounding boxes, generating noisy boxes for training, thus enhancing model robustness on testing data. We propose a new paradigm to formulate the visual object tracking problem as a denoising learning process. However, tracking algorithms are usually asked to run in real-time, directly applying the diffusion model to object tracking would severely impair tracking speed. Therefore, we decompose the denoising learning process into every denoising block within a model, not by running the model multiple times, and thus we summarize the proposed paradigm as an in-model latent denoising learning process. Specifically, we propose a denoising Vision Transformer (ViT), which is composed of multiple denoising blocks. In the denoising block, template and search embeddings are projected into every denoising block as conditions. A denoising block is responsible for removing the noise in a predicted bounding box, and multiple stacked denoising blocks cooperate to accomplish the whole denoising process. Subsequently, weutilize image features and trajectory information to refine the denoised bounding box. Besides, we also utilize trajectory memory and visual memory to improve tracking stability. Experimental results validate the effectiveness of our approach, achieving competitive performance on several challenging datasets. The proposed in-model latent denoising tracker achieve real-time speed, rendering denoising learning applicable in the visual object tracking community.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-208" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-208', event_id='93124', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6900</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93124">An Analysis of Tokenization: Transformers under Markov Data</a></strong></h5>


                        <p class="text-muted">
                            Nived Rajaraman &middot; Jiantao Jiao &middot; Kannan Ramchandran
                        </p>

                    </div>
                    <div class="abstract">
                        <p>While there has been a large body of research attempting to circumvent tokenization for language modeling (Clark et al. 2022, Xue et al. 2022), the current consensus is that it is a necessary initial step for designing state-of-the-art performant language models. In this paper, we investigate tokenization from a theoretical point of view by studying the behavior of transformers on simple data generating processes. When trained on data drawn from certain simple $k^{\text{th}}$-order Markov processes for $k > 1$, transformers exhibit a surprising phenomenon - in the absence of tokenization, they empirically are incredibly slow or fail to learn the right distribution and predict characters according to a unigram model (Makkuva et al. 2024). With the addition of tokenization, however, we empirically observe that transformers break through this barrier and are able to model the probabilities of sequences drawn from the source near-optimally, achieving small cross-entropy loss. With this observation as starting point, we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization. With the appropriate tokenization, we show that even the simplest unigram models (over tokens) learnt by transformers are able to model the probability of sequences drawn from $k^{\text{th}}$-order Markov sources near optimally. Our analysis provides a justification for the use of tokenization in practice through studying the behavior of transformers on Markovian data.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-209" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-209', event_id='93546', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6901</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93546">Data Free Backdoor Attacks</a></strong></h5>


                        <p class="text-muted">
                            Bochuan Cao &middot; Jinyuan Jia &middot; Chuxuan Hu &middot; Wenbo Guo &middot; Zhen Xiang &middot; Jinghui Chen &middot; Bo Li &middot; Dawn Song
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Backdoor attacks aim to inject a backdoor into a classifier such that it predicts any input with an attacker-chosen backdoor trigger as an attacker-chosen target class. Existing backdoor attacks require either retraining the classifier with some clean data or modifying the model's architecture.As a result, they are 1) not applicable when clean data is unavailable, 2) less efficient when the model is large, and 3) less stealthy due to architecture changes. In this work, we propose DFBA, a novel retraining-free and data-free backdoor attack without changing the model architecture. Technically, our proposed method modifies a few parameters of a classifier to inject a backdoor. Through theoretical analysis, we verify that our injected backdoor is provably undetectable and unremovable by various state-of-the-art defenses under mild assumptions. Our evaluation on multiple datasets further demonstrates that our injected backdoor: 1) incurs negligible classification loss, 2) achieves 100\% attack success rates, and 3) bypasses six existing state-of-the-art defenses. Moreover, our comparison with a state-of-the-art non-data-free backdoor attack shows our attack is more stealthy and effective against various defenses while achieving less classification accuracy loss.We will release our code upon paper acceptance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-210" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-210', event_id='93811', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6902</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93811">Shuffling Gradient-Based Methods for Nonconvex-Concave Minimax Optimization</a></strong></h5>


                        <p class="text-muted">
                            Quoc Tran Dinh &middot; Trang H. Tran &middot; Lam Nguyen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper aims at developing novel shuffling gradient-based methods for tackling two classes of minimax problems: nonconvex-linear and nonconvex-strongly concave settings. The first algorithm addresses the nonconvex-linear minimax model and achieves the state-of-the-art oracle complexity typically observed in nonconvex optimization.  It also employs a new shuffling estimator for the ``hyper-gradient'', departing from standard shuffling techniques in optimization. The second method consists of two variants: semi-shuffling and full-shuffling schemes.  These variants tackle the nonconvex-strongly concave minimax setting.  We establish their oracle complexity bounds under standard assumptions, which, to our best knowledge, are the best-known for this specific setting. Numerical examples demonstrate the performance of our algorithms and compare them with two other methods.  Our results show that the new methods achieve comparable performance with SGD, supporting the potential of incorporating shuffling strategies into minimax algorithms.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-211" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-211', event_id='93850', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6903</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93850">QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization</a></strong></h5>


                        <p class="text-muted">
                            Qi Song &middot; Tianxiang Gong &middot; Shiqi Gao &middot; Haoyi Zhou &middot; Jianxin Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multimodal contrastive learning (MCL) has recently demonstrated significant success across various tasks. However, the existing MCL treats all negative samples equally and ignores the potential semantic association with positive samples, which limits the model's ability to achieve fine-grained alignment. In multi-view scenarios, MCL tends to prioritize shared information while neglecting modality-specific unique information across different views, leading to feature suppression and suboptimal performance in downstream tasks. To address these limitations, we propose a novel contrastive framework name <em>QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization</em>. In the QUEST framework, we propose quaternion contrastive objectives and orthogonal constraints to extract sufficient unique information. Meanwhile, a shared information-guided penalization is introduced to ensure that shared information does not excessively influence the optimization of unique information. Our method leverages quaternion vector spaces to simultaneously optimize shared and unique information. Experiments on multiple datasets show that our method achieves superior performance in multimodal contrastive learning benchmarks. On public benchmark, our approach achieves state-of-the-art performance, and on synthetic shortcut datasets, we outperform existing baseline methods by an average of 97.95\% on the CLIP model.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-212" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-212', event_id='93855', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6904</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93855">Parameter Competition Balancing for Model Merging</a></strong></h5>


                        <p class="text-muted">
                            Guodong DU &middot; Junlin Lee &middot; Jing Li &middot; Runhua Jiang &middot; Yifei Guo &middot; Shuyang Yu &middot; Hanting Liu &middot; Sim Kuan Goh &middot; Ho-Kin Tang &middot; Daojing He &middot; Min Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named <strong>PCB-Merging</strong> (Parameter Competition Balancing), a <em>lightweight</em> and <em>training-free</em> technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-213" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-213', event_id='93860', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6905</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93860">Acceleration Exists! Optimization Problems When Oracle Can Only Compare Objective Function Values</a></strong></h5>


                        <p class="text-muted">
                            Aleksandr Lobanov &middot; Alexander Gasnikov &middot; Andrey Krasnov
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Frequently, the burgeoning field of black-box optimization encounters challenges due to a limited understanding of the mechanisms of the objective function. To address such problems, in this work we focus on the deterministic concept of Order Oracle, which only utilizes order access between function values (possibly with some bounded noise), but without assuming access to their values. As theoretical results, we propose a new approach to create non-accelerated optimization algorithms (obtained by integrating Order Oracle into existing optimization tools) in non-convex, convex, and strongly convex settings that are as good as both SOTA coordinate algorithms with first-order oracle and SOTA algorithms with Order Oracle up to logarithm factor. Moreover, using the proposed approach, <em>we provide the first accelerated optimization algorithm using the Order Oracle</em>. And also, using an already different approach we provide the asymptotic convergence of <em>the first algorithm with the stochastic Order Oracle concept</em>. Finally, our theoretical results demonstrate effectiveness of proposed algorithms through numerical experiments.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-214" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-214', event_id='94048', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6906</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94048">A Bayesian Approach for Personalized Federated Learning in Heterogeneous Settings</a></strong></h5>


                        <p class="text-muted">
                            Disha Makhija &middot; Joydeep Ghosh &middot; Nhat Ho
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Federated learning (FL), through its privacy-preserving collaborative learning approach, has significantly empowered decentralized devices. However,  constraints in either data and/or computational resources among participating clients introduce several challenges in learning, including the inability to train large model architectures, heightened risks of overfitting, and more. In this work, we present a novel FL framework grounded in Bayesian learning to address these challenges. Our approach involves training personalized Bayesian models at each client tailored to the unique complexities of the clients' datasets and efficiently collaborating across these clients. By leveraging Bayesian neural networks and their uncertainty quantification capabilities, our local training procedure robustly learns from small datasets. And the novel collaboration procedure utilizing priors in the functional (output) space of the networks facilitates collaboration across models of varying sizes, enabling the framework to adapt well in heterogeneous data and computational settings. Furthermore, we present a differentially private version of the algorithm, accompanied by formal differential privacy guarantees that apply without any assumptions on the learning algorithm. Through experiments on popular FL datasets, we demonstrate that our approach outperforms strong baselines in both homogeneous and heterogeneous settings, and under strict privacy constraints.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-215" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-215', event_id='94077', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6907</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94077">On the Target-kernel Alignment: a Unified Analysis with Kernel Complexity</a></strong></h5>


                        <p class="text-muted">
                            Chao Wang &middot; Xin HE &middot; Yuwen Wang &middot; Junhui Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper investigates the impact of alignment between the target function of interest and the kernel matrix on a variety of kernel-based methods based on a general loss belonging to a rich loss function family, which covers many commonly used methods in regression and classification problems. We consider the truncated kernel-based method (TKM) which is estimated within a reduced function space constructed by using the spectral truncation of the kernel matrix and compare its theoretical behavior to that of the standard kernel-based method (KM) under various settings. By using the kernel complexity function that quantifies the complexity of the induced function space, we derive the upper bounds for both TKM and KM, and further reveal their dependencies on the degree of target-kernel alignment. Specifically, for the alignment with polynomial decay, the established results indicate that under the just-aligned and weakly-aligned regimes, TKM and KM share the same learning rate. Yet, under the strongly-aligned regime, KM suffers the saturation effect, while TKM can be continuously improved as the alignment becomes stronger. This further implies that TKM has a strong ability to capture the strong alignment and provide a theoretically guaranteed solution to eliminate the phenomena of saturation effect. The minimax lower bound is also established for the squared loss to confirm the optimality of TKM. Extensive numerical experiments further support our theoretical findings. The Python code for reproducing the numerical experiments is available at https://github.com/wywangen.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-216" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-216', event_id='94108', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6908</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94108">KptLLM: Unveiling the Power of Large Language Model for Keypoint Comprehension</a></strong></h5>


                        <p class="text-muted">
                            Jie Yang &middot; Wang ZENG &middot; Sheng Jin &middot; Lumin Xu &middot; Wentao Liu &middot; Chen Qian &middot; Ruimao Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advancements in Multimodal Large Language Models (MLLMs) have greatly improved their abilities in image understanding. However, these models often struggle with grasping pixel-level semantic details, e.g., the keypoints of an object. To bridge this gap, we introduce the novel challenge of Semantic Keypoint Comprehension, which aims to comprehend keypoints across different task scenarios, including keypoint semantic understanding, visual prompt-based keypoint detection, and textual prompt-based keypoint detection. Moreover, we introduce KptLLM, a unified multimodal model that utilizes an identify-then-detect strategy to effectively address these challenges. KptLLM underscores the initial discernment of semantics in keypoints, followed by the precise determination of their positions through a chain-of-thought process. With several carefully designed modules, KptLLM adeptly handles various modality inputs, facilitating the interpretation of both semantic contents and keypoint locations. Our extensive experiments demonstrate KptLLM's superiority in various keypoint detection benchmarks and its unique semantic capabilities in interpreting keypoints.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-217" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-217', event_id='94126', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6909</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94126">MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes</a></strong></h5>


                        <p class="text-muted">
                            Zhenhui Ye &middot; Tianyun Zhong &middot; Yi Ren &middot; Ziyue Jiang &middot; Jiawei Huang &middot; Rongjie Huang &middot; Jinglin Liu &middot; Jinzheng He &middot; Chen Zhang &middot; Zehan Wang &middot; Xize Cheng &middot; Xiang Yin &middot; Zhou Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Talking face generation (TFG) aims to animate a target identity's face to create realistic talking videos. Personalized TFG is a variant that emphasizes the perceptual identity similarity of the synthesized result (from the perspective of appearance and talking style). While previous works typically solve this problem by learning an individual neural radiance field (NeRF) for each identity to implicitly store its static and dynamic information, we find it inefficient and non-generalized due to the per-identity-per-training framework and the limited training data. To this end, we propose MimicTalk, the first attempt that exploits the rich knowledge from a NeRF-based person-agnostic generic model for improving the efficiency and robustness of personalized TFG. To be specific, (1) we first come up with a person-agnostic 3D TFG model as the base model and propose to adapt it into a specific identity; (2) we propose a static-dynamic-hybrid adaptation pipeline to help the model learn the personalized static appearance and facial dynamic features; (3) To generate the facial motion of the personalized talking style, we propose an in-context stylized audio-to-motion model that mimics the implicit talking style provided in the reference video without information loss by an explicit style representation. The adaptation process to an unseen identity can be performed in 15 minutes, which is 47 times faster than previous person-dependent methods. Experiments show that our MimicTalk surpasses previous baselines regarding video quality, efficiency, and expressiveness. Video samples are available at https://mimictalk.github.io .</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-218" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-218', event_id='94291', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#6910</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94291">Unlock the Intermittent Control Ability of Model Free Reinforcement Learning</a></strong></h5>


                        <p class="text-muted">
                            Jiashun Liu &middot; Jianye Hao &middot; Xiaotian Hao &middot; Yi Ma &middot; YAN ZHENG &middot; Yujing Hu &middot; Tangjie Lv
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Intermittent control problems are common in real world. The interactions between the decision maker and the executor can be discontinuous (intermittent) due to various types of interruptions, e.g. unstable communication channel. Due to intermittent interaction, agents are unable to acquire the state sent by the executor and cannot transmit actions to the executor within a period of time step, i.e. bidirectional blockage, which may lead to inefficiencies of reinforcement learning policies and prevent the executors from completing the task. Such problem is not well studied in the RL community. In this paper, we model Intermittent control problem as an Intermittent Control Markov Decision Process, i.e agents are expected to generate action sequences corresponding to the unavailable states and transmit them before disabling interactions to ensure the smooth and effective motion of executors. However, directly generating multiple future actions in the original action space has unnatural motion issue and exploration difficulty. We propose <strong>M</strong>ulti-step <strong>A</strong>ction <strong>R</strong>epre<strong>S</strong>entation (<strong>MARS</strong>), which encodes a sequence of actions from the original action space to a compact and decodable latent space. Then based on the latent action sequence representation, the mainstream RL methods can be easily optimized to learn a smooth and efficient motion policy. Extensive experiments on simulation tasks and real-world robotic grasping tasks show that MARS significantly improves the learning efficiency and final performances compared with existing baselines.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-219" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-219', event_id='92964', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#7000</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92964">Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Masatoshi Uehara &middot; Yulai Zhao &middot; Ehsan Hajiramezanali &middot; Gabriele Scalia &middot; Gokcen Eraslan &middot; Avantika Lal &middot; Sergey Levine &middot; Tommaso Biancalani
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>AI-driven design problems, such as DNA/protein sequence design, are commonly tackled from two angles: generative modeling, which efficiently captures the feasible design space (e.g., natural images or biological sequences), and model-based optimization, which utilizes reward models for extrapolation. To combine the strengths of both approaches, we adopt a hybrid method that fine-tunes cutting-edge diffusion models by optimizing reward models through RL. Although prior work has explored similar avenues, they primarily focus on scenarios where accurate reward models are accessible. In contrast, we concentrate on an offline setting where a reward model is unknown, and we must learn from static offline datasets, a common scenario in scientific domains. In offline scenarios, existing approaches tend to suffer from overoptimization, as they may be misled by the reward model in out-of-distribution regions. To address this, we introduce a conservative fine-tuning approach, BRAID, by optimizing a conservative reward model, which includes additional penalization outside of offline data distributions. Through empirical and theoretical analysis, we demonstrate the capability of our approach to outperform the best designs in offline data, leveraging the extrapolation capabilities of reward models while avoiding the generation of invalid designs through pre-trained diffusion models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-220" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-220', event_id='92953', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#7001</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92953">Neural Gaffer: Relighting Any Object via Diffusion</a></strong></h5>


                        <p class="text-muted">
                            Haian Jin &middot; Yuan Li &middot; Fujun Luan &middot; Yuanbo Xiangli &middot; Sai Bi &middot; Kai Zhang &middot; Zexiang Xu &middot; Jin Sun &middot; Noah Snavely
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Single-image relighting is a challenging task that involves reasoning about the complex interplay between geometry, materials, and lighting. Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight. Alternatively, some methods explicitly decompose a scene into intrinsic components, such as normals and BRDFs, which can be inaccurate or under-expressive. In this work, we propose a novel end-to-end 2D relighting diffusion model, called Neural Gaffer, that takes a single image of any object and can synthesize an accurate, high-quality relit image under any novel environmental lighting condition, simply by conditioning an image generator on a target environment map, without an explicit scene decomposition. Our method builds on a pre-trained diffusion model, and fine-tunes it on a synthetic relighting dataset, revealing and harnessing the inherent understanding of lighting present in the diffusion model. We evaluate our model on both synthetic and in-the-wild Internet imagery and demonstrate its advantages in terms of generalization and accuracy. Moreover, by combining with other generative methods, our model enables many downstream 2D tasks, such as text-based relighting and object insertion. Our model can also operate as a strong relighting prior for 3D tasks, such as relighting a radiance field.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-221" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-221', event_id='92924', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#7002</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92924">Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging</a></strong></h5>


                        <p class="text-muted">
                            JIAMIAN WANG &middot; Zongliang Wu &middot; Yulun Zhang &middot; Xin Yuan &middot; Tao Lin &middot; Zhiqiang Tao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Existing reconstruction models in snapshot compressive imaging systems (SCI) are trained with a single well-calibrated hardware instance, making their perfor- mance vulnerable to hardware shifts and limited in adapting to multiple hardware configurations. To facilitate cross-hardware learning, previous efforts attempt to directly collect multi-hardware data and perform centralized training, which is impractical due to severe user data privacy concerns and hardware heterogeneity across different platforms/institutions. In this study, we explicitly consider data privacy and heterogeneity in cooperatively optimizing SCI systems by proposing a Federated Hardware-Prompt learning (FedHP) framework. Rather than mitigating the client drift by rectifying the gradients, which only takes effect on the learning manifold but fails to solve the heterogeneity rooted in the input data space, FedHP learns a hardware-conditioned prompter to align inconsistent data distribution across clients, serving as an indicator of the data inconsistency among different hardware (e.g., coded apertures). Extensive experimental results demonstrate that the proposed FedHP coordinates the pre-trained model to multiple hardware con- figurations, outperforming prevalent FL frameworks for 0.35dB under challenging heterogeneous settings. Moreover, a Snapshot Spectral Heterogeneous Dataset has been built upon multiple practical SCI systems. Data and code are aveilable at https://github.com/Jiamian-Wang/FedHP-Snapshot-Compressive-Imaging.git</p>
</p>
                    </div>

                </div>
            
        </div>



    




    
        </div>
    

</main>
<!--END BLOCK CONTENT-->


<!--Footer for the edit button-->

    <div id="editFooter" class="noprint" style="width:70px;">

        <button class="editFooterButton btn btn-outline-primary" title="Enable editing of content where possible"
                id="editpage"
                onclick="start_edit();"><i class="far fa-edit"></i></button>
        <button class="editFooterButton btn btn-outline-primary" title="Save edited content and reload the page"
                id="noeditpage"
                onclick="stop_edit();" style="display:none"><i class="fas fa-save"></i>
        </button>
    </div>


<script>

    $(function () {
        if ($(".editable").length == 0) {
            $("#editFooter").hide();
        }
    })
</script>

<script src="/static/core/js/fastclick.min.js" ></script>

<!--We don't know if there are editable tags on the page until after the django template engine has rendered the page. So,
test in javascript for "editable" tags and if present, load the ckeditor engine dynamically. -->

<script>
    if (document.getElementsByClassName('editable').length > 0) {
        var script = document.createElement("script");
        script.type = "text/javascript";
        script.src = "/static/core/ckeditor/4.18/ckeditor.js";    // use this for linked script
        script.text = "alert('voila!');"               // use this for inline script
        document.body.appendChild(script);
    }

</script>


<script>
    function fetchContent() {
        $(".editable").each(function (index) {
            debugger
            var myself = this;
            var docvID = this.getAttribute('id').replace("id_", "");
            var blurbtext = this.getAttribute("blurbtext");
            $.ajax({
                url: "/Admin/RetrieveDocumentVersion",
                type: "POST",
                data: {
                    docvID: docvID,
                    blurbtext: blurbtext,
                    csrfmiddlewaretoken: csrftoken,
                },
                success: function (data, textStatus, jqXHR) {
                    myself.setAttribute("contenteditable", "true");
                    myself.innerHTML = data;
                    CKEDITOR.inline(myself.id);
                },
            });
        })
    }

    $("#nopageedit").hide();

    function start_edit() {

        $(".editable").addClass("warning-ring");

        //At the beginning of an edit, we need to replace the content of the
        //editable div with it's databased content in order to preserve the
        //template tags. We want the tag, not the rendered tag.

        /* You must remove any countdown.js timers on the page before replacing the page with it's
        document version otherwise, Javascript will throw an exception.  */


        $("[class$='-countdown']").parent().remove();
        fetchContent();
        $(".editable").attr("onblur", "ckeditorsave(this)");
        window.status.bold();
        window.status = "Click outside the editable area to save. Changes are LIVE!! Refresh page to discard changes.";
        $("#editpage").hide();
        $("#noeditpage").show();
    }


    function stop_edit() {
        ckeditorsave();
        $("#noeditpage").hide();
        $("#editpage").show();
        window.location.reload();
    }

    function ckeditorsave(event) {
        for (var name in CKEDITOR.instances) {
            if (CKEDITOR.instances[name].checkDirty()) {
                editor = CKEDITOR.instances[name];
                saveEditable(editor);
            }
        }
    }

    function saveEditable(editor) {
        var content = editor.getData();
        var contentId = editor.name;
        var pageId = window.location.pathname;
        var originalContent = "N/A";
        var documentversion = editor.container.getAttribute("id").replace("id_", "");
        var blurbtext = editor.container.getAttribute("blurbtext");
        if (contentId.match(/-aloha$/gi)) {
            contentId = contentId.replace(/-aloha/gi, '');
        }  /*I'm not sure what this does but it seems like it would matter*/
        var request = jQuery.ajax({
            url: "/Admin/SaveDocument",
            type: "POST",
            async: false,
            data: {
                content: content,
                originalContent: originalContent,
                contentId: contentId,
                pageId: pageId,
                documentversion: documentversion,
                blurbtext: blurbtext,
                csrfmiddlewaretoken: csrftoken
            },
            success: function (data) {
                if (data['message']) {
                    alert(data['message']);
                }
            },
            error: function (xqXHR, textStatus) {
                window.status = textStatus;
                debugger;
            }

        });

    };


</script>

<script>
    jQuery(document).ajaxSend(function (event, xhr, settings) {
        function getCookie(name) {
            var cookieValue = null;
            if (document.cookie && document.cookie != '') {
                var cookies = document.cookie.split(';');
                for (var i = 0; i < cookies.length; i++) {
                    var cookie = jQuery.trim(cookies[i]);
                    // Does this cookie string begin with the name we want?
                    if (cookie.substring(0, name.length + 1) == (name + '=')) {
                        cookieValue = decodeURIComponent(cookie.substring(name.length + 1));
                        break;
                    }
                }
            }
            return cookieValue;
        }

        function sameOrigin(url) {
            // url could be relative or scheme relative or absolute
            var host = document.location.host; // host + port
            var protocol = document.location.protocol;
            var sr_origin = '//' + host;
            var origin = protocol + sr_origin;
            // Allow absolute or scheme relative URLs to same origin
            return (url == origin || url.slice(0, origin.length + 1) == origin + '/') ||
                (url == sr_origin || url.slice(0, sr_origin.length + 1) == sr_origin + '/') ||
                // or any other URL that isn't scheme relative or absolute i.e relative.
                !(/^(\/\/|http:|https:).*/.test(url));
        }

        function safeMethod(method) {
            return (/^(GET|HEAD|OPTIONS|TRACE)$/.test(method));
        }

        if (!safeMethod(settings.type) && sameOrigin(settings.url)) {
            xhr.setRequestHeader("X-CSRFToken", getCookie('csrftoken'));
        }
    });
</script>





<div id="successful-page-load" class='hidden'>Successful Page Load</div>





    
        <link href="/static/conf_gdpr/css/conf_gdpr.css" rel="stylesheet">
        <div id="cookie-bar" style="z-index: 8">
            <table class="gdpr-statement">
                <col>
                <col style="width:120px">
                <tr>
                    <td style="padding:5px">
                        NeurIPS uses cookies for essential functions only. We do not sell your personal
                        information.
                        <a href="/public/PrivacyPolicy">Our Privacy Policy &raquo;&nbsp;</a>
                    </td>
                    <td>
                        <button float-end class="btn btn-light btn-sm btn btn-outline-dark" onClick="accept_cookies();">Accept
                            Cookies
                        </button>
                    </td>
                </tr>
            </table>
        </div>

        <script>
            function accept_cookies() {

                $.ajax({
                    method: "POST",
                    url: "/conf_gdpr/accept",
                    data: {
                        csrfmiddlewaretoken: csrftoken,
                    },
                }).done(function (data) {
                    console.log(data);
                    $("#cookie-bar").fadeOut();
                }).fail(function (jqXHR, textStatus) {
                    alert(textStatus);
                });
            }
        </script>

    







<br>
<div class="noprint">
    <footer id="bootstrap-footer" class="text-center text-lg-start bg-light text-muted noprint">

        <div class="text-center p-1 border-top border-dark">
        </div>
        <!-- Section: Links  -->
        <section class="pt-1">
            <div class="container text-center text-md-start mt-3">
                <!-- Grid row -->
                <div class="row mt-3">
                    <!-- Grid column -->
                    <div class="col-md-3 col-lg-3 col-xl-3 mx-auto mb-3">
                        <!-- Content -->
                        <h6 class="text-uppercase fw-bold mb-4">
                            <img src="/static/core/img/NeurIPS-logo.svg" alt="NeurIPS logo" height='30'>
                        </h6>
                        <p>
                            The NeurIPS Logo above may be used on presentations. Right-click and choose
                            download. It is a vector graphic and may be used at any scale.
                        </p>

                    </div>


                    <!-- Grid column -->
                    <div class="col-md-5 col-lg-4 col-xl-3 mx-auto mb-4" style="max-width: 300px;">
                        <!-- Links -->
                        <h6 class="text-uppercase fw-bold mb-4 text-center">
                            Useful links
                        </h6>
                        <div>
             <ul>
	<li><a href="/Conferences/2024/Press">Press</a></li>
	<li><a href="/Exhibitors/exhibitorinfo">Exhibitor Information</a></li>
</ul>

            </div>
                    </div>
                    <!-- Grid column -->

                    <!-- Grid column -->
                    <div class="col-md-4 col-lg-3 col-xl-3 mx-auto mb-md-0 mb-4">
                        <!-- Links -->
                        <h6 class="text-uppercase fw-bold mb-4">Contact</h6>
                        
                            <p>
                                <i class="fas fa-home me-3"></i> 1269 Law St, San Diego CA 92109
                            </p>
                        
                        <p>
                            <i class="fas fa-envelope me-3"></i> <a href="/Help/Contact">Email</a>
                        </p>
                        
                        


                    </div>
                    <!-- Grid column -->
                </div>
                <!-- Grid row -->
            </div>
        </section>
        <!-- Section: Links  -->

        <!-- Copyright -->
        <div class="text-center p-4" style="background-color: rgba(0, 0, 0, 0.05);">
            <div>
             <p><a href="https://proceedings.neurips.cc">NeurIPS Proceedings</a></p>

            </div>
        </div>
        <!-- Copyright -->
    </footer>
</div>
<!-- Footer -->


<!-- Footer -->

</body>
</html>
