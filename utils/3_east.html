





<!DOCTYPE html>
<html lang="en" style="scroll-padding-top: 70px;"> 

<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="/static/virtual/js/virtual.js"></script>
    <meta name="google-site-verification" content="0jwPnVXIAk4FvFdT37dwMmd-kjHF86e5DKwvqlStUW0">


    
    <link rel="stylesheet" href="/static/core/css/core.css" type="text/css">
    <link rel="stylesheet" href="/static/virtual/css/virtual.css" type="text/css">
     <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">

    <link rel="stylesheet" href="/static/core/css/custom.css" type="text/css">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.14.0-beta2/dist/css/bootstrap-select.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Exo:wght@400;700&family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
      "tex2jax": {
        "inlineMath": [["$","$"], ["\\(","\\)"]],
        "displayMath": [["\\[","\\]"]],
        "processEscapes": true
      }
    }
    );
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <!--This script keeps local links inside the web app rather than opening them
in Safari, and has nothing to do with editing or Aloha.-->

<script >
	(function(document,navigator,standalone) {
		// prevents links from apps from opening in mobile safari
		// this javascript must be the first script in your <head>
		if ((standalone in navigator) && navigator[standalone]) {
			var curnode, location=document.location, stop=/^(a|html)$/i;
			document.addEventListener('click', function(e) {
				curnode=e.target;
				while (!(stop).test(curnode.nodeName)) {
					curnode=curnode.parentNode;
				}
				// Conditions to do this only on links to your own app
				// if you want all links, use if('href' in curnode) instead.
				if(
					'href' in curnode && // is a link
					(chref=curnode.href).replace(location.href,'').indexOf('#') && // is not an anchor
					(	!(/^[a-z\+\.\-]+:/i).test(chref) ||                       // either does not have a proper scheme (relative links)
						chref.indexOf(location.protocol+'//'+location.host)===0 ) // or is in the same protocol and domain
				) {
					e.preventDefault();
					location.href = curnode.href;
				}
			},false);
		}
	})(document,window.navigator,'standalone');
</script>        

<!-- This style sets the minimum size of a blurb to 260 px unless there is a
template context variable blurb_min_height that sets it otherwise. If blurbs
aren't all about the same size, they don't flow well when the window is
resized.-->


<style>
/*This is here rather that in a .css file for a reason.*/
    @media screen and (min-width: 767px) {
        .blurb {
            min-height:260px;
        }
    }
</style>
    

<script src="https://code.jquery.com/jquery-3.6.1.min.js"
        integrity="sha256-o88AwQnZB+VDvE9tvIXrMQaPlFFSUTR+nldQm1LuPXQ=" crossorigin="anonymous">
</script>

<script>
    if (typeof jQuery === 'undefined') {
        var script = document.createElement('script');
        script.type = 'text/javascript';
        script.src = "/static/core/js/jquery-3.6.1.min.js";
        document.head.appendChild(script);
    }
</script>


    <script>
        var $ = jQuery;
        /*Store a pointer to jquery2, so I can reference it later.  Aloha loads jquery 1.7 and much
        of bootstrap 3 is not compatible. This comment is deprecated. */
    </script>

    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4" crossorigin="anonymous"></script>

    <script src="/static/core/js/ajax-csrf-snippet.js"></script>
    <script src="https://kit.fontawesome.com/be44b7e05d.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.14.0-beta3/dist/js/bootstrap-select.min.js"></script>


    <style>
        body {
            font-family: Exo;}
    </style>








        


    <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">
    <link href="https://fonts.googleapis.com/css2?family=Exo:wght@400;700&family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/static/virtual/css/virtual.css">
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/corejs-typeahead/1.3.1/typeahead.bundle.min.js" integrity="sha512-lEb9Vp/rkl9g2E/LdHIMFTqz21+LA79f84gqP75fbimHqVTu6483JG1AwJlWLLQ8ezTehty78fObKupq3HSHPQ==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
            integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ="
            crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script>
    <script src="/static/core/js/ajax-csrf-snippet.js" ></script>
    <script src="/static/virtual/js/virtual.js"></script>
    


    
    <title>Track: Poster Session 3 East</title>
    <script src='https://slideslive.com/embed_presentation.js'></script>

    <title>NeurIPS 2024</title>
</head>

<body>




<div class="noprint">
    
        <!--Navbar start-->
<header>
    <a href="#child-menu" class="off-screen">Skip to yearly menu bar</a>
    <a href="#main" class="off-screen">Skip to main content</a>
    <div id="id_navbar" class="navbar navbar-expand-sm navbar-dark" aria-label="Main Navigation"
         style="background-color:#212529">
        <h2 class="off-screen">Main Navigation</h2>
        <div class="container-fluid">
            <div><a class="navbar-brand" href="/">

                <img src="/static/core/img/neurips-navbar-logo.svg" alt="conference_logo" height="40"></a></div>


            <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                    data-bs-target="#navbarToggler1"
                    aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarToggler1">
                <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                    
    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" 
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            NeurIPS
        </a>
        <ul class="dropdown-menu dropdown-menu-dark">
            
    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/FAQ">
                    <span >
                        Help/FAQ
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Help/Contact">
                    <span >
                        Contact NeurIPS
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Conferences/2023/EthicsGuidelines">
                    <span >
                        Code of Ethics
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/public/CodeOfConduct">
                    <span >
                        Code of Conduct
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Profile/create">
                    <span >
                        Create Profile
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/public/JournalToConference">
                    <span >
                        Journal To Conference Track
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/public/DiversityInclusion">
                    <span >
                        Diversity &amp; Inclusion
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="https://proceedings.neurips.cc/">
                    <span >
                        Proceedings
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Conferences/FutureMeetings">
                    <span >
                        Future Meetings
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Conferences/2024/Press">
                    <span >
                        Press
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Exhibitors/exhibitorinfo">
                    <span >
                        Exhibitor Information
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/public/PrivacyPolicy">
                    <span >
                        Privacy Policy
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/Downloads">
                    <span >
                        Downloads
                    </span>
                </a>
                
            </li>

        

    



        </ul>
    </li>
    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/MyStuff">
                    <span >
                        My Stuff
                    </span>
                </a>
                
            </li>

        

    



                </ul>

                <form class="d-flex mx-2" aria-label="Search" role="search" action="/search">
                    <div class="input-group" role="search" style="outline-color:green;">
                        <input type="text" class="form-control" placeholder="Search" name="q"
                               value=""
                               aria-label="Search" aria-describedby="btnGroupAddon"
                                id="navbar-search">
                        <div class="input-group-text btn-primary" id="btnGroupAddon">
                            <button style="border: none; background-color: transparent; padding: 0;" type="submit">
                                <i class="fa-solid fa-magnifying-glass"></i>
                            </button>
                        </div>
                    </div>
                </form>

                
                    
                    <div class="btn-group d-none d-sm-block nav-item" role="group"
                         aria-label="Button group with nested dropdown">
                        <div class="btn-group" role="group">
                            <button type="button" class="btn btn-light dropdown-toggle" data-bs-toggle="dropdown"
                               id="id_navbar_username" aria-expanded="false">
                                Mei
                            </button>
                            <ul class="dropdown-menu dropdown-menu-end dropdown-menu-dark">
                                
                                <li>
                                    <a class="dropdown-item" href="/MyStuff"> <i class="fa-regular fa-user"></i> &nbsp;My
                                        Stuff</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li>
                                    <a class="dropdown-item" href="/virtual/2024/mycalendar"> <i class="fa-regular fa-user"></i> &nbsp;My
                                        Bookmarks</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li>
                                    <a class="dropdown-item" href="/Profile/change-password"> <i
                                            class="fa-solid fa-lock"></i> &nbsp;Change Password</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li>
                                    <a class="dropdown-item" href="/resetpassword"> <i class="fa-solid fa-unlock"></i>
                                        &nbsp;Reset Password</a>
                                </li>

                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/EditProfile"> <i
                                        class="fa-regular fa-address-card"></i> &nbsp;Edit Profile </a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/MergeAccounts"> <i
                                        class="fa-solid fa-code-merge"></i> &nbsp;Merge Profiles </a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item"
                                       href="/set_timezone?nextp=/virtual/2024/session/108366"> <i
                                        class="fa-solid fa-earth-americas"></i>
                                    &nbsp;TZ: America/Chicago</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/logout"> <i
                                        class="fa-solid fa-right-from-bracket"></i> &nbsp;Log Out</a></li>
                            </ul>
                        </div>
                    </div>
                    <br>
                    
                    <div class="btn-group d-block d-sm-none " role="group"
                         aria-label="Button group with nested dropdown">

                        <div class="btn-group" role="group">
                            <button class="btn btn-light dropdown-toggle" data-bs-toggle="dropdown"
                                    aria-expanded="false">
                                Mei
                            </button>
                            <ul class="dropdown-menu dropdown-menu-dark">

                                <li>
                                    <a class="dropdown-item" href="/MyStuff"> <span
                                            class="glyphicon glyphicon-cog"></span> My Stuff</a>
                                </li>
                                <li>
                                    <a class="dropdown-item" href="/virtual/2024/mycalendar"> <span
                                            class="glyphicon glyphicon-cog"></span> My Bookmarks</a>
                                </li>
                                <li>
                                    <a class="dropdown-item" href="/Profile/change-password"> <span
                                            class="glyphicon glyphicon-cog"></span> Change Password</a>
                                </li>

                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/EditProfile"><span
                                        class="fa-solid fa-pen-to-square"></span> Edit Profile </a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item"
                                       href="/set_timezone?nextp=/virtual/2024/session/108366">TZ: America/Chicago</a>
                                </li>
                                <hr class="dropdown-divider" aria-hidden="true">
                                <li><a class="dropdown-item" href="/logout"><span
                                        class="fa-solid fa-right-from-bracket"></span> Log Out</a></li>
                            </ul>
                        </div>
                    </div>
                

            </div>
        </div>
    </div>
</header>
<!--Navbar end-->
    
</div><!--noprint div-->

<!--This holds the whole page including the navbar-->

<main id="main">
    
        <div class="container-fluid">
            <!--Navbar start-->

<div class="dropdown" id="child-menu">
    <nav class="align-middle navbar navbar-expand-md  rounded-bottom"
         style="min-height: 57px; background-image: url(/static/virtual/img/navbackground.png); background-repeat: repeat-x;">
        <div class="container-fluid">

            <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                    data-bs-target="#navbarToggler987"
                    aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarToggler987">
                <ul class="navbar-nav me-auto mb-lg-0">
                    


    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle border-3  btn btn-primary text-white p-1" style= "background-color: #070bff; font-size: 1.2 em;"
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Select Year: (2024)
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2024">2024
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2023">2023
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2022">2022
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2021">2021
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2020">2020
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2019">2019
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2018">2018
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2017">2017
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2016">2016
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2015">2015
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2014">2014
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2013">2013
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2012">2012
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2011">2011
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2010">2010
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2009">2009
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2008">2008
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2007">2007
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>
        

    



    
        
            <li class="dropdown-item " >
                <a class="dropdown-item p-1"
                   href="/Conferences/2006">2006
                </a>
                
            </li>
        

    



        </ul>
    </li>
    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/index.html">
                    <span >
                        Start Here
                    </span>
                </a>
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/calendar">
                    <span >
                        Schedule
                    </span>
                </a>
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/tutorial">
                    <span >
                        Tutorials
                    </span>
                </a>
                
            </li>

        

    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" 
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Main Conference
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/eventlistwithbios/Invited%20Talk">
                    <span >
                        Invited Talks
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/oral">
                    <span >
                        Orals
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/spotlight-posters-2024">
                    <span >
                        Spotlights
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/papers.html">
                    <span >
                        Papers
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="https://neurips2024.vizhub.ai">
                    <span >
                        Paper Visualization
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/Competition">
                    <span >
                        Competitions
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/datasets-benchmarks-2024">
                    <span >
                        Datasets &amp; Benchmarks
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/journal_track_2024">
                    <span >
                        Journal Track
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/creative-ai-2024">
                    <span >
                        Creative AI Track
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/awards_detail">
                    <span >
                        Outstanding Paper Awards
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/affinity%20workshop">
                    <span >
                        Affinity Workshops
                    </span>
                </a>
                
            </li>

        

    



        </ul>
    </li>
    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" 
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Community
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/affinity_events">
                    <span >
                        Affinity Events
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/community?show=mentorship">
                    <span >
                        Mentorship Event
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/session/101095">
                    <span >
                        Bridging the Future
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/social">
                    <span >
                        Socials
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/careers">
                    <span >
                        Careers
                    </span>
                </a>
                
            </li>

        

    



        </ul>
    </li>
    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/events/workshop">
                    <span >
                        Workshops
                    </span>
                </a>
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/sponsor_list">
                    <span >
                        Exhibitors
                    </span>
                </a>
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/search">
                    <span >
                        <i class="fas fa-search"></i>
                    </span>
                </a>
                
            </li>

        

    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" 
           href="#"
           role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Help
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/FAQ">
                    <span >
                        FAQ
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="https://chat.neurips.cc/channel/HelpDesk">
                    <span >
                        Helpdesk in RocketChat
                    </span>
                </a>
                
                    <hr class="dropdown-divider" aria-hidden="true">
                
            </li>

        

    



    
        
            <li class="dropdown-item  pe-2" >
                <a class="nav-link p-1"  href="/virtual/2024/organizers">
                    <span >
                        Organizers
                    </span>
                </a>
                
            </li>

        

    



        </ul>
    </li>
    



                </ul>
            </div>
        </div>
    </nav>
</div>
    <!--Navbar end-->
        </div>
        <br><br>
    
    
        
        <div class="container">
    
    

    

    

        <div class="container">
            
                

    <span id="the-bookmark-1" class="bump20 bookmark-right fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='the-bookmark-1', event_id='108366', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>



            
            <!-- Title -->
            <div class="" style="">
                <div class="card-header">
                    <h3 class="text-center ">Poster Session</h3>
                    <h2 class="card-title main-title text-center" style="">
                        Poster Session 3 East
                    </h2>
                    
                        <h5 class="text-center text-muted">East Exhibit Hall A-C</h5>
                    
                    


                    
                    

                    


                    <div class="text-center">
                        

                        <div>
                            
                        </div>
                        <div>
                            
                        </div>
                        <div>
                            
                        </div>

                        
                        
                        

                        

                        

                        

                            
                                <div>Thu 12 Dec 1 p.m. CST 
                                    &mdash; 4 p.m. CST  

    <span id="bookmark-number-0" class="bump20 " title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-0', event_id='108366', bookmark_event_number='1',
                  alt_bookmark_element_id='the-bookmark-1');">
        
            <span class="green">(Bookmark)</span>
        
    </span>



                                    

                                </div>
                                <div class="schedule-html-detail"></div>

                            

                            

                        
                        

                        
                    </div>
                    <div class=" text-center text-muted text-monospace ">
                        <div> 
                        </div>
                    </div>
                </div>
            </div>
            <div id="details" class="pp-card m-3 collapse">
                <div class="card-body p-3">

                    <div id="abstractExample">
                        <span class="font-weight-bold">Abstract:</span> 
                    </div>


                </div>
            </div>
        </div>
        <!-- SlidesLive -->

        
            <div class="container">
                <div class="col-xs-12 my-auto p-2 admin centered">
                    
                </div>
            </div>

            

            
         

        <!--Children in this session -->
        <p></p>

        <div class="container" style="padding-bottom: 30px; padding-top:30px">
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-1" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-1', event_id='94067', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1000</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94067">Propensity Score Alignment of Unpaired Multimodal Data</a></strong></h5>


                        <p class="text-muted">
                            Johnny Xi &middot; Jana Osea &middot; Zuheng Xu &middot; Jason Hartford
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multimodal representation learning techniques typically require paired samples to learn shared representations, but collecting paired samples can be challenging in fields like biology, where measurement devices often destroy the samples. This paper presents an approach to address the challenge of aligning unpaired samples across disparate modalities in multimodal representation learning. We draw an analogy between potential outcomes in causal inference and potential views in multimodal observations, allowing us to leverage Rubin's framework to estimate a common space for matching samples. Our approach assumes experimentally perturbed samples by treatments, and uses this to estimate a propensity score from each modality. We show that the propensity score encapsulates all shared information between a latent state and treatment, and can be used to define a distance between samples. We experiment with two alignment techniques that leverage this distance---shared nearest neighbours (SNN) and optimal transport (OT) matching---and find that OT matching results in significant improvements over state-of-the-art alignment approaches in on synthetic multi-modal tasks, in real-world data from NeurIPS Multimodal Single-Cell Integration Challenge, and on a single cell microscopy to expression prediction task.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-2" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-2', event_id='94728', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1001</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94728">xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology</a></strong></h5>


                        <p class="text-muted">
                            Julius Hense &middot; Mina Jamshidi Idaji &middot; Oliver Eberle &middot; Thomas Schnake &middot; Jonas Dippel &middot; Laure Ciernik &middot; Oliver Buchstab &middot; Andreas Mock &middot; Frederick Klauschen &middot; Klaus-Robert MÃ¼ller
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multiple instance learning (MIL) is an effective and widely used approach for weakly supervised machine learning. In histopathology, MIL models have achieved remarkable success in tasks like tumor detection, biomarker prediction, and outcome prognostication. However, MIL explanation methods are still lagging behind, as they are limited to small bag sizes or disregard instance interactions. We revisit MIL through the lens of explainable AI (XAI) and introduce xMIL, a refined framework with more general assumptions. We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets. Our approach consistently outperforms previous explanation attempts with particularly improved faithfulness scores on challenging biomarker prediction tasks. Finally, we showcase how xMIL explanations enable pathologists to extract insights from MIL models, representing a significant advance for knowledge discovery and model debugging in digital histopathology.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-3" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-3', event_id='96550', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1002</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96550">Exploring Molecular Pretraining Model at Scale</a></strong></h5>


                        <p class="text-muted">
                            xioahong ji &middot; Zhen Wang &middot; Zhifeng Gao &middot; Hang Zheng &middot; Linfeng Zhang &middot; Guolin Ke &middot; Weinan E
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In recent years, pretraining models have made significant advancements in the fields of natural language processing (NLP), computer vision (CV), and life sciences. The significant advancements in NLP and CV are predominantly driven by the expansion of model parameters and data size, a phenomenon now recognized as the scaling laws. However, research exploring scaling law in molecular pretraining model remains unexplored. In this work, we present an innovative molecular pretraining model that leverages a two-track transformer to effectively integrate features at the atomic level, graph level, and geometry structure level. Along with this, we systematically investigate the scaling law within molecular pretraining models, examining the power-law correlations between validation loss and model size, dataset size, and computational resources. Consequently, we successfully scale the model to 1.1 billion parameters through pretraining on 800 million conformations, making it the largest molecular pretraining model to date. Extensive experiments show the consistent improvement on the downstream tasks as the model size grows up. The model with 1.1 billion parameters also outperform over existing methods, achieving an average 27\% improvement on the QM9 and 14\% on COMPAS-1D dataset.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-4" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-4', event_id='97558', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1003</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97558">PROSPECT PTMs: Rich Labeled Tandem Mass Spectrometry Dataset of Modified Peptides for Machine Learning in Proteomics</a></strong></h5>


                        <p class="text-muted">
                            Wassim Gabriel &middot; Omar Shouman &middot; Eva Ayla SchrÃ¶der &middot; Florian BÃ¶Ãl &middot; Mathias Wilhelm
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Post-Translational Modifications (PTMs) are changes that occur in proteins after synthesis, influencing their structure, function, and cellular behavior. PTMs are essential in cell biology; they regulate protein function and stability, are involved in various cellular processes, and are linked to numerous diseases. A particularly interesting class of PTMs are chemical modifications such as phosphorylation introduced on amino acid side chains because they can drastically alter the physicochemical properties of the peptides once they are present. One or more PTMs can be attached to each amino acid of the peptide sequence. The most commonly applied technique to detect PTMs on proteins is bottom-up Mass Spectrometry-based proteomics (MS), where proteins are digested into peptides and subsequently analyzed using Tandem Mass Spectrometry (MS/MS). While an increasing number of machine learning models are published focusing on MS/MS-related property prediction of unmodified peptides, high-quality reference data for modified peptides is missing, impeding model development for this important class of peptides. To enable researchers to train machine learning models that can accurately predict the properties of modified peptides, we introduce four high-quality labeled datasets for applying machine and deep learning to tasks in MS-based proteomics. The four datasets comprise several subgroups of peptides with 1.2 million unique modified peptide sequences and 30 unique pairs of (amino-acid, PTM), covering both experimentally introduced and naturally occurring modifications on various amino acids. We evaluate the utility and importance of the dataset by providing benchmarking results on models trained with and without modifications and highlighting the impact of including modified sequences on downstream tasks. We demonstrate that predicting the properties of modified peptides is more challenging but has a broad impact since they are often the core of protein functionality and its regulation, and they have a potential role as biomarkers in clinical applications. Our datasets contribute to applied machine learning in proteomics by enabling the research community to experiment with methods to encode PTMs as model inputs and to benchmark against reference data for model comparison. With a proper data split for three common tasks in proteomics, we provide a robust way to evaluate model performance and assess generalization on unseen modified sequences.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-5" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-5', event_id='97655', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1004</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97655">A benchmark for prediction of transcriptomic responses to chemical perturbations across cell types</a></strong></h5>


                        <p class="text-muted">
                            Artur SzaÅata &middot; Andrew Benz &middot; Robrecht Cannoodt &middot; Mauricio Cortes &middot; Jason Fong &middot; Sunil Kuppasani &middot; Richard Lieberman &middot; Tianyu Liu &middot; Javier Mas-Rosario &middot; Rico Meinl &middot; Jalil Nourisa &middot; Jared Tumiel &middot; Tin M. Tunjic &middot; Mengbo Wang &middot; Noah Weber &middot; Hongyu Zhao &middot; Benedict Anchang &middot; Fabian Theis &middot; Malte Luecken &middot; Daniel Burkhardt
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Single-cell transcriptomics has revolutionized our understanding of cellular heterogeneity and drug perturbation effects. However, its high cost and the vast chemical space of potential drugs present barriers to experimentally characterizing the effect of chemical perturbations in all the myriad cell types of the human body. To overcome these limitations, several groups have proposed using machine learning methods to directly predict the effect of chemical perturbations either across cell contexts or chemical space. However, advances in this field have been hindered by a lack of well-designed evaluation datasets and benchmarks. To drive innovation in perturbation modeling, the Open Problems Perturbation Prediction (OP3) benchmark introduces a framework for predicting the effects of small molecule perturbations on cell type-specific gene expression. OP3 leverages the Open Problems in Single-cell Analysis benchmarking infrastructure and is enabled by a new single-cell perturbation dataset, encompassing 146 compounds tested on human blood cells. The benchmark includes diverse data representations, evaluation metrics, and winning methods from our "Single-cell perturbation prediction: generalizing experimental interventions to unseen contexts" competition at NeurIPS 2023. We envision that the OP3 benchmark and competition will drive innovation in single-cell perturbation prediction by improving the accessibility, visibility, and feasibility of this challenge, thereby promoting the impact of machine learning in drug discovery.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-6" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-6', event_id='93097', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1005</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93097">Med-Real2Sim: Non-Invasive Medical Digital Twins using Physics-Informed Self-Supervised Learning</a></strong></h5>


                        <p class="text-muted">
                            Keying Kuang &middot; Frances Dean &middot; Jack B. Jedlicki &middot; David Ouyang &middot; Anthony Philippakis &middot; David Sontag &middot; Ahmed Alaa
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>A digital twin is a virtual replica of a real-world physical phenomena that uses mathematical modeling to characterize and simulate its defining features. By constructing digital twins for disease processes, we can perform in-silico simulations that mimic patients' health conditions and counterfactual outcomes under hypothetical interventions in a virtual setting. This eliminates the need for invasive procedures or uncertain treatment decisions. In this paper, we propose a method to identify digital twin model parameters using only noninvasive patient health data. We approach the digital twin modeling as a composite inverse problem, and observe that its structure resembles pretraining and finetuning in self-supervised learning (SSL). Leveraging this, we introduce a physics-informed SSL algorithm that initially pretrains a neural network on the pretext task of learning a differentiable simulator of a physiological process. Subsequently, the model is trained to reconstruct physiological measurements from noninvasive modalities while being constrained by the physical equations learned in pretraining. We apply our method to identify digital twins of cardiac hemodynamics using noninvasive echocardiogram videos, and demonstrate its utility in unsupervised disease detection and in-silico clinical trials.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-7" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-7', event_id='95027', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1006</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95027">Random Representations Outperform Online Continually Learned Representations</a></strong></h5>


                        <p class="text-muted">
                            Ameya Prabhu &middot; Shiven Sinha &middot; Ponnurangam Kumaraguru &middot; Philip Torr &middot; Ozan Sener &middot; Puneet Dokania
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Continual learning has primarily focused on the issue of catastrophic forgetting and the associated stability-plasticity tradeoffs. However, little attention has been paid to the efficacy of continually learned representations, as representations are learned alongside classifiers throughout the learning process. Our primary contribution is empirically demonstrating that existing online continually trained deep networks produce inferior representations compared to a simple pre-defined random transforms. Our approach embeds raw pixels using a fixed random transform, approximating an RBF-Kernel initialized before any data is seen. We then train a simple linear classifier on top without storing any exemplars, processing one sample at a time in an online continual learning setting. This method, called RanDumb,  significantly outperforms state-of-the-art continually learned representations across all standard online continual learning benchmarks. Our study reveals the significant limitations of representation learning, particularly in low-exemplar and online continual learning scenarios. Extending our investigation to popular exemplar-free scenarios with pretrained models, we find that training only a linear classifier on top of pretrained representations surpasses most continual fine-tuning and prompt-tuning strategies. Overall, our investigation challenges the prevailing assumptions about effective representation learning in the online continual learning.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-8" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-8', event_id='93541', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1007</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93541">Sharing Key Semantics in Transformer Makes Efficient Image Restoration</a></strong></h5>


                        <p class="text-muted">
                            Bin Ren &middot; Yawei Li &middot; Jingyun Liang &middot; Rakesh Ranjan &middot; Mengyuan Liu &middot; Rita Cucchiara &middot; Luc V Gool &middot; Ming-Hsuan Yang &middot; Nicu Sebe
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Image Restoration (IR), a classic low-level vision task, has witnessed significant advancements through deep models that effectively model global information. Notably, the emergence of Vision Transformers (ViTs) has further propelled these advancements. When computing, the self-attention mechanism, a cornerstone of ViTs, tends to encompass all global cues, even those from semantically unrelated objects or regions. This inclusivity introduces computational inefficiencies, particularly noticeable with high input resolution, as it requires processing irrelevant information, thereby impeding efficiency. Additionally, for IR, it is commonly noted that small segments of a degraded image, particularly those closely aligned semantically, provide particularly relevant information to aid in the restoration process, as they contribute essential contextual cues crucial for accurate reconstruction. To address these challenges, we propose boosting IR's performance by sharing the key semantics via Transformer for IR (i.e., SemanIR) in this paper. Specifically, SemanIR initially constructs a sparse yet comprehensive key-semantic dictionary within each transformer stage by establishing essential semantic connections for every degraded patch. Subsequently, this dictionary is shared across all subsequent transformer blocks within the same stage. This strategy optimizes attention calculation within each block by focusing exclusively on semantically related components stored in the key-semantic dictionary. As a result, attention calculation achieves linear computational complexity within each window. Extensive experiments across 6 IR tasks confirm the proposed SemanIR's state-of-the-art performance, quantitatively and qualitatively showcasing advancements. The visual results, code, and trained models are available at: https://github.com/Amazingren/SemanIR.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-9" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-9', event_id='94090', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1008</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94090">Detecting Bugs with Substantial Monetary Consequences by LLM and Rule-based Reasoning</a></strong></h5>


                        <p class="text-muted">
                            Brian Zhang &middot; Zhuo Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Financial transactions are increasingly being handled by automated programs called <em>smart contracts</em>. However, one challenge in the adaptation of smart contracts is the presence of vulnerabilities, which can cause significant monetary loss.In  2024, $247.88 M was lost in 20 smart contract exploits.According to a recent study, accounting bugs (i.e., incorrect implementations of domain-specific financial models) are the most prevalent type of vulnerability, and are one of the most difficult to find, requiring substantial human efforts.While Large Language Models (LLMs) have shown promise in identifying these bugs, they often suffer from lack of generalization of vulnerability types, hallucinations, and problems with representing smart contracts in limited token context space.This paper proposes a hybrid system combining LLMs and rule-based reasoning to detect accounting error vulnerabilities in smart contracts. In particular, it utilizes the understanding capabilities of LLMs to annotate the financial meaning of variables in smart contracts, and employs rule-based reasoning to propagate the information throughout a contract's logic and to validate potential vulnerabilities.To remedy hallucinations, we propose a feedback loop where validation is performed by providing the reasoning trace of vulnerabilities to the LLM for iterative self-reflection. We achieve 75.6% accuracy on the labelling of financial meanings against human annotations. Furthermore, we achieve a recall of 90.5% from running on 23 real-world smart contract projects containing 21 accounting error vulnerabilities.Finally, we apply the automated technique on 8 recent projects, finding 4 known and 2 unknown bugs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-10" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-10', event_id='93942', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1009</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93942">FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion</a></strong></h5>


                        <p class="text-muted">
                            Xing Han &middot; Huy Nguyen &middot; Carl Harris &middot; Nhat Ho &middot; Suchi Saria
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in the real world is validated by a diverse set of challenging prediction tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-11" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-11', event_id='94767', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1010</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94767">PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging</a></strong></h5>


                        <p class="text-muted">
                            Xin Cai &middot; Zhiyuan You &middot; Hailong Zhang &middot; Jinwei Gu &middot; Wentao Liu &middot; Tianfan Xue
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Lensless cameras offer significant advantages in size, weight, and cost compared to traditional lens-based systems. Without a focusing lens, lensless cameras rely on computational algorithms to recover the scenes from multiplexed measurements. However, current algorithms struggle with inaccurate forward imaging models and insufficient priors to reconstruct high-quality images. To overcome these limitations, we introduce a novel two-stage approach for consistent and photorealistic lensless image reconstruction. The first stage of our approach ensures data consistency by focusing on accurately reconstructing the low-frequency content with a spatially varying deconvolution method that adjusts to changes in the Point Spread Function (PSF) across the camera's field of view. The second stage enhances photorealism by incorporating a generative prior from pre-trained diffusion models. By conditioning on the low-frequency content retrieved in the first stage, the diffusion model effectively reconstructs the high-frequency details that are typically lost in the lensless imaging process, while also maintaining image fidelity. Our method achieves a superior balance between data fidelity and visual quality compared to existing methods, as demonstrated with two popular lensless systems, PhlatCam and DiffuserCam.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-12" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-12', event_id='94942', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1011</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94942">End-to-End Ontology Learning with Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Andy Lo &middot; Albert Q. Jiang &middot; Wenda Li &middot; Mateja Jamnik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Ontologies are useful for automatic machine processing of domain knowledge as they represent it in a structured format. Yet, constructing ontologies requires substantial manual effort. To automate part of this process, large language models (LLMs) have been applied to solve various subtasks of ontology learning. However, this partial ontology learning does not capture the interactions between subtasks. We address this gap by introducing OLLM, a general and scalable method for building the taxonomic backbone of an ontology from scratch. Rather than focusing on subtasks, like individual relations between entities, we model entire subcomponents of the target ontology by finetuning an LLM with a custom regulariser that reduces overfitting on high-frequency concepts. We introduce a novel suite of metrics for evaluating the quality of the generated ontology by measuring its semantic and structural similarity to the ground truth. In contrast to standard metrics, our metrics use deep learning techniques to define more robust distance measures between graphs. Both our quantitative and qualitative results on Wikipedia show that OLLM outperforms subtask composition methods, producing more semantically accurate ontologies while maintaining structural integrity. We further demonstrate that our model can be effectively adapted to new domains, like arXiv, needing only a small number of training examples. Our source code and datasets are available at https://github.com/andylolu2/ollm.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-13" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-13', event_id='94040', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1100</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94040">RGFN: Synthesizable Molecular Generation Using GFlowNets</a></strong></h5>


                        <p class="text-muted">
                            MichaÅ Koziarski &middot; Andrei Rekesh &middot; Dmytro Shevchuk &middot; Almer van der Sloot &middot; Piotr GaiÅski &middot; Yoshua Bengio &middot; Chenghao Liu &middot; Mike Tyers &middot; Robert Batey
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Generative models hold great promise for small molecule discovery, significantly increasing the size of search space compared to traditional in silico screening libraries. However, most existing machine learning methods for small molecule generation suffer from poor synthesizability of candidate compounds, making experimental validation difficult. In this paper we propose Reaction-GFlowNet (RGFN), an extension of the GFlowNet framework that operates directly in the space of chemical reactions, thereby allowing out-of-the-box synthesizability while maintaining comparable quality of generated candidates. We demonstrate that with the proposed set of reactions and building blocks, it is possible to obtain a search space of molecules orders of magnitude larger than existing screening libraries coupled with low cost of synthesis. We also show that the approach scales to very large fragment libraries, further increasing the number of potential molecules. We demonstrate the effectiveness of the proposed approach across a range of oracle models, including pretrained proxy models and GPU-accelerated docking.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-14" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-14', event_id='93555', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1101</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93555">MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training</a></strong></h5>


                        <p class="text-muted">
                            Bo Chen &middot; Zhilei Bei &middot; Xingyi Cheng &middot; Pan Li &middot; Jie Tang &middot; Le Song
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multiple Sequence Alignment (MSA) plays a pivotal role in unveiling the evolutionary trajectories of protein families. The accuracy of protein structure predictions is often compromised for protein sequences that lack sufficient homologous information to construct high-quality MSA. Although various methods have been proposed to generate high-quality MSA under these conditions, they fall short in comprehensively capturing the intricate co-evolutionary patterns within MSA or require guidance from external oracle models. Here we introduce MSAGPT, a novel approach to prompt protein structure predictions via MSA generative pre-training in a low-MSA regime. MSAGPT employs a simple yet effective 2D evolutionary positional encoding scheme to model the complex evolutionary patterns. Endowed by this, the flexible 1D MSA decoding framework facilitates zero- or few-shot learning. Moreover, we demonstrate leveraging the feedback from AlphaFold2 (AF2) can further enhance the modelâs capacity via Rejective Fine-tuning (RFT) and Reinforcement Learning from AF2 Feedback (RLAF). Extensive experiments confirm the efficacy of MSAGPT in generating faithful and informative MSA (up to +8.5% TM-Score on few-shot scenarios). The transfer learning also demonstrates its great potential for the wide range of tasks resorting to the quality of MSA.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-15" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-15', event_id='97726', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1102</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97726">Harmony4D: A Video Dataset for In-The-Wild Close Human Interactions</a></strong></h5>


                        <p class="text-muted">
                            Rawal Khirodkar &middot; Jyun-Ting Song &middot; Jinkun Cao &middot; Zhengyi Luo &middot; Kris Kitani
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Understanding how humans interact with each other is key to building realistic multi-human virtual reality systems. This area remains relatively unexplored due to the lack of large-scale datasets. Recent datasets focusing on this issue mainly consist of activities captured entirely in controlled indoor environments with choreographed actions, significantly affecting their diversity. To address this, we introduce Harmony4D, a multi-view video dataset for human-human interaction featuring in-the-wild activities such as wrestling, dancing, MMA,and more. We use a flexible multi-view capture system to record these dynamic activities and provide annotations for human detection, tracking, 2D/3D pose estimation, and mesh recovery for closely interacting subjects. We propose a novel markerless algorithm to track 3D human poses in severe occlusion and close interaction to obtain our annotations with minimal manual intervention. Harmony4D consists of 1.66 million images and 3.32 million human instances from more than 20 synchronized cameras with 208 video sequences spanning diverse environments and 24 unique subjects. We rigorously evaluate existing state-of-the-art methods for mesh recovery and highlight their significant limitations in modeling close interaction scenarios. Additionally, we fine-tune a pre-trained HMR2.0 model on Harmony4D and demonstrate an improved performance of 54.8% PVE in scenes with severe occlusion and contact. âHarmonyâa cohesive alignment of human behaviors."</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-16" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-16', event_id='97631', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1103</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97631">A Benchmark Dataset for Event-Guided Human Pose Estimation and Tracking in Extreme Conditions</a></strong></h5>


                        <p class="text-muted">
                            Hoonhee Cho &middot; Taewoo Kim &middot; Yuhwan Jeong &middot; Kuk-Jin Yoon
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multi-person pose estimation and tracking have been actively researched by the computer vision community due to their practical applicability. However, existing human pose estimation and tracking datasets have only been successful in typical scenarios, such as those without motion blur or with well-lit conditions. These RGB-based datasets are limited to learning under extreme motion blur situations or poor lighting conditions, making them inherently vulnerable to such scenarios.As a promising solution, bio-inspired event cameras exhibit robustness in extreme scenarios due to their high dynamic range and micro-second level temporal resolution. Therefore, in this paper, we introduce a new hybrid dataset encompassing both RGB and event data for human pose estimation and tracking in two extreme scenarios: low-light and motion blur environments. The proposed Event-guided Human Pose Estimation and Tracking in eXtreme Conditions (EHPT-XC) dataset covers cases of motion blur caused by dynamic objects and low-light conditions individually as well as both simultaneously. With EHPT-XC, we aim to inspire researchers to tackle pose estimation and tracking in extreme conditions by leveraging the advantageous of the event camera.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-17" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-17', event_id='97507', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1104</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97507">ImageNet3D: Towards General-Purpose Object-Level 3D Understanding</a></strong></h5>


                        <p class="text-muted">
                            Wufei Ma &middot; Guanning Zeng &middot; Guofeng Zhang &middot; Qihao Liu &middot; Letian Zhang &middot; Adam Kortylewski &middot; Yaoyao Liu &middot; Alan Yuille
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>A vision model with general-purpose object-level 3D understanding should be capable of inferring both 2D (e.g., class name and bounding box) and 3D information (e.g., 3D location and 3D viewpoint) for arbitrary rigid objects in natural images. This is a challenging task, as it involves inferring 3D information from 2D signals and most importantly, generalizing to rigid objects from unseen categories. However, existing datasets with object-level 3D annotations are often limited by the number of categories or the quality of annotations. Models developed on these datasets become specialists for certain categories or domains, and fail to generalize. In this work, we present ImageNet3D, a large dataset for general-purpose object-level 3D understanding. ImageNet3D augments 200 categories from the ImageNet dataset with 2D bounding box, 3D pose, 3D location annotations, and image captions interleaved with 3D information. With the new annotations available in ImageNet3D, we could (i) analyze the object-level 3D awareness of visual foundation models, and (ii) study and develop general-purpose models that infer both 2D and 3D information for arbitrary rigid objects in natural images, and (iii) integrate unified 3D models with large language models for 3D-related reasoning.. We consider two new tasks, probing of object-level 3D awareness and open vocabulary pose estimation, besides standard classification and pose estimation. Experimental results on ImageNet3D demonstrate the potential of our dataset in building vision models with stronger general-purpose object-level 3D understanding.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-18" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-18', event_id='96927', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1105</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96927">ScaleKD: Strong Vision Transformers Could Be Excellent Teachers</a></strong></h5>


                        <p class="text-muted">
                            Jiawei Fan &middot; Chao Li &middot; Xiaolong Liu &middot; Anbang Yao
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In this paper, we question if well pre-trained vision transformer (ViT) models could be used as teachers that exhibit scalable properties to advance cross architecture knowledge distillation research, in the context of adopting mainstream large-scale visual recognition datasets for evaluation. To make this possible, our analysis underlines the importance of seeking effective strategies to align (1) feature computing paradigm differences, (2) model scale differences, and (3) knowledge density differences. By combining three closely coupled components namely *cross attention projector*, *dual-view feature mimicking* and *teacher parameter perception* tailored to address the alignment problems stated above, we present a simple and effective knowledge distillation method, called *ScaleKD*. Our method can train student backbones that span across a variety of convolutional neural network (CNN), multi-layer perceptron (MLP), and ViT architectures on image classification datasets, achieving state-of-the-art knowledge distillation performance. For instance, taking a well pre-trained Swin-L as the teacher model, our method gets 75.15\%|82.03\%|84.16\%|78.63\%|81.96\%|83.93\%|83.80\%|85.53\%  top-1 accuracies for MobileNet-V1|ResNet-50|ConvNeXt-T|Mixer-S/16|Mixer-B/16|ViT-S/16|Swin-T|ViT-B/16 models trained on ImageNet-1K dataset from scratch, showing 3.05\%|3.39\%|2.02\%|4.61\%|5.52\%|4.03\%|2.62\%|3.73\% absolute gains to the individually trained counterparts. Intriguingly, when scaling up the size of teacher models or their pre-training datasets, our method showcases the desired scalable properties, bringing increasingly larger gains to student models. We also empirically show that the student backbones trained by our method transfer well on downstream MS-COCO and ADE20K datasets. More importantly, our method could be used as a more efficient alternative to the time-intensive pre-training paradigm for any target student model on large-scale datasets if a strong pre-trained ViT is available, reducing the amount of viewed training samples up to 195$\times$. The code is available at *https://github.com/deep-optimization/ScaleKD*.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-19" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-19', event_id='96823', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1106</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96823">Not Just Object, But State: Compositional Incremental Learning without Forgetting</a></strong></h5>


                        <p class="text-muted">
                            Yanyi Zhang &middot; Binglin Qiu &middot; Qi Jia &middot; Yu Liu &middot; Ran He
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Most incremental learners excessively prioritize object classes while neglecting various kinds of states (e.g. color and material) attached to the objects. As a result, they are limited in the ability to model state-object compositionality accurately. To remedy this limitation, we propose a novel task called Compositional Incremental Learning (composition-IL), which enables the model to recognize a variety of state-object compositions in an incremental learning fashion. Since the lack of suitable datasets, we re-organize two existing datasets and make them tailored for composition-IL. Then, we propose a prompt-based Composition Incremental Learner (CompILer), to overcome the ambiguous composition boundary. Specifically, we exploit multi-pool prompt learning, and ensure the inter-pool prompt discrepancy and intra-pool prompt diversity. Besides, we devise object-injected state prompting which injects object prompts to guide the selection of state prompts. Furthermore, we fuse the selected prompts by a generalized-mean strategy, to eliminate irrelevant information learned in the prompts. Extensive experiments on two datasets exhibit state-of-the-art performance achieved by CompILer. Code and datasets are available at: https://github.com/Yanyi-Zhang/CompILer.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-20" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-20', event_id='96772', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1107</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96772">BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular Depth Estimation</a></strong></h5>


                        <p class="text-muted">
                            Xiang Zhang &middot; Bingxin Ke &middot; Hayko Riemenschneider &middot; Nando Metzger &middot; Anton Obukhov &middot; Markus Gross &middot; Konrad Schindler &middot; Christopher Schroers
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>By training over large-scale datasets, zero-shot monocular depth estimation (MDE) methods show robust performance in the wild but often suffer from insufficient detail. Although recent diffusion-based MDE approaches exhibit a superior ability to extract details, they struggle in geometrically complex scenes that challenge their geometry prior, trained on less diverse 3D  data. To leverage the complementary merits of both worlds, we propose BetterDepth to achieve geometrically correct affine-invariant MDE while capturing fine details. Specifically, BetterDepth is a conditional diffusion-based refiner that takes the prediction from pre-trained MDE models as depth conditioning, in which the global depth layout is well-captured, and iteratively refines details based on the input image. For the training of such a refiner, we propose global pre-alignment and local patch masking methods to ensure BetterDepth remains faithful to the depth conditioning while learning to add fine-grained scene details. With efficient training on small-scale synthetic datasets, BetterDepth achieves state-of-the-art zero-shot MDE performance on diverse public datasets and on in-the-wild scenes. Moreover, BetterDepth can improve the performance of other MDE models in a plug-and-play manner without further re-training.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-21" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-21', event_id='96717', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1108</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96717">Learning to Decouple the Lights for 3D Face Texture Modeling</a></strong></h5>


                        <p class="text-muted">
                            Tianxin Huang &middot; Zhenyu Zhang &middot; Ying Tai &middot; Gim Hee Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Existing research has made impressive strides in reconstructing human facial shapes and textures from images with well-illuminated faces and minimal external occlusions. Nevertheless, it remains challenging to recover accurate facial textures from scenarios with complicated illumination affected by external occlusions, \eg a face that is partially obscured by items such as a hat. Existing works based on the assumption of single and uniform illumination cannot correctly process these data.In this work, we introduce a novel approach to model 3D facial textures under such unnatural illumination. Instead of assuming single illumination, our framework learns to imitate the unnatural illumination as a composition of multiple separate light conditions combined with learned neural representations, named Light Decoupling.According to experiments on both single images and video sequences, we demonstrate the effectiveness of our approach in modeling facial textures under challenging illumination affected by occlusions.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-22" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-22', event_id='96714', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1109</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96714">D-MiSo: Editing Dynamic 3D Scenes using Multi-Gaussians Soup</a></strong></h5>


                        <p class="text-muted">
                            Joanna Waczynska &middot; Piotr Borycki &middot; Joanna Kaleta &middot; Slawomir Tadeja &middot; PrzemysÅaw Spurek
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Over the past years, we have observed an abundance of approaches for modeling dynamic 3D scenes using Gaussian Splatting (GS). These solutions use GS to represent the scene's structure and the neural network to model dynamics. Such approaches allow fast rendering and extracting each element of such a dynamic scene. However, modifying such objects over time is challenging. SC-GS (Sparse Controlled Gaussian Splatting) enhanced with Deformed Control Points partially solves this issue. However, this approach necessitates selecting elements that need to be kept fixed, as well as centroids that should be adjusted throughout editing. Moreover, this task poses additional difficulties regarding the re-productivity of such editing. To address this, we propose Dynamic Multi-Gaussian Soup (D-MiSo), which allows us to model the mesh-inspired representation of dynamic GS. Additionally, we propose a strategy of linking parameterized Gaussian splats, forming a Triangle Soup with the estimated mesh. Consequently, we can separately construct new trajectories for the 3D objects composing the scene. Thus, we can make the scene's dynamic editable over time or while maintaining partial dynamics.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-23" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-23', event_id='96544', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1110</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96544">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</a></strong></h5>


                        <p class="text-muted">
                            Zhiwen Fan &middot; Kevin Wang &middot; Kairun Wen &middot; Zehao Zhu &middot; Dejia Xu &middot; Zhangyang &amp;quot;Atlas&amp;quot; Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advances in real-time neural rendering using point-based techniques have enabled broader adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting impose substantial storage overhead, as Structure-from-Motion (SfM) points can grow to millions, often requiring gigabyte-level disk space for a single unbounded scene. This growth presents scalability challenges and hinders splatting efficiency. To address this, we introduce LightGaussian, a method for transforming 3D Gaussians into a more compact format. Inspired by Network Pruning, LightGaussian identifies Gaussians with minimal global significance on scene reconstruction, and applies a pruning and recovery process to reduce redundancy while preserving visual quality. Knowledge distillation and pseudo-view augmentation then transfer spherical harmonic coefficients to a lower degree, yielding compact representations. Gaussian Vector Quantization, based on each Gaussianâs global significance, further lowers bitwidth with minimal accuracy loss. LightGaussian achieves an average 15 times compression rate while boosting FPS from 144 to 237 within the 3D-GS framework, enabling efficient complex scene representation on the Mip-NeRF 360 and Tank &amp; Temple datasets. The proposed Gaussian pruning approach is also adaptable to other 3D representations (e.g., Scaffold-GS), demonstrating strong generalization capabilities.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-24" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-24', event_id='96431', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1111</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96431">DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features</a></strong></h5>


                        <p class="text-muted">
                            Letian Wang &middot; Seung Wook Kim &middot; Jiawei Yang &middot; Cunjun Yu &middot; Boris Ivanovic &middot; Steven Waslander &middot; Yue Wang &middot; Sanja Fidler &middot; Marco Pavone &middot; Peter Karkus
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose DistillNeRF, a self-supervised learning framework addressing the challenge of understanding 3D environments from limited 2D observations in outdoor autonomous driving scenes. Our method is a generalizable feedforward model that predicts a rich neural scene representation from sparse, single-frame multi-view camera inputs with limited view overlap, and is trained self-supervised with differentiable rendering to reconstruct RGB, depth, or feature images. Our first insight is to exploit per-scene optimized Neural Radiance Fields (NeRFs) by generating dense depth and virtual camera targets from them, which helps our model to learn enhanced 3D geometry from sparse non-overlapping image inputs. Second, to learn a semantically rich 3D representation, we propose distilling features from pre-trained 2D foundation models, such as CLIP or DINOv2, thereby enabling various downstream tasks without the need for costly 3D human annotations. To leverage these two insights, we introduce a novel model architecture with a two-stage lift-splat-shoot encoder and a parameterized sparse hierarchical voxel representation. Experimental results on the NuScenes and Waymo NOTR datasets demonstrate that DistillNeRF significantly outperforms existing comparable state-of-the-art self-supervised methods for scene reconstruction, novel view synthesis, and depth estimation; and it allows for competitive zero-shot 3D semantic occupancy prediction, as well as open-world scene understanding through distilled foundation model features. Demos and code will be available at https://distillnerf.github.io/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-25" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-25', event_id='95520', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1200</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95520">Learning Disentangled Representations for Perceptual Point Cloud Quality Assessment via Mutual Information Minimization</a></strong></h5>


                        <p class="text-muted">
                            Ziyu Shan &middot; Yujie Zhang &middot; Yipeng Liu &middot; YILING XU
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>No-Reference Point Cloud Quality Assessment (NR-PCQA) aims to objectively assess the human perceptual quality of point clouds without relying on pristine-quality point clouds for reference. It is becoming increasingly significant with the rapid advancement of immersive media applications such as virtual reality (VR) and augmented reality (AR). However, current NR-PCQA models attempt to indiscriminately learn point cloud content and distortion representations within a single network, overlooking their distinct contributions to quality information. To address this issue, we propose DisPA, a novel disentangled representation learning framework for NR-PCQA. The framework trains a dual-branch disentanglement network to minimize mutual information (MI) between representations of point cloud content and distortion. Specifically, to fully disentangle representations, the two branches adopt different philosophies: the content-aware encoder is pretrained by a masked auto-encoding strategy, which can allow the encoder to capture semantic information from rendered images of distorted point clouds; the distortion-aware encoder takes a mini-patch map as input, which forces the encoder to focus on low-level distortion patterns. Furthermore, we utilize an MI estimator to estimate the tight upper bound of the actual MI and further minimize it to achieve explicit representation disentanglement. Extensive experimental results demonstrate that DisPA outperforms state-of-the-art methods on multiple PCQA datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-26" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-26', event_id='95744', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1201</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95744">ChatCam: Empowering Camera Control through Conversational AI</a></strong></h5>


                        <p class="text-muted">
                            Xinhang Liu &middot; Yu-Wing Tai &middot; Chi-Keung Tang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Cinematographers adeptly capture the essence of the world, crafting compelling visual narratives through intricate camera movements. Witnessing the strides made by large language models in perceiving and interacting with the 3D world, this study explores their capability to control cameras with human language guidance. We introduce ChatCam, a system that navigates camera movements through conversations with users, mimicking a professional cinematographer's workflow. To achieve this, we propose CineGPT, a GPT-based autoregressive model for text-conditioned camera trajectory generation. We also develop an Anchor Determinator to ensure precise camera trajectory placement. ChatCam understands user requests and employs our proposed tools to generate trajectories, which can be used to render high-quality video footage on radiance field representations. Our experiments, including comparisons to state-of-the-art approaches and user studies, demonstrate our approach's ability to interpret and execute complex instructions for camera operation, showing promising applications in real-world production settings. Project page: https://xinhangliu.com/chatcam.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-27" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-27', event_id='95800', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1202</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95800">bit2bit: 1-bit quanta video reconstruction via self-supervised photon prediction</a></strong></h5>


                        <p class="text-muted">
                            Yehe Liu &middot; Alexander Krull &middot; Hector Basevi &middot; Ales Leonardis &middot; Michael Jenkins
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Quanta image sensors, such as SPAD arrays, are an emerging sensor technology, producing 1-bit arrays representing photon detection events over exposures as short as a few nanoseconds. In practice, raw data are post-processed using heavy spatiotemporal binning to create more useful and interpretable images at the cost of degrading spatiotemporal resolution. In this work, we propose bit2bit, a new method for reconstructing high-quality image stacks at the original spatiotemporal resolution from sparse binary quanta image data. Inspired by recent work on Poisson denoising, we developed an algorithm that creates a dense image sequence from sparse binary photon data by predicting the photon arrival location probability distribution. However, due to the binary nature of the data, we show that the assumption of a Poisson distribution is inadequate. Instead, we model the process with a Bernoulli lattice process from the truncated Poisson. This leads to the proposal of a novel self-supervised solution based on a masked loss function. We evaluate our method using both simulated and real data. On simulated data from a conventional video, we achieve 34.35 mean PSNR with extremely photon-sparse binary input (&lt;0.06 photons per pixel per frame). We also present a novel dataset containing a wide range of real SPAD high-speed videos under various challenging imaging conditions. The scenes cover strong/weak ambient light, strong motion, ultra-fast events, etc., which will be made available to the community, on which we demonstrate the promise of our approach. Both reconstruction quality and throughput substantially surpass the state-of-the-art methods (e.g., Quanta Burst Photography (QBP)). Our approach significantly enhances the visualization and usability of the data, enabling the application of existing analysis techniques.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-28" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-28', event_id='95850', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1203</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95850">Animate3D: Animating Any 3D Model with Multi-view Video Diffusion</a></strong></h5>


                        <p class="text-muted">
                            Yanqin Jiang &middot; Chaohui Yu &middot; Chenjie Cao &middot; Fan Wang &middot; Weiming Hu &middot; Jin Gao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advances in 4D generation mainly focus on generating 4D content by distilling pre-trained text or single-view image conditioned models. It is inconvenient for them to take advantage of various off-the-shelf 3D assets with multi-view attributes, and their results suffer from spatiotemporal inconsistency owing to the inherent ambiguity in the supervision signals. In this work, we present Animate3D, a novel framework for animating any static 3D model. The core idea is two-fold: 1) We propose a novel multi-view video diffusion model (MV-VDM) conditioned on multi-view renderings of the static 3D object, which is trained on our presented large-scale multi-view video dataset (MV-Video). 2) Based on MV-VDM, we introduce a framework combining reconstruction and 4D Score Distillation Sampling (4D-SDS) to leverage the multi-view video diffusion priors for animating 3D objects. Specifically, for MV-VDM, we design a new spatiotemporal attention module to enhance spatial and temporal consistency by integrating 3D and video diffusion models. Additionally, we leverage the static 3D modelâs multi-view renderings as conditions to preserve its identity. For animating 3D models, an effective two-stage pipeline is proposed: we first reconstruct coarse motions directly from generated multi-view videos, followed by the introduced 4D-SDS to model fine-level motions. Benefiting from accurate motion learning, we could achieve straightforward mesh animation. Qualitative and quantitative experiments demonstrate that Animate3D significantly outperforms previous approaches. Data, code, and models are open-released.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-29" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-29', event_id='96022', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1204</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96022">CRT-Fusion: Camera, Radar, Temporal Fusion Using  Motion Information  for 3D Object Detection</a></strong></h5>


                        <p class="text-muted">
                            Jisong Kim &middot; Minjae Seong &middot; Jun Won Choi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Accurate and robust 3D object detection is a critical component in autonomous vehicles and robotics. While recent radar-camera fusion methods have made significant progress by fusing information in the bird's-eye view (BEV) representation, they often struggle to effectively capture the motion of dynamic objects, leading to limited performance in real-world scenarios. In this paper, we introduce CRT-Fusion, a novel framework that integrates temporal information into radar-camera fusion to address this challenge. Our approach comprises three key modules: Multi-View Fusion (MVF), Motion Feature Estimator (MFE), and Motion Guided Temporal Fusion (MGTF). The MVF module fuses radar and image features within both the camera view and bird's-eye view, thereby generating a more precise unified BEV representation. The MFE module conducts two simultaneous tasks: estimation of pixel-wise velocity information and BEV segmentation. Based on the velocity and the occupancy score map obtained from the MFE module, the MGTF module aligns and fuses feature maps across multiple timestamps in a recurrent manner. By considering the motion of dynamic objects, CRT-Fusion can produce robust BEV feature maps, thereby improving detection accuracy and robustness. Extensive evaluations on the challenging nuScenes dataset demonstrate that CRT-Fusion achieves state-of-the-art performance for radar-camera-based 3D object detection. Our approach outperforms the previous best method in terms of NDS by +1.7%, while also surpassing the leading approach in mAP by +1.4%. These significant improvements in both metrics showcase the effectiveness of our proposed fusion strategy in enhancing the reliability and accuracy of 3D object detection.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-30" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-30', event_id='96027', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1205</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96027">Enhancing Feature Diversity Boosts Channel-Adaptive Vision Transformers</a></strong></h5>


                        <p class="text-muted">
                            Chau Pham &middot; Bryan Plummer
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multi-Channel Imaging (MCI) contains an array of challenges for encoding useful feature representations not present in traditional images. For example, images from two different satellites may both contain RGB channels, but the remaining channels can be different for each imaging source. Thus, MCI models must support a variety of channel configurations at test time. Recent work has extended traditional visual encoders for MCI, such as Vision Transformers (ViT), by supplementing pixel information with an encoding representing the channel configuration. However, these methods treat each channel equally, i.e., they do not consider the unique properties of each channel type, which can result in needless and potentially harmful redundancies in the learned features. For example, if RGB channels are always present, the other channels can focus on extracting information that cannot be captured by the RGB channels. To this end, we propose DiChaViT, which aims to enhance the diversity in the learned features of MCI-ViT models. This is achieved through a novel channel sampling strategy that encourages the selection of more distinct channel sets for training. Additionally, we employ regularization and initialization techniques to increase the likelihood that new information is learned from each channel. Many of our improvements are architecture agnostic and can be incorporated into new architectures as they are developed. Experiments on both satellite and cell microscopy datasets, CHAMMI, JUMP-CP, and So2Sat, report DiChaViT yields a 1.5 - 5.0% gain over the state-of-the-art. Our code is publicly available at https://github.com/chaudatascience/diverse<em>channel</em>vit.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-31" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-31', event_id='96081', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1206</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96081">HiCoM: Hierarchical Coherent Motion for Dynamic Streamable Scenes with 3D Gaussian Splatting</a></strong></h5>


                        <p class="text-muted">
                            Qiankun Gao &middot; Jiarui Meng &middot; Chengxiang Wen &middot; Jie Chen &middot; Jian Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The online reconstruction of dynamic scenes from multi-view streaming videos faces significant challenges in training, rendering and storage efficiency. Harnessing superior learning speed and real-time rendering capabilities, 3D Gaussian Splatting (3DGS) has recently demonstrated considerable potential in this field. However, 3DGS can be inefficient in terms of storage and prone to overfitting by excessively growing Gaussians, particularly with limited views. This paper proposes an efficient framework, dubbed HiCoM, with three key components. First, we construct a compact and robust initial 3DGS representation using a perturbation smoothing strategy. Next, we introduce a Hierarchical Coherent Motion mechanism that leverages the inherent non-uniform distribution and local consistency of 3D Gaussians to swiftly and accurately learn motions across frames. Finally, we continually refine the 3DGS with additional Gaussians, which are later merged into the initial 3DGS to maintain consistency with the evolving scene. To preserve a compact representation, an equivalent number of low-opacity Gaussians that minimally impact the representation are removed before processing subsequent frames. Extensive experiments conducted on two widely used datasets show that our framework improves learning efficiency of the state-of-the-art methods by about 20% and reduces the data storage by 85%, achieving competitive free-viewpoint video synthesis quality but with higher robustness and stability. Moreover, by parallel learning multiple frames simultaneously, our HiCoM decreases the average training wall time to &lt;2 seconds per frame with negligible performance degradation, substantially boosting real-world applicability and responsiveness.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-32" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-32', event_id='96171', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1207</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96171">Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand Avatars</a></strong></h5>


                        <p class="text-muted">
                            Xuan Huang &middot; Hanhui Li &middot; Wanquan Liu &middot; Xiaodan Liang &middot; Yiqiang Yan &middot; Yuhao Cheng &middot; CHENQIANG GAO
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we propose to create animatable avatars for interacting hands with 3D Gaussian Splatting (GS) and single-image inputs. Existing GS-based methods designed for single subjects often yield unsatisfactory results due to limited input views, various hand poses, and occlusions. To address these challenges, we introduce a novel two-stage interaction-aware GS framework that exploits cross-subject hand priors and refines 3D Gaussians in interacting areas. Particularly, to handle hand variations, we disentangle the 3D presentation of hands into optimization-based identity maps and learning-based latent geometric features and neural texture maps. Learning-based features are captured by trained networks to provide reliable priors for poses, shapes, and textures, while optimization-based identity maps enable efficient one-shot fitting of out-of-distribution hands. Furthermore, we devise an interaction-aware attention module and a self-adaptive Gaussian refinement module. These modules enhance image rendering quality in areas with intra- and inter-hand interactions, overcoming the limitations of existing GS-based methods. Our proposed method is validated via extensive experiments on the large-scale InterHand2.6M dataset, and it significantly improves the state-of-the-art performance in image quality. Code and models will be released upon acceptance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-33" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-33', event_id='96247', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1208</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96247">Causal Context Adjustment Loss for Learned Image Compression</a></strong></h5>


                        <p class="text-muted">
                            Minghao Han &middot; Shiyin Jiang &middot; Shengxi Li &middot; Xin Deng &middot; Mai Xu &middot; Ce Zhu &middot; Shuhang Gu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In recent years, learned image compression (LIC) technologies have surpassed conventional methods notably in terms of rate-distortion (RD) performance. Most present learned techniques are VAE-based with an autoregressive entropy model, which obviously promotes the RD performance by utilizing the decoded causal context. However, extant methods are highly dependent on the fixed hand-crafted causal context. The question of how to guide the auto-encoder to generate a more effective causal context benefit for the autoregressive entropy models is worth exploring. In this paper, we make the first attempt in investigating the way to explicitly adjust the causal context with our proposed Causal Context Adjustment loss (CCA-loss). By imposing the CCA-loss, we enable the neural network to spontaneously adjust important information into the early stage of the autoregressive entropy model. Furthermore, as transformer technology develops remarkably, variants of which have been adopted by many state-of-the-art (SOTA) LIC techniques. The existing computing devices have not adapted the calculation of the attention mechanism well, which leads to a burden on computation quantity and inference latency. To overcome it, we establish a convolutional neural network (CNN) image compression model and adopt the unevenly channel-wise grouped strategy for high efficiency. Ultimately, the proposed CNN-based LIC network trained with our Causal Context Adjustment loss attains a great trade-off between inference latency and rate-distortion performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-34" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-34', event_id='96276', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1209</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96276">Learning Transferable Features for Implicit Neural Representations</a></strong></h5>


                        <p class="text-muted">
                            Kushal Kardam Vyas &middot; Imtiaz Humayun &middot; Aniket Dashpute &middot; Richard Baraniuk &middot; Ashok Veeraraghavan &middot; Guha Balakrishnan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Implicit neural representations (INRs) have demonstrated success in a variety of applications, including inverse problems and neural rendering. An INR is typically trained to capture one signal of interest, resulting in learned neural features that are highly attuned to that signal. Assumed to be less generalizable, we explore the aspect of transferability of such learned neural features for fitting similar signals. We introduce a new INR training framework, STRAINER that learns transferable features for fitting INRs to new signals from a given distribution, faster and with better reconstruction quality. Owing to the sequential layer-wise affine operations in an INR, we propose to learn transferable representations by sharing initial encoder layers across multiple INRs with independent decoder layers. At test time, the learned encoder representations are transferred as initialization for an otherwise randomly initialized INR. We find STRAINER to yield extremely powerful initialization for fitting images from the same domain and allow for a â +10dB gain in signal quality early on compared to an untrained INR itself. STRAINER also provides a simple way to encode data-driven priors in INRs. We evaluate STRAINER on multiple in-domain and out-of-domain signal fitting tasks and inverse problems and further provide detailed analysis and discussion on the transferability of STRAINERâs features.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-35" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-35', event_id='96283', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1210</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96283">Optimal Transport-based Labor-free Text Prompt Modeling for Sketch Re-identification</a></strong></h5>


                        <p class="text-muted">
                            Rui Li &middot; Tingting Ren &middot; Jie Wen &middot; Jinxing Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Sketch Re-identification (Sketch Re-ID), which aims to retrieve target person from an image gallery based on a sketch query, is crucial for criminal investigation, law enforcement, and missing person searches. Existing methods aim to alleviate the modality gap by employing semantic metrics constraints or auxiliary modal guidance. However, they incur expensive labor costs and inevitably omit fine-grained modality-consistent information due to the abstraction of sketches.To address this issue, this paper proposes a novel $\textit{Optimal Transport-based Labor-free Text Prompt Modeling}$ (OLTM) network, which hierarchically extracts coarse- and fine-grained similarity representations guided by textual semantic information without any additional annotations. Specifically, multiple target attributes are flexibly obtained by a pre-trained visual question answering (VQA) model. Subsequently, a text prompt reasoning module employs learnable prompt strategy and optimal transport algorithm to extract discriminative global and local text representations, which serve as a bridge for hierarchical and multi-granularity modal alignment between sketch and image modalities.Additionally, instead of measuring the similarity of two samples by only computing their distance, a novel triplet assignment loss is further proposed, in which the whole data distribution also contributes to optimizing the inter/intra-class distances. Extensive experiments conducted on two public benchmarks consistently demonstrate the robustness and superiority of our OLTM over state-of-the-art methods.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-36" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-36', event_id='96321', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1211</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96321">Articulate your NeRF: Unsupervised articulated object modeling via conditional view synthesis</a></strong></h5>


                        <p class="text-muted">
                            Jianning Deng &middot; Kartic Subr &middot; Hakan Bilen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose a novel unsupervised method to learn pose and part-segmentation of articulated objects with rigid parts.    Given two observations of an object in different articulation states, our method learns the geometry and appearance of object parts by using an implicit model from the first observation, distills the part segmentation and articulation from the second observation while rendering the latter observation.    Additionally, to tackle the complexities in the joint optimization of part segmentation and articulation, we propose a voxel grid based initialization strategy and a decoupled optimization procedure.    Compared to the prior unsupervised work, our model obtains significantly better performance, generalizes to objects with multiple parts while it can be efficiently from few views for the latter observation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-37" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-37', event_id='95458', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1300</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95458">DiPEx: Dispersing Prompt Expansion for Class-Agnostic Object Detection</a></strong></h5>


                        <p class="text-muted">
                            Jia S Lim &middot; Zhuoxiao Chen &middot; Zhi Chen &middot; Mahsa Baktashmotlagh &middot; Xin Yu &middot; Zi Huang &middot; Yadan Luo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Class-agnostic object detection (OD) can be a cornerstone or a bottleneck for many downstream vision tasks. Despite considerable advancements in bottom-up and multi-object discovery methods that leverage basic visual cues to identify salient objects, consistently achieving a high recall rate remains difficult due to the diversity of object types and their contextual complexity. In this work, we investigate using vision-language models (VLMs) to enhance object detection via a self-supervised prompt learning strategy. Our initial findings indicate that manually crafted text queries often result in undetected objects, primarily because detection confidence diminishes when the query words exhibit semantic overlap. To address this, we propose a Dispersing Prompt Expansion (DiPEx) approach. DiPEx progressively learns to expand a set of distinct, non-overlapping hyperspherical prompts to enhance recall rates, thereby improving performance in downstream tasks such as out-of-distribution OD. Specifically, DiPEx initiates the process by self-training generic parent prompts and selecting the one with the highest semantic uncertainty for further expansion. The resulting child prompts are expected to inherit semantics from their parent prompts while capturing more fine-grained semantics. We apply dispersion losses to ensure high inter-class discrepancy among child prompts while preserving semantic consistency between parent-child prompt pairs. To prevent excessive growth of the prompt sets, we utilize the maximum angular coverage (MAC) of the semantic space as a criterion for early termination. We demonstrate the effectiveness of DiPEx through extensive class-agnostic OD and OOD-OD experiments on MS-COCO and LVIS, surpassing other prompting methods by up to 20.1% in AR and achieving a 21.3% AP improvement over SAM.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-38" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-38', event_id='95437', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1301</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95437">Dual-Diffusion for Binocular 3D Human Pose Estimation</a></strong></h5>


                        <p class="text-muted">
                            Xiaoyue Wan &middot; Zhuo Chen &middot; Bingzhi Duan &middot; Xu Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Binocular 3D human pose estimation (HPE), reconstructing a 3D pose from 2D poses of two views, offers practical advantages by combining multiview geometry with the convenience of a monocular setup. However, compared to a multiview setup, the reduction in the number of cameras increases uncertainty in 3D reconstruction. To address this issue, we leverage the diffusion model, which has shown success in monocular 3D HPE by recovering 3D poses from noisy data with high uncertainty. Yet, the uncertainty distribution of initial 3D poses remains unknown. Considering that 3D errors stem from 2D errors within geometric constraints, we recognize that the uncertainties of 3D and 2D are integrated in a binocular configuration, with the initial 2D uncertainty being well-defined. Based on this insight, we propose Dual-Diffusion specifically for Binocular 3D HPE, simultaneously denoising the uncertainties in 2D and 3D, and recovering plausible and accurate results. Additionally, we introduce Z-embedding as an additional condition for denoising and implement baseline-width-related pose normalization to enhance the model flexibility for various baseline settings. This is crucial as 3D error influence factors encompass depth and baseline width. Extensive experiments validate the effectiveness of our Dual-Diffusion in 2D refinement and 3D estimation. The code and models are available at https://github.com/sherrywan/Dual-Diffusion.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-39" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-39', event_id='95412', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1302</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95412">When does perceptual alignment benefit vision representations?</a></strong></h5>


                        <p class="text-muted">
                            Shobhita Sundaram &middot; Stephanie Fu &middot; Lukas Muttenthaler &middot; Netanel Tamir &middot; Lucy Chai &middot; Simon Kornblith &middot; Trevor Darrell &middot; Phillip Isola
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose. Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception. While vision representations have previously benefited from human preference alignment in contexts like image generation, the utility of perceptually aligned representations in more general-purpose settings remains unclear. Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability in standard computer vision tasks. We finetune state-of-the-art models on a dataset of human similarity judgments for synthetic image triplets and evaluate them across diverse computer vision tasks. We find that aligning models to perceptual judgments yields representations that improve upon the original backbones across many downstream tasks, including counting, semantic segmentation, depth estimation, instance retrieval, and retrieval-augmented generation. In addition, we find that performance is widely preserved on other tasks, including specialized out-of-distribution domains such as in medical imaging and 3D environment frames. Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can make them better representation learners.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-40" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-40', event_id='95406', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1303</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95406">GaussianCut: Interactive segmentation via graph cut for 3D Gaussian Splatting</a></strong></h5>


                        <p class="text-muted">
                            Umangi Jain &middot; Ashkan Mirzaei &middot; Igor Gilitschenski
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce GaussianCut, a new method for interactive multiview segmentation of scenes represented as 3D Gaussians. Our approach allows for selecting the objects to be segmented by interacting with a single view. It accepts intuitive user input, such as point clicks, coarse scribbles, or text. Using 3D Gaussian Splatting (3DGS) as the underlying scene representation simplifies the extraction of objects of interest which are considered to be a subset of the scene's Gaussians.  Our key idea is to represent the scene as a graph and use the graph-cut algorithm to minimize an energy function to effectively partition the Gaussians into foreground and background. To achieve this, we construct a graph based on scene Gaussians and devise a segmentation-aligned energy function on the graph to combine user inputs with scene properties. To obtain an initial coarse segmentation, we leverage 2D image/video segmentation models and further refine these coarse estimates using our graph construction. Our empirical evaluations show the adaptability of GaussianCut across a diverse set of scenes. GaussianCut achieves competitive performance with state-of-the-art approaches for 3D segmentation without requiring any additional segmentation-aware training</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-41" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-41', event_id='95149', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1304</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95149">Neural Localizer Fields for Continuous 3D Human Pose and Shape Estimation</a></strong></h5>


                        <p class="text-muted">
                            IstvÃ¡n SÃ¡rÃ¡ndi &middot; Gerard Pons-Moll
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>With the explosive growth of available training data, single-image 3D human modeling is ahead of a transition to a data-centric paradigm.A key to successfully exploiting data scale is to design flexible models that can be supervised from various heterogeneous data sources produced by different researchers or vendors.To this end, we propose a simple yet powerful paradigm for seamlessly unifying different human pose and shape-related tasks and datasets.Our formulation is centered on the ability - both at training and test time - to query any arbitrary point of the human volume, and obtain its estimated location in 3D.We achieve this by learning a continuous neural field of body point localizer functions, each of which is a differently parameterized 3D heatmap-based convolutional point localizer (detector).For generating parametric output, we propose an efficient post-processing step for fitting SMPL-family body models to nonparametric joint and vertex predictions.With this approach, we can naturally exploit differently annotated data sources including mesh, 2D/3D skeleton and dense pose, without having to convert between them, and thereby train large-scale 3D human mesh and skeleton estimation models that outperform the state-of-the-art on several public benchmarks including 3DPW, EMDB, EHF, SSP-3D and AGORA by a considerable margin.We release our code and models to foster downstream research.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-42" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-42', event_id='94371', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1305</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94371">Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models</a></strong></h5>


                        <p class="text-muted">
                            Jiayu Wang &middot; Yifei Ming &middot; Zhenmei Shi &middot; Vibhav Vineet &middot; Xin Wang &middot; Sharon Li &middot; Neel Joshi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoningâa fundamental component of human cognitionâremains under-explored. We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting. We conduct a comprehensive evaluation of competitive language and vision-language models. Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided. Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence. Our code is available at https://github.com/jiayuww/SpatialEval.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-43" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-43', event_id='94539', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1306</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94539">SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection</a></strong></h5>


                        <p class="text-muted">
                            Yuxuan Li &middot; Xiang Li &middot; Weijie Li &middot; Qibin Hou &middot; Li Liu &middot; Ming-Ming Cheng &middot; Jian Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising &lt;2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at \url{https://github.com/zcablii/SARDet_100K}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-44" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-44', event_id='94478', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1307</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94478">SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition</a></strong></h5>


                        <p class="text-muted">
                            Feng Lu &middot; Xinyao Zhang &middot; Canming Ye &middot; Shuting Dong &middot; Lijun Zhang &middot; Xiangyuan Lan &middot; Chun Yuan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Visual place recognition (VPR) is an essential task for multiple applications such as augmented reality and robot localization. Over the past decade, mainstream methods in the VPR area have been to use feature representation based on global aggregation, as exemplified by NetVLAD. These features are suitable for large-scale VPR and robust against viewpoint changes. However, the VLAD-based aggregation methods usually learn a large number of (e.g., 64) clusters and their corresponding cluster centers, which directly leads to a high dimension of the yielded global features. More importantly, when there is a domain gap between the data in training and inference, the cluster centers determined on the training set are usually improper for inference, resulting in a performance drop. To this end, we first attempt to improve NetVLAD by removing the cluster center and setting only a small number of (e.g., only 4) clusters. The proposed method not only simplifies NetVLAD but also enhances the generalizability across different domains. We name this method SuperVLAD. In addition, by introducing ghost clusters that will not be retained in the final output, we further propose a very low-dimensional 1-Cluster VLAD descriptor, which has the same dimension as the output of GeM pooling but performs notably better. Experimental results suggest that, when paired with a transformer-based backbone, our SuperVLAD shows better domain generalization performance than NetVLAD with significantly fewer parameters. The proposed method also surpasses state-of-the-art methods with lower feature dimensions on several benchmark datasets. The code is available at https://github.com/lu-feng/SuperVLAD.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-45" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-45', event_id='94470', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1308</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94470">Multistable Shape from Shading Emerges from Patch Diffusion</a></strong></h5>


                        <p class="text-muted">
                            Xinran Han &middot; Todd Zickler &middot; Ko Nishino
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Models for inferring monocular shape of surfaces with diffuse reflection---shape from shading---ought to produce distributions of outputs, because there are fundamental mathematical ambiguities of both continuous (e.g., bas-relief) and discrete (e.g., convex/concave) types that are also experienced by humans. Yet, the outputs of current models are limited to point estimates or tight distributions around single modes, which prevent them from capturing these effects. We introduce a model that reconstructs a multimodal distribution of shapes from a single shading image, which aligns with the human experience of multistable perception. We train a small denoising diffusion process to generate surface normal fields from $16\times 16$ patches of synthetic images of everyday 3D objects. We deploy this model patch-wise at multiple scales, with guidance from inter-patch shape consistency constraints. Despite its relatively small parameter count and predominantly bottom-up structure, we show that multistable shape explanations emerge from this model for ambiguous test images that humans experience as being multistable. At the same time, the model produces veridical shape estimates for object-like images that include distinctive occluding contours and appear less ambiguous. This may inspire new architectures for stochastic 3D shape perception that are more efficient and better aligned with human experience.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-46" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-46', event_id='94353', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1309</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94353">A Unified Framework for 3D Scene Understanding</a></strong></h5>


                        <p class="text-muted">
                            Wei Xu &middot; Chunsheng Shi &middot; Sifan Tu &middot; Xin Zhou &middot; Dingkang Liang &middot; Xiang Bai
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose UniSeg3D, a unified 3D segmentation framework that achieves panoptic, semantic, instance, interactive, referring, and open-vocabulary segmentation tasks within a single model. Most previous 3D segmentation approaches are typically tailored to a specific task, limiting their understanding of 3D scenes to a task-specific perspective. In contrast, the proposed method unifies six tasks into unified representations processed by the same Transformer. It facilitates inter-task knowledge sharing, thereby promoting comprehensive 3D scene understanding. To take advantage of multi-task unification, we enhance performance by leveraging inter-task connections. Specifically, we design knowledge distillation and contrastive learning methods to transfer task-specific knowledge across different tasks. Benefiting from extensive inter-task knowledge sharing, our UniSeg3D becomes more powerful. Experiments on three benchmarks, including ScanNet20, ScanRefer, and ScanNet200, demonstrate that the UniSeg3D consistently outperforms current SOTA methods, even those specialized for individual tasks. We hope UniSeg3D can serve as a solid unified baseline and inspire future work. Code and models are available at \url{https://dk-liang.github.io/UniSeg3D/}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-47" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-47', event_id='94280', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1310</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94280">GeoLRM: Geometry-Aware Large Reconstruction Model for High-Quality 3D Gaussian Generation</a></strong></h5>


                        <p class="text-muted">
                            Chubin Zhang &middot; Hongliang Song &middot; Yi Wei &middot; Chen Yu &middot; Jiwen Lu &middot; Yansong Tang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this work, we introduce the Geometry-Aware Large Reconstruction Model (GeoLRM), an approach which can predict high-quality assets with 512k Gaussians and 21 input images in only 11 GB GPU memory. Previous works neglect the inherent sparsity of 3D structure and do not utilize explicit geometric relationships between 3D and 2D images. This limits these methods to a low-resolution representation and makes it difficult to scale up to the dense views for better quality. GeoLRM tackles these issues by incorporating a novel 3D-aware transformer structure that directly processes 3D points and uses deformable cross-attention mechanisms to effectively integrate image features into 3D representations. We implement this solution through a two-stage pipeline: initially, a lightweight proposal network generates a sparse set of 3D anchor points from the posed image inputs; subsequently, a specialized reconstruction transformer refines the geometry and retrieves textural details. Extensive experimental results demonstrate that GeoLRM significantly outperforms existing models, especially for dense view inputs. We also demonstrate the practical applicability of our model with 3D generation tasks, showcasing its versatility and potential for broader adoption in real-world applications. The project page: https://linshan-bin.github.io/GeoLRM/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-48" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-48', event_id='94214', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1311</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94214">R$^2$-Gaussian: Rectifying Radiative Gaussian Splatting for Tomographic Reconstruction</a></strong></h5>


                        <p class="text-muted">
                            Ruyi Zha &middot; Tao Jun Lin &middot; Yuanhao Cai &middot; Jiwen Cao &middot; Yanhao Zhang &middot; Hongdong Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p>3D Gaussian splatting (3DGS) has shown promising results in image rendering and surface reconstruction. However, its potential in volumetric reconstruction tasks, such as X-ray computed tomography, remains under-explored. This paper introduces R$^2$-Gaussian, the first 3DGS-based framework for sparse-view tomographic reconstruction. By carefully deriving X-ray rasterization functions, we discover a previously unknown \emph{integration bias} in the standard 3DGS formulation, which hampers accurate volume retrieval. To address this issue, we propose a novel rectification technique via refactoring the projection from 3D to 2D Gaussians. Our new method presents three key innovations: (1) introducing tailored Gaussian kernels, (2) extending rasterization to X-ray imaging, and (3) developing a CUDA-based differentiable voxelizer. Experiments on synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art approaches in accuracy and efficiency. Crucially, it delivers high-quality results in 4 minutes, which is 12$\times$ faster than NeRF-based methods and on par with traditional algorithms.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-49" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-49', event_id='93056', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1400</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93056">Toward Approaches to Scalability in 3D Human Pose Estimation</a></strong></h5>


                        <p class="text-muted">
                            Jun-Hui Kim &middot; Seong-Whan Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In the field of 3D Human Pose Estimation (HPE), scalability and generalization across diverse real-world scenarios remain significant challenges. This paper addresses two key bottlenecks to scalability: limited data diversity caused by 'popularity bias' and increased 'one-to-many' depth ambiguity arising from greater pose diversity. We introduce the Biomechanical Pose Generator (BPG), which leverages biomechanical principles, specifically the normal range of motion, to autonomously generate a wide array of plausible 3D poses without relying on a source dataset, thus overcoming the restrictions of popularity bias. To address depth ambiguity, we propose the Binary Depth Coordinates (BDC), which simplifies depth estimation into a binary classification of joint positions (front or back). This method decomposes a 3D pose into three core elementsâ2D pose, bone length, and binary depth decisionâsubstantially reducing depth ambiguity and enhancing model robustness and accuracy, particularly in complex poses. Our results demonstrate that these approaches increase the diversity and volume of pose data while consistently achieving performance gains, even amid the complexities introduced by increased pose diversity.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-50" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-50', event_id='93236', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1401</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93236">Learning to Edit Visual Programs with Self-Supervision</a></strong></h5>


                        <p class="text-muted">
                            R. Kenny Jones &middot; Renhao Zhang &middot; Aditya Ganeshan &middot; Daniel Ritchie
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We design a system that learns how to edit visual programs. Our edit network consumes a complete input program and a visual target. From this input, we task our network with predicting a local edit operation that could be applied to the input program to improve its similarity to the target. In order to apply this scheme for domains that lack program annotations, we develop a self-supervised learning approach that integrates this edit network into a bootstrapped finetuning loop along with a network that predicts entire programs in one-shot. Our joint finetuning scheme, when coupled with an inference procedure that initializes a population from the one-shot model and evolves members of this population with the edit network, helps to infer more accurate visual programs. Over multiple domains, we experimentally compare our method against the alternative of using only the one-shot model, and find that even under equal search-time budgets, our editing-based paradigm provides significant advantages.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-51" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-51', event_id='93493', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1402</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93493">Prune and Repaint: Content-Aware Image Retargeting for any Ratio</a></strong></h5>


                        <p class="text-muted">
                            Feihong Shen &middot; Chao Li &middot; Yifeng Geng &middot; Yongjian Deng &middot; Hao Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Image retargeting is the task of adjusting the aspect ratio of images to suit different display devices or presentation environments. However, existing retargeting methods often struggle to balance the preservation of key semantics and image quality, resulting in either deformation or loss of important objects, or the introduction of local artifacts such as discontinuous pixels and inconsistent regenerated content. To address these issues, we propose a content-aware retargeting method called PruneRepaint. It incorporates semantic importance for each pixel to guide the identification of regions that need to be pruned or preserved in order to maintain key semantics. Additionally, we introduce an adaptive repainting module that selects image regions for repainting based on the distribution of pruned pixels and the proportion between foreground size and target aspect ratio, thus achieving local smoothness after pruning. By focusing on the content and structure of the foreground, our PruneRepaint approach adaptively avoids key content loss and deformation, while effectively mitigating artifacts with local repainting. We conduct experiments on the public RetargetMe benchmark and demonstrate through objective experimental results and subjective user studies that our method outperforms previous approaches in terms of preserving semantics and aesthetics, as well as better generalization across diverse aspect ratios. Codes will be available at https://github.com/fhshen2022/PruneRepaint.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-52" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-52', event_id='93549', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1403</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93549">Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers</a></strong></h5>


                        <p class="text-muted">
                            Dong Hoon Lee &middot; Seunghoon Hong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent token reduction methods for Vision Transformers (ViTs) incorporate token merging, which measures the similarities between token embeddings and combines the most similar pairs.However, their merging policies are directly dependent on intermediate features in ViTs, which prevents exploiting features tailored for merging and requires end-to-end training to improve token merging.In this paper, we propose Decoupled Token Embedding for Merging (DTEM) that enhances token merging through a decoupled embedding learned via a continuously relaxed token merging process.Our method introduces a lightweight embedding module decoupled from the ViT forward pass to extract dedicated features for token merging, thereby addressing the restriction from using intermediate features.The continuously relaxed token merging, applied during training, enables us to learn the decoupled embeddings in a differentiable manner.Thanks to the decoupled structure, our method can be seamlessly integrated into existing ViT backbones and trained either modularly by learning only the decoupled embeddings or end-to-end by fine-tuning. We demonstrate the applicability of DTEM on various tasks, including classification, captioning, and segmentation, with consistent improvement in token merging.Especially in the ImageNet-1k classification, DTEM achieves a 37.2\% reduction in FLOPs while maintaining a top-1 accuracy of 79.85\% with DeiT-small.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-53" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-53', event_id='93574', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1404</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93574">Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba</a></strong></h5>


                        <p class="text-muted">
                            Haoye Dong &middot; Aviral Chharia &middot; Wenbo Gou &middot; Francisco Vicente Carrasco &middot; Fernando D De la Torre
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, yet they do not fully achieve robust and accurate performance, primarily due to inefficiently modeling spatial relations between joints. To address this problem, we propose a novel graph-guided Mamba framework, named Hamba, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba's scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to efficiently learn the spatial relationships between joints for improving reconstruction performance. Specifically, we design a Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5\% fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space features and jointly considers global and local features to improve performance. Experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND. At the time of this paper's acceptance, Hamba holds the top position, Rank 1, in two competition leaderboards on 3D hand reconstruction.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-54" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-54', event_id='93643', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1405</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93643">LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate</a></strong></h5>


                        <p class="text-muted">
                            Anthony Fuller &middot; Daniel Kyrollos &middot; Yousef Yassin &middot; James Green
                        </p>

                    </div>
                    <div class="abstract">
                        <p>High-resolution images offer more information about scenes that can improve model accuracy. However, the dominant model architecture in computer vision, the vision transformer (ViT), cannot effectively leverage larger images without finetuning â ViTs poorly extrapolate to more patches at test time, although transformers offer sequence length flexibility. We attribute this shortcoming to the current patch position encoding methods, which create a distribution shift when extrapolating.We propose a drop-in replacement for the position encoding of plain ViTs that restricts attention heads to fixed fields of view, pointed in different directions, using 2D attention masks. Our novel method, called LookHere, provides translation-equivariance, ensures attention head diversity, and limits the distribution shift that attention heads face when extrapolating. We demonstrate that LookHere improves performance on classification (avg. 1.6%), against adversarial attack (avg. 5.4%), and decreases calibration error (avg. 1.5%) â on ImageNet without extrapolation. With extrapolation, LookHere outperforms the current SoTA position encoding method, 2D-RoPE, by 21.7% on ImageNet when trained at $224^2$ px and tested at $1024^2$ px. Additionally, we release a high-resolution test set to improve the evaluation of high-resolution image classifiers, called ImageNet-HR.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-55" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-55', event_id='93701', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1406</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93701">DiffSF: Diffusion Models for Scene Flow Estimation</a></strong></h5>


                        <p class="text-muted">
                            Yushan Zhang &middot; Bastian Wandt &middot; Maria Magnusson &middot; Michael Felsberg
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Scene flow estimation is an essential ingredient for a variety of real-world applications, especially for autonomous agents, such as self-driving cars and robots. While recent scene flow estimation approaches achieve reasonable accuracy, their applicability to real-world systems additionally benefits from a reliability measure. Aiming at improving accuracy while additionally providing an estimate for uncertainty, we propose DiffSF that combines transformer-based scene flow estimation with denoising diffusion models. In the diffusion process, the ground truth scene flow vector field is gradually perturbed by adding Gaussian noise. In the reverse process, starting from randomly sampled Gaussian noise, the scene flow vector field prediction is recovered by conditioning on a source and a target point cloud. We show that the diffusion process greatly increases the robustness of predictions compared to prior approaches resulting in state-of-the-art performance on standard scene flow estimation benchmarks. Moreover, by sampling multiple times with different initial states, the denoising process predicts multiple hypotheses, which enables measuring the output uncertainty, allowing our approach to detect a majority of the inaccurate predictions. The code is available at https://github.com/ZhangYushan3/DiffSF.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-56" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-56', event_id='93705', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1407</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93705">Rad-NeRF: Ray-decoupled Training of Neural Radiance Field</a></strong></h5>


                        <p class="text-muted">
                            Lidong Guo &middot; Xuefei Ning &middot; Yonggan Fu &middot; Tianchen Zhao &middot; Zhuoliang Kang &middot; Jincheng Yu &middot; Yingyan (Celine) Lin &middot; Yu Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Although the neural radiance field (NeRF) exhibits high-fidelity visualization on the rendering task, it still suffers from rendering defects, especially in complex scenes. In this paper, we delve into the reason for the unsatisfactory performance and conjecture that it comes from interference in the training process. Due to occlusions in complex scenes, a 3D point may be invisible to some rays. On such a point, training with those rays that do not contain valid information about the point might interfere with the NeRF training. Based on the above intuition, we decouple the training process of NeRF in the ray dimension softly and propose a Ray-decoupled Training Framework for neural rendering (Rad-NeRF). Specifically, we construct an ensemble of sub-NeRFs and train a soft gate module to assign the gating scores to these sub-NeRFs based on specific rays. The gate module is jointly optimized with the sub-NeRF ensemble to learn the preference of sub-NeRFs for different rays automatically. Furthermore, we introduce depth-based mutual learning to enhance the rendering consistency among multiple sub-NeRFs and mitigate the depth ambiguity. Experiments on five datasets demonstrate that Rad-NeRF can enhance the rendering performance across a wide range of scene types compared with existing single-NeRF and multi-NeRF methods. With only 0.2% extra parameters, Rad-NeRF improves rendering performance by up to 1.5dB. Code is available at https://github.com/thu-nics/Rad-NeRF.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-57" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-57', event_id='93806', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1408</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93806">FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training</a></strong></h5>


                        <p class="text-muted">
                            Ruihong Yin &middot; Vladimir Yugay &middot; Yue Li &middot; Sezer Karaoglu &middot; Theo Gevers
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The field of novel view synthesis from images has seen rapid advancements with the introduction of Neural Radiance Fields (NeRF) and more recently with 3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its efficiency and ability to render novel views accurately. While Gaussian Splatting performs well when a sufficient amount of training images are available, its unstructured explicit representation tends to overfit in scenarios with sparse input images, resulting in poor rendering performance. To address this, we present a 3D Gaussian-based novel view synthesis method using sparse input images that can accurately render the scene from the viewpoints not covered by the training images. We propose a multi-stage training scheme with matching-based consistency constraints imposed on the novel views without relying on pre-trained depth estimation or diffusion models. This is achieved by using the matches of the available training images to supervise the generation of the novel views sampled between the training frames with color, geometry, and semantic losses. In addition, we introduce a locality preserving regularization for 3D Gaussians which removes rendering artifacts by preserving the local color structure of the scene. Evaluation on synthetic and real-world datasets demonstrates competitive or superior performance of our method in few-shot novel view synthesis compared to existing state-of-the-art methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-58" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-58', event_id='93890', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1409</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93890">Text-Infused Attention and Foreground-Aware Modeling for Zero-Shot Temporal Action Detection</a></strong></h5>


                        <p class="text-muted">
                            Yearang Lee &middot; Ho-Joong Kim &middot; Seong-Whan Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Zero-Shot Temporal Action Detection (ZSTAD) aims to classify and localize action segments in untrimmed videos for unseen action categories. Most existing ZSTAD methods utilize a foreground-based approach, limiting the integration of text and visual features due to their reliance on pre-extracted proposals. In this paper, we introduce a cross-modal ZSTAD baseline with mutual cross-attention, integrating both text and visual information throughout the detection process. Our simple approach results in superior performance compared to previous methods. Despite this improvement, we further identify a common-action bias issue that the cross-modal baseline over-focus on common sub-actions due to a lack of ability to discriminate text-related visual parts. To address this issue, we propose Text-infused attention and Foreground-aware Action Detection (Ti-FAD), which enhances the ability to focus on text-related sub-actions and distinguish relevant action segments from the background. Our extensive experiments demonstrate that Ti-FAD outperforms the state-of-the-art methods on ZSTAD benchmarks by a large margin:  41.2\% (+ 11.0\%) on THUMOS14 and 32.0\% (+ 5.4\%) on ActivityNet v1.3. Code is available at: https://github.com/YearangLee/Ti-FAD.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-59" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-59', event_id='94005', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1410</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94005">UDON: Universal Dynamic Online distillatioN for generic image representations</a></strong></h5>


                        <p class="text-muted">
                            Nikolaos-Antonios Ypsilantis &middot; Kaifeng Chen &middot; Andre Araujo &middot; Ondrej Chum
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Universal image representations are critical in enabling real-world fine-grained and instance-level recognition applications, where objects and entities from any domain must be identified at large scale.Despite recent advances, existing methods fail to capture important domain-specific knowledge, while also ignoring differences in data distribution across different domains.This leads to a large performance gap between efficient universal solutions and expensive approaches utilising a collection of specialist models, one for each domain.In this work, we make significant strides towards closing this gap, by introducing a new learning technique, dubbed UDON (Universal Dynamic Online distillatioN).UDON employs multi-teacher distillation, where each teacher is specialized in one domain, to transfer detailed domain-specific knowledge into the student universal embedding.UDON's distillation approach is not only effective, but also very efficient, by sharing most model parameters between the student and all teachers, where all models are jointly trained in an online manner.UDON also comprises a sampling technique which adapts the training process to dynamically allocate batches to domains which are learned slower and require more frequent processing.This boosts significantly the learning of complex domains which are characterised by a large number of classes and long-tail distributions.With comprehensive experiments, we validate each component of UDON, and showcase significant improvements over the state of the art in the recent UnED benchmark.Code: https://github.com/nikosips/UDON.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-60" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-60', event_id='94008', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1411</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94008">Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View Synthesis</a></strong></h5>


                        <p class="text-muted">
                            Zhiyuan Min &middot; Yawei Luo &middot; Jianwen Sun &middot; Yi Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Generalizable 3D Gaussian splitting (3DGS) can reconstruct new scenes from sparse-view observations in a feed-forward inference manner, eliminating the need for scene-specific retraining required in conventional 3DGS. However, existing methods rely heavily on epipolar priors, which can be unreliable in complex real-world scenes, particularly in non-overlapping and occluded regions. In this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based model for generalizable novel view synthesis that operates independently of epipolar line constraints. To enhance multiview feature extraction with 3D perception, we employ a self-supervised Vision Transformer (ViT) with cross-view completion pre-training on large-scale datasets. Additionally, we introduce an Iterative Cross-view Gaussians Alignment method to ensure consistent depth scales across different views. Our eFreeSplat represents a new paradigm for generalizable novel view synthesis. We evaluate eFreeSplat on wide-baseline novel view synthesis tasks using the RealEstate10K and ACID datasets. Extensive experiments demonstrate that eFreeSplat surpasses state-of-the-art baselines that rely on epipolar priors, achieving superior geometry reconstruction and novel view synthesis quality.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-61" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-61', event_id='93050', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1500</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93050">ManiPose: Manifold-Constrained Multi-Hypothesis 3D Human Pose Estimation</a></strong></h5>


                        <p class="text-muted">
                            CÃ©dric ROMMEL &middot; Victor Letzelter &middot; Nermin Samet &middot; Renaud Marlet &middot; Matthieu Cord &middot; Patrick Perez &middot; Eduardo Valle
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose ManiPose, a manifold-constrained multi-hypothesis model for human-pose 2D-to-3D lifting. We provide theoretical and empirical evidence that, due to the depth ambiguity inherent to monocular 3D human pose estimation, traditional regression models suffer from pose-topology consistency issues, which standard evaluation metrics (MPJPE, P-MPJPE and PCK) fail to assess.  ManiPose addresses depth ambiguity by proposing multiple candidate 3D poses for each 2D input, each with its estimated plausibility. Unlike previous multi-hypothesis approaches, ManiPose forgoes generative models, greatly facilitating its training and usage. By constraining the outputs to lie on the human pose manifold, ManiPose guarantees the consistency of all hypothetical poses, in contrast to previous works. We showcase the performance of ManiPose on real-world datasets, where it outperforms state-of-the-art models in pose consistency by a large margin while being very competitive on the MPJPE metric.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-62" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-62', event_id='92937', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1501</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92937">Unsupervised Homography Estimation on Multimodal Image Pair via Alternating Optimization</a></strong></h5>


                        <p class="text-muted">
                            Sanghyeob Song &middot; Jaihyun Lew &middot; Hyemi Jang &middot; Sungroh Yoon
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Estimating the homography between two images is crucial for mid- or high-level vision tasks, such as image stitching and fusion. However, using supervised learning methods is often challenging or costly due to the difficulty of collecting ground-truth data. In response, unsupervised learning approaches have emerged. Most early methods, though, assume that the given image pairs are from the same camera or have minor lighting differences. Consequently, while these methods perform effectively under such conditions, they generally fail when input image pairs come from different domains, referred to as multimodal image pairs.To address these limitations, we propose AltO, an unsupervised learning framework for estimating homography in multimodal image pairs. Our method employs a two-phase alternating optimization framework, similar to Expectation-Maximization (EM), where one phase reduces the geometry gap and the other addresses the modality gap. To handle these gaps, we use Barlow Twins loss for the modality gap and propose an extended version, Geometry Barlow Twins, for the geometry gap. As a result, we demonstrate that our method, AltO, can be trained on multimodal datasets without any ground-truth data. It not only outperforms other unsupervised methods but is also compatible with various architectures of homography estimators.The source code can be found at: https://github.com/songsang7/AltO</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-63" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-63', event_id='94159', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1502</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94159">TrAct: Making First-layer Pre-Activations Trainable</a></strong></h5>


                        <p class="text-muted">
                            Felix Petersen &middot; Christian Borgelt &middot; Stefano Ermon
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We consider the training of the first layer of vision models and notice the clear relationship between pixel values and gradient update magnitudes: the gradients arriving at the weights of a first layer are by definition directly proportional to (normalized) input pixel values. Thus, an image with low contrast has a smaller impact on learning than an image with higher contrast, and a very bright or very dark image has a stronger impact on the weights than an image with moderate brightness. In this work, we propose performing gradient descent on the embeddings produced by the first layer of the model. However, switching to discrete inputs with an embedding layer is not a reasonable option for vision models. Thus, we propose the conceptual procedure of (i) a gradient descent step on first layer activations to construct an activation proposal, and (ii) finding the optimal weights of the first layer, i.e., those weights which minimize the squared distance to the activation proposal. We provide a  closed form solution of the procedure and adjust it for robust stochastic training while computing everything efficiently. Empirically, we find that TrAct (Training Activations) speeds up training by factors between 1.25x and 4x while requiring only a small computational overhead. We demonstrate the utility of TrAct with different optimizers for a range of different vision models including convolutional and transformer architectures.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-64" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-64', event_id='96467', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1503</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96467">Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation</a></strong></h5>


                        <p class="text-muted">
                            Xin Yuan &middot; Michael Maire
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We develop a neural network architecture which, trained in an unsupervised manner as a denoising diffusion model, simultaneously learns to both generate and segment images.  Learning is driven entirely by the denoising diffusion objective, without any annotation or prior knowledge about regions during training.  A computational bottleneck, built into the neural architecture, encourages the denoising network to partition an input into regions, denoise them in parallel, and combine the results.  Our trained model generates both synthetic images and, by simple examination of its internal predicted partitions, semantic segmentations of those images.  Without fine-tuning, we directly apply our unsupervised model to the downstream task of segmenting real images via noising and subsequently denoising them.  Experiments demonstrate that our model achieves accurate unsupervised image segmentation and high-quality synthetic image generation across multiple datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-65" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-65', event_id='95513', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1504</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95513">ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise Optimization</a></strong></h5>


                        <p class="text-muted">
                            Luca Eyring &middot; Shyamgopal Karthik &middot; Karsten Roth &middot; Alexey Dosovitskiy &middot; Zeynep Akata
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Text-to-Image (T2I) models have made significant advancements in recent years, but they still struggle to accurately capture intricate details specified in complex compositional prompts. While fine-tuning T2I models with reward objectives has shown promise, it suffers from "reward hacking" and may not generalize well to unseen prompt distributions. In this work, we propose Reward-based Noise Optimization (ReNO), a novel approach that enhances T2I models at inference by optimizing the initial noise based on the signal from one or multiple human preference reward models. Remarkably, solving this optimization problem with gradient ascent for 50 iterations yields impressive results on four different one-step models across two competitive benchmarks, T2I-CompBench and GenEval. Within a computational budget of 20-50 seconds, ReNO-enhanced one-step models consistently surpass the performance of all current open-source Text-to-Image models. Extensive user studies demonstrate that our model is preferred nearly twice as often compared to the popular SDXL model and is on par with the proprietary Stable Diffusion 3 with 8B parameters. Moreover, given the same computational resources, a ReNO-optimized one-step model outperforms widely-used open-source models such as SDXL and PixArt-alpha, highlighting the efficiency and effectiveness of ReNO in enhancing T2I model performance at inference time.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-66" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-66', event_id='94905', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1505</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94905">Autoregressive Image Generation without Vector Quantization</a></strong></h5>


                        <p class="text-muted">
                            Tianhong Li &middot; Yonglong Tian &middot; He Li &middot; Mingyang Deng &middot; Kaiming He
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at <a href="https://github.com/LTH14/mar">https://github.com/LTH14/mar</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-67" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-67', event_id='94408', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1506</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94408">Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference</a></strong></h5>


                        <p class="text-muted">
                            Senmao Li &middot; Taihang Hu &middot; Joost van de Weijer &middot; Fahad Shahbaz Khan &middot; Tao Liu &middot; Linxuan Li &middot; Shiqi Yang &middot; Yaxing Wang &middot; Ming-Ming Cheng &middot; jian Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>One of the main drawback of diffusion models is the slow inference time for image generation. Among the most successful approaches to addressing this problem are distillation methods. However, these methods require considerable computational resources. In this paper, we take another approach to diffusion model acceleration. We conduct a comprehensive study of the UNet encoder and empirically analyze the encoder features. This provides insights regarding their changes during the inference process. In particular, we find that encoder features change minimally, whereas the decoder features exhibit substantial variations across different time-steps. This insight motivates us to omit encoder computation at certain adjacent time-steps and reuse encoder features of previous time-steps as input to the decoder in multiple time-steps. Importantly, this allows us to perform decoder computation in parallel, further accelerating the denoising process. Additionally,  we introduce a prior noise injection method to improve the texture details in the generated image. Besides the standard text-to-image task, we also validate our approach on other tasks: text-to-video, personalized generation and reference-guided generation. Without utilizing any knowledge distillation technique, our approach accelerates both the Stable Diffusion (SD) and DeepFloyd-IF model sampling by 41$\%$ and 24$\%$ respectively, and DiT model sampling by 34$\%$, while maintaining high-quality generation performance. Our code will be publicly released.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-68" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-68', event_id='94290', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1507</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94290">Neural Assets: 3D-Aware Multi-Object Scene Synthesis with Image Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Ziyi Wu &middot; Yulia Rubanova &middot; Rishabh Kabra &middot; Drew Hudson &middot; Igor Gilitschenski &middot; Yusuf Aytar &middot; Sjoerd van Steenkiste &middot; Kelsey Allen &middot; Thomas Kipf
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We address the problem of multi-object 3D pose control in image diffusion models. Instead of conditioning on a sequence of text tokens, we propose to use a set of per-object representations, <em>Neural Assets</em>, to control the 3D pose of individual objects in a scene. Neural Assets are obtained by pooling visual representations of objects from a reference image, such as a frame in a video, and are trained to reconstruct the respective objects in a different image, e.g., a later frame in the video. Importantly, we encode object visuals from the reference image while conditioning on object poses from the target frame, which enables learning disentangled appearance and position features. Combining visual and 3D pose representations in a sequence-of-tokens format allows us to keep the text-to-image interface of existing models, with Neural Assets in place of text tokens. By fine-tuning a pre-trained text-to-image diffusion model with this information, our approach enables fine-grained 3D pose and placement control of individual objects in a scene. We further demonstrate that Neural Assets can be transferred and recomposed across different scenes. Our model achieves state-of-the-art multi-object editing results on both synthetic 3D scene datasets, as well as two real-world video datasets (Objectron, Waymo Open).</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-69" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-69', event_id='94025', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1508</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94025">HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image Generation</a></strong></h5>


                        <p class="text-muted">
                            Bocheng &middot; Yuhang Ma &middot; wuliebucha &middot; Shanyuan Liu &middot; Ao Ma &middot; Xiaoyu Wu &middot; Dawei Leng &middot; Yuhui Yin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The task of layout-to-image generation involves synthesizing images based on the captions of objects and their spatial positions. Existing methods still struggle in complex layout generation, where common bad cases include object missing, inconsistent lighting, conflicting view angles, etc. To effectively address these issues, we propose a \textbf{Hi}erarchical \textbf{Co}ntrollable (HiCo) diffusion model for layout-to-image generation, featuring object seperable conditioning branch structure. Our key insight is to achieve spatial disentanglement through hierarchical modeling of layouts. We use a multi branch structure to represent hierarchy and aggregate them in fusion module. To evaluate the performance of multi-objective controllable layout generation in natural scenes, we introduce the HiCo-7K benchmark, derived from the GRIT-20M dataset and manually cleaned. https://github.com/360CVGroup/HiCo_T2I.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-70" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-70', event_id='93631', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1509</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93631">Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Cong Wan &middot; Yuhang He &middot; Xiang Song &middot; Yihong Gong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion models have revolutionized customized text-to-image generation, allowing for efficient synthesis of photos from personal data with textual descriptions. However, these advancements bring forth risks including privacy breaches and unauthorized replication of artworks. Previous researches primarily center around using âprompt-specific methodsâ to generate adversarial examples to protect personal images, yet the effectiveness of existing methods is hindered by constrained adaptability to different prompts.In this paper, we introduce a Prompt-Agnostic Adversarial Perturbation (PAP) method for customized diffusion models. PAP first models the prompt distribution using a Laplace Approximation, and then produces prompt-agnostic perturbations by maximizing a disturbance expectation based on the modeled distribution.This approach effectively tackles the prompt-agnostic attacks, leading to improved defense stability.Extensive experiments in face privacy and artistic style protection, demonstrate the superior generalization of our method in comparison to existing techniques.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-71" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-71', event_id='93588', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1510</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93588">PromptFix: You Prompt and We Fix the Photo</a></strong></h5>


                        <p class="text-muted">
                            yongsheng yu &middot; Ziyun Zeng &middot; Hang Hua &middot; Jianlong Fu &middot; Jiebo Luo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions. However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks. Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images. To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks. First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation. Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization. Experimental results show that PromptFix outperforms previous methods in various image-processing tasks. Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-72" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-72', event_id='95600', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1511</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95600">NeuralFluid: Nueral Fluidic System Design and Control with Differentiable Simulation</a></strong></h5>


                        <p class="text-muted">
                            Yifei Li &middot; Yuchen Sun &middot; Pingchuan Ma &middot; Eftychios Sifakis &middot; Tao Du &middot; Bo Zhu &middot; Wojciech Matusik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present NeuralFluid, a novel framework to explore neural control and design of complex fluidic systems with dynamic solid boundaries. Our system features a fast differentiable Navier-Stokes solver with solid-fluid interface handling, a low-dimensional differentiable parametric geometry representation, a control-shape co-design algorithm, and gym-like simulation environments to facilitate various fluidic control design applications. Additionally, we present a benchmark of design, control, and learning tasks on high-fidelity, high-resolution dynamic fluid environments that pose challenges for existing differentiable fluid simulators. These tasks include designing the control of artificial hearts, identifying robotic end-effector shapes, and controlling a fluid gate. By seamlessly incorporating our differentiable fluid simulator into a learning framework, we demonstrate successful design, control, and learning results that surpass gradient-free solutions in these benchmark tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-73" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-73', event_id='93472', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1600</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93472">Variational Multi-scale Representation for Estimating Uncertainty in 3D Gaussian Splatting</a></strong></h5>


                        <p class="text-muted">
                            Ruiqi Li &middot; Yiu-ming Cheung
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recently, 3D Gaussian Splatting (3DGS) has become popular in reconstructing dense 3D representations of appearance and geometry. However, the learning pipeline in 3DGS inherently lacks the ability to quantify uncertainty, which is an important factor in applications like robotics mapping and navigation. In this paper, we propose an uncertainty estimation method built upon the Bayesian inference framework. Specifically, we propose a method to build variational multi-scale 3D Gaussians, where we leverage explicit scale information in 3DGS parameters to construct diversified parameter space samples. We develop an offset table technique to draw local multi-scale samples efficiently by offsetting selected attributes and sharing other base attributes. Then, the offset table is learned by variational inference with multi-scale prior. The learned offset posterior can quantify the uncertainty of each individual Gaussian component, and be used in the forward pass to infer the predictive uncertainty. Extensive experimental results on various benchmark datasets show that the proposed method provides well-aligned calibration performance on estimated uncertainty and better rendering quality compared with the previous methods that enable uncertainty quantification with view synthesis. Besides, by leveraging the model parameter uncertainty estimated by our method, we can remove noisy Gaussians automatically, thereby obtaining a high-fidelity part of the reconstructed scene, which is of great help in improving the visual quality.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-74" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-74', event_id='95233', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1601</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95233">Tetrahedron Splatting for 3D Generation</a></strong></h5>


                        <p class="text-muted">
                            Chun Gu &middot; Zeyu Yang &middot; Zijie Pan &middot; Xiatian Zhu &middot; Li Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>3D representation is essential to the significant advance of 3D generation with 2D diffusion priors. As a flexible representation, NeRF has been first adopted for 3D representation. With density-based volumetric rendering, it however suffers both intensive computational overhead and inaccurate mesh extraction. Using a signed distance field and Marching Tetrahedra, DMTet allows for precise mesh extraction and real-time rendering but is limited in handling large topological changes in meshes, leading to optimization challenges. Alternatively, 3D Gaussian Splatting (3DGS) is favored in both training and rendering efficiency while falling short in mesh extraction. In this work, we introduce a novel 3D representation, Tetrahedron Splatting (TeT-Splatting), that supports easy convergence during optimization, precise mesh extraction, and real-time rendering simultaneously. This is achieved by integrating surface-based volumetric rendering within a structured tetrahedral grid while preserving the desired ability of precise mesh extraction, and a tile-based differentiable tetrahedron rasterizer. Furthermore, we incorporate eikonal and normal consistency regularization terms for the signed distance field to improve generation quality and stability. Critically, our representation can be trained without mesh extraction, making the optimization process easier to converge. Our TeT-Splatting can be readily integrated in existing 3D generation pipelines, along with polygonal mesh for texture optimization. Extensive experiments show that our TeT-Splatting strikes a superior tradeoff among convergence speed, render efficiency, and mesh quality as compared to previous alternatives under varying 3D generation settings.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-75" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-75', event_id='96214', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1602</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96214">Model Sensitivity Aware Continual Learning</a></strong></h5>


                        <p class="text-muted">
                            Zhenyi Wang &middot; Heng Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Continual learning (CL) aims to adapt to non-stationary data distributions while retaining previously acquired knowledge. However, CL models typically face a trade-off between preserving old task knowledge and excelling in new task performance. Existing approaches often sacrifice one for the other. To overcome this limitation, orthogonal to existing approaches, we propose a novel perspective that views the CL model ability in preserving old knowledge and performing well in new task as a matter of model sensitivity to parameter updates. \textit{Excessive} parameter sensitivity can lead to two drawbacks: (1) significant forgetting of previous knowledge; and (2) overfitting to new tasks. To reduce parameter sensitivity, we optimize the model's performance based on the parameter distribution, which achieves the worst-case CL performance within a distribution neighborhood. This innovative learning paradigm offers dual benefits: (1) reduced forgetting of old knowledge by mitigating drastic changes in model predictions under small parameter updates; and (2) enhanced new task performance by preventing overfitting to new tasks. Consequently, our method achieves superior ability in retaining old knowledge and achieving excellent new task performance simultaneously.Importantly, our approach is compatible with existing CL methodologies, allowing seamless integration while delivering significant improvements in effectiveness, efficiency, and versatility with both theoretical and empirical supports.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-76" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-76', event_id='93186', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1603</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93186">Learning Segmentation from Point Trajectories</a></strong></h5>


                        <p class="text-muted">
                            Laurynas Karazija &middot; Iro Laina &middot; Christian Rupprecht &middot; Andrea Vedaldi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We consider the problem of segmenting objects in videos based on their motion and no other forms of supervision. Prior work has often approached this problem by using the principle of common fate, namely the fact that the motion of points that belong to the same object is strongly correlated. However, most authors have only considered instantaneous motion from optical flow. In this work, we present a way to train a segmentation network using long-term point trajectories as a supervisory signal to complement optical flow. The key difficulty is that long-term motion, unlike instantaneous motion, is difficult to model -- any parametric approximation is unlikely to capture complex motion patterns over long periods of time. We instead draw inspiration from subspace clustering approaches, proposing a loss function that seeks to group the trajectories into low-rank matrices where the motion of object points can be approximately explained as a linear combination of other point tracks. Our method outperforms the prior art on motion-based segmentation, which shows the utility of long-term motion and the effectiveness of our formulation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-77" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-77', event_id='94242', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1604</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94242">A Simple Image Segmentation Framework via In-Context Examples</a></strong></h5>


                        <p class="text-muted">
                            Yang Liu &middot; Chenchen Jing &middot; Hengtao Li &middot; Muzhi Zhu &middot; Hao Chen &middot; Xinlong Wang &middot; Chunhua Shen
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Recently, there have been explorations of generalist segmentation models that can effectively tackle a variety of image segmentation tasks within a unified in-context learning framework. However, these methods still struggle with task ambiguity in in-context segmentation, as not all in-context examples can accurately convey the task information. In order to address this issue, we present SINE, a simple image $\textbf{S}$egmentation framework utilizing $\textbf{in}$-context $\textbf{e}$xamples. Our approach leverages a Transformer encoder-decoder structure, where the encoder provides high-quality image representations, and the decoder is designed to yield multiple task-specific output masks to eliminate task ambiguity effectively. Specifically, we introduce an In-context Interaction module to complement in-context information and produce correlations between the target image and the in-context example and a Matching Transformer that uses fixed matching and a Hungarian algorithm to eliminate differences between different tasks. In addition, we have further perfected the current evaluation system for in-context image segmentation, aiming to facilitate a holistic appraisal of these models. Experiments on various segmentation tasks show the effectiveness of the proposed method.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-78" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-78', event_id='94323', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1605</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94323">Geometric Exploitation for Indoor Panoramic Semantic Segmentation</a></strong></h5>


                        <p class="text-muted">
                            Duc Cao Dinh &middot; Seok Joon Kim &middot; Kyusung Cho
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>PAnoramic Semantic Segmentation (PASS) is an important task in computer vision,as it enables semantic understanding of a 360Â° environment. Currently,most of existing works have focused on addressing the distortion issues in 2Dpanoramic images without considering spatial properties of indoor scene. Thisrestricts PASS methods in perceiving contextual attributes to deal with the ambiguitywhen working with monocular images. In this paper, we propose a novelapproach for indoor panoramic semantic segmentation. Unlike previous works,we consider the panoramic image as a composition of segment groups: oversampledsegments, representing planar structures such as floors and ceilings, andunder-sampled segments, representing other scene elements. To optimize eachgroup, we first enhance over-sampled segments by jointly optimizing with a densedepth estimation task. Then, we introduce a transformer-based context modulethat aggregates different geometric representations of the scene, combinedwith a simple high-resolution branch, it serves as a robust hybrid decoder forestimating under-sampled segments, effectively preserving the resolution of predictedmasks while leveraging various indoor geometric properties. Experimentalresults on both real-world (Stanford2D3DS, Matterport3D) and synthetic (Structured3D)datasets demonstrate the robustness of our framework, by setting newstate-of-the-arts in almost evaluations, The code and updated results are availableat: https://github.com/caodinhduc/vertical<em>relative</em>distance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-79" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-79', event_id='94354', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1606</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94354">UniDSeg: Unified Cross-Domain 3D Semantic Segmentation via Visual Foundation Models Prior</a></strong></h5>


                        <p class="text-muted">
                            Yao Wu &middot; Mingwei Xing &middot; Yachao Zhang &middot; Xiaotong Luo &middot; Yuan Xie &middot; Yanyun Qu
                        </p>

                    </div>
                    <div class="abstract">
                        <p>3D semantic segmentation using an adapting model trained from a source domain with or without accessing unlabeled target-domain data is the fundamental task in computer vision, containing domain adaptation and domain generalization.The essence of simultaneously solving cross-domain tasks is to enhance the generalizability of the encoder.In light of this, we propose a groundbreaking universal method with the help of off-the-shelf Visual Foundation Models (VFMs) to boost the adaptability and generalizability of cross-domain 3D semantic segmentation, dubbed $\textbf{UniDSeg}$.Our method explores the VFMs prior and how to harness them, aiming to inherit the recognition ability of VFMs.Specifically, this method introduces layer-wise learnable blocks to the VFMs, which hinges on alternately learning two representations during training: (i) Learning visual prompt. The 3D-to-2D transitional prior and task-shared knowledge is captured from the prompt space, and then (ii) Learning deep query. Spatial Tunability is constructed to the representation of distinct instances driven by prompts in the query space.Integrating these representations into a cross-modal learning framework, UniDSeg efficiently mitigates the domain gap between 2D and 3D modalities, achieving unified cross-domain 3D semantic segmentation.Extensive experiments demonstrate the effectiveness of our method across widely recognized tasks and datasets, all achieving superior performance over state-of-the-art methods. Remarkably, UniDSeg achieves 57.5\%/54.4\% mIoU on ``A2D2/sKITTI'' for domain adaptive/generalized tasks. Code is available at https://github.com/Barcaaaa/UniDSeg.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-80" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-80', event_id='95872', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1607</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95872">MVSDet: Multi-View Indoor 3D Object Detection via Efficient Plane Sweeps</a></strong></h5>


                        <p class="text-muted">
                            Yating Xu &middot; Chen Li &middot; Gim Hee Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The key challenge of multi-view indoor 3D object detection is to infer accurate geometry information from images for precise 3D detection. Previous method relies on NeRF for geometry reasoning. However, the geometry extracted from NeRF is generally inaccurate, which leads to sub-optimal detection performance. In this paper, we propose MVSDet which utilizes plane sweep for geometry-aware 3D object detection. To circumvent the requirement for a large number of depth planes for accurate depth prediction, we design a probabilistic sampling and soft weighting mechanism to decide the placement of pixel features on the 3D volume. We select multiple locations that score top in the probability volume for each pixel and use their probability score to indicate the confidence. We further apply recent pixel-aligned Gaussian Splatting to regularize depth prediction and improve detection performance with little computation overhead. Extensive experiments on ScanNet and ARKitScenes datasets are conducted to show the superiority of our model. Our code is available at https://github.com/Pixie8888/MVSDet.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-81" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-81', event_id='96551', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1608</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96551">Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models</a></strong></h5>


                        <p class="text-muted">
                            Yifan Zhang &middot; Junhui Hou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Contrastive image-to-LiDAR knowledge transfer, commonly used for learning 3D representations with synchronized images and point clouds, often faces a self-conflict dilemma. This issue arises as contrastive losses unintentionally dissociate features of unmatched points and pixels that share semantic labels, compromising the integrity of learned representations. To overcome this, we harness Visual Foundation Models (VFMs), which have revolutionized the acquisition of pixel-level semantics, to enhance 3D representation learning. Specifically, we utilize off-the-shelf VFMs to generate semantic labels for weakly-supervised pixel-to-point contrastive distillation. Additionally, we employ von Mises-Fisher distributions to structure the feature space, ensuring semantic embeddings within the same class remain consistent across varying inputs. Furthermore, we adapt sampling probabilities of points to address imbalances in spatial distribution and category frequency, promoting comprehensive and balanced learning. Extensive experiments demonstrate that our approach mitigates the challenges posed by traditional methods and consistently surpasses existing image-to-LiDAR contrastive distillation methods in downstream tasks. We have included the code in supplementary materials.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-82" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-82', event_id='97821', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1609</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97821">Infusing Synthetic Data with Real-World Patterns for Zero-Shot Material State Segmentation</a></strong></h5>


                        <p class="text-muted">
                            sagi eppel &middot; Jolina Li &middot; Manuel Drehwald &middot; Alan Aspuru-Guzik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Visual recognition of materials and their states is essential for understanding the physical world, from identifying wet regions on surfaces or stains on fabrics to detecting infected areas or minerals in rocks. Collecting data that captures this vast variability is complex due to the scattered and gradual nature of material states. Manually annotating real-world images is constrained by cost and precision, while synthetic data, although accurate and inexpensive, lacks real-world diversity. This work aims to bridge this gap by infusing patterns automatically extracted from real-world images into synthetic data. Hence, patterns collected from natural images are used to generate and map materials into synthetic scenes. This unsupervised approach captures the complexity of the real world while maintaining the precision and scalability of synthetic data. We also present the first comprehensive benchmark for zero-shot material state segmentation, utilizing real-world images across a diverse range of domains, including food, soils, construction, plants, liquids, and more, each appears in various states such as wet, dry, infected, cooked, burned, and many others. The annotation includes partial similarity between regions with similar but not identical materials and hard segmentation of only identical material states. This benchmark eluded top foundation models, exposing the limitations of existing data collection methods. Meanwhile, nets trained on the infused data performed significantly better on this and related tasks. The dataset, code, and trained model are publicly available. We also share 300,000 extracted textures and SVBRDF/PBR materials to facilitate future datasets generation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-83" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-83', event_id='92954', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1610</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92954">Towards Understanding the Working Mechanism of Text-to-Image Diffusion Model</a></strong></h5>


                        <p class="text-muted">
                            Mingyang Yi &middot; Aoxue Li &middot; Yi Xin &middot; Zhenguo Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recently, the strong latent Diffusion Probabilistic Model (DPM) has been applied to high-quality Text-to-Image (T2I) generation (e.g., Stable Diffusion), by injecting the encoded target text prompt into the gradually denoised diffusion image generator. Despite the success of DPM in practice, the mechanism behind it remains to be explored. To fill this blank, we begin by examining the intermediate statuses during the gradual denoising generation process in DPM. The empirical observations indicate, the shape of image is reconstructed after the first few denoising steps, and then the image is filled with details (e.g., texture). The phenomenon is because the low-frequency signal (shape relevant) of the noisy image is not corrupted until the final stage in the forward process (initial stage of generation) of adding noise in DPM. Inspired by the observations, we proceed to explore the influence of each token in the text prompt during the two stages. After a series of experiments of T2I generations conditioned on a set of text prompts. We conclude that in the earlier generation stage, the image is mostly decided by the special token [\texttt{EOS}] in the text prompt, and the information in the text prompt is already conveyed in this stage. After that, the diffusion model completes the details of generated images by information from themselves. Finally, we propose to apply this observation to accelerate the process of T2I generation by properly removing text guidance, which finally accelerates the sampling up to 25\%+.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-84" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-84', event_id='93222', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1611</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93222">LG-VQ: Language-Guided Codebook Learning</a></strong></h5>


                        <p class="text-muted">
                            Liang Guotao &middot; Baoquan Zhang &middot; Yaowei Wang &middot; Yunming Ye &middot; Xutao Li &middot; Wanghuaibin &middot; Luo Chuyao &middot; kolaye &middot; luolinfeng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Vector quantization (VQ) is a key technique in high-resolution and high-fidelity image synthesis, which aims to learn a codebook to encode an image with a sequence of discrete codes and then generate an image in an auto-regression manner.   Although existing methods have shown superior performance, most methods prefer to learn a single-modal codebook (\emph{e.g.}, image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (\emph{e.g.}, text-to-image, image captioning) due to the existence of modal gaps.  In this paper, we propose a novel language-guided codebook learning framework, called LG-VQ, which aims to learn a codebook that can be aligned with the text to improve the performance of multi-modal downstream tasks. Specifically, we first introduce pre-trained text semantics as prior knowledge, then design two novel alignment modules (\emph{i.e.}, Semantic Alignment Module, and Relationship Alignment Module) to transfer such prior knowledge into codes for achieving codebook text alignment.     In particular, our LG-VQ method is model-agnostic, which can be easily integrated into existing VQ models. Experimental results show that our method achieves superior performance on reconstruction and various multi-modal downstream tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-85" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-85', event_id='97601', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1700</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97601">HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</a></strong></h5>


                        <p class="text-muted">
                            Zhenzhi Wang &middot; Yixuan Li &middot; Yanhong Zeng &middot; Youqing Fang &middot; Yuwei Guo &middot; Wenran Liu &middot; Jing Tan &middot; Kai Chen &middot; Bo Dai &middot; Tianfan Xue &middot; Dahua Lin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Human image animation involves generating videos from a character photo, allowing user control and unlocking potential for video and movie production.While recent approaches yield impressive results using high-quality training data, the inaccessibility of these datasets hampers fair and transparent benchmarking. Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation.To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data. For the real-world data, we compile a vast collection of copyright-free real-world videos from the internet. Through a carefully designed rule-based filtering strategy, we ensure the inclusion of high-quality videos, resulting in 20K human-centric videos in 1080P resolution. Human and camera motion annotation is accomplished using a 2D pose estimator and a SLAM-based method.For the synthetic data, we gather 2,300 copyright-free 3D avatar assets to augment existing available 3D assets. Notably, we introduce a rule-based camera trajectory generation method, enabling the synthetic pipeline to incorporate diverse and precise camera motion annotation, which can rarely found in real-world data. To verify the effectiveness of HumanVid, we establish a baseline model that considers both human and camera motions as conditions. Through extensive experimentation, we demonstrate that training with the real-world portion of HumanVid achieves state-of-the-art performance. Moreover, incorporating the synthetic data enhances user control over both human and camera motions, setting a new benchmark.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-86" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-86', event_id='96337', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1701</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96337">CV-VAE: A Compatible Video VAE for Latent Generative Video Models</a></strong></h5>


                        <p class="text-muted">
                            Sijie Zhao &middot; Yong Zhang &middot; Xiaodong Cun &middot; Shaoshu Yang &middot; Muyao Niu &middot; Xiaoyu Li &middot; Wenbo HU &middot; Ying Shan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Spatio-temporal compression of videos, utilizing networks such as Variational Autoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other video generative models. For instance, many LLM-like video models learn the distribution of discrete tokens derived from 3D VAEs within the VQVAE framework, while most diffusion-based video models capture the distribution of continuous latent extracted by 2D VAEs without quantization. The temporal compression is simply realized by uniform frame sampling which results in unsmooth motion between consecutive frames. Currently, there lacks of a commonly used continuous video (3D) VAE for latent diffusion-based video models in the research community. Moreover, since current diffusion-based approaches are often implemented using pre-trained text-to-image (T2I) models, directly training a video VAE without considering the compatibility with existing T2I models will result in a latent space gap between them, which will take huge computational resources for training to bridge the gap even with the T2I models as initialization. To address this issue, we propose a method for training a video VAE of latent video models, namely CV-VAE, whose latent space is compatible with that of a given image VAE, e.g., image VAE of Stable Diffusion (SD). The compatibility is achieved by the proposed novel latent space regularization, which involves formulating a regularization loss using the image VAE. Benefiting from the latent space compatibility, video models can be trained seamlessly from pre-trained T2I or video models in a truly spatio-temporally compressed latent space, rather than simply sampling video frames at equal intervals. To improve the training efficiency, we also design a novel architecture for the video VAE. With our CV-VAE, existing video models can generate four times more frames with minimal finetuning. Extensive experiments are conducted to demonstrate the effectiveness of the proposed video VAE.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-87" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-87', event_id='95885', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1702</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95885">HOI-Swap: Swapping Objects in Videos with Hand-Object Interaction Awareness</a></strong></h5>


                        <p class="text-muted">
                            Zihui (Sherry) Xue &middot; Romy Luo &middot; Changan Chen &middot; Kristen Grauman
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We study the problem of precisely swapping objects in videos, with a focus on those interacted with by hands, given one user-provided reference object image. Despite the great advancements that diffusion models have made in video editing recently, these models often fall short in handling the intricacies of hand-object interactions (HOI), failing to produce realistic edits---especially when object swapping results in object shape or functionality changes. To bridge this gap, we present HOI-Swap, a novel diffusion-based video editing framework trained in a self-supervised manner.  Designed in two stages, the first stage focuses on object swapping in a single frame with HOI awareness; the model learns to adjust the interaction patterns, such as the hand grasp, based on changes in the object's properties. The second stage extends the single-frame edit across the entire sequence; we achieve controllable motion alignment with the original video by: (1) warping a new sequence from the stage-I edited frame based on sampled motion points and (2) conditioning video generation on the warped sequence. Comprehensive qualitative and quantitative evaluations demonstrate that HOI-Swap significantly outperforms existing methods, delivering high-quality video edits with realistic HOIs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-88" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-88', event_id='95305', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1703</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95305">SF-V: Single Forward Video Generation Model</a></strong></h5>


                        <p class="text-muted">
                            Zhixing Zhang &middot; Yanyu Li &middot; Yushu Wu &middot; yanwu xu &middot; Anil Kag &middot; Ivan Skorokhodov &middot; Willi Menapace &middot; Aliaksandr Siarohin &middot; Junli Cao &middot; Dimitris Metaxas &middot; Sergey Tulyakov &middot; Jian Ren
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Diffusion-based video generation models have demonstrated remarkable success in obtaining high-fidelity videos through the iterative denoising process. However, these models require multiple denoising steps during sampling, resulting in high computational costs. In this work, we propose a novel approach to obtain single-step video generation models by leveraging adversarial training to fine-tune pre-trained video diffusion models. We show that, through the adversarial training, the multi-steps video diffusion model, i.e., Stable Video Diffusion (SVD), can be trained to perform single forward pass to synthesize high-quality videos, capturing both temporal and spatial dependencies in the video data. Extensive experiments demonstrate that our method achieves competitive generation quality of synthesized videos with significantly reduced computational overhead for the denoising process (i.e., around $23\times$ speedup compared with SVD and $6\times$ speedup compared with existing works, with even better generation quality), paving the way for real-time video synthesis and editing.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-89" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-89', event_id='94114', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1704</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94114">Diffusion4D: Fast Spatial-temporal Consistent 4D generation via Video Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            HANWEN LIANG &middot; Yuyang Yin &middot; Dejia Xu &middot; hanxue liang &middot; Zhangyang &amp;quot;Atlas&amp;quot; Wang &middot; Konstantinos N Plataniotis &middot; Yao Zhao &middot; Yunchao Wei
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The availability of large-scale multimodal datasets and advancements in diffusion models have significantly accelerated progress in 4D content generation. Most prior approaches rely on multiple images or video diffusion models, utilizing score distillation sampling for optimization or generating pseudo novel views for direct supervision. However, these methods are hindered by slow optimization speeds and multi-view inconsistency issues. Spatial and temporal consistency in 4D geometry has been extensively explored respectively in 3D-aware diffusion models and traditional monocular video diffusion models. Building on this foundation, we propose a strategy to migrate the temporal consistency in video diffusion models to the spatial-temporal consistency required for 4D generation. Specifically, we present a novel framework, \textbf{Diffusion4D}, for efficient and scalable 4D content generation. Leveraging a meticulously curated dynamic 3D dataset, we develop a 4D-aware video diffusion model capable of synthesizing orbital views of dynamic 3D assets. To control the dynamic strength of these assets, we introduce a 3D-to-4D motion magnitude metric as guidance. Additionally, we propose a novel motion magnitude reconstruction loss and 3D-aware classifier-free guidance to refine the learning and generation of motion dynamics. After obtaining orbital views of the 4D asset, we perform explicit 4D construction with Gaussian splatting in a coarse-to-fine manner. Extensive experiments demonstrate that our method surpasses prior state-of-the-art techniques in terms of generation efficiency and 4D geometry consistency across various prompt modalities.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-90" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-90', event_id='94009', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1705</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94009">Fast and Memory-Efficient Video Diffusion Using Streamlined Inference</a></strong></h5>


                        <p class="text-muted">
                            Zheng Zhan &middot; Yushu Wu &middot; Yifan Gong &middot; Zichong Meng &middot; Zhenglun Kong &middot; Changdi Yang &middot; Geng Yuan &middot; Pu Zhao &middot; Wei Niu &middot; Yanzhi Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The rapid progress in artificial intelligence-generated content (AIGC), especially with diffusion models, has significantly advanced development of high-quality video generation. However, current video diffusion models exhibit demanding computational requirements and high peak memory usage, especially for generating longer and higher-resolution videos. These limitations greatly hinder the practical application of video diffusion models on standard hardware platforms. To tackle this issue, we present a novel, training-free framework named Streamlined Inference, which leverages the temporal and spatial properties of video diffusion models. Our approach integrates three core components: Feature Slicer, Operator Grouping, and Step Rehash. Specifically, Feature Slicer effectively partitions input features into sub-features and Operator Grouping processes each sub-feature with a group of consecutive operators, resulting in significant memory reduction without sacrificing the quality or speed. Step Rehash further exploits the similarity between adjacent steps in diffusion, and accelerates inference through skipping unnecessary steps. Extensive experiments demonstrate that our approach significantly reduces peak memory and computational overhead, making it feasible to generate high-quality videos on a single consumer GPU (e.g., reducing peak memory of Animatediff from 42GB to 11GB, featuring faster inference on 2080Ti).</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-91" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-91', event_id='93792', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1706</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93792">MotionCraft: Physics-Based Zero-Shot Video Generation</a></strong></h5>


                        <p class="text-muted">
                            Antonio Montanaro &middot; Luca Savant Aira &middot; Emanuele Aiello &middot; Diego Valsesia &middot; Enrico Magli
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Generating videos with realistic and physically plausible motion is one of the main recent challenges in computer vision. While diffusion models are achieving compelling results in image generation, video diffusion models are limited by heavy training and huge models, resulting in videos that are still biased to the training dataset. In this work we propose MotionCraft, a new zero-shot video generator to craft physics-based and realistic videos. MotionCraft is able to warp the noise latent space of an image diffusion model, such as Stable Diffusion, by applying an optical flow derived from a physics simulation. We show that warping the noise latent space results in coherent application of the desired motion while allowing the model to generate missing elements consistent with the scene evolution, which would otherwise result in artefacts or missing content if the flow was applied in the pixel space.We compare our method with the state-of-the-art Text2Video-Zero reporting qualitative and quantitative improvements, demonstrating the effectiveness of our approach to generate videos with finely-prescribed complex motion dynamics.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-92" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-92', event_id='93082', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1707</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93082">ReVideo: Remake a Video with Motion and Content Control</a></strong></h5>


                        <p class="text-muted">
                            Chong Mou &middot; Mingdeng Cao &middot; Xintao Wang &middot; Zhaoyang Zhang &middot; Ying Shan &middot; Jian Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite significant advancements in video generation and editing using diffusion models, achieving accurate and localized video editing remains a substantial challenge. Additionally, most existing video editing methods primarily focus on altering visual content, with limited research dedicated to motion editing. In this paper, we present a novel attempt to Remake a Video (ReVideo) which stands out from existing methods by allowing precise video editing in specific areas through the specification of both content and motion. Content editing is facilitated by modifying the first frame, while the trajectory-based motion control offers an intuitive user interaction experience. ReVideo addresses a new task involving the coupling and training imbalance between content and motion control. To tackle this, we develop a three-stage training strategy that progressively decouples these two aspects from coarse to fine. Furthermore, we propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations. Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories. Our method can also seamlessly extend these applications to multi-area editing without specific training, demonstrating its flexibility and robustness.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-93" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-93', event_id='97673', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1708</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97673">TAPVid-3D: A Benchmark for Tracking Any Point in 3D</a></strong></h5>


                        <p class="text-muted">
                            Skanda Koppula &middot; Ignacio Rocco &middot; Yi Yang &middot; joseph heyward &middot; Joao Carreira &middot; Andrew Zisserman &middot; Gabriel Brostow &middot; Carl Doersch
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce a new benchmark, TAPVid-3D, for evaluating the task of long-range Tracking Any Point in 3D (TAP-3D). While point tracking in two dimensions (TAP-2D) has many benchmarks measuring performance on real-world videos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To this end, leveraging existing footage, we build a new benchmark for 3D point tracking featuring 4,000+ real-world videos, composed of three different data sources spanning a variety of object types, motion patterns, and indoor and outdoor environments. To measure performance on the TAP-3D task, we formulate a collection of metrics that extend the Jaccard-based metric used in TAP-2D to handle the complexities of ambiguous depth scales across models, occlusions, and multi-track spatio-temporal smoothness. We manually verify a large sample of trajectories to ensure correct video annotations, and assess the current state of the TAP-3D task by constructing competitive baselines using existing tracking models. We anticipate this benchmark will serve as a guidepost to improve our ability to understand precise 3D motion and surface deformation from monocular video.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-94" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-94', event_id='97580', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1709</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97580">CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes</a></strong></h5>


                        <p class="text-muted">
                            Paritosh Parmar &middot; Eric Peh &middot; Ruirui Chen &middot; Ting En Lam &middot; Yuhan Chen &middot; Elston Tan &middot; Basura Fernando
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. Cartoons use the principles of animation that allow animators to create expressive, unambiguous causal relationships between events to form a coherent storyline. Utilizing these properties, along with thought-provoking questions and multi-level answers (answer and detailed causal explanation), our questions involve causal chains that interconnect multiple dynamic interactions between characters and visual scenes. These factors demand models to solve more challenging, yet well-defined causal relationships. We also introduce hard incorrect answer mining, including a causally confusing version that is even more challenging. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling \&amp; joint modeling of vision and language as the immediate areas for future efforts to focus upon. Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field. Link to dataset provided in Appendix.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-95" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-95', event_id='93812', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1710</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93812">OPEL: Optimal Transport Guided ProcedurE Learning</a></strong></h5>


                        <p class="text-muted">
                            Sayeed Shafayet Chowdhury &middot; Soumyadeep Chandra &middot; Kaushik Roy
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Procedure learning refers to the task of identifying the key-steps and determining their logical order, given several videos of the same task. For both third-person and first-person (egocentric) videos, state-of-the-art (SOTA) methods aim at finding correspondences across videos in time to accomplish procedure learning. However, to establish temporal relationships within the sequences, these methods often rely on frame-to-frame mapping, or assume monotonic alignment of video pairs, leading to sub-optimal results. To this end, we propose to treat the video frames as  samples from an unknown distribution, enabling us to frame their distance calculation as an optimal transport (OT) problem. Notably, the OT-based formulation allows us to relax the previously mentioned assumptions. To further improve performance, we enhance the OT formulation by introducing two regularization terms. The first,  inverse difference moment regularization, promotes transportation between instances that are homogeneous in the embedding space as well as being temporally closer. The second, regularization based on the KL-divergence with an exponentially decaying prior smooths the alignment while enforcing conformity to the optimality (alignment obtained from vanilla OT optimization) and temporal priors. The resultant optimal transport guided procedure learning framework (`OPEL') significantly outperforms the SOTA on benchmark datasets. Specifically, we achieve 22.4\% (IoU) and 26.9\% (F1) average improvement compared to the current SOTA on large scale egocentric benchmark, EgoProceL. Furthermore, for the third person benchmarks (ProCeL and CrossTask), the proposed approach obtains 46.2\% (F1) average enhancement over SOTA.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-96" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-96', event_id='97787', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1711</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97787">Biomedical Visual Instruction Tuning with Clinician Preference Alignment</a></strong></h5>


                        <p class="text-muted">
                            Hejie Cui &middot; Lingjun Mao &middot; Xin Liang &middot; Jieyu Zhang &middot; Hui Ren &middot; Quanzheng Li &middot; Xiang Li &middot; Carl Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advancements in multimodal foundation models have showcased impressive capabilities in understanding and reasoning with visual and textual information. Adapting these foundation models trained for general usage to specialized domains like biomedicine requires large-scale domain-specific instruction datasets. While existing works have explored curating such datasets automatically, the resultant datasets are not explicitly aligned with domain expertise. In this work, we propose a data-centric framework, Biomedical Visual Instruction Tuning with Clinician Preference Alignment (BioMed-VITAL), that incorporates clinician preferences into both stages of generating and selecting instruction data for tuning biomedical multimodal foundation models. First, during the generation stage, we prompt the GPT-4V generator with a diverse set of clinician-selected demonstrations for preference-aligned data candidate generation. Then, during the selection phase, we train a separate selection model, which explicitly distills clinician and policy-guided model preferences into a rating function to select high-quality data for medical instruction tuning. Results show that the model tuned with the instruction-following data from our method demonstrates a significant improvement in open visual chat (18.5% relatively) and medical VQA (win rate up to 81.73%). Our instruction-following data and models are available at https://BioMed-VITAL.github.io.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-97" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-97', event_id='96934', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1800</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96934">Local Superior Soups: A Catalyst for Model Merging in Cross-Silo Federated Learning</a></strong></h5>


                        <p class="text-muted">
                            Minghui Chen &middot; Meirui Jiang &middot; Xin Zhang &middot; DOU QI &middot; Zehua Wang &middot; Xiaoxiao Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Federated learning (FL) is a learning paradigm that enables collaborative training of models using decentralized data. Recently, the utilization of pre-trained weight initialization in FL has been demonstrated to effectively improve model performance. However, the evolving complexity of current pre-trained models, characterized by a substantial increase in parameters, markedly intensifies the challenges associated with communication rounds required for their adaptation to FL. To address these communication cost issues and increase the performance of pre-trained model adaptation in FL, we propose an innovative model interpolation-based local training technique called ``Local Superior Soups.''Our method enhances local training across different clients, encouraging the exploration of a connected low-loss basin within a few communication rounds through regularized model interpolation. This approach acts as a catalyst for the seamless adaptation of pre-trained models in in FL.We demonstrated its effectiveness and efficiency across diverse widely-used FL datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-98" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-98', event_id='97437', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1801</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97437">Data curation via joint example selection further accelerates multimodal learning</a></strong></h5>


                        <p class="text-muted">
                            Talfan Evans &middot; Nikhil Parthasarathy &middot; Hamza Merzic &middot; Olivier Henaff
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly prioritizing batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individually-prioritized data points. As performance improves by selecting from large super-batches, we  also leverage recent advances in model approximation to reduce the computational overhead of scoring. As a result, our approachâmultimodal contrastive learning with joint example selection (JEST)âsurpasses state-of-the-art pretraining methods with up to 13$\times$ fewer iterations and 10$\times$ less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing data curation as a new dimension for neural scaling laws.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-99" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-99', event_id='97453', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1802</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97453">Intrinsic Self-Supervision for Data Quality Audits</a></strong></h5>


                        <p class="text-muted">
                            Fabian GrÃ¶ger &middot; Simone Lionetti &middot; Philippe Gottfrois &middot; Alvaro Gonzalez-Jimenez &middot; Ludovic Amruthalingam &middot; Matthew Groh &middot; Alexander Navarini &middot; Marc Pouly
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Benchmark datasets in computer vision often contain off-topic images, near duplicates, and label errors, leading to inaccurate estimates of model performance. In this paper, we revisit the task of data cleaning and formalize it as either a ranking problem, which significantly reduces human inspection effort, or a scoring problem, which allows for automated decisions based on score distributions. We find that a specific combination of context-aware self-supervised representation learning and distance-based indicators is effective in finding issues without annotation biases. This methodology, which we call SelfClean, surpasses state-of-the-art performance in detecting off-topic images, near duplicates, and label errors within widely-used computer vision datasets, such as ImageNet-1k, Food-101N, and STL-10, both for synthetic issues and real contamination. We apply the detailed method to multiple image benchmarks, identify up to 16% of issues, and confirm an improvement in evaluation reliability upon cleaning.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-100" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-100', event_id='92927', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1803</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92927">IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation</a></strong></h5>


                        <p class="text-muted">
                            Fan Lin &middot; Shuyi Xie &middot; Yong Dai &middot; Wenlin Yao &middot; TianJiao Lang &middot; Yu Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>As Large Language Models (LLMs) become more capable of handling increasingly complex tasks, the evaluation set must keep pace with these advancements to ensure it remains sufficiently discriminative. Item Discrimination (ID) theory, which is widely used in educational assessment, measures the ability of individual test items to differentiate between high and low performers. Inspired by this theory, we propose an ID-induced prompt synthesis framework for evaluating LLMs so that the evaluation set continually updates and refines according to model abilities. Our data synthesis framework prioritizes both breadth and specificity. It can generate prompts that comprehensively evaluate the capabilities of LLMs while revealing meaningful performance differences between models, allowing for effective discrimination of their relative strengths and weaknesses across various tasks and domains.To produce high-quality data, we incorporate a self-correct mechanism into our generalization framework and develop two models to predict prompt discrimination and difficulty score to facilitate our data synthesis framework, contributing valuable tools to evaluation data synthesis research. We apply our generated data to evaluate five SOTA models. Our data achieves an average score of 51.92, accompanied by a variance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and WizardLM) obtain an average score exceeding 67, with a variance below 3.2.The results demonstrate that the data generated by our framework is more challenging and discriminative compared to previous works.We will release a dataset of over 3,000 carefully crafted prompts to facilitate evaluation research of LLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-101" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-101', event_id='95014', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1804</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95014">DiffAug: A Diffuse-and-Denoise Augmentation for Training Robust Classifiers</a></strong></h5>


                        <p class="text-muted">
                            Chandramouli Shama Sastry &middot; Sri Harsha Dumpala &middot; Sageev Oore
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce DiffAug, a simple and efficient diffusion-based augmentation technique to train image classifiers for the crucial yet challenging goal of improved classifier robustness. Applying DiffAug to a given example consists of one forward-diffusion step followed by one reverse-diffusion step. Using both ResNet-50 and Vision Transformer architectures, we comprehensively evaluate classifiers trained with DiffAug and demonstrate the surprising effectiveness of single-step reverse diffusion in improving robustness to covariate shifts, certified adversarial accuracy and out of distribution detection. When we combine DiffAug with other augmentations such as AugMix and DeepAugment we demonstrate further improved robustness. Finally, building on this approach, we also improve classifier-guided diffusion wherein we observe improvements in: (i) classifier-generalization, (ii) gradient quality (i.e., improved perceptual alignment) and (iii) image generation performance. We thus introduce a computationally efficient technique for training with improved robustness that does not require any additional data, and effectively complements existing augmentation approaches.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-102" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-102', event_id='95647', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1805</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95647">Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale</a></strong></h5>


                        <p class="text-muted">
                            Tianyue Ou &middot; Frank F. Xu &middot; Aman Madaan &middot; Jiarui Liu &middot; Robert Lo &middot; Abishek Sridhar &middot; Sudipta Sengupta &middot; Dan Roth &middot; Graham Neubig &middot; Shuyan Zhou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>LLMs can now act as autonomous agents that interact with digital environments and complete specific objectives (e.g., arranging an online meeting). However, accuracy is still far from satisfactory, partly due to a lack of large-scale, direct demonstrations for digital tasks. Obtaining supervised data from humans is costly, and automatic data collection through exploration or reinforcement learning relies on complex environmental and content setup, resulting in datasets that lack comprehensive coverage of various scenarios. On the other hand, there is abundant knowledge that may indirectly assist task completion, such as online tutorials that were created for human consumption. In this work, we present Synatra, an approach that effectively transforms this indirect knowledge into direct supervision at scale. We define different types of indirect knowledge, and carefully study the available sources to obtain it, methods to encode the structure of direct demonstrations, and finally methods to transform indirect knowledge into direct demonstrations. We use 100k such synthetically-created demonstrations to finetune a 7B CodeLlama, and demonstrate that the resulting agent surpasses all comparably sized models on three web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well as surpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic demonstrations prove to be only 3% the cost of human demonstrations (at $0.031 each), we show that the synthetic demonstrations can be more effective than an identical number of human demonstrations collected from limited domains.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-103" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-103', event_id='96477', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1806</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96477">Right this way: Can VLMs Guide Us to See More to Answer Questions?</a></strong></h5>


                        <p class="text-muted">
                            Li Liu &middot; Diji Yang &middot; Sijia Zhong &middot; Kalyana Suma Sree Tholeti &middot; Lei Ding &middot; Yi Zhang &middot; Leilani Gilpin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In question-answering scenarios, humans can assess whether the available information is sufficient and seek additional information if necessary, rather than providing a forced answer. In contrast, Vision Language Models (VLMs) typically generate direct, one-shot responses without evaluating the sufficiency of the information. To investigate this gap, we identify a critical and challenging task in the Visual Question Answering (VQA) scenario: can VLMs indicate how to adjust an image when the visual information is insufficient to answer a question? This capability is especially valuable for assisting visually impaired individuals who often need guidance to capture images correctly. To evaluate this capability of current VLMs, we introduce a human-labeled dataset as a benchmark for this task. Additionally, we present an automated framework that generates synthetic training data by simulating ``where to know'' scenarios. Our empirical results show significant performance improvements in mainstream VLMs when fine-tuned with this synthetic data. This study demonstrates the potential to narrow the gap between information assessment and acquisition in VLMs, bringing their performance closer to humans.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-104" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-104', event_id='94518', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1807</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94518">Elucidating the Design Space of Dataset Condensation</a></strong></h5>


                        <p class="text-muted">
                            Shitong Shao &middot; Zikai Zhou &middot; Huanran Chen &middot; Zhiqiang Shen
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Dataset condensation, a concept within data-centric learning, efficiently transfers critical attributes from an original dataset to a synthetic version, maintaining both diversity and realism. This approach significantly improves model training efficiency and is adaptable across multiple application areas. Previous methods in dataset condensation have faced challenges: some incur high computational costs which limit scalability to larger datasets (e.g., MTT, DREAM, and TESLA), while others are restricted to less optimal design spaces, which could hinder potential improvements, especially in smaller datasets (e.g., SRe$^2$L, G-VBSM, and RDED). To address these limitations, we propose a comprehensive design framework that includes specific, effective strategies like implementing soft category-aware matching and adjusting the learning rate schedule. These strategies are grounded in empirical evidence and theoretical backing. Our resulting approach, Elucidate Dataset Condensation (EDC), establishes a benchmark for both small and large-scale dataset condensation. In our testing, EDC achieves state-of-the-art accuracy, reaching 48.6% on ImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a compression ratio of 0.78%. This performance exceeds those of SRe$^2$L, G-VBSM, and RDED by margins of 27.3%, 17.2%, and 6.6%, respectively.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-105" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-105', event_id='95413', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1808</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95413">Just Add $100 More: Augmenting Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem</a></strong></h5>


                        <p class="text-muted">
                            Mincheol Chang &middot; Siyeong Lee &middot; Jinkyu Kim &middot; Namil Kim
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Typical LiDAR-based 3D object detection models are trained with real-world data collection, which is often imbalanced over classes.To deal with it, augmentation techniques are commonly used, such as copying ground truth LiDAR points and pasting them into scenes.However, existing methods struggle with the lack of sample diversity for minority classes and the limitation of suitable placement.In this work, we introduce a novel approach that utilizes pseudo LiDAR point clouds generated from low-cost miniatures or real-world videos, which is called Pseudo Ground Truth augmentation (PGT-Aug).PGT-Aug involves three key steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D view synthesis model, (ii) object-level domain alignment with LiDAR intensity simulation, and (iii) a hybrid context-aware placement method from ground and map information. We demonstrate the superiority and generality of our method through performance improvements in extensive experiments conducted on popular benchmarks, i.e., nuScenes, KITTI, and Lyft, especially for the datasets with large domain gaps captured by different LiDAR configurations.The project webpage is https://just-add-100-more.github.io.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-106" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-106', event_id='96389', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1809</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96389">Offline Behavior Distillation</a></strong></h5>


                        <p class="text-muted">
                            Shiye Lei &middot; Sen Zhang &middot; Dacheng Tao
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Massive reinforcement learning (RL) data are typically collected to train policies offline without the need for interactions, but the large data volume can cause training inefficiencies. To tackle this issue, we formulate offline behavior distillation (OBD), which synthesizes limited expert behavioral data from sub-optimal RL data, enabling rapid policy learning. We propose two naive OBD objectives, DBC and PBC, which measure distillation performance via the decision difference between policies trained on distilled data and either offline data or a near-expert policy. Due to intractable bi-level optimization, the OBD objective is difficult to minimize to small values, which deteriorates PBC by its distillation performance guarantee with quadratic discount complexity $\mathcal{O}(1/(1-\gamma)^2)$. We theoretically establish the equivalence between the policy performance and action-value weighted decision difference, and introduce action-value weighted PBC (Av-PBC) as a more effective OBD objective. By optimizing the weighted decision difference, Av-PBC achieves a superior distillation guarantee with linear discount complexity $\mathcal{O}(1/(1-\gamma))$. Extensive experiments on multiple D4RL datasets reveal that Av-PBC offers significant improvements in OBD performance, fast distillation convergence speed, and robust cross-architecture/optimizer generalization.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-107" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-107', event_id='97493', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1810</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97493">EEVR: A Virtual Reality-Based Emotion Dataset Featuring Paired Physiological Signals and Textual Descriptions</a></strong></h5>


                        <p class="text-muted">
                            Pragya Singh &middot; Ritvik Budhiraja &middot; Ankush Gupta &middot; Anshul Goswami &middot; Mohan Kumar &middot; Pushpendra Singh
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>EEVR (Emotion Elicitation in Virtual Reality) is a novel dataset specifically designed for language supervision-based pre-training and emotion recognition tasks, such as valence and arousal classification. It features high-quality physiological signals, including electrodermal activity (EDA) and photoplethysmography (PPG), acquired through emotion elicitation via 360-degree virtual reality (VR) videos. Additionally, it includes subject-wise textual descriptions of emotions experienced during each stimulus gathered from qualitative interviews. The emotional stimuli were carefully selected to induce a range of emotions covering all four quadrants of Russell's circumplex model. The dataset consists of recordings from 37 participants and is the first to pair raw text with physiological signals, providing additional contextual information that objective labels cannot offer. Baseline models for arousal, valence, and emotion classification are provided, along with code for data cleaning and feature extraction pipelines. We show that augmenting our signals with self-reported textual annotations can improve performance on physiological signal-based emotion recognition tasks. The dataset is available at https://melangelabiiitd.github.io/EEVR/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-108" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-108', event_id='97627', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1811</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97627">Croissant: A Metadata Format for ML-Ready Datasets</a></strong></h5>


                        <p class="text-muted">
                            Mubashara Akhtar &middot; Omar Benjelloun &middot; Costanza Conforti &middot; Luca Foschini &middot; Joan Giner-Miguelez &middot; Pieter Gijsbers &middot; Sujata Goswami &middot; Nitisha Jain &middot; Michalis Karamousadakis &middot; Michael Kuchnik &middot; Satyapriya Krishna &middot; Sylvain Lesage &middot; Quentin Lhoest &middot; Pierre Marcenac &middot; Manil Maskey &middot; Peter Mattson &middot; Luis Oala &middot; Hamidah Oderinwale &middot; Pierre Ruyssen &middot; Tim Santos &middot; Rajat Shinde &middot; Elena Simperl &middot; Arjun Suresh &middot; Goeffry Thomas &middot; Slava Tykhonov &middot; Joaquin Vanschoren &middot; Susheel Varma &middot; Jos van der Velde &middot; Steffen Vogler &middot; Carole-Jean Wu &middot; Luyao Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that creates a shared representation across ML tools, frameworks, and platforms.Croissant makes datasets more discoverable, portable, and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, enabling easy loading without changes into the most commonly-used ML frameworks, regardless of where the data is stored. Our initial evaluation shows that Croissant metadata is deemed readable, understandable, complete, yet concise by human raters.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-109" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-109', event_id='96735', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1900</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96735">On the Inductive Bias of Stacking Towards Improving Reasoning</a></strong></h5>


                        <p class="text-muted">
                            Nikunj Saunshi &middot; Stefani Karp &middot; Shankar Krishnan &middot; Sobhan Miryoosefi &middot; Sashank Jakkam Reddi &middot; Sanjiv Kumar
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Given the increasing scale of model sizes, efficient training strategies like gradual stacking have garnered interest. Stacking enables efficient training by gradually growing the depth of a model in stages and using layers from a smaller model in an earlier stage to initialize the next stage. Although efficient for training, the model biases induced by such growing approaches are largely unexplored. In this work, we examine this fundamental aspect of gradual stacking, going beyond its efficiency benefits. We propose a variant of gradual stacking called MIDAS that can speed up language model training by up to 40\%. Furthermore we discover an intriguing phenomenon: MIDAS is not only training-efficient but surprisingly also has an inductive bias towards improving downstream tasks, especially tasks that require reasoning abilities like reading comprehension and math problems, despite having similar or slightly worse perplexity compared to baseline training. To further analyze this inductive bias, we construct {\em reasoning primitives} â simple synthetic tasks that are building blocks for reasoning â and find that a model pretrained with stacking is significantly better than standard pretraining on these primitives, with and without fine-tuning. This provides stronger and more robust evidence for this inductive bias towards reasoning. These findings of training efficiency and inductive bias towards reasoning are verified at 1B, 2B and 8B parameter language models. Finally, we conjecture the underlying reason for this inductive bias by exploring the connection of stacking to looped models and provide strong supporting empirical analysis.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-110" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-110', event_id='96092', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1901</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96092">B$\oplus$LD: Boolean Logic Deep Learning</a></strong></h5>


                        <p class="text-muted">
                            Van Minh NGUYEN &middot; Cristian Ocampo-Blandon &middot; Aymen Askri &middot; Louis Leconte &middot; Ba-Hien Tran
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Computational intensiveness of deep learning has motivated low-precision arithmetic designs. However, the current quantized/binarized training approaches are limited by: (1) significant performance loss due to arbitrary approximations of the latent weight gradient through its discretization/binarization function, and (2) training computational intensiveness due to the reliance on full-precision latent weights. This paper proposes a novel mathematical principle by introducing the notion of Boolean variation such that neurons made of Boolean weights and/or activations can be trained ---for the first time--- natively in Boolean domain instead of latent-weight gradient descent and real arithmetic. We explore its convergence, conduct extensively experimental benchmarking, and provide consistent complexity evaluation by considering chip architecture, memory hierarchy, dataflow, and arithmetic precision. Our approach achieves baseline full-precision accuracy in ImageNet classification and surpasses state-of-the-art results in semantic segmentation, with notable performance in image super-resolution, and natural language understanding with transformer-based models. Moreover, it significantly reduces energy consumption during both training and inference.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-111" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-111', event_id='95906', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1902</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95906">Simulation-Free Training of Neural ODEs on Paired Data</a></strong></h5>


                        <p class="text-muted">
                            Semin Kim &middot; Jaehoon Yoo &middot; Jinwoo Kim &middot; Yeonwoo Cha &middot; Saehoon Kim &middot; Seunghoon Hong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this work, we investigate a method for simulation-free training of Neural Ordinary Differential Equations (NODEs) for learning deterministic mappings between paired data. Despite the analogy of NODEs as continuous-depth residual networks, their application in typical supervised learning tasks has not been popular, mainly due to the large number of function evaluations required by ODE solvers and numerical instability in gradient estimation. To alleviate this problem, we employ the flow matching framework for simulation-free training of NODEs, which directly regresses the parameterized dynamics function to a predefined target velocity field. Contrary to generative tasks, however, we show that applying flow matching directly between paired data can often lead to an ill-defined flow that breaks the coupling of the data pairs (e.g., due to crossing trajectories). We propose a simple extension that applies flow matching in the embedding space of data pairs, where the embeddings are learned jointly with the dynamic function to ensure the validity of the flow which is also easier to learn. We demonstrate the effectiveness of our method on both regression and classification tasks, where our method outperforms existing NODEs with a significantly lower number of function evaluations. The code is available at https://github.com/seminkim/simulation-free-node.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-112" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-112', event_id='95624', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1903</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95624">Counter-Current Learning: A Biologically Plausible Dual Network Approach for Deep Learning</a></strong></h5>


                        <p class="text-muted">
                            Chia-Hsiang Kao &middot; Bharath Hariharan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite its widespread use in neural networks, error backpropagation has faced criticism for its lack of biological plausibility, suffering from issues such as the backward locking problem and the weight transport problem. These limitations have motivated researchers to explore more biologically plausible learning algorithms that could potentially shed light on how biological neural systems adapt and learn. Inspired by the counter-current exchange mechanisms observed in biological systems, we propose counter-current learning (CCL), a biologically plausible framework for credit assignment in deep learning. This framework employs a feedforward network to process input data and a feedback network to process targets, with each network enhancing the other through anti-parallel signal propagation. By leveraging the more informative signals from the bottom layer of the feedback network to guide the updates of the top layer of the feedforward network and vice versa, CCL enables the simultaneous transformation of source inputs to target outputs and the dynamic mutual influence of these transformations.Experimental results on MNIST, FashionMNIST, CIFAR10, CIFAR100, and STL-10 datasets using multi-layer perceptrons and convolutional neural networks demonstrate that CCL achieves comparable performance to other biological plausible algorithms while offering a more biologically realistic learning mechanism. Furthermore, we showcase the applicability of our approach to an autoencoder task, underscoring its potential for unsupervised representation learning.Our work presents a promising direction for biologically inspired and plausible learning algorithms, offering insights into the mechanisms of learning and adaptation in neural networks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-113" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-113', event_id='95440', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1904</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95440">Adaptive Depth Networks with Skippable Sub-Paths</a></strong></h5>


                        <p class="text-muted">
                            Woochul Kang &middot; HYUNGSEOP LEE
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Predictable adaptation of network depths can be an effective way to control inference latency and meet the resource condition of various devices. However, previous adaptive depth networks do not provide general principles and a formal explanation on why and which layers can be skipped, and, hence, their approaches are hard to be generalized and require long and complex training steps. In this paper, we present a practical approach to adaptive depth networks that is applicable to various networks with minimal training effort. In our approach, every hierarchical residual stage is divided into two sub-paths, and they are trained to acquire different properties through a simple self-distillation strategy. While the first sub-path is essential for hierarchical feature learning, the second one is trained to refine the learned features and minimize performance degradation if it is skipped. Unlike prior adaptive networks, our approach does not train every target sub-network in an iterative manner. At test time, however, we can connect these sub-paths in a combinatorial manner to select sub-networks of various accuracy-efficiency trade-offs from a single network. We provide a formal rationale for why the proposed training method can reduce overall prediction errors while minimizing the impact of skipping sub-paths. We demonstrate the generality and effectiveness of our approach with convolutional neural networks and transformers.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-114" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-114', event_id='95119', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1905</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95119">Stepping on the Edge: Curvature Aware Learning Rate Tuners</a></strong></h5>


                        <p class="text-muted">
                            Vincent Roulet &middot; Atish Agarwala &middot; Jean-Bastien Grill &middot; Grzegorz Swirszcz &middot; Mathieu Blondel &middot; Fabian Pedregosa
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Curvature information -- particularly, the largest eigenvalue of the lossHessian, known as the sharpness -- often forms the basis for learning ratetuners. However, recent work has shown that the curvature information undergoescomplex dynamics during training, going from a phase of increasing sharpness toeventual stabilization. We analyze the closed-loop feedback effect betweenlearning rate tuning and curvature. We find that classical learning rate tunersmay yield greater one-step loss reduction, yet they ultimately underperform inthe long term when compared to constant learning rates in the full batch regime.These models break the stabilization of the sharpness, which we explain using asimplified model of the joint dynamics of the learning rate and the curvature.To further investigate these effects, we introduce a new learning rate tuningmethod, Curvature Dynamics Aware Tuning (CDAT), which prioritizes long termcurvature stabilization over instantaneous progress on the objective. In thefull batch regime, CDAT shows behavior akin to prefixed warm-up schedules on deeplearning objectives, outperforming tuned constant learning rates. In the minibatch regime, we observe that stochasticity introduces confounding effects thatexplain the previous success of some learning rate tuners at appropriate batchsizes. Our findings highlight the critical role of understanding the jointdynamics of the learning rate and curvature, beyond greedy minimization, todiagnose failures and design effective adaptive learning rate tuners.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-115" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-115', event_id='94427', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1906</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94427">Training Binary Neural Networks via Gaussian Variational Inference and Low-Rank Semidefinite Programming</a></strong></h5>


                        <p class="text-muted">
                            Lorenzo Orecchia &middot; Jiawei Hu &middot; Xue He &middot; Wang Mark &middot; Xulei Yang &middot; Min Wu &middot; Xue Geng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Current methods for training Binarized Neural Networks (BNNs) heavily rely on the heuristic straight-through estimator (STE), which crucially enables the application of SGD-based optimizers to the combinatorial training problem. Although the STE heuristics and their variants have led to significant improvements in BNN performance, their theoretical underpinnings remain unclear and relatively understudied. In this paper, we propose a theoretically motivated optimization framework for BNN training based on Gaussian variational inference. In its simplest form, our approach yields a non-convex linear programming formulation whose variables and associated gradients motivate the use of latent weights and STE gradients. More importantly, our framework allows us to  formulate  semidefinite programming (SDP) relaxations to the BNN training task. Such formulations are able to explicitly models pairwise correlations between weights during training, leading to a more accurate optimization characterization of the training problem. As the size of such formulations grows quadratically in the number of weights, quickly becoming intractable for large networks, we apply the Burer-Monteiro approach and only optimize over linear-size low-rank SDP solutions. Our empirical evaluation on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet datasets shows our method consistently outperforming all state-of-the-art algorithms for training BNNs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-116" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-116', event_id='94139', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1907</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94139">Alias-Free Mamba Neural Operator</a></strong></h5>


                        <p class="text-muted">
                            Jianwei Zheng &middot; Wei Li &middot; Ni Xu &middot; Junwei Zhu &middot; XiaoxuLin &middot; Xiaoqin Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Benefiting from the booming deep learning techniques, neural operators (NO) are considered as an ideal alternative to break the traditions of solving Partial Differential Equations (PDE) with expensive cost.Yet with the remarkable progress, current solutions concern little on the holistic function features--both global and local information-- during the process of solving PDEs.Besides, a meticulously designed kernel integration to meet desirable performance often suffers from a severe computational burden, such as GNO with $O(N(N-1))$, FNO with $O(NlogN)$, and Transformer-based NO with $O(N^2)$.To counteract the dilemma, we propose a mamba neural operator with $O(N)$ computational complexity, namely MambaNO.Functionally, MambaNO achieves a clever balance between global integration, facilitated by state space model of Mamba that scans the entire function, and local integration, engaged with an alias-free architecture. We prove a property of continuous-discrete equivalence to show the capability ofMambaNO in approximating operators arising from universal PDEs to desired accuracy. MambaNOs are evaluated on a diverse set of benchmarks with possibly multi-scale solutions and set new state-of-the-art scores, yet with fewer parameters and better efficiency.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-117" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-117', event_id='94003', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1908</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94003">Learn To be Efficient: Build Structured Sparsity in Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Haizhong Zheng &middot; Xiaoyan Bai &middot; Xueshen Liu &middot; Zhuoqing Morley Mao &middot; Beidi Chen &middot; Fan Lai &middot; Atul Prakash
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. However, existing methods only focus on utilizing this naturally formed activation sparsity in a post-training setting, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity. To achieve this, we introduce a novel training algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like LLaMA using non-ReLU activations. Extensive evaluation on language understanding, language generation, and instruction tuning tasks show that LTE consistently outperforms SOTA baselines. Along with our hardware-aware custom kernel implementation, LTE reduces LLaMA2-7B inference latency by 25% at 50% sparsity.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-118" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-118', event_id='93966', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1909</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93966">MOTE-NAS: Multi-Objective Training-based Estimate for Efficient Neural Architecture Search</a></strong></h5>


                        <p class="text-muted">
                            Yuming Zhang &middot; Jun Hsieh &middot; Xin Li &middot; Ming-Ching Chang &middot; Chun-Chieh Lee &middot; Kuo-Chin Fan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Neural Architecture Search (NAS) methods seek effective optimization toward performance metrics regarding model accuracy and generalization while facing challenges regarding search costs and GPU resources. Recent Neural Tangent Kernel (NTK) NAS methods achieve remarkable search efficiency based on a training-free model estimate; however, they overlook the non-convex nature of the DNNs in the search process. In this paper, we develop Multi-Objective Training-based Estimate (MOTE) for efficient NAS, retaining search effectiveness and achieving the new state-of-the-art in the accuracy and cost trade-off. To improve NTK and inspired by the Training Speed Estimation (TSE) method, MOTE is designed to model the actual performance of DNNs from macro to micro perspective by draw loss landscape and convergence speed simultaneously. Using two reduction strategies, the MOTE is generated based on a reduced architecture and a reduced dataset. Inspired by evolutionary search, our iterative ranking-based, coarse-to-fine architecture search is highly effective. Experiments on NASBench-201 show MOTE-NAS achieves 94.32% accuracy on CIFAR-10, 72.81% on CIFAR-100, and 46.38% on ImageNet-16-120, outperforming NTK-based NAS approaches. An evaluation-free (EF) version of MOTE-NAS delivers high efficiency in only 5 minutes, delivering a model more accurate than KNAS.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-119" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-119', event_id='93805', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1910</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93805">Compressing Large Language Models using Low Rank and Low Precision Decomposition</a></strong></h5>


                        <p class="text-muted">
                            Rajarshi Saha &middot; Naomi Sagan &middot; Varun Srivastava &middot; Andrea Goldsmith &middot; Mert Pilanci
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices. This work introduces $\rm CALDERA$ -- a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix $\mathbf{W}$ by approximating it via a low-rank, low-precision decomposition as $\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$. Here, $\mathbf{L}$ and $\mathbf{R}$ are low rank factors, and the entries of $\mathbf{Q}$, $\mathbf{L}$ and $\mathbf{R}$ are quantized. The model is compressed by substituting each layer with its $\mathbf{Q} + \mathbf{L}\mathbf{R}$ decomposition, and the zero-shot performance of the compressed model is evaluated. Additionally, $\mathbf{L}$ and $\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance. $\rm CALDERA$ obtains this decomposition by formulating it as an optimization problem $\min_{\mathbf{Q},\mathbf{L},\mathbf{R}}\lVert(\mathbf{Q} + \mathbf{L}\mathbf{R} - \mathbf{W})\mathbf{X}^\top\rVert_{\rm F}^2$, where $\mathbf{X}$ is the calibration data, and $\mathbf{Q}, \mathbf{L}, \mathbf{R}$ are constrained to be representable using low-precision formats. Theoretical upper bounds on the approximation error of $\rm CALDERA$ are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget. Results illustrate that compressing LlaMa-$2$ $7$B/$13$B/$70$B and LlaMa-$3$ $8$B models obtained using $\rm CALDERA$ outperforms existing post-training LLM compression techniques in the regime of less than $2.5$ bits per parameter.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-120" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-120', event_id='93727', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#1911</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93727">DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs</a></strong></h5>


                        <p class="text-muted">
                            Haokun Lin &middot; Haobo Xu &middot; Yichen WU &middot; Jingzhi Cui &middot; Yingtao Zhang &middot; Linzhan Mou &middot; Linqi Song &middot; Zhenan Sun &middot; Ying Wei
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Quantization of large language models (LLMs) faces significant challenges, particularly due to the presence of outlier activations that impede efficient low-bit representation. Traditional approaches predominantly address Normal Outliers, which are activations across all tokens with relatively large magnitudes. However, these methods struggle with smoothing Massive Outliers that display significantly larger values, which leads to significant performance degradation in low-bit quantization. In this paper, we introduce DuQuant, a novel approach that utilizes rotation and permutation transformations to more effectively mitigate both massive and normal outliers. First, DuQuant starts by constructing the rotation matrix, using specific outlier dimensions as prior knowledge, to redistribute outliers to adjacent channels by block-wise rotation. Second, We further employ a zigzag permutation to balance the distribution of outliers across blocks, thereby reducing block-wise variance. A subsequent rotation further smooths the activation landscape, enhancing model performance. DuQuant simplifies the quantization process and excels in managing outliers, outperforming the state-of-the-art baselines across various sizes and types of LLMs on multiple tasks, even with 4-bit weight-activation quantization. Our code is available at https://github.com/Hsu1023/DuQuant.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-121" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-121', event_id='94298', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2000</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94298">PLIP: Language-Image Pre-training for Person Representation Learning</a></strong></h5>


                        <p class="text-muted">
                            Jialong Zuo &middot; Jiahao Hong &middot; Feng Zhang &middot; Changqian Yu &middot; Hanyu Zhou &middot; Changxin Gao &middot; Nong Sang &middot; Jingdong Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Language-image pre-training is an effective technique for learning powerful representations in general domains. However, when directly turning to person representation learning, these general pre-training methods suffer from unsatisfactory performance. The reason is that they neglect critical person-related characteristics, i.e., fine-grained attributes and identities. To address this issue, we propose a novel language-image pre-training framework for person representation learning, termed PLIP. Specifically, we elaborately design three pretext tasks: 1) Text-guided Image Colorization, aims to establish the correspondence between the person-related image regions and the fine-grained color-part textual phrases. 2) Image-guided Attributes Prediction, aims to mine fine-grained attribute information of the person body in the image; and 3) Identity-based Vision-Language Contrast, aims to correlate the cross-modal representations at the identity level rather than the instance level. Moreover, to implement our pre-train framework, we construct a large-scale person dataset with image-text pairs named SYNTH-PEDES by automatically generating textual annotations. We pre-train PLIP on SYNTH-PEDES and evaluate our models by spanning downstream person-centric tasks. PLIP not only significantly improves existing methods on all these tasks, but also shows great ability in the zero-shot and domain generalization settings. The code, dataset and weight will be made publicly available.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-122" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-122', event_id='95451', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2001</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95451">Discrete Dictionary-based Decomposition Layer for Structured Representation Learning</a></strong></h5>


                        <p class="text-muted">
                            Taewon Park &middot; Hyun-Chul Kim &middot; Minho Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Neuro-symbolic neural networks have been extensively studied to integrate symbolic operations with neural networks, thereby improving systematic generalization. Specifically, Tensor Product Representation (TPR) framework enables neural networks to perform differentiable symbolic operations by encoding the symbolic structure of data within vector spaces. However, TPR-based neural networks often struggle to decompose unseen data into structured TPR representations, undermining their symbolic operations. To address this decomposition problem, we propose a Discrete Dictionary-based Decomposition (D3) layer designed to enhance the decomposition capabilities of TPR-based models. D3 employs discrete, learnable key-value dictionaries trained to capture symbolic features essential for decomposition operations. It leverages the prior knowledge acquired during training to generate structured TPR representations by mapping input data to pre-learned symbolic features within these dictionaries. D3 is a straightforward drop-in layer that can be seamlessly integrated into any TPR-based model without modifications. Our experimental results demonstrate that D3 significantly improves the systematic generalization of various TPR-based models while requiring fewer additional parameters. Notably, D3 outperforms baseline models on the synthetic task that demands the systematic decomposition of unseen combinatorial data.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-123" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-123', event_id='95883', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2002</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95883">Learning Low-Rank Feature for Thorax Disease Classification</a></strong></h5>


                        <p class="text-muted">
                            Yancheng Wang &middot; Rajeev Goel &middot; Utkarsh Nath &middot; Alvin Silva &middot; Teresa Wu &middot; Yingzhen Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deep neural networks, including Convolutional Neural Networks (CNNs) and Visual Transformers (ViT), have achieved stunning success in the medical image domain. We study thorax disease classification in this paper. Effective extraction of features for the disease areas is crucial for disease classification on radiographic images. While various neural architectures and training techniques, such as self-supervised learning with contrastive/restorative learning, have been employed for disease classification on radiographic images, there are no principled methods that can effectively reduce the adverse effect of noise and background or non-disease areas on the radiographic images for disease classification. To address this challenge, we propose a novel Low-Rank Feature Learning (LRFL) method in this paper, which is universally applicable to the training of all neural networks. The LRFL method is both empirically motivated by a Low Frequency Property (LFP) and theoretically motivated by our sharp generalization bound for neural networks with low-rank features. LFP not only widely exists in deep neural networks for generic machine learning but also exists in all the thorax medical datasets studied in this paper. In the empirical study, using a neural network such as a ViT or a CNN pre-trained on unlabeled chest X-rays by Masked Autoencoders (MAE), our novel LRFL method is applied on the pre-trained neural network and demonstrates better classification results in terms of both multi-class area under the receiver operating curve (mAUC) and classification accuracy than the current state-of-the-art. The code is available at https://github.com/Statistical-Deep-Learning/LRFL.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-124" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-124', event_id='95936', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2003</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95936">Linguistic Collapse: Neural Collapse in (Large) Language Models</a></strong></h5>


                        <p class="text-muted">
                            Robert Wu &middot; Vardan Papyan
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Neural collapse ($\mathcal{NC}$) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers.These behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension.Recent studies have explored $\mathcal{NC}$ in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries.Language modeling presents a curious frontier, as \textit{training by token prediction} constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs.This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards $\mathcal{NC}$.We find that $\mathcal{NC}$ properties that develop with scale (and regularization) are linked to generalization.Moreover, there is evidence of some relationship between $\mathcal{NC}$ and generalization independent of scale.Our work thereby underscores the generality of $\mathcal{NC}$ as it extends to the novel and more challenging setting of language modeling.Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on $\mathcal{NC}$-related properties.Our code is hosted on GitHub: [`https://github.com/rhubarbwu/linguistic-collapse`](https://github.com/rhubarbwu/linguistic-collapse).</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-125" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-125', event_id='93271', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2004</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93271">Nonlocal Attention Operator: Materializing Hidden Knowledge Towards Interpretable Physics Discovery</a></strong></h5>


                        <p class="text-muted">
                            Yue Yu &middot; Ning Liu &middot; Fei Lu &middot; Tian Gao &middot; Siavash Jafarzadeh &middot; Stewart A Silling
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite recent popularity of attention-based neural architectures in core AI fields like natural language processing (NLP) and computer vision (CV), their potential in modeling complex physical systems remains under-explored. Learning problems in physical systems are often characterized as discovering operators that map between function spaces based on a few instances of function pairs. This task frequently presents a severely ill-posed PDE inverse problem. In this work, we propose a novel neural operator architecture based on the attention mechanism, which we coin Nonlocal Attention Operator (NAO), and explore its capability towards developing a foundation physical model. In particular, we show that the attention mechanism is equivalent to a double integral operator that enables nonlocal interactions among spatial tokens, with a data-dependent kernel characterizing the inverse mapping from data to the hidden parameter field of the underlying operator. As such, the attention mechanism extracts global prior information from training data generated by multiple systems, and suggests the exploratory space in the form of a nonlinear kernel map. Consequently, NAO can address ill-posedness and rank deficiency in inverse PDE problems by encoding regularization and achieving generalizability. Lastly, we empirically demonstrate the advantages of NAO over baseline neural models in terms of the generalizability to unseen data resolutions and system states. Our work not only suggests a novel neural operator architecture for learning an interpretable foundation model of physical systems, but also offers a new perspective towards understanding the attention mechanism.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-126" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-126', event_id='93809', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2005</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93809">In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness</a></strong></h5>


                        <p class="text-muted">
                            Liam Collins &middot; Advait Parulekar &middot; Aryan Mokhtari &middot; Sujay Sanghavi &middot; Sanjay Shakkottai
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such, that learner must adapt to the context without additional training. We explore the role of <em>softmax</em> attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activation and thus cannot be replicated by the linear activation often studied in prior theoretical analyses.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-127" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-127', event_id='94135', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2006</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94135">Rough Transformers: Lightweight Continuous-Time Sequence Modelling with Path Signatures</a></strong></h5>


                        <p class="text-muted">
                            Fernando Moreno-Pino &middot; Alvaro Arroyo &middot; Harrison Waldon &middot; Xiaowen Dong &middot; Alvaro Cartea
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Time-series data in real-world settings typically exhibit long-range dependencies and are observed at non-uniform intervals. In these settings, traditional sequence-based recurrent models struggle. To overcome this, researchers often replace recurrent models with Neural ODE-based architectures to account for irregularly sampled data and use  Transformer-based architectures to account for long-range dependencies. Despite the success of these two approaches, both incur very high computational costs for input sequences of even moderate length. To address this challenge, we introduce the Rough Transformer, a variation of the Transformer model that operates on continuous-time representations of input sequences and incurs significantly lower computational costs. In particular, we propose multi-view signature attention, which uses path signatures to augment vanilla attention and to capture both local and global (multi-scale) dependencies in the input data, while remaining robust to changes in the sequence length and sampling frequency and yielding improved spatial processing. We find that, on a variety of time-series-related tasks, Rough Transformers consistently outperform their vanilla attention counterparts while obtaining the representational benefits of Neural ODE-based models, all at a fraction of the computational time and memory resources.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-128" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-128', event_id='96404', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2007</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96404">SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention</a></strong></h5>


                        <p class="text-muted">
                            RÃ³bert CsordÃ¡s &middot; Piotr PiÄkos &middot; Kazuki Irie &middot; JÃ¼rgen Schmidhuber
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite many recent works on Mixture of Experts (MoEs) for resource-efficient Transformer language models, existing methods mostly focus on MoEs for feedforward layers. Previous attempts at extending MoE to the self-attention layer fail to match the performance of the parameter-matched baseline. Our novel SwitchHead is an effective MoE method for the attention layer that successfully reduces both the compute and memory requirements, achieving wall-clock speedup, while matching the language modeling performance of the baseline Transformer. Our novel MoE mechanism allows SwitchHead to compute up to 8 times fewer attention matrices than the standard Transformer. SwitchHead can also be combined with MoE feedforward layers, resulting in fully-MoE "SwitchAll" Transformers. For our 262M parameter model trained on C4, SwitchHead matches the perplexity of standard models with only 44% compute and 27% memory usage. Zero-shot experiments on downstream tasks confirm the performance of SwitchHead, e.g., achieving more than 3.5% absolute improvements on BliMP compared to the baseline with an equal compute resource.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-129" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-129', event_id='96936', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2008</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96936">KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</a></strong></h5>


                        <p class="text-muted">
                            Coleman Hooper &middot; Sehoon Kim &middot; Hiva Mohammadzadeh &middot; Michael Mahoney &middot; Sophia Shao &middot; Kurt Keutzer &middot; Amir Gholami
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>LLMs are seeing growing use for applications which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in sub-4-bit precision. Our work, KVQuant, facilitates low precision KV cache quantization by incorporating several novel methods: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better represent the distributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we isolate outliers separately for each vector to minimize skews in quantization ranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral models, we achieve &lt; 0.1 perplexity degradation with 3-bit quantization on both Wikitext-2 and C4, outperforming existing approaches. Our method enables serving LLaMA-7B with a context length of up to 1 million on a single A100-80GB GPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for KVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline fp16 matrix-vector multiplications, for the LLaMA-7B model.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-130" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-130', event_id='93129', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2009</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93129">Scaling White-Box Transformers for Vision</a></strong></h5>


                        <p class="text-muted">
                            Jinrui Yang &middot; Xianhang Li &middot; Druv Pai &middot; Yuyin Zhou &middot; Yi Ma &middot; Yaodong Yu &middot; Cihang Xie
                        </p>

                    </div>
                    <div class="abstract">
                        <p>CRATE, a white-box transformer architecture designed to learn compressed and sparse representations, offers an intriguing alternative to standard vision transformers (ViTs) due to its inherent mathematical interpretability. Despite extensive investigations into the scaling behaviors of language and vision transformers, the scalability of CRATE remains an open question which this paper aims to address. Specifically, we propose CRATE-$\alpha$, featuring strategic yet minimal modifications to the sparse coding block in the CRATE architecture design, and a light training recipe designed to improve the scalability of CRATE.Through extensive experiments, we demonstrate that CRATE-$\alpha$ can effectively scale with larger model sizes and datasets. For example, our CRATE-$\alpha$-B substantially outperforms the prior best CRATE-B model accuracy on ImageNet classification by 3.7%, achieving an accuracy of 83.2%. Meanwhile, when scaling further, our CRATE-$\alpha$-L obtains an ImageNet classification accuracy of 85.1%. More notably, these model performance improvements are achieved while preserving, and potentially even enhancing the interpretability of learned CRATE models, as we demonstrate through showing that the learned token representations of increasingly larger trained CRATE-$\alpha$ models yield increasingly higher-quality unsupervised object segmentation of images.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-131" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-131', event_id='93255', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2010</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93255">Incorporating Test-Time Optimization into Training with Dual Networks for Human Mesh Recovery</a></strong></h5>


                        <p class="text-muted">
                            Yongwei Nie &middot; Mingxian Fan &middot; Chengjiang Long &middot; Qing Zhang &middot; Jian Zhu &middot; Xuemiao Xu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Human Mesh Recovery (HMR) is the task of estimating a parameterized 3D human mesh from an image. There is a kind of methods first training a regression model for this problem, then further optimizing the pretrained regression model for any specific sample individually at test time. However, the pretrained model may not provide an ideal optimization starting point for the test-time optimization. Inspired by meta-learning, we incorporate the test-time optimization into training, performing a step of test-time optimization for each sample in the training batch before really conducting the training optimization over all the training samples. In this way, we obtain a meta-model, the meta-parameter of which is friendly to the test-time optimization. At test time, after several test-time optimization steps starting from the meta-parameter, we obtain much higher HMR accuracy than the test-time optimization starting from the simply pretrained regression model. Furthermore, we find test-time HMR objectives are different from training-time objectives, which reduces the effectiveness of the learning of the meta-model. To solve this problem, we propose a dual-network architecture that unifies the training-time and test-time objectives. Our method, armed with meta-learning and the dual networks, outperforms state-of-the-art regression-based and optimization-based HMR approaches, as validated by the extensive experiments. The codes are available at https://github.com/fmx789/Meta-HMR.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-132" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-132', event_id='93262', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2011</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93262">Convergence Analysis of Split Federated Learning on Heterogeneous Data</a></strong></h5>


                        <p class="text-muted">
                            Pengchao Han &middot; Chao Huang &middot; Geng Tian &middot; Ming Tang &middot; Xin Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Split federated learning (SFL) is a recent distributed approach for collaborative model training among multiple clients. In SFL, a global model is typically split into two parts, where clients train one part in a parallel federated manner, and a main server trains the other. Despite the recent research on SFL algorithm development, the convergence analysis of SFL is missing in the literature, and this paper aims to fill this gap. The analysis of SFL can be more challenging than that of federated learning (FL), due to the potential dual-paced updates at the clients and the main server. We provide convergence analysis of SFL for strongly convex and general convex objectives on heterogeneous data. The convergence rates are $O(1/T)$ and $O(1/\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds for SFL training. We further extend the analysis to non-convex objectives and where some clients may be unavailable during training. Numerical experiments validate our theoretical results and show that SFL outperforms FL and split learning (SL) when data is highly heterogeneous across a large number of clients.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-133" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-133', event_id='93747', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2100</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93747">Once Read is Enough: Domain-specific Pretraining-free Language Models with Cluster-guided Sparse Experts for Long-tail Domain Knowledge</a></strong></h5>


                        <p class="text-muted">
                            Fang Dong &middot; Mengyi Chen &middot; Jixian Zhou &middot; Yubin Shi &middot; Yixuan Chen &middot; Mingzhi Dong &middot; Yujiang Wang &middot; Dongsheng Li &middot; Xiaochen Yang &middot; Rui Zhu &middot; Robert Dick &middot; Qin Lv &middot; Fan Yang &middot; Tun Lu &middot; Ning Gu &middot; Li Shang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Language models (LMs) only pretrained on a general and massive corpus usually cannot attain satisfying performance on domain-specific downstream tasks, and hence, applying domain-specific pretraining to LMs is a common and indispensable practice.However, domain-specific pretraining can be costly and time-consuming, hindering LMs' deployment in real-world applications.In this work, we consider the incapability to memorize domain-specific knowledge embedded in the general corpus with rare occurrences and long-tail distributions as the leading cause for pretrained LMs' inferior downstream performance. Analysis of Neural Tangent Kernels (NTKs) reveals that those long-tail data are commonly overlooked in the model's gradient updates and, consequently, are not effectively memorized, leading to poor domain-specific downstream performance.Based on the intuition that data with similar semantic meaning are closer in the embedding space, we devise a Cluster-guided Sparse Expert (CSE) layer to actively learn long-tail domain knowledge typically neglected in previous pretrained LMs.During pretraining, a CSE layer efficiently clusters domain knowledge together and assigns long-tail knowledge to designate extra experts. CSE is also a lightweight structure that only needs to be incorporated in several deep layers.With our training strategy, we found that during pretraining, data of long-tail knowledge gradually formulate isolated, outlier clusters in an LM's representation spaces, especially in deeper layers. Our experimental results show that only pretraining CSE-based LMs is enough to achieve superior performance than regularly pretrained-finetuned LMs on various downstream tasks, implying the prospects of domain-specific-pretraining-free language models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-134" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-134', event_id='93207', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2101</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93207">The Benefits of Balance: From Information Projections to Variance Reduction</a></strong></h5>


                        <p class="text-muted">
                            Lang Liu &middot; Ronak Mehta &middot; Soumik Pal &middot; Zaid Harchaoui
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Data balancing across multiple modalities and sources appears in various forms in foundation models in machine learning and AI, e.g. in CLIP and DINO. We show that data balancing across modalities and sources actually offers an unsuspected benefit: variance reduction. We present a non-asymptotic statistical bound that quantifies this variance reduction effect and relates it to the eigenvalue decay of Markov operators. Furthermore, we describe how various forms of data balancing in contrastive multimodal learning and self-supervised clustering can be better understood, and even improved upon, owing to our variance reduction viewpoint.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-135" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-135', event_id='93148', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2102</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93148">Neural Experts: Mixture of Experts for Implicit Neural Representations</a></strong></h5>


                        <p class="text-muted">
                            Yizhak Ben-Shabat &middot; Chamin Hewa Koneputugodage &middot; Sameera Ramasinghe &middot; Stephen Gould
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Implicit neural representations (INRs) have proven effective in various tasks including image, shape, audio, and video reconstruction. These INRs typically learn the implicit field from sampled input points. This is often done using a single network for the entire domain, imposing many global constraints on a single function. In this paper, we propose a mixture of experts (MoE) implicit neural representation approach that enables learning local piece-wise continuous functions that simultaneously learns to subdivide the domain and fit it locally. We show that incorporating a mixture of experts architecture into existing INR formulations provides a boost in speed, accuracy, and memory requirements. Additionally, we introduce novel conditioning and pretraining methods for the gating network that improves convergence to the desired solution. We evaluate the effectiveness of our approach on multiple reconstruction tasks, including surface reconstruction, image reconstruction, and audio signal reconstruction and show improved performance compared to non-MoE methods. Code is available at our project page https://sitzikbs.github.io/neural-experts-projectpage/ .</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-136" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-136', event_id='92990', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2103</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92990">Neural Concept Binder</a></strong></h5>


                        <p class="text-muted">
                            Wolfgang Stammer &middot; Antonia WÃ¼st &middot; David Steinmann &middot; Kristian Kersting
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The challenge in object-based visual reasoning lies in generating concept representations that are both descriptive and distinct. Achieving this in an unsupervised manner requires human users to understand the model's learned concepts and, if necessary, revise incorrect ones. To address this challenge, we introduce the Neural Concept Binder (NCB), a novel framework for deriving both discrete and continuous concept representations, which we refer to as "concept-slot encodings". NCB employs two types of binding: "soft binding", which leverages the recent SysBinder mechanism to obtain object-factor encodings, and subsequent "hard binding", achieved through hierarchical clustering and retrieval-based inference. This enables obtaining expressive, discrete representations from unlabeled images. Moreover, the structured nature of NCB's concept representations allows for intuitive inspection and the straightforward integration of external knowledge, such as human input or insights from other AI models like GPT-4. Additionally, we demonstrate that incorporating the hard binding mechanism preserves model performance while enabling seamless integration into both neural and symbolic modules for complex reasoning tasks. We validate the effectiveness of NCB through evaluations on our newly introduced CLEVR-Sudoku dataset.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-137" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-137', event_id='99343', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2104</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/99343">Reproducibility Study on Adversarial Attacks Against Robust Transformer Trackers</a></strong></h5>


                        <p class="text-muted">
                            Fatemeh Nourilenjan Nokabadi &middot; Christian GagnÃ© &middot; Jean-Francois Lalonde
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>New transformer networks have been integrated into object tracking pipelines and have demonstrated strong performance on the latest benchmarks. This paper focuses on understanding how transformer trackers behave under adversarial attacks and how different attacks perform on tracking datasets as their parameters change. We conducted a series of experiments to evaluate the effectiveness of existing adversarial attacks on object trackers with transformer and non-transformer backbones. We experimented on 7 different trackers, including 3 that are transformer-based, and 4 which leverage other architectures. These trackers are tested against 4 recent attack methods to assess their performance and robustness on VOT2022ST, UAV123 and GOT10k datasets. Our empirical study focuses on evaluating adversarial robustness of object trackers based on bounding box versus binary mask predictions, and attack methods at different levels of perturbations. Interestingly, our study found that altering the perturbation level may not significantly affect the overall object tracking results after the attack. Similarly, the sparsity and imperceptibility of the attack perturbations may remain stable against perturbation level shifts. By applying a specific attack on all transformer trackers, we show that new transformer trackers having a stronger cross-attention modeling achieve a greater adversarial robustness on tracking datasets, such as VOT2022ST and GOT10k. Our results also indicate the necessity for new attack methods to effectively tackle the latest types of transformer trackers. The codes necessary to reproduce this study are available at https://github.com/fatemehN/ReproducibilityStudy.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-138" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-138', event_id='96554', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2105</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96554">ECLipsE: Efficient Compositional Lipschitz Constant Estimation for Deep Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Yuezhu Xu &middot; S Sivaranjani
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The Lipschitz constant plays a crucial role in certifying the robustness of neural networks to input perturbations. Since calculating the exact Lipschitz constant is NP-hard, efforts have been made to obtain tight upper bounds on the Lipschitz constant. Typically, this involves solving a large matrix verification problem, the computational cost of which grows significantly for both deeper and wider networks. In this paper, we provide a compositional approach to estimate Lipschitz constants for deep feed-forward neural networks. We first obtain an exact decomposition of the large matrix verification problem into smaller sub-problems. Then, leveraging the underlying cascade structure of the network, we develop two algorithms. The first algorithm explores the geometric features of the problem and enables us to provide Lipschitz estimates that are comparable to existing methods by solving small semidefinite programs (SDPs) that are only as large as the size of each layer. The second algorithm relaxes these sub-problems and provides a closed-form solution to each sub-problem for extremely fast estimation, altogether eliminating the need to solve SDPs. The two algorithms represent different levels of trade-offs between efficiency and accuracy. Finally, we demonstrate that our approach provides a steep reduction in computation time (as much as several thousand times faster, depending on the algorithm for deeper networks) while yielding Lipschitz bounds that are very close to or even better than those achieved by state-of-the-art approaches in a broad range of experiments. In summary, our approach considerably advances the scalability and efficiency of certifying neural network robustness, making it particularly attractive for online learning tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-139" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-139', event_id='96530', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2106</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96530">HYDRA-FL: Hybrid Knowledge Distillation for Robust and Accurate Federated Learning</a></strong></h5>


                        <p class="text-muted">
                            Momin Ahmad Khan &middot; Yasra Chandio &middot; Fatima Anwar
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Data heterogeneity among Federated Learning (FL) users poses a significant challenge, resulting in reduced global model performance. The community has designed various techniques to tackle this issue, among which Knowledge Distillation (KD)-based techniques are common.    While these techniques effectively improve performance under high heterogeneity, they inadvertently cause higher accuracy degradation under model poisoning attacks (known as \emph{attack amplification}). This paper presents a case study to reveal this critical vulnerability in KD-based FL systems. We show why KD causes this issue through empirical evidence and use it as motivation to design a hybrid distillation technique. We introduce a novel algorithm, Hybrid Knowledge Distillation for Robust and Accurate FL (HYDRA-FL), which reduces the impact of attacks in attack scenarios by offloading some of the KD loss to a shallow layer via an auxiliary classifier. We model HYDRA-FL as a generic framework and adapt it to two KD-based FL algorithms, FedNTD and MOON. Using these two as case studies, we demonstrate that our technique outperforms baselines in attack settings while maintaining comparable performance in benign settings.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-140" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-140', event_id='96193', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2107</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96193">AdanCA: Neural Cellular Automata As Adaptors For More Robust Vision Transformer</a></strong></h5>


                        <p class="text-muted">
                            Yitao Xu &middot; Tong Zhang &middot; Sabine SÃ¼sstrunk
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Vision Transformers (ViTs) demonstrate remarkable performance in image classification through visual-token interaction learning, particularly when equipped with local information via region attention or convolutions. Although such architectures improve the feature aggregation from different granularities, they often fail to contribute to the robustness of the networks. Neural Cellular Automata (NCA) enables the modeling of global visual-token representations through local interactions, with its training strategies and architecture design conferring strong generalization ability and robustness against noisy input. In this paper, we propose Adaptor Neural Cellular Automata (AdaNCA) for Vision Transformers that uses NCA as plug-and-play adaptors between ViT layers, thus enhancing ViT's performance and robustness against adversarial samples as well as out-of-distribution inputs. To overcome the large computational overhead of standard NCAs, we propose Dynamic Interaction for more efficient interaction learning. Using our analysis of AdaNCA placement and robustness improvement, we also develop an algorithm for identifying the most effective insertion points for AdaNCA. With less than a 3% increase in parameters, AdaNCA contributes to more than 10% absolute improvement in accuracy under adversarial attacks on the ImageNet1K benchmark. Moreover, we demonstrate with extensive evaluations across eight robustness benchmarks and four ViT architectures that AdaNCA, as a plug-and-play module, consistently improves the robustness of ViTs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-141" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-141', event_id='96104', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2108</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96104">DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms</a></strong></h5>


                        <p class="text-muted">
                            Oryan Yehezkel &middot; Alon Zolfi &middot; Amit Baras &middot; Yuval Elovici &middot; Asaf Shabtai
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Vision transformers have shown remarkable advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. Token sparsification mechanisms have been proposed to address this issue. These mechanisms employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the modelâs efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector â carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating systemâs resources, while maintaining its stealthiness. Our evaluation demonstrates the attackâs effectiveness on three token sparsification mechanisms and examines the attackâs transferability between them and its effect on the GPU resources. To mitigate the impact of the attack, we propose various countermeasures.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-142" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-142', event_id='95047', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2109</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95047">On the Scalability of Certified Adversarial Robustness with Generated Data</a></strong></h5>


                        <p class="text-muted">
                            Thomas Altstidl &middot; David Dobre &middot; Arthur Kosmala &middot; Bjoern Eskofier &middot; Gauthier Gidel &middot; Leo Schwinn
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Certified defenses against adversarial attacks offer formal guarantees on the robustness of a model, making them more reliable than empirical methods such as adversarial training, whose effectiveness is often later reduced by unseen attacks. Still, the limited certified robustness that is currently achievable has been a bottleneck for their practical adoption. Gowal et al. and Wang et al. have shown that generating additional training data using state-of-the-art diffusion models can considerably improve the robustness of adversarial training. In this work, we demonstrate that a similar approach can substantially improve deterministic certified defenses but also reveal notable differences in the scaling behavior between certified and empirical methods. In addition, we provide a list of recommendations to scale the robustness of certified training approaches. Our approach achieves state-of-the-art deterministic robustness certificates on CIFAR-10 for the $\ell_2$ ($\epsilon = 36/255$) and $\ell_{\infty}$ ($\epsilon = 8/255$) threat models, outperforming the previous results by $+3.95$ and $+1.39$ percentage points, respectively. Furthermore, we report similar improvements for CIFAR-100.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-143" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-143', event_id='94947', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2110</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94947">ProTransformer: Robustify Transformers via Plug-and-Play Paradigm</a></strong></h5>


                        <p class="text-muted">
                            Zhichao Hou &middot; Weizhi Gao &middot; Yuchen Shen &middot; Feiyi Wang &middot; Xiaorui Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Transformer-based architectures have dominated various areas of machine learning in recent years. In this paper, we introduce a novel robust attention mechanism designed to enhance the resilience of transformer-based architectures. Crucially, this technique can be integrated into existing transformers as a plug-and-play layer, improving their robustness without the need for additional training or fine-tuning. Through comprehensive experiments and ablation studies, we demonstrate that our ProTransformer significantly enhances the robustness of transformer models across a variety of prediction tasks, attack mechanisms, backbone architectures, and data domains. Notably, without further fine-tuning, the ProTransformer consistently improves the performance of vanilla transformers by 19.5\%, 28.3\%, 16.1\%, and 11.4\% for BERT, ALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler attack. Furthermore, ProTransformer shows promising resilience in large language models (LLMs) against prompting-based attacks, improving the performance of T5 and LLaMA by 24.8\% and 17.8\%, respectively, and enhancing Vicuna by an average of 10.4\% against the Jailbreaking attack. Beyond the language domain, ProTransformer also demonstrates outstanding robustness in both vision and graph domains.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-144" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-144', event_id='94909', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2111</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94909">Energy-based Hopfield Boosting for Out-of-Distribution Detection</a></strong></h5>


                        <p class="text-muted">
                            Claus Hofmann &middot; Simon Schmid &middot; Bernhard Lehner &middot; Daniel Klotz &middot; Sepp Hochreiter
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world. Outlier exposure methods, which incorporate auxiliary outlier data in the training process, can drastically improve OOD detection performance compared to approaches without advanced training strategies. We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy to sharpen the decision boundary between the in-distribution and OOD data. Hopfield Boosting encourages the model to focus on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data. Our method achieves a new state-of-the-art in OOD detection with outlier exposure, improving the FPR95 from 2.28 to 0.92 on CIFAR-10, from 11.76 to 7.94 on CIFAR-100, and from 50.74 to 36.60 on ImageNet-1K.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-145" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-145', event_id='95275', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2200</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95275">Learning and Transferring Sparse Contextual Bigrams with Linear Transformers</a></strong></h5>


                        <p class="text-muted">
                            Yunwei Ren &middot; Zixuan Wang &middot; Jason Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Transformers have achieved significant success in natural language modeling because of their exceptional capabilities to combine contextual information and global knowledge, yet their theoretical basis remains unclear. In this paper, we first propose Sparse Contextual Bigram (SCB), a natural extension to the classical bigram model, where the generation of the next token depends on a sparse set of earlier positions determined by the last token. We investigate the training dynamics and sample complexity of learning SCB using a one-layer linear transformer with a gradient-based algorithm. We show that when trained from scratch, the training process can be split into an initial sample-intensive stage where the correlation is boosted from zero to a nontrivial value, followed by a more sample-efficient stage of further improvement. Additionally, we prove that, provided a nontrivial correlation between the downstream and pretraining tasks, finetuning from a pretrained model allows us to bypass the initial sample-intensive stage. We also empirically demonstrate that our algorithm can outperform SGD in our setting.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-146" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-146', event_id='95785', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2201</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95785">How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective</a></strong></h5>


                        <p class="text-muted">
                            Qiaozhe Zhang &middot; Ruijie Zhang &middot; Jun Sun &middot; Yingzhuang Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Network pruning is a commonly used measure to alleviate the storage and computational burden of deep neural networks. However, the fundamental limit of network pruning is still lacking. To close the gap, in this work we'll take a first-principles approach, i.e. we'll directly impose the sparsity constraint on the loss function and leverage the framework of <em>statistical dimension</em> in convex geometry, thus we're able to characterize the sharp phase transition point, i.e. the fundamental limit of the pruning ratio. Through this fundamental limit, we're able to identify two key factors that determine the pruning ratio limit, namely, <em>weight magnitude</em> and <em>network flatness</em>. Generally speaking, the flatter the loss landscape or the smaller the weight magnitude, the smaller pruning ratio. Moreover, we provide efficient countermeasures to address the challenges in the computation of the pruning limit, which involves accurate spectrum estimation of a large-scale and non-positive Hessian matrix. Moreover, through the lens of the pruning ratio threshold,  we can provide rigorous interpretations on several heuristics in existing pruning algorithms. Extensive experiments are performed that demonstrate that our theoretical pruning ratio threshold coincides very well with the experiments. All codes are available at: https://anonymous.4open.science/r/Global-One-shot-Pruning-BC7B</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-147" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-147', event_id='96392', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2202</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96392">Benign overfitting in leaky ReLU networks with moderate input dimension</a></strong></h5>


                        <p class="text-muted">
                            Kedar Karhadkar &middot; Erin George &middot; Michael Murray &middot; Guido Montufar &middot; Deanna Needell
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well. We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task. We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs. We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with gradient descent (GD) satisfy this property. In contrast to prior work we do not require the training data to be nearly orthogonal. Notably, for input dimension $d$ and training sample size $n$, while results in prior work require $d = \Omega(n^2 \log n)$, here we require only $d = \Omega(n)$.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-148" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-148', event_id='95313', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2203</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95313">No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations</a></strong></h5>


                        <p class="text-muted">
                            Walter Simoncini &middot; Andrei Bursuc &middot; Spyridon Gidaris &middot; Yuki Asano
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper introduces FUNGI, <strong>F</strong>eatures from <strong>UN</strong>supervised <strong>G</strong>rad<strong>I</strong>ents, a method to enhance the features of transformer encoders by leveraging self-supervised gradients. Our method is simple: given any pretrained model, we first compute gradients from various self-supervised objectives for each input. These gradients are projected to a lower dimension and then concatenated with the model's output embedding. The resulting features are evaluated on k-nearest neighbor classification over 11 datasets from vision, 5 from natural language processing, and 2 from audio. Across backbones spanning various sizes and pretraining strategies, FUNGI features provide consistent performance improvements over the embeddings. We also show that using FUNGI features can benefit linear classification, clustering and image retrieval, and that they significantly improve the retrieval-based in-context scene understanding abilities of pretrained models, for example improving upon DINO by +17% for semantic segmentation - without any training. Code is available at https://github.com/WalterSimoncini/fungivision.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-149" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-149', event_id='96235', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2204</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96235">Contrastive-Equivariant Self-Supervised Learning Improves Alignment with Primate Visual Area IT</a></strong></h5>


                        <p class="text-muted">
                            Thomas Yerxa &middot; Jenelle Feather &middot; Eero Simoncelli &middot; SueYeon Chung
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Models trained with self-supervised learning objectives have recently matched or surpassed models trained with traditional supervised object recognition in their ability to predict neural responses of object-selective neurons in the primate visual system. A self-supervised learning objective is arguably a more biologically plausible organizing principle, as the optimization does not require a large number of labeled examples. However, typical self-supervised objectives may result in network representations that are overly invariant to changes in the input. Here, we show that a representation with structured variability to the input transformations is better aligned with known features of visual perception and neural computation. We introduce a novel framework for converting standard invariant SSL losses into "contrastive-equivariant" versions that encourage preserving aspects of the input transformation without supervised access to the transformation parameters. We further demonstrate that our proposed method systematically increases models' ability to predict responses in macaque inferior temporal cortex. Our results demonstrate the promise of incorporating known features of neural computation into task-optimization for building better models of visual cortex.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-150" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-150', event_id='96398', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2205</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96398">Learning from Pattern Completion: Self-supervised Controllable Generation</a></strong></h5>


                        <p class="text-muted">
                            Zhiqiang Chen &middot; Guofan Fan &middot; Jinying Gao &middot; Lei Ma &middot; Bo Lei &middot; Tiejun Huang &middot; Shan Yu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The human brain exhibits a strong ability to spontaneously associate different visual attributes of the same or similar visual scene, such as associating sketches and graffiti with real-world visual objects, usually without supervising information. In contrast, in the field of artificial intelligence, controllable generation methods like ControlNet heavily rely on annotated training datasets such as depth maps, semantic segmentation maps, and poses, which limits the methodâs scalability. Inspired by the neural mechanisms that may contribute to the brainâs associative power, specifically the cortical modularization and hippocampal pattern completion, here we propose a self-supervised controllable generation (SCG) framework. Firstly, we introduce an equivariance constraint to promote inter-module independence and intra-module correlation in a modular autoencoder network, thereby achieving functional specialization. Subsequently, based on these specialized modules, we employ a self-supervised pattern completion approach for controllable generation training. Experimental results demonstrate that the proposed modular autoencoder effectively achieves functional specialization, including the modular processing of color, brightness, and edge detection, and exhibits brain-like features including orientation selectivity, color antagonism, and center-surround receptive fields. Through self-supervised training, associative generation capabilities spontaneously emerge in SCG, demonstrating excellent zero-shot generalization ability to various tasks such as superresolution, dehaze and associative or conditional generation on painting, sketches, and ancient graffiti. Compared to the previous representative method ControlNet, our proposed approach not only demonstrates superior robustness in more challenging high-noise scenarios but also possesses more promising scalability potential due to its self-supervised manner. Codes are released on Github and Gitee.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-151" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-151', event_id='96517', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2206</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96517">PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Fanxu Meng &middot; Zhaohui Wang &middot; Muhan Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the low-rank adaptation (LoRA) method approximates the model changes $\Delta W \in \mathbb{R}^{m \times n}$ through the product of two matrices $A \in \mathbb{R}^{m \times r}$ and $B \in \mathbb{R}^{r \times n}$, where $r \ll \min(m, n)$, $A$ is initialized with Gaussian noise, and $B$ with zeros. LoRA **freezes the original model $W$** and **updates the "Noise \& Zero" adapter**, which may lead to slow convergence. To overcome this limitation, we introduce **P**r**i**ncipal **S**ingular values and **S**ingular vectors **A**daptation (PiSSA). PiSSA shares the same architecture as LoRA, but initializes the adaptor matrices $A$ and $B$ with the principal components of the original matrix $W$, and put the remaining components into a residual matrix $W^{res} \in \mathbb{R}^{m \times n}$ which is frozen during fine-tuning.Compared to LoRA, PiSSA **updates the principal components** while **freezing the "residual" parts**, allowing faster convergence and enhanced performance. Comparative experiments of PiSSA and LoRA across 11 different models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks, reveal that PiSSA consistently outperforms LoRA under identical experimental setups. On the GSM8K benchmark, Gemma-7B fine-tuned with PiSSA achieves an accuracy of 77.7\%, surpassing LoRA's 74.53\% by 3.25\%. Due to the same architecture, PiSSA is also compatible with quantization to further reduce the memory requirement of fine-tuning. Compared to QLoRA, QPiSSA (PiSSA with 4-bit quantization) exhibits smaller quantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K, QPiSSA attains an accuracy of 86.05\%, exceeding the performances of QLoRA at 81.73\%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few seconds, presenting a negligible cost for transitioning from LoRA to PiSSA.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-152" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-152', event_id='93300', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2207</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93300">RAMP: Boosting Adversarial Robustness Against Multiple $l_p$ Perturbations for Universal Robustness</a></strong></h5>


                        <p class="text-muted">
                            Enyi Jiang &middot; Gagandeep Singh
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Most existing works focus on improving robustness against adversarial attacks bounded by a single $l_p$ norm using adversarial training (AT). However, these AT models' multiple-norm robustness (union accuracy) is still low, which is crucial since in the real-world an adversary is not necessarily bounded by a single norm. The tradeoffs among robustness against multiple $l_p$ perturbations and accuracy/robustness make obtaining good union and clean accuracy challenging. We design a logit pairing loss to improve the union accuracy by analyzing the tradeoffs from the lens of distribution shifts. We connect natural training (NT) with AT via gradient projection, to incorporate useful information from NT into AT, where we empirically and theoretically show it moderates the accuracy/robustness tradeoff. We propose a novel training framework \textbf{RAMP}, to boost the robustness against multiple $l_p$ perturbations. \textbf{RAMP} can be easily adapted for robust fine-tuning and full AT. For robust fine-tuning, \textbf{RAMP} obtains a union accuracy up to $53.3\%$ on CIFAR-10, and $29.1\%$ on ImageNet. For training from scratch, \textbf{RAMP} achieves a union accuracy of $44.6\%$ and good clean accuracy of $81.2\%$ on ResNet-18 against AutoAttack on CIFAR-10. Beyond multi-norm robustness \textbf{RAMP}-trained models achieve superior \textit{universal robustness}, effectively generalizing against a range of unseen adversaries and natural corruptions.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-153" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-153', event_id='93623', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2208</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93623">A theoretical design of concept sets: improving the predictability of concept bottleneck models</a></strong></h5>


                        <p class="text-muted">
                            Max Ruiz Luyten &middot; Mihaela van der Schaar
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Concept-based learning, a promising approach in machine learning, emphasizes the value of high-level representations called concepts. However, despite growing interest in concept-bottleneck models (CBMs), there is a lack of clear understanding regarding the properties of concept sets and their impact on model performance. In this work, we define concepts within the machine learning context, highlighting their core properties: 'expressiveness' and 'model-aware inductive bias', and we make explicit the underlying assumption of CBMs. We establish theoretical results for concept-bottleneck models (CBMs), revealing how these properties guide the design of concept sets that optimize model performance. Specifically, we demonstrate that well-chosen concept sets can improve sample efficiency and out-of-distribution robustness in the appropriate regimes. Based on these insights, we propose a method to effectively identify informative and non-redundant concepts. We validate our approach with experiments on CIFAR-10 and MetaShift, showing that concept-bottleneck models outperform the foundational embedding counterpart, particularly in low-data regimes and under distribution shifts. We also examine failure modes and discuss how they can be tackled.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-154" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-154', event_id='93664', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2209</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93664">NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes</a></strong></h5>


                        <p class="text-muted">
                            Hao-Lun Sun &middot; Lei Hsiung &middot; Nandhini Chandramoorthy &middot; Pin-Yu Chen &middot; Tsung-Yi Ho
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deep neural networks (DNNs) have become ubiquitous in machine learning, but their energy consumption remains problematically high. An effective strategy for reducing such consumption is supply-voltage reduction, but if done too aggressively, it can lead to accuracy degradation. This is due to random bit-flips in static random access memory (SRAM), where model parameters are stored. To address this challenge, we have developed NeuralFuse, a novel add-on module that handles the energy-accuracy tradeoff in low-voltage regimes by learning input transformations and using them to generate error-resistant data representations, thereby protecting DNN accuracy in both nominal and low-voltage scenarios. As well as being easy to implement, NeuralFuse can be readily applied to DNNs with limited access, such cloud-based APIs that are accessed remotely or non-configurable hardware. Our experimental results demonstrate that, at a 1% bit-error rate, NeuralFuse can reduce SRAM access energy by up to 24% while recovering accuracy by up to 57%. To the best of our knowledge, this is the first approach to addressing low-voltage-induced bit errors that requires no model retraining.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-155" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-155', event_id='94143', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2210</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94143">Wormhole Loss for Partial Shape Matching</a></strong></h5>


                        <p class="text-muted">
                            Amit Bracha &middot; Thomas DagÃ¨s &middot; Ron Kimmel
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>When matching parts of a surface to its whole, a fundamental question arises: Which points should be included in the matching process? The issue is intensified when using isometry to measure similarity, as it requires the validation of whether distances measured between pairs of surface points should influence the matching process. The approach we propose treats surfaces as manifolds equipped with geodesic distances, and addresses the partial shape matching challenge by introducing a novel criterion to meticulously search for consistent distances between pairs of points. The new criterion explores the relation between intrinsic geodesic distances between the points, geodesic distances between the points and surface boundaries, and extrinsic distances between boundary points measured in the embedding space. It is shown to be less restrictive compared to previous measures and achieves state-of-the-art results when used as a loss function in training networks for partial shape matching.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-156" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-156', event_id='94634', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2211</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94634">A Simple Remedy for Dataset Bias via Self-Influence: A Mislabeled Sample Perspective</a></strong></h5>


                        <p class="text-muted">
                            Yeonsung Jung &middot; Jaeyun Song &middot; June Yong Yang &middot; Jin-Hwa Kim &middot; Sung-Yub Kim &middot; Eunho Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Learning generalized models from biased data is an important undertaking toward fairness in deep learning. To address this issue, recent studies attempt to identify and leverage bias-conflicting samples free from spurious correlations without prior knowledge of bias or an unbiased set. However, spurious correlation remains an ongoing challenge, primarily due to the difficulty in correctly detecting these samples. In this paper, inspired by the similarities between mislabeled samples and bias-conflicting samples, we approach this challenge from a novel perspective of mislabeled sample detection. Specifically, we delve into Influence Function, one of the standard methods for mislabeled sample detection, for identifying bias-conflicting samples and propose a simple yet effective remedy for biased models by leveraging them. Through comprehensive analysis and experiments on diverse datasets, we demonstrate that our new perspective can boost the precision of detection and rectify biased models effectively. Furthermore, our approach is complementary to existing methods, showing performance improvement even when applied to models that have already undergone recent debiasing techniques.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-157" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-157', event_id='94952', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2300</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94952">Improving Adaptivity via Over-Parameterization in Sequence Models</a></strong></h5>


                        <p class="text-muted">
                            Yicheng Li &middot; Qian Lin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>It is well known that eigenfunctions of a kernel play a crucial role in kernel regression.    Through several examples, we demonstrate that even with the same set of eigenfunctions, the order of these functions significantly impacts regression outcomes.    Simplifying the model by diagonalizing the kernel, we introduce an over-parameterized gradient descent in the realm of sequence model to capture the effects of various orders of a fixed set of eigen-functions.    This method is designed to explore the impact of varying eigenfunction orders.    Our theoretical results show that the over-parameterization gradient flow can adapt to the underlying structure of the signal and significantly outperform the vanilla gradient flow method.    Moreover, we also demonstrate that deeper over-parameterization can further enhance the generalization capability of the model.    These results not only provide a new perspective on the benefits of over-parameterization and but also offer insights into the adaptivity and generalization potential of neural networks beyond the kernel regime.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-158" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-158', event_id='94810', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2301</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94810">The Prevalence of Neural Collapse in Neural Multivariate  Regression</a></strong></h5>


                        <p class="text-muted">
                            George Andriopoulos &middot; Zixuan Dong &middot; Li Guo &middot; Zifan Zhao &middot; Keith Ross
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Recently it has been observed that neural networks exhibit Neural Collapse (NC) during the final stage of training for the classification problem. We empirically show that multivariate regression, as employed in imitation learning and other applications, exhibits Neural Regression Collapse (NRC), a new form of neural collapse: (NRC1) The last-layer feature vectors collapse to the subspace spanned by the $n$ principal components of the feature vectors, where $n$ is the dimension of the targets (for univariate regression, $n=1$); (NRC2) The last-layer feature vectors also collapse to the subspace spanned by the last-layer weight vectors; (NRC3) The Gram matrix for the weight vectors converges to a specific functional form that depends on the covariance matrix of the targets. After empirically establishing the prevalence of (NRC1)-(NRC3) for a variety of datasets and network architectures, we provide an explanation of these phenomena by modeling the regression task in the context of the  Unconstrained Feature Model (UFM), in which the last layer feature vectors are treated as free variables when minimizing the loss function. We show that when the regularization parameters in the UFM model are strictly positive, then (NRC1)-(NRC3) also emerge as solutions in the UFM optimization problem. We also show that if the regularization parameters are equal to zero, then there is no collapse. To our knowledge, this is the first empirical and theoretical study of neural collapse in the context of regression. This extension is significant not only because it broadens the applicability of neural collapse to a new category of problems but also because it suggests that the phenomena of neural collapse could be a universal behavior in deep learning.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-159" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-159', event_id='94247', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2302</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94247">Mission Impossible: A Statistical Perspective on Jailbreaking LLMs</a></strong></h5>


                        <p class="text-muted">
                            Jingtong Su &middot; Julia Kempe &middot; Karen Ullrich
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) are trained on a deluge of text data with limited quality control. As a result, LLMs can exhibit unintended or even harmful behaviours, such as leaking information, fake news or hate speech. Countermeasures, commonly referred to as preference alignment, include fine-tuning the pretrained LLMs with carefully crafted text examples of desired behaviour. Even then, empirical evidence shows preference aligned LLMs can be enticed to harmful behaviour. This so called jailbreaking of LLMs is typically achieved by adversarially modifying the input prompt to the LLM. Our paper provides theoretical insights into the phenomenon of preference alignment and jailbreaking from a statistical perspective. Under our framework, we first show that pretrained LLMs will mimic harmful behaviour if present in the training corpus. \textbf{Under that same framework, we then introduce a statistical notion of alignment, and lower-bound the jailbreaking probability, showing that it is unpreventable under reasonable assumptions.} Based on our insights, we propose an alteration to the currently prevalent alignment strategy RLHF. Specifically, we introduce a  simple modification to the RLHF objective, we call \emph{E-RLHF}, that aims to increase the likelihood of safe responses. \emph{E-RLHF} brings no additional training cost, and is compatible with other methods. Empirically, we demonstrate that \emph{E-RLHF} outperforms RLHF on all alignment problems put forward by the AdvBench \citep{zou2023universal} and HarmBench project \citep{mazeika2024harmbench} without sacrificing model performance as measured by the MT-Bench project \citep{zheng2024judging}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-160" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-160', event_id='93217', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2303</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93217">Optimal deep learning of holomorphic operators between Banach spaces</a></strong></h5>


                        <p class="text-muted">
                            Ben Adcock &middot; Nick Dexter &middot; Sebastian Moraga Scheuermann
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Operator learning problems arise in many key areas of scientific computing where Partial Differential Equations (PDEs) are used to model physical systems. In such scenarios, the operators map between Banach or Hilbert spaces. In this work, we tackle the problem of learning operators between Banach spaces, in contrast to the vast majority of past works considering only Hilbert spaces. We focus on learning holomorphic operators -- an important class of problems with many applications. We combine arbitrary approximate encoders and decoders with standard feedforward Deep Neural Network (DNN) architectures -- specifically, those with constant width exceeding the depth -- under standard $\ell^2$-loss minimization. We first identify a family of  DNNs such that the resulting Deep Learning (DL) procedure achieves optimal generalization bounds for such operators. For standard fully-connected architectures, we then show that there are uncountably many minimizers of the training problem that yield equivalent optimal performance. The DNN architectures we consider are `problem agnostic', with width and depth only depending on the amount of training data $m$ and not on regularity assumptions of the target operator. Next, we show that DL is optimal for this problem: no recovery procedure can surpass these generalization bounds up to log terms. Finally, we present numerical results demonstrating the practical performance on challenging problems including the parametric diffusion, Navier-Stokes-Brinkman and Boussinesq PDEs.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-161" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-161', event_id='95512', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2304</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95512">SLTrain: a sparse plus low rank approach for parameter and memory efficient pretraining</a></strong></h5>


                        <p class="text-muted">
                            Andi Han &middot; Jiaxiang Li &middot; Wei Huang &middot; Mingyi Hong &middot; Akiko Takeda &middot; Pratik Kumar Jawanpuria &middot; Bamdev Mishra
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) have shown impressive capabilities across various tasks. However, training LLMs from scratch requires significant computational power and extensive memory capacity. Recent studies have explored low-rank structures on weights for efficient fine-tuning in terms of parameters and memory, either through low-rank adaptation or factorization. While effective for fine-tuning, low-rank structures are generally less suitable for pretraining because they restrict parameters to a low-dimensional subspace. In this work, we propose to parameterize the weights as a sum of low-rank and sparse matrices for pretraining, which we call SLTrain. The low-rank component is learned via matrix factorization, while for the sparse component, we employ a simple strategy of uniformly selecting the sparsity support at random and learning only the non-zero entries with the fixed support. While being simple, the random fixed-support sparse learning strategy significantly enhances pretraining when combined with low-rank learning. Our results show that SLTrain adds minimal extra parameters and memory costs compared to pretraining with low-rank parameterization, yet achieves substantially better performance, which is comparable to full-rank training. Remarkably, when combined with quantization and per-layer updates, SLTrain can reduce memory requirements by up to 73% when pretraining the LLaMA 7B model.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-162" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-162', event_id='94699', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2305</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94699">Cross-model Control: Improving Multiple Large Language Models in One-time Training</a></strong></h5>


                        <p class="text-muted">
                            Jiayi Wu &middot; Hao Sun &middot; Hengyi Cai &middot; Lixin Su &middot; Shuaiqiang Wang &middot; Dawei Yin &middot; Xiang Li &middot; Ming Gao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The number of large language models (LLMs) with varying parameter scales and vocabularies is increasing. While they deliver powerful performance, they also face a set of common optimization needs to meet specific requirements or standards, such as instruction following or avoiding the output of sensitive information from the real world. However, how to reuse the fine-tuning outcomes of one model to other models to reduce training costs remains a challenge. To bridge this gap, we introduce Cross-model Control (CMC), a method that improves multiple LLMs in one-time training with a portable tiny language model. Specifically, we have observed that the logit shift before and after fine-tuning is remarkably similar across different models. Based on this insight, we incorporate a tiny language model with a minimal number of parameters. By training alongside a frozen template LLM, the tiny model gains the capability to alter the logits output by the LLMs. To make this tiny language model applicable to models with different vocabularies, we propose a novel token mapping strategy named PM-MinED. We have conducted extensive experiments on instruction tuning and unlearning tasks, demonstrating the effectiveness of CMC. Our code is available at https://github.com/wujwyi/CMC</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-163" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-163', event_id='92936', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2306</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92936">Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints</a></strong></h5>


                        <p class="text-muted">
                            Jay Bear &middot; Adam Prugel-Bennett &middot; Jonathon Hare
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Iterative algorithms solve problems by taking steps until a solution is reached. Models in the form of Deep Thinking (DT) networks have been demonstrated to learn iterative algorithms in a way that can scale to different sized problems at inference time using recurrent computation and convolutions. However, they are often unstable during training, and have no guarantees of convergence/termination at the solution. This paper addresses the problem of instability by analyzing the growth in intermediate representations, allowing us to build models (referred to as Deep Thinking with Lipschitz Constraints (DT-L)) with many fewer parameters and providing more reliable solutions. Additionally our DT-L formulation provides guarantees of convergence of the learned iterative procedure to a unique solution at inference time. We demonstrate DT-L is capable of robustly learning algorithms which extrapolate to harder problems than in the training set. We benchmark on the traveling salesperson problem to evaluate the capabilities of the modified system in an NP-hard problem where DT fails to learn.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-164" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-164', event_id='96503', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2307</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96503">Sparse High Rank Adapters</a></strong></h5>


                        <p class="text-muted">
                            Kartikeya Bhardwaj &middot; Nilesh Pandey &middot; Sweta Priyadarshi &middot; Viswanath Ganapathy &middot; Shreya Kadambi &middot; Rafael Esteves &middot; Shubhankar Borse &middot; Paul Whatmough &middot; Risheek Garrepalli &middot; Mart van Baalen &middot; Harris Teague &middot; Markus Nagel
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Low Rank Adaptation (LoRA) has gained massive attention in the recent generative AI research. One of the main advantages of LoRA is its ability to be fused with  pretrained models, adding no overhead during inference. However, from a mobile deployment standpoint, we can either avoid inference overhead in the fused mode but lose the ability to switch adapters rapidly, or suffer significant (up to 30% higher) inference latency while enabling rapid switching in the unfused mode. LoRA also exhibits concept-loss when multiple adapters are used concurrently. In this paper, we propose Sparse High Rank Adapters (SHiRA), a new paradigm which incurs no inference overhead, enables rapid switching, and significantly reduces concept-loss. Specifically, SHiRA can be trained by directly tuning only 1-2% of the base model weights while leaving others unchanged. This results in a highly sparse adapter which can be switched directly in the fused mode. We further provide theoretical and empirical insights on how high sparsity in SHiRA can aid multi-adapter fusion by reducing concept loss. Our extensive experiments on LVMs and LLMs demonstrate that finetuning only a small fraction of the parameters in the base model significantly outperforms LoRA while enabling both rapid switching and multi-adapter fusion. Finally, we provide a latency- and memory-efficient SHiRA implementation based on Parameter-Efficient Finetuning (PEFT) Library which trains at nearly the same speed as LoRA while consuming up to 16% lower peak GPU memory, thus making SHiRA easy to adopt for practical use cases. To demonstrate rapid switching benefits during inference, we show that loading SHiRA on a base model can be 5x-16x faster than LoRA fusion on a CPU.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-165" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-165', event_id='95892', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2308</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95892">Instruction Tuning With Loss Over Instructions</a></strong></h5>


                        <p class="text-muted">
                            Zhengxiang Shi &middot; Adam Yang &middot; Bin Wu &middot; Laurence Aitchison &middot; Emine Yilmaz &middot; Aldo Lipani
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Instruction tuning plays a crucial role in shaping the outputs of language models (LMs) to desired styles. In this work, we propose a simple yet effective method, Instruction Modelling (IM), which trains LMs by applying a loss function to the instruction and prompt part rather than solely to the output part. Through experiments across 21 diverse benchmarks, we show that, in many scenarios, IM can effectively improve the LM performance on both NLP tasks (<em>e.g.,</em> MMLU, TruthfulQA, and HumanEval) and open-ended generation benchmarks (<em>e.g.,</em> MT-Bench and AlpacaEval). Remarkably, in the most advantageous case, IM boosts model performance on AlpacaEval 1.0 by over 100%. We identify two key factors influencing the effectiveness of IM: (1) The ratio between instruction length and output length in the training data; and (2) The number of training examples. We observe that IM is especially beneficial when trained on datasets with lengthy instructions paired with brief outputs, or under the Superficial Alignment Hypothesis (SAH) where a small amount of training examples are used for instruction tuning. Further analysis substantiates our hypothesis that our improvement can be attributed to reduced overfitting to instruction tuning datasets. It is worth noting that we are not proposing \ours as a replacement for the current instruction tuning process.Instead, our work aims to provide practical guidance for instruction tuning LMs, especially in low-resource scenarios.Our code is available at https://github.com/ZhengxiangShi/InstructionModelling.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-166" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-166', event_id='96795', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2309</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96795">Adaptive Experimentation When You Can&#x27;t Experiment</a></strong></h5>


                        <p class="text-muted">
                            Yao Zhao &middot; Kwang-Sung Jun &middot; Tanner Fiez &middot; Lalit Jain
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper introduces the confounded pure exploration transductive linear bandit (CPET-LB) problem. As a motivating example,  often online services cannot directly assign users to specific control or treatment experiences either for business or practical reasons. In these settings, naively comparing treatment and control groups that may result from self-selection can lead to biased estimates of underlying treatment effects. Instead, online services can employ a properly randomized encouragement that incentivizes users toward a specific treatment. Our methodology provides online services with an adaptive experimental design approach for learning the best-performing treatment for such encouragement designs. We consider a more general underlying model captured by a linear structural equation and formulate pure exploration linear bandits in this setting. Though pure exploration has been extensively studied in standard adaptive experimental design settings, we believe this is the first work considering a setting where noise is confounded.  Elimination-style algorithms using experimental design methods in combination with a novel finite-time confidence interval on an instrumental variable style estimator are presented with sample complexity upper bounds nearly matching a minimax lower bound. Finally, experiments are conducted that demonstrate the efficacy of our approach.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-167" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-167', event_id='95625', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2310</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95625">Adversarial SchrÃ¶dinger Bridge Matching</a></strong></h5>


                        <p class="text-muted">
                            Nikita Gushchin &middot; Daniil Selikhanovych &middot; Sergei Kholkin &middot; Evgeny Burnaev &middot; Aleksandr Korotin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The SchrÃ¶dinger Bridge (SB) problem offers a powerful framework for combining optimal transport and diffusion models. A promising recent approach to solve the SB problem is the Iterative Markovian Fitting (IMF) procedure, which alternates between Markovian and reciprocal projections of continuous-time stochastic processes. However, the model built by the IMF procedure has a long inference time due to using many steps of numerical solvers for stochastic differential equations. To address this limitation, we propose a novel Discrete-time IMF (D-IMF) procedure in which learning of stochastic processes is replaced by learning just a few transition probabilities in discrete time. Its great advantage is that in practice it can be naturally implemented using the Denoising Diffusion GAN (DD-GAN), an already well-established adversarial generative modeling technique. We show that our D-IMF procedure can provide the same quality of unpaired domain translation as the IMF, using only several generation steps instead of hundreds.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-168" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-168', event_id='94284', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2311</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94284">Transcendence: Generative Models Can Outperform The Experts That Train Them</a></strong></h5>


                        <p class="text-muted">
                            Edwin Zhang &middot; Vincent Zhu &middot; Naomi Saphra &middot; Anat Kleiman &middot; Benjamin Edelman &middot; Milind Tambe &middot; Sham Kakade &middot; Eran Malach
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Generative models are trained with the simple objective of imitating the conditional probability distribution induced by the data they are trained on. Therefore, when trained on data generated by humans, we may not expect the artificial model to outperform the humans on their original objectives. In this work, we study the phenomenon of <em>transcendence</em>: when a generative model achieves capabilities that surpass the abilities of the experts generating its data. We demonstrate transcendence by training an autoregressive transformer to play chess from game transcripts, and show that the trained model can sometimes achieve better performance than all players in the dataset. We theoretically prove that transcendence is enabled by low-temperature sampling, and rigorously assess this experimentally. Finally, we discuss other sources of transcendence, laying the groundwork for future investigation of this phenomenon in a broader setting.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-169" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-169', event_id='96269', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2400</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96269">InterControl: Zero-shot Human Interaction Generation by Controlling Every Joint</a></strong></h5>


                        <p class="text-muted">
                            Zhenzhi Wang &middot; Jingbo Wang &middot; Yixuan Li &middot; Dahua Lin &middot; Bo Dai
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Text-conditioned motion synthesis has made remarkable progress with the emergence of diffusion models. However, the majority of these motion diffusion models are primarily designed for a single character and overlook multi-human interactions. In our approach, we strive to explore this problem by synthesizing human motion with interactions for a group of characters of any size in a zero-shot manner. The key aspect of our approach is the adaptation of human-wise interactions as pairs of human joints that can be either in contact or separated by a desired distance. In contrast to existing methods that necessitate training motion generation models on multi-human motion datasets with a fixed number of characters, our approach inherently possesses the flexibility to model human interactions involving an arbitrary number of individuals, thereby transcending the limitations imposed by the training data. We introduce a novel controllable motion generation method, InterControl, to encourage the synthesized motions maintaining the desired distance between joint pairs. It consists of a motion controller and an inverse kinematics guidance module that realistically and accurately aligns the joints of synthesized characters to the desired location.  Furthermore, we demonstrate that the distance between joint pairs for human-wise interactions can be generated using an off-the-shelf Large Language Model (LLM). Experimental results highlight the capability of our framework to generate interactions with multiple human characters and its potential to work with off-the-shelf physics-based character simulators. Code is available at https://github.com/zhenzhiwang/intercontrol.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-170" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-170', event_id='96349', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2401</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96349">Graph Diffusion Policy Optimization</a></strong></h5>


                        <p class="text-muted">
                            Yijing Liu &middot; Chao Du &middot; Tianyu Pang &middot; Chongxuan LI &middot; Min Lin &middot; Wei Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent research has made significant progress in optimizing diffusion models for downstream objectives, which is an important pursuit in fields such as graph generation for drug design. However, directly applying these models to graph presents challenges, resulting in suboptimal performance. This paper introduces graph diffusion policy optimization (GDPO), a novel approach to optimize graph diffusion models for arbitrary (e.g., non-differentiable) objectives using reinforcement learning. GDPO is based on an eager policy gradient tailored for graph diffusion models, developed through meticulous analysis and promising improved performance. Experimental results show that GDPO achieves state-of-the-art performance in various graph generation tasks with complex and diverse objectives. Code is available at https://github.com/sail-sg/GDPO.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-171" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-171', event_id='96682', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2402</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96682">Score Distillation via Reparametrized DDIM</a></strong></h5>


                        <p class="text-muted">
                            Artem Lukoianov &middot; Haitz SÃ¡ez de OcÃ¡riz Borde &middot; Kristjan Greenewald &middot; Vitor Guizilini &middot; Timur Bagautdinov &middot; Vincent Sitzmann &middot; Justin Solomon
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While 2D diffusion models generate realistic, high-detail images, 3D shape generation methods like Score Distillation Sampling (SDS)  built on these 2D diffusion models  produce cartoon-like, over-smoothed shapes. To help explain this discrepancy, we show that the image guidance used in Score Distillation can be understood as the velocity field of a 2D denoising generative process, up to the choice of a noise term. In particular, after a change of variables, SDS resembles a high-variance version of Denoising Diffusion Implicit Models (DDIM) with a differently-sampled noise term: SDS introduces noise i.i.d. randomly at each step, while DDIM infers it from the previous noise predictions. This excessive variance can lead to over-smoothing and unrealistic outputs. We show that a better noise approximation can be recovered by inverting DDIM in each SDS update step. This modification makes SDS's generative process for 2D images almost identical to DDIM. In 3D, it removes over-smoothing, preserves higher-frequency detail, and brings the generation quality closer to that of 2D samplers. Experimentally, our method achieves better or similar 3D generation quality compared to other state-of-the-art Score Distillation methods, all without training additional neural networks or multi-view supervision, and providing useful insights into relationship between 2D and 3D asset generation with diffusion models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-172" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-172', event_id='96702', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2403</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96702">Equivariant Neural Diffusion for Molecule Generation</a></strong></h5>


                        <p class="text-muted">
                            FranÃ§ois Cornet &middot; Grigory Bartosh &middot; Mikkel Schmidt &middot; Christian Andersson Naesseth
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce Equivariant Neural Diffusion (END), a novel diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Compared to current state-of-the-art equivariant diffusion models, the key innovation in END lies in its learnable forward process for enhanced generative modelling. Rather than pre-specified, the forward process is parameterized through a time- and data-dependent transformation that is equivariant to rigid transformations.  Through a series of experiments on standard molecule generation benchmarks, we demonstrate the competitive performance of END compared to several strong baselines for both unconditional and conditional generation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-173" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-173', event_id='96707', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2404</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96707">Zero-to-Hero: Enhancing Zero-Shot Novel View Synthesis via Attention Map Filtering</a></strong></h5>


                        <p class="text-muted">
                            Ido Sobol &middot; Chenfeng Xu &middot; Or Litany
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Generating realistic images from arbitrary views based on a single source image remains a significant challenge in computer vision, with broad applications ranging from e-commerce to immersive virtual experiences. Recent advancements in diffusion models, particularly the Zero-1-to-3 model, have been widely adopted for generating plausible views, videos, and 3D models. However, these models still struggle with inconsistencies and implausibility in new views generation, especially for challenging changes in viewpoint. In this work, we propose Zero-to-Hero, a novel test-time approach that enhances view synthesis by manipulating attention maps during the denoising process of Zero-1-to-3. By drawing an analogy between the denoising process and stochastic gradient descent (SGD), we implement a filtering mechanism that aggregates attention maps, enhancing generation reliability and authenticity. This process improves geometric consistency without requiring retraining or significant computational resources. Additionally, we modify the self-attention mechanism to integrate information from the source view, reducing shape distortions. These processes are further supported by a specialized sampling schedule. Experimental results demonstrate substantial improvements in fidelity and consistency, validated on a diverse set of out-of-distribution objects. Additionally, we demonstrate the general applicability and effectiveness of Zero-to-Hero in multi-view, and image generation conditioned on semantic maps and pose.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-174" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-174', event_id='96807', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2405</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96807">IR-CM: The Fast and Universal Image Restoration Method Based on Consistency Model</a></strong></h5>


                        <p class="text-muted">
                            Xiaoxuan Gong &middot; Jie Ma
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper proposes a fast and general-purpose image restoration method. The key idea is to achieve few-step or even one-step inference by conducting consistency distilling or training on a specific mean-reverting stochastic differential equations. Furthermore, based on this, we propose a novel linear-nonlinear decoupling training strategy, significantly enhancing training effectiveness and surpassing consistency distillation on inference performance. This allows our method to be independent of any pre-trained checkpoint, enabling it to serve as an effective standalone image-to-image transformation model. Finally, to avoid trivial solutions and stabilize model training, we introduce a simple origin-guided loss. To validate the effectiveness of our proposed method, we conducted experiments on tasks including image deraining, denoising, deblurring, and low-light image enhancement. The experiments show that our method achieves highly competitive results with only one-step inference. And with just two-step inference, it can achieve state-of-the-art performance in low-light image enhancement. Furthermore, a number of ablation experiments demonstrate the effectiveness of the proposed training strategy. our code is available at https://github.com/XiaoxuanGong/IR-CM.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-175" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-175', event_id='93475', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2406</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93475">Identifiable Object-Centric Representation Learning via Probabilistic Slot Attention</a></strong></h5>


                        <p class="text-muted">
                            Avinash Kori &middot; Francesco Locatello &middot; Ainkaran Santhirasekaram &middot; Francesca Toni &middot; Ben Glocker &middot; Fabio De Sousa Ribeiro
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Learning modular object-centric representations is said to be crucial for systematic generalization. Existing methods show promising object-binding capabilities empirically, but theoretical identifiability guarantees remain relatively underdeveloped. Understanding when object-centric representations can theoretically be identified is important for scaling slot-based methods to high-dimensional images with correctness guarantees. To that end, we propose a probabilistic slot-attention algorithm that imposes an <em>aggregate</em> mixture prior over object-centric slot representations, thereby providing slot identifiability guarantees without supervision, up to an equivalence relation. We provide empirical verification of our theoretical identifiability result using both simple 2-dimensional data and high-resolution imaging datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-176" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-176', event_id='93507', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2407</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93507">HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning</a></strong></h5>


                        <p class="text-muted">
                            Chunlin Tian &middot; Zhan Shi &middot; Zhijiang Guo &middot; Li Li &middot; Cheng-Zhong Xu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA. However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets. This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance. Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA. Building on these insights, we have developed HydraLoRA, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise. Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases. Our anonymous codes are submitted with the paper and will be publicly available. Code is available: https://github.com/Clin0212/HydraLoRA.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-177" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-177', event_id='93777', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2408</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93777">Are More LLM Calls All You Need? Towards the Scaling Properties of Compound AI Systems</a></strong></h5>


                        <p class="text-muted">
                            Lingjiao Chen &middot; Jared Quincy Davis &middot; Boris Hanin &middot; Peter Bailis &middot; Ion Stoica &middot; Matei A Zaharia &middot; James Zou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Language Model (LM) calls and aggregate their responses. However, there is little understanding of how the number of LM calls -- e.g., when asking the LM to answer each question multiple times and taking a majority vote -- affects such a compound system's performance. In this paper, we initiate the study of scaling properties of compound inference systems. We analyze, theoretically and empirically, how the number of LM calls affects the performance of Vote and Filter-Vote, two of the simplest compound system designs, which aggregate LM responses via majority voting, optionally applying LM filters. We find, surprisingly, that across multiple language tasks, the performance of both Vote and Filter-Vote can first increase but then decrease as a function of the number of LM calls. Our theoretical results suggest that this non-monotonicity is due to the diversity of query difficulties within a task: more LM calls lead to higher performance on "easy" queries, but lower performance on "hard" queries, and non-monotone behavior can emerge when a task contains both types of queries. This insight then allows us to compute, from a small number of samples, the number of LM calls that maximizes system performance, and define an analytical scaling model for both systems. Experiments show that our scaling model can accurately predict the performance of Vote and Filter-Vote systems and thus find the optimal number of LM calls to make.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-178" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-178', event_id='93922', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2409</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93922">Physically Compatible 3D Object Modeling from a Single Image</a></strong></h5>


                        <p class="text-muted">
                            Minghao Guo &middot; Bohan Wang &middot; Pingchuan Ma &middot; Tianyuan Zhang &middot; Crystal Owens &middot; Chuang Gan &middot; Josh Tenenbaum &middot; Kaiming He &middot; Wojciech Matusik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present a computational framework that transforms single images into 3D physical objects. The visual geometry of a physical object in an image is determined by three orthogonal attributes: mechanical properties, external forces, and rest-shape geometry. Existing single-view 3D reconstruction methods often overlook this underlying composition, presuming rigidity or neglecting external forces. Consequently, the reconstructed objects fail to withstand real-world physical forces, resulting in instability or undesirable deformation -- diverging from their intended designs as depicted in the image. Our optimization framework addresses this by embedding physical compatibility into the reconstruction process. We explicitly decompose the three physical attributes and link them through static equilibrium, which serves as a hard constraint, ensuring that the optimized physical shapes exhibit desired physical behaviors. Evaluations on a dataset collected from Objaverse demonstrate that our framework consistently enhances the physical realism of 3D models over existing methods. The utility of our framework extends to practical applications in dynamic simulations and 3D printing, where adherence to physical compatibility is paramount.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-179" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-179', event_id='94183', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2410</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94183">Harmonizing Visual Text Comprehension and Generation</a></strong></h5>


                        <p class="text-muted">
                            Zhen Zhao &middot; Jingqun Tang &middot; Binghong Wu &middot; Chunhui Lin &middot; Shu Wei &middot; Hao Liu &middot; Xin Tan &middot; zhizhong zhang &middot; Can Huang &middot; Yuan Xie
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this work, we present TextHarmony, a unified and versatile multimodal generative model proficient in comprehending and generating visual text. Simultaneously generating images and texts typically results in performance degradation due to the inherent inconsistency between vision and language modalities. To overcome this challenge, existing approaches resort to modality-specific data for supervised fine-tuning, necessitating distinct model instances. We propose Slide-LoRA, which dynamically aggregates modality-specific and modality-agnostic LoRA experts, partially decoupling the multimodal generation space. Slide-LoRA harmonizes the generation of vision and language within a singular model instance, thereby facilitating a more unified generative process. Additionally, we develop a high-quality image caption dataset, DetailedTextCaps-100K, synthesized with a sophisticated closed-source MLLM to enhance visual text generation capabilities further. Comprehensive experiments across various benchmarks demonstrate the effectiveness of the proposed approach. Empowered by Slide-LoRA, TextHarmony achieves comparable performance to modality-specific fine-tuning results with only a 2% increase in parameters and shows an average improvement of 2.5% in visual text comprehension tasks and 4.0% in visual text generation tasks. Our work delineates the viability of an integrated approach to multimodal generation within the visual text domain, setting a foundation for subsequent inquiries. Code is available at https://github.com/bytedance/TextHarmony.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-180" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-180', event_id='94221', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2411</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94221">Metric Flow Matching for Smooth Interpolations on the Data Manifold</a></strong></h5>


                        <p class="text-muted">
                            Kacper Kapusniak &middot; Peter Potaptchik &middot; Teodora Reu &middot; Leo Zhang &middot; Alexander Tong &middot; Michael Bronstein &middot; Joey Bose &middot; Francesco Di Giovanni
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Matching objectives underpin the success of modern generative models and rely on constructing conditional paths that transform a source distribution into a target distribution. Despite being a fundamental building block, conditional paths have been designed principally under the assumption of $\textit{Euclidean geometry}$, resulting in straight interpolations. However, this can be particularly restrictive for tasks such as trajectory inference, where straight paths might lie outside the data manifold, thus failing to capture the underlying dynamics giving rise to the observed marginals. In this paper, we propose Metric Flow Matching (MFM), a novel simulation-free framework for conditional flow matching where interpolants are approximate geodesics learned by minimizing the kinetic energy of a data-induced Riemannian metric. This way, the generative model matches vector fields on the data manifold, which corresponds to lower uncertainty and more meaningful interpolations. We prescribe general metrics to instantiate MFM, independent of the task, and test it on a suite of challenging problems including LiDAR navigation, unpaired image translation, and modeling cellular dynamics. We observe that MFM outperforms the Euclidean baselines, particularly achieving SOTA on single-cell trajectory prediction.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-181" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-181', event_id='96267', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2500</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96267">DEFT: Efficient Fine-tuning of Diffusion Models by Learning the Generalised $h$-transform</a></strong></h5>


                        <p class="text-muted">
                            Alexander Denker &middot; Francisco Vargas &middot; Shreyas Padhy &middot; Kieran Didi &middot; Simon Mathis &middot; Riccardo Barbano &middot; Vincent Dutordoir &middot; Emile Mathieu &middot; Urszula Julia Komorowska &middot; Pietro LiÃ³
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for conditional sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling.Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood Doob's h-transform. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional $h$-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On image reconstruction tasks, we achieve speedups of up to 1.6$\times$, while having the best perceptual quality on natural images and reconstruction performance on medical images. Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-182" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-182', event_id='96100', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2501</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96100">Interpreting the Weight Space of Customized Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Amil Dravid &middot; Yossi Gandelsman &middot; Kuan-Chieh Wang &middot; Rameen Abdal &middot; Gordon Wetzstein &middot; Alexei Efros &middot; Kfir Aberman
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We investigate the space of weights spanned by a large collection of customized diffusion models. We populate this space by creating a dataset of over 60,000 models, each of which is a base model fine-tuned to insert a different person's visual identity. We model the underlying manifold of these weights as a subspace, which we term $\textit{weights2weights}$. We demonstrate three immediate applications of this space that result in new diffusion models -- sampling, editing, and inversion. First, sampling a set of weights from this space results in a new model encoding a novel identity. Next, we find linear directions in this space corresponding to semantic edits of the identity (e.g., adding a beard), resulting in a new model with the original identity edited. Finally, we show that inverting a single image into this space encodes a realistic identity into a model, even if the input image is out of distribution (e.g., a painting). We further find that these linear properties of the diffusion model weight space extend to other visual concepts. Our results indicate that the weight space of fine-tuned diffusion models can behave as an interpretable $\textit{meta}$-latent space producing new models.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-183" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-183', event_id='96029', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2502</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96029">Aligning Target-Aware Molecule Diffusion Models with Exact Energy Optimization</a></strong></h5>


                        <p class="text-muted">
                            Siyi Gu &middot; Minkai Xu &middot; Alexander Powers &middot; Weili Nie &middot; Tomas Geffner &middot; Karsten Kreis &middot; Jure Leskovec &middot; Arash Vahdat &middot; Stefano Ermon
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Generating ligand molecules for specific protein targets, known as structure-based drug design, is a fundamental problem in therapeutics development and biological discovery. Recently, target-aware generative models, especially diffusion models, have shown great promise in modeling protein-ligand interactions and generating candidate drugs. However, existing models primarily focus on learning the chemical distribution of all drug candidates, which lacks effective steerability on the chemical quality of model generations. In this paper, we propose a novel and general alignment framework to align pretrained target diffusion models with preferred functional properties, named AliDiff. AliDiff shifts the target-conditioned chemical distribution towards regions with higher binding affinity and structural rationality, specified by user-defined reward functions, via the preference optimization approach. To avoid the overfitting problem in common preference optimization objectives, we further develop an improved Exact Energy Preference Optimization method to yield an exact and efficient alignment of the diffusion models, and provide the closed-form expression for the converged distribution. Empirical studies on the CrossDocked2020 benchmark show that AliDiff can generate molecules with state-of-the-art binding energies with up to -7.07 Avg. Vina Score, while maintaining strong molecular properties. Code is available at https://github.com/MinkaiXu/AliDiff.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-184" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-184', event_id='95942', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2503</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95942">Constrained Synthesis with Projected Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Jacob K Christopher &middot; Stephen Baek &middot; Nando Fioretto
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper introduces an approach to endow generative diffusion processes the ability to satisfy and certify compliance with constraints and physical principles. The proposed method recast the traditional sampling process of generative diffusion models as a constrained optimization problem, steering the generated data distribution to remain within a specified region to ensure adherence to the given constraints.These capabilities are validated on applications featuring both convex and challenging, non-convex, constraints as well as ordinary differential equations, in domains spanning from synthesizing new materials with precise morphometric properties, generating physics-informed motion, optimizing paths in planning scenarios, and human motion synthesis.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-185" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-185', event_id='94841', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2504</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94841">Provable and Efficient Dataset Distillation for Kernel Ridge Regression</a></strong></h5>


                        <p class="text-muted">
                            Yilan Chen &middot; Wei Huang &middot; Lily Weng
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Deep learning models are now trained on increasingly larger datasets, making it crucial to reduce computational costs and improve data quality. Dataset distillation aims to distill a large dataset into a small synthesized dataset such that models trained on it can achieve similar performance to those trained on the original dataset. While there have been many empirical efforts to improve dataset distillation algorithms, a thorough theoretical analysis and provable, efficient algorithms are still lacking. In this paper, by focusing on dataset distillation for kernel ridge regression (KRR), we show that one data point per class is already necessary and sufficient to recover the original model's performance in many settings. For linear ridge regression and KRR with surjective feature mappings, we provide necessary and sufficient conditions for the distilled dataset to recover the original model's parameters. For KRR with injective feature mappings of deep neural networks, we show that while one data point per class is not sufficient in general, $k+1$ data points can be sufficient for deep linear neural networks, where $k$ is the number of classes. Our theoretical results enable directly constructing analytical solutions for distilled datasets, resulting in a provable and efficient dataset distillation algorithm for KRR. We verify our theory experimentally and show that our algorithm outperforms previous work such as KIP while being significantly more efficient, e.g. 15840$\times$ faster on CIFAR-100.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-186" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-186', event_id='95622', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2505</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95622">Simple and Effective Masked Diffusion Language Models</a></strong></h5>


                        <p class="text-muted">
                            Subham Sahoo &middot; Marianne Arriola &middot; Aaron Gokaslan &middot; Edgar Marroquin &middot; Alexander Rush &middot; Yair Schiff &middot; Justin Chiu &middot; Volodymyr Kuleshov
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive (AR) methods in language modeling.In this work, we show that simple masked discrete diffusion is more performant than previously thought.We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements.Our objective has a simple form&mdash;it is a mixture of classical masked language modeling losses&mdash;and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model.On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches AR perplexity. We provide the code, along with a blog post and video tutorial on the project page: https://s-sahoo.com/mdlm</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-187" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-187', event_id='95462', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2506</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95462">TFG: Unified Training-Free Guidance for Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Haotian Ye &middot; Haowei Lin &middot; Jiaqi Han &middot; Minkai Xu &middot; Sheng Liu &middot; Yitao Liang &middot; Jianzhu Ma &middot; James Zou &middot; Stefano Ermon
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Given an unconditional diffusion model and a predictor for a target property of interest (e.g., a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training. Existing methods, though effective in various individual applications, often lack theoretical grounding and rigorous testing on extensive benchmarks. As a result, they could even fail on simple tasks, and applying them to a new problem becomes unavoidably difficult. This paper introduces a novel algorithmic framework encompassing existing methods as special cases, unifying the study of training-free guidance into the analysis of an algorithm-agnostic design space. Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy that can be readily applied to any downstream task. We systematically benchmark across 7 diffusion models on 16 tasks with 40 targets, and improve performance by 8.5% on average. Our framework and benchmark offer a solid foundation for conditional generation in a training-free manner.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-188" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-188', event_id='95365', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2507</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95365">CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching</a></strong></h5>


                        <p class="text-muted">
                            DONGZHI JIANG &middot; Guanglu Song &middot; Xiaoshi Wu &middot; Renrui Zhang &middot; Dazhong Shen &middot; ZHUOFAN ZONG &middot; Yu Liu &middot; Hongsheng Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion models have demonstrated great success in the field of text-to-image generation. However, alleviating the misalignment between the text prompts and images is still challenging. We break down the problem into two causes: concept ignorance and concept mismapping. To tackle the two challenges, we propose CoMat, an end-to-end diffusion model fine-tuning strategy with the image-to-text concept matching mechanism. Firstly, we introduce a novel image-to-text concept activation module to guide the diffusion model in revisiting ignored concepts. Additionally, an attribute concentration module is proposed to map the text conditions of each entity to its corresponding image area correctly. Extensive experimental evaluations, conducted across three distinct text-to-image alignment benchmarks, demonstrate the superior efficacy of our proposed method, CoMat-SDXL, over the baseline model, SDXL~\cite{podell2023sdxl}. We also show that our method enhances general condition utilization capability and generalizes to the long and complex prompt despite not specifically training on it.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-189" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-189', event_id='94953', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2508</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94953">DreamSteerer: Enhancing Source Image Conditioned Editability using Personalized Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Zhengyang Yu &middot; Zhaoyuan Yang &middot; Jing Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent text-to-image (T2I) personalization methods have shown great premise in teaching a diffusion model user-specified concepts given a few images for reusing the acquired concepts in a novel context. With massive efforts being dedicated to personalized generation, a promising extension is personalized editing, namely to edit an image using personalized concepts, which can provide more precise guidance signal than traditional textual guidance. To address this, one straightforward solution is to incorporate a personalized diffusion model with a text-driven editing framework. However, such solution often shows unsatisfactory editability on the source image. To address this, we propose DreamSteerer, a plug-in method for augmenting existing T2I personalization methods. Specifically, we enhance the source image conditioned editability of a personalized diffusion model via a novel Editability Driven Score Distillation (EDSD) objective. Moreover, we identify a mode trapping issue with EDSD, and propose a mode shifting regularization with spatial feature guided sampling to avoid such issue. We further employ two key modifications on the Delta Denoising Score framework that enable high-fidelity local editing with personalized concepts. Extensive experiments validate that DreamSteerer can significantly improve the editability of several T2I personalization baselines while being computationally efficient.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-190" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-190', event_id='94930', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2509</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94930">Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models</a></strong></h5>


                        <p class="text-muted">
                            Sangwoong Yoon &middot; Himchan Hwang &middot; Dohyun Kwon &middot; Yung-Kyun Noh &middot; Frank Park
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present a maximum entropy inverse reinforcement learning (IRL) approach for improving the sample quality of diffusion generative models, especially when the number of generation time steps is small. Similar to how IRL trains a policy based on the reward function learned from expert demonstrations, we train (or fine-tune) a diffusion model using the log probability density estimated from training data. Since we employ an energy-based model (EBM) to represent the log density, our approach boils down to the joint training of a diffusion model and an EBM. Our IRL formulation, named Diffusion by Maximum Entropy IRL (DxMI), is a minimax problem that reaches equilibrium when both models converge to the data distribution. The entropy maximization plays a key role in DxMI, facilitating the exploration of the diffusion model and ensuring the convergence of the EBM. We also propose Diffusion by Dynamic Programming (DxDP), a novel reinforcement learning algorithm for diffusion models, as a subroutine in DxMI. DxDP makes the diffusion model update in DxMI efficient by transforming the original problem into an optimal control formulation where value functions replace back-propagation in time. Our empirical studies show that diffusion models fine-tuned using DxMI can generate high-quality samples in as few as 4 and 10 steps.  Additionally, DxMI enables the training of an EBM without MCMC, stabilizing EBM training dynamics and enhancing anomaly detection performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-191" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-191', event_id='94802', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2510</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94802">Stochastic Optimal Control for Diffusion Bridges in Function Spaces</a></strong></h5>


                        <p class="text-muted">
                            Byoungwoo Park &middot; Jungwon Choi &middot; Sungbin Lim &middot; Juho Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Recent advancements in diffusion models and diffusion bridges primarily focus on finite-dimensional spaces, yet many real-world problems necessitate operations in infinite-dimensional function spaces for more natural and interpretable formulations. In this paper, we present a theory of stochastic optimal control (SOC) tailored to infinite-dimensional spaces, aiming to extend diffusion-based algorithms to function spaces. Specifically, we demonstrate how Doobâs $h$-transform, the fundamental tool for constructing diffusion bridges, can be derived from the SOC perspective and expanded to infinite dimensions. This expansion presents a challenge, as infinite-dimensional spaces typically lack closed-form densities. Leveraging our theory, we establish that solving the optimal control problem with a specific objective function choice is equivalent to learning diffusion-based generative models. We propose two applications: 1) learning bridges between two infinite-dimensional distributions and 2) generative models for sampling from an infinite-dimensional distribution. Our approach proves effective for diverse problems involving continuous function space representations, such as resolution-free images, time-series data, and probability density functions.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-192" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-192', event_id='94781', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2511</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94781">Self-Refining Diffusion Samplers: Enabling Parallelization via Parareal Iterations</a></strong></h5>


                        <p class="text-muted">
                            Nikil Selvam &middot; Amil Merchant &middot; Stefano Ermon
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In diffusion models, samples are generated through an iterative refinement process, requiring hundreds of sequential model evaluations. Several recent methods have introduced approximations (fewer discretization steps or distillation) to trade off speed at the cost of sample quality. In contrast, we introduce Self-Refining Diffusion Samplers (SRDS) that retain sample quality and can improve latency at the cost of additional parallel compute. We take inspiration from the Parareal algorithm, a popular numerical method for parallel-in-time integration of differential equations. In SRDS, a quick but rough estimate of a sample is first created and then iteratively refined in parallel through Parareal iterations. SRDS is not only guaranteed to accurately solve the ODE and converge to the serial solution but also benefits from parallelization across the diffusion trajectory, enabling batched inference and pipelining. As we demonstrate for pre-trained diffusion models, the early convergence of this refinement procedure drastically reduces the number of steps required to produce a sample, speeding up generation for instance by up to 1.7x on a 25-step StableDiffusion-v2 benchmark and up to 4.3x on longer trajectories.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-193" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-193', event_id='93165', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2600</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93165">Diffusion Models are Certifiably Robust Classifiers</a></strong></h5>


                        <p class="text-muted">
                            Huanran Chen &middot; Yinpeng Dong &middot; Shitong Shao &middot; Hao Zhongkai &middot; Xiao Yang &middot; Hang Su &middot; Jun Zhu
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Generative learning, recognized for its effective modeling of data distributions, offers inherent advantages in handling out-of-distribution instances, especially for enhancing robustness to adversarial attacks. Among these, diffusion classifiers, utilizing powerful diffusion models, have demonstrated superior empirical robustness. However, a comprehensive theoretical understanding of their robustness is still lacking, raising concerns about their vulnerability to stronger future attacks. In this study, we prove that diffusion classifiers possess $O(1)$ Lipschitzness, and establish their certified robustness, demonstrating their inherent resilience. To achieve non-constant Lipschitzness, thereby obtaining much tighter certified robustness, we generalize diffusion classifiers to classify Gaussian-corrupted data. This involves deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. Experimental results show the superior certified robustness of these Noised Diffusion Classifiers (NDCs). Notably, we achieve over 80\% and 70\% certified robustness on CIFAR-10 under adversarial perturbations with \(\ell_2\) norms less than 0.25 and 0.5, respectively, using a single off-the-shelf diffusion model without any additional data.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-194" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-194', event_id='95468', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2601</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95468">Towards Global Optimal Visual In-Context Learning Prompt Selection</a></strong></h5>


                        <p class="text-muted">
                            Chengming Xu &middot; Chen Liu &middot; Yikai Wang &middot; Yuan Yao &middot; Yanwei Fu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Visual In-Context Learning (VICL) is a prevailing way to transfer visual foundation models to new tasks by leveraging contextual information contained in in-context examples to enhance learning and prediction of query sample. The fundamental problem in VICL is how to select the best prompt to activate its power as much as possible, which is equivalent to the ranking problem to test the in-context behavior of each candidate in the alternative set and select the best one. To utilize more appropriate ranking metric and leverage more comprehensive information among the alternative set, we propose a novel in-context example selection framework to approximately identify the global optimal prompt, i.e. choosing the best performing in-context examples from all alternatives for each query sample. Our method, dubbed Partial2Global, adopts a transformer-based list-wise ranker to provide a more comprehensive comparison within several alternatives, and a consistency-aware ranking aggregator to generate globally consistent ranking. The effectiveness of Partial2Global is validated through experiments on foreground segmentation, single object detection and image colorization, demonstrating that Partial2Global selects consistently better in-context examples compared with other methods, and thus establish the new state-of-the-arts.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-195" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-195', event_id='93374', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2602</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93374">Adapting Diffusion Models for Improved Prompt Compliance and Controllable Image Synthesis</a></strong></h5>


                        <p class="text-muted">
                            Deepak Sridhar &middot; Abhishek Peri &middot; Rohith Rachala &middot; Nuno Vasconcelos
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Recent advances in generative modeling with diffusion processes (DPs) enabled breakthroughs in image synthesis. Despite impressive image quality, these models have various prompt compliance problems, including low recall in generating multiple objects, difficulty in generating text in images, and meeting constraints like object locations and pose. For fine-grained editing and manipulation, they also require fine-grained semantic or instance maps that are tedious to produce manually. While prompt compliance can be enhanced by addition of loss functions at inference, this is time consuming and does not scale to complex scenes.   To overcome these limitations, this work introduces a new family of  $\textit{Factor Graph Diffusion Models}$ (FG-DMs) that models the joint distribution of images and conditioning variables, such as semantic, sketch, depth or normal maps via a factor graph decomposition. This joint structure has several advantages, including support for efficient sampling based prompt compliance schemes, which produce images of high object recall, semi-automated fine-grained editing, explainability at intermediate levels, ability to produce labeled datasets for the training of downstream models such as segmentation or depth, training with missing data, and continual learning where new conditioning variables can be added with minimal or no modifications to the existing structure. We propose an implementation of FG-DMs by adapting a pre-trained Stable Diffusion (SD) model to implement all FG-DM factors, using only COCO dataset, and show that it is effective in generating images with 15\% higher recall than SD while retaining its generalization ability. We introduce an attention distillation loss that encourages consistency among the attention maps of all factors, improving the fidelity of the generated conditions and image. We also show that training FG-DMs from scratch on MM-CelebA-HQ, Cityscapes, ADE20K, and COCO produce images of high quality (FID) and diversity (LPIPS).</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-196" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-196', event_id='93464', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2603</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93464">Fast samplers for Inverse Problems in Iterative Refinement models</a></strong></h5>


                        <p class="text-muted">
                            Kushagra Pandey &middot; Ruihan Yang &middot; Stephan Mandt
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Constructing fast samplers for unconditional diffusion and flow-matching models has received much attention recently; however, existing methods for solving <em>inverse problems</em>, such as super-resolution, inpainting, or deblurring, still require hundreds to thousands of iterative steps to obtain high-quality results. We propose a plug-and-play framework for constructing efficient samplers for inverse problems, requiring only <em>pre-trained</em> diffusion or flow-matching models. We present <em>Conditional Conjugate Integrators</em>, which leverage the specific form of the inverse problem to project the respective conditional diffusion/flow dynamics into a more amenable space for sampling. Our method complements popular posterior approximation methods for solving inverse problems using diffusion/flow models. We evaluate the proposed method's performance on various linear image restoration tasks across multiple datasets, employing diffusion and flow-matching models. Notably, on challenging inverse problems like 4x super-resolution on the ImageNet dataset, our method can generate high-quality samples in as few as <em>5</em> conditional sampling steps and outperforms competing baselines requiring 20-1000 steps. Our code will be publicly available at https://github.com/mandt-lab/c-pigdm.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-197" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-197', event_id='93756', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2604</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93756">LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Seyedmorteza Sadat &middot; Jakob Buhmann &middot; Derek Bradley &middot; Otmar Hilliges &middot; Romann M Weber
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Advances in latent diffusion models (LDMs) have revolutionized high-resolution image generation, but the design space of the autoencoder that is central to these systems remains underexplored. In this paper, we introduce LiteVAE, a new autoencoder design for LDMs, which leverages the 2D discrete wavelet transform to enhance scalability and computational efficiency over standard variational autoencoders (VAEs) with no sacrifice in output quality. We investigate the training methodologies and the decoder architecture of LiteVAE and propose several enhancements that improve the training dynamics and reconstruction quality. Our base LiteVAE model matches the quality of the established VAEs in current LDMs with a six-fold reduction in encoder parameters, leading to faster training and lower GPU memory requirements, while our larger model outperforms VAEs of comparable complexity across all evaluated metrics (rFID, LPIPS, PSNR, and SSIM).</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-198" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-198', event_id='93758', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2605</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93758">Improving the Training of Rectified Flows</a></strong></h5>


                        <p class="text-muted">
                            Sangyun Lee &middot; Zinan Lin &middot; Giulia Fanti
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Diffusion models have shown great promise for image and video generation, but sampling from state-of-the-art models requires expensive numerical integration of a generative ODE.    One approach for tackling this problem is rectified flows, which iteratively learn smooth ODE paths that are less susceptible to truncation error.    However, rectified flows still require a relatively large number of function evaluations (NFEs).    In this work, we propose improved techniques for training rectified flows, allowing them to compete with knowledge distillation methods even in the low NFE setting.    Our main insight is that under realistic settings, a single iteration of the Reflow algorithm for training rectified flows is sufficient to learn nearly straight trajectories; hence, the current practice of using multiple Reflow iterations is unnecessary.    We thus propose techniques to improve one-round training of rectified flows, including a U-shaped timestep distribution and LPIPS-Huber premetric.    With these techniques, we improve the FID of the previous 2-rectified flow by up to 75\% in the 1 NFE setting on CIFAR-10.    On ImageNet 64$\times$64, our improved rectified flow outperforms the state-of-the-art distillation methods    such as consistency distillation and progressive distillation in both one-step and two-step settings and rivals the performance of improved consistency training (iCT) in FID.    Code is available at https://github.com/sangyun884/rfpp.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-199" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-199', event_id='93906', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2606</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93906">Immiscible Diffusion: Accelerating Diffusion Training with Noise Assignment</a></strong></h5>


                        <p class="text-muted">
                            Yiheng Li &middot; Heyang Jiang &middot; Akio Kodaira &middot; Masayoshi TOMIZUKA &middot; Kurt Keutzer &middot; Chenfeng Xu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we point out that suboptimal noise-data mapping leads to slow training of diffusion models. During diffusion training, current methods diffuse each image across the entire noise space, resulting in a mixture of all images at every point in the noise layer. We emphasize that this random mixture of noise-data mapping complicates the optimization of the denoising function in diffusion models. Drawing inspiration from the immiscibility phenomenon in physics, we propose <em>Immiscible Diffusion</em>, a simple and effective method to improve the random mixture of noise-data mapping. In physics, miscibility can vary according to various intermolecular forces. Thus, immiscibility means that the mixing of molecular sources is distinguishable. Inspired by this concept, we propose an assignment-then-diffusion training strategy to achieve <em>Immiscible Diffusion</em>. As one example, prior to diffusing the image data into noise, we assign diffusion target noise for the image data by minimizing the total image-noise pair distance in a mini-batch. The assignment functions analogously to external forces to expel the diffuse-able areas of images, thus mitigating the inherent difficulties in diffusion training. Our approach is remarkably simple, requiring only <em>one line of code</em> to restrict the diffuse-able area for each image while preserving the Gaussian distribution of noise. In this way, each image is preferably projected to nearby noise. To address the high complexity of the assignment algorithm, we employ a quantized assignment strategy, which significantly reduces the computational overhead to a negligible level (e.g. 22.8ms for a large batch size of 1024 on an A6000). Experiments demonstrate that our method can achieve up to 3x faster training for unconditional Consistency Models on the CIFAR dataset, as well as for DDIM and Stable Diffusion on CelebA and ImageNet dataset, and in class-conditional training and fine-tuning. In addition, we conducted a thorough analysis that sheds light on how it improves diffusion training speed while improving fidelity. The code is available at https://yhli123.github.io/immiscible-diffusion</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-200" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-200', event_id='94225', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2607</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94225">AdjointDEIS: Efficient Gradients for Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Zander W. Blasingame &middot; Chen Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The optimization of the latents and parameters of diffusion models with respect to some differentiable metric defined on the output of the model is a challenging and complex problem. The sampling for diffusion models is done by solving either the <em>probability flow</em> ODE or diffusion SDE wherein a neural network approximates the score function allowing a numerical ODE/SDE solver to be used. However, naive backpropagation techniques are memory intensive, requiring the storage of all intermediate states, and face additional complexity in handling the injected noise from the diffusion term of the diffusion SDE. We propose a novel family of bespoke ODE solvers to the continuous adjoint equations for diffusion models, which we call <em>AdjointDEIS</em>. We exploit the unique construction of diffusion SDEs to further simplify the formulation of the continuous adjoint equations using <em>exponential integrators</em>. Moreover, we provide convergence order guarantees for our bespoke solvers. Significantly, we show that continuous adjoint equations for diffusion SDEs actually simplify to a simple ODE. Lastly, we demonstrate the effectiveness of AdjointDEIS for guided generation with an adversarial attack in the form of the face morphing problem. Our code will be released on our project page <a href="https://zblasingame.github.io/AdjointDEIS/">https://zblasingame.github.io/AdjointDEIS/</a></p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-201" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-201', event_id='94238', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2608</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94238">Leveraging Drift to Improve Sample Complexity of Variance Exploding Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Ruofeng Yang &middot; Zhijie Wang &middot; Bo Jiang &middot; Shuai Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Variance exploding (VE) based diffusion models, an important class of diffusion models, have shown state-of-the-art (SOTA) performance. However, only a few theoretical works analyze VE-based models, and those works suffer from a worse forward convergence rate $1/\text{poly}(T)$ than the $\exp{(-T)}$ of variance preserving (VP) based models, where $T$ is the forward diffusion time and the rate measures the distance between forward marginal distribution $q_T$ and pure Gaussian noise. The slow rate is due to the Brownian Motion without a drift term. In this work, we design a new drifted VESDE forward process, which allows a faster $\exp{(-T)}$ forward convergence rate. With this process, we achieve the first efficient polynomial sample complexity for a series of VE-based models with reverse SDE under the manifold hypothesis. Furthermore, unlike previous works, we allow the diffusion coefficient to be unbounded instead of a constant, which is closer to the SOTA models. Besides the reverse SDE, the other common reverse process is the probability flow ODE  (PFODE) process, which is deterministic and enjoys faster sample speed. To deepen the understanding of VE-based models, we consider a more general setting considering reverse SDE and PFODE simultaneously, propose a unified tangent-based analysis framework, and prove the first quantitative convergence guarantee for SOTA VE-based models with reverse PFODE.We also show that the drifted VESDE can balance different error terms and improve generated samples without training through synthetic and real-world experiments.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-202" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-202', event_id='94416', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2609</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94416">Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy</a></strong></h5>


                        <p class="text-muted">
                            Hancheng Ye &middot; Jiakang Yuan &middot; Renqiu Xia &middot; Xiangchao Yan &middot; Tao Chen &middot; Junchi Yan &middot; Botian Shi &middot; Bo Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion models have recently achieved great success in the synthesis of high-quality images and videos. However, the existing denoising techniques in diffusion models are commonly based on step-by-step noise predictions, which suffers from high computation cost, resulting in a prohibitive latency for interactive applications. In this paper, we propose AdaptiveDiffusion to relieve this bottleneck by adaptively reducing the noise prediction steps during the denoising process. Our method considers the potential of skipping as many noise prediction steps as possible while keeping the final denoised results identical to the original full-step ones. Specifically, the skipping strategy is guided by the third-order latent difference that indicates the stability between timesteps during the denoising process, which benefits the reusing of previous noise prediction results. Extensive experiments on image and video diffusion models demonstrate that our method can significantly speed up the denoising process while generating identical results to the original process, achieving up to an average 2-5x speedup without quality degradation. The code is available at https://github.com/UniModal4Reasoning/AdaptiveDiffusion</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-203" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-203', event_id='94674', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2610</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94674">Discrete-state Continuous-time Diffusion for Graph Generation</a></strong></h5>


                        <p class="text-muted">
                            Zhe Xu &middot; Ruizhong Qiu &middot; Yuzhong Chen &middot; Huiyuan Chen &middot; Xiran Fan &middot; Menghai Pan &middot; Zhichen Zeng &middot; Mahashweta Das &middot; Hanghang Tong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graph is a prevalent discrete data structure, whose generation has wide applications such as drug discovery and circuit design. Diffusion generative models, as an emerging research focus, have been applied to graph generation tasks. Overall, according to the space of states and time steps, diffusion generative models can be categorized into discrete-/continuous-state discrete-/continuous-time fashions. In this paper, we formulate the graph diffusion generation in a discrete-state continuous-time setting, which has never been studied in previous graph diffusion models. The rationale of such a formulation is to preserve the discrete nature of graph-structured data and meanwhile provide flexible sampling trade-offs between sample quality and efficiency. Analysis shows that our training objective is closely related to the generation quality and our proposed generation framework enjoys ideal invariant/equivariant properties concerning the permutation of node ordering. Our proposed model shows competitive empirical performance against other state-of-the-art graph generation solutions on various benchmarks while at the same time can flexibly trade off the generation quality and efficiency in the sampling phase.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-204" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-204', event_id='94763', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2611</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94763">MoLE: Enhancing Human-centric Text-to-image Diffusion via Mixture of  Low-rank Experts</a></strong></h5>


                        <p class="text-muted">
                            Jie Zhu &middot; Yixiong Chen &middot; Mingyu Ding &middot; Ping Luo &middot; Leye Wang &middot; Jingdong Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Text-to-image diffusion has attracted vast attention due to its impressive image-generation capabilities. However, when it comes to human-centric text-to-image generation, particularly in the context of faces and hands, the results often fall short of naturalness due to insufficient training priors. We alleviate the issue in this work from two perspectives. 1) From the data aspect, we carefully collect a human-centric dataset comprising over one million high-quality human-in-the-scene images and two specific sets of close-up images of faces and hands. These datasets collectively provide a rich prior knowledge base to enhance the human-centric image generation capabilities of the diffusion model. 2) On the methodological front, we propose a simple yet effective method called Mixture of Low-rank Experts (MoLE) by considering low-rank modules trained on close-up hand and face images respectively as experts. This concept draws inspiration from our observation of low-rank refinement, where a low-rank module trained by a customized close-up dataset has the potential to enhance the corresponding image part when applied at an appropriate scale. To validate the superiority of MoLE in the context of human-centric image generation compared to state-of-the-art, we construct two benchmarks and perform evaluations with diverse metrics and human studies. Datasets, model, and code are released at https://sites.google.com/view/mole4diffuser/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-205" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-205', event_id='93104', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2700</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93104">Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation</a></strong></h5>


                        <p class="text-muted">
                            Lingxiao Zhao &middot; Xueying Ding &middot; Leman Akoglu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely un-ordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each blockâs probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN (Maronet al., 2019). Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-206" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-206', event_id='93029', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2701</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93029">Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion</a></strong></h5>


                        <p class="text-muted">
                            Boyuan Chen &middot; Diego MartÃ­ MonsÃ³ &middot; Yilun Du &middot; Max Simchowitz &middot; Russ Tedrake &middot; Vincent Sitzmann
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture,  and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. Project website: https://boyuan.space/diffusion-forcing/</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-207" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-207', event_id='96962', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2702</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96962">On the Noise Robustness of In-Context Learning for Text Generation</a></strong></h5>


                        <p class="text-muted">
                            hongfu gao &middot; Feipeng Zhang &middot; Wenyu Jiang &middot; Jun Shu &middot; Feng Zheng &middot; Hongxin Wei
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) have shown impressive performance on downstream tasks by in-context learning (ICL), which heavily relies on the quality of demonstrations selected from a large set of annotated examples. Recent works claim that in-context learning is robust to noisy demonstrations in text classification. In this work, we show that, on text generation tasks, noisy annotations significantly hurt the performance of in-context learning. To circumvent the issue, we propose a simple and effective approach called Local Perplexity Ranking (LPR), which replaces the "noisy" candidates with their nearest neighbors that are more likely to be clean. Our method is motivated by analyzing the perplexity deviation caused by noisy labels and decomposing perplexity into inherent perplexity and matching perplexity. Our key idea behind LPR is thus to decouple the matching perplexity by performing the ranking among the neighbors in semantic space. Our approach can prevent the selected demonstrations from including mismatched input-label pairs while preserving the effectiveness of the original selection methods. Extensive experiments demonstrate the effectiveness of LPR, improving the EM score by up to 18.75 on common benchmarks with noisy annotations.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-208" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-208', event_id='96206', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2703</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96206">BERTs are Generative In-Context Learners</a></strong></h5>


                        <p class="text-muted">
                            David Samuel
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While in-context learning is commonly associated with causal language models, such as GPT, we demonstrate that this capability also 'emerges' in masked language models. Through an embarrassingly simple inference technique, we enable an existing masked model, DeBERTa, to perform generative tasks without additional training or architectural changes. Our evaluation reveals that the masked and causal language models behave very differently, as they clearly outperform each other on different categories of tasks. These complementary strengths suggest that the field's focus on causal models for in-context learning may be limiting â both architectures can develop these capabilities, but with distinct advantages; pointing toward promising hybrid approaches that combine the strengths of both objectives.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-209" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-209', event_id='96107', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2704</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96107">Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving</a></strong></h5>


                        <p class="text-muted">
                            Aniket Didolkar &middot; Anirudh Goyal &middot; Nan Rosemary Ke &middot; Siyuan Guo &middot; Michal Valko &middot; Timothy Lillicrap &middot; Danilo Jimenez Rezende &middot; Yoshua Bengio &middot; Michael Mozer &middot; Sanjeev Arora
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>\emph{Metacognitive knowledge} refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also  have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure  to get a powerful  LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans.To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH.  (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label.  This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic,  even though this article applies it to math problems.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-210" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-210', event_id='95875', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2705</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95875">Universal In-Context Approximation By Prompting Fully Recurrent Models</a></strong></h5>


                        <p class="text-muted">
                            Aleksandar Petrov &middot; Tom Lamb &middot; Alasdair Paren &middot; Philip Torr &middot; Adel Bibi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Zero-shot and in-context learning enable solving tasks without model fine-tuning, making them essential for developing generative model solutions. Therefore, it is crucial to understand whether a pretrained model can be prompted to approximate any function, i.e., whether it is a universal in-context approximator. While it was recently shown that transformer models do possess this property, these results rely on their attention mechanism. Hence, these findings do not apply to fully recurrent architectures like RNNs, LSTMs, and the increasingly popular SSMs. We demonstrate that RNNs, LSTMs, GRUs, Linear RNNs, and linear gated architectures such as Mamba and Hawk/Griffin can also serve be universal in-context approximators. To streamline our argument, we introduce a programming language called LSRL that compiles to these fully recurrent architectures. LSRL may be of independent interest for further studies of fully recurrent models, such as constructing interpretability benchmarks. We also study the role of multiplicative gating and observe that architectures incorporating such gating (e.g., LSTMs, GRUs, Hawk/Griffin) can implement certain operations more stably, making them more viable candidates for practical in-context universal approximation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-211" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-211', event_id='96545', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2707</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96545">MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures</a></strong></h5>


                        <p class="text-muted">
                            Jinjie Ni &middot; Fuzhao Xue &middot; Xiang Yue &middot; Yuntian Deng &middot; Mahir Shah &middot; Kabir Jain &middot; Graham Neubig &middot; Yang You
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Evaluating large language models (LLMs) is challenging. Traditional ground-truth- based benchmarks fail to capture the comprehensiveness and nuance of real-world queries, while LLM-as-judge benchmarks suffer from grading biases and limited query quantity. Both of them may also become contaminated over time. User- facing evaluation, such as Chatbot Arena, provides reliable signals but is costly and slow. In this work, we propose MixEval, a new paradigm for establishing efficient, gold-standard LLM evaluation by strategically mixing off-the-shelf bench- marks. It bridges (1) comprehensive and well-distributed real-world user queries and (2) efficient and fairly-graded ground-truth-based benchmarks, by matching queries mined from the web with similar queries from existing benchmarks. Based on MixEval, we further build MixEval-Hard, which offers more room for model improvement. Our benchmarksâ advantages lie in (1) a 0.96 model ranking correlation with Chatbot Arena arising from the highly impartial query distribution and grading mechanism, (2) fast, cheap, and reproducible execution (6% of the time and cost of MMLU), and (3) dynamic evaluation enabled by the rapid and stable data update pipeline. We provide extensive meta-evaluation and analysis for our and existing LLM benchmarks to deepen the communityâs understanding of LLM evaluation and guide future research directions.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-212" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-212', event_id='95348', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2708</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95348">Questioning the Survey Responses of Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Ricardo Dominguez-Olmedo &middot; Moritz Hardt &middot; Celestine Mendler-DÃ¼nner
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Surveys have recently gained popularity as a tool to study large language models. By comparing modelsâ survey responses to those of different human reference populations, researchers aim to infer the demographics, political opinions, or values best represented by current language models. In this work, we critically examine language models' survey responses on the basis of the well-established American Community Survey by the U.S. Census Bureau. Evaluating 43 different language models using de-facto standard prompting methodologies, we establish two dominant patterns. First, models' responses are governed by ordering and labeling biases, for example, towards survey responses labeled with the letter âAâ. Second, when adjusting for these systematic biases through randomized answer ordering, models across the board trend towards uniformly random survey responses, irrespective of model size or training data. As a result, models consistently appear to better represent subgroups whose aggregate statistics are closest to uniform for the survey under consideration, leading to potentially misguided conclusions about model alignment.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-213" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-213', event_id='93820', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2709</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93820">Cascade Speculative Drafting for Even Faster LLM Inference</a></strong></h5>


                        <p class="text-muted">
                            Ziyi Chen &middot; Xiaocong Yang &middot; Jiacheng Lin &middot; Chenkai Sun &middot; Kevin Chang &middot; Jie Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The <em>Vertical Cascade</em> eliminates autoregressive generation from neural models, while the <em>Horizontal Cascade</em> optimizes time allocation in drafting for improved efficiency. Combining both cascades, CS Drafting achieves greater speedup compared to the baselines in our experiments, while preserving the same output distribution as the target model. Our code is publicly available at https://github.com/lfsszd/CS-Drafting.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-214" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-214', event_id='95225', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2710</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95225">Learning Distributions on Manifolds with Free-Form Flows</a></strong></h5>


                        <p class="text-muted">
                            Peter Sorrenson &middot; Felix Draxler &middot; Armand Rousselot &middot; Sander Hummerich &middot; Ullrich KÃ¶the
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose Manifold Free-Form Flows (M-FFF), a simple new generative model for data on manifolds. The existing approaches to learning a distribution on arbitrary manifolds are expensive at inference time, since sampling requires solving a differential equation. Our method overcomes this limitation by sampling in a single function evaluation. The key innovation is to optimize a neural network via maximum likelihood on the manifold, possible by adapting the free-form flow framework to Riemannian manifolds. M-FFF is straightforwardly adapted to any manifold with a known projection. It consistently matches or outperforms previous single-step methods specialized to specific manifolds. It is typically two orders of magnitude faster than multi-step methods based on diffusion or flow matching, achieving better likelihoods in several experiments. We provide our code at https://github.com/vislearn/FFF.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-215" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-215', event_id='95067', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2711</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95067">Learning Goal-Conditioned Representations for Language Reward Models</a></strong></h5>


                        <p class="text-muted">
                            Vaskar Nath &middot; Dylan Slack &middot; Jeff Da &middot; Yuntao Ma &middot; Hugh Zhang &middot; Spencer Whitehead &middot; Sean Hendryx
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Techniques that learn improved representations via offline data or self-supervised objectives have shown impressive results in traditional reinforcement learning.Nevertheless, it is unclear how improved representation learning can benefit reinforcement learning from human feedback on language models.In this work, we propose training reward models (RMs) in a contrastive, $\textit{goal-conditioned}$ fashion by increasing the representation similarity of future states along sampled preferred trajectories and decreasing the similarity along randomly sampled dispreferred trajectories.This objective significantly improves reward model performance by up to 0.09 AUROC across challenging benchmarks, such as MATH and GSM8k. These findings extend to general alignment as well -- on the Helpful-Harmless dataset, we observe 2.3\% increase in accuracy.Beyond improving reward model performance, we show this way of training RM representations enables improved steerability because it allows us to evaluate the likelihood of an action achieving a particular goal-state (e.g. whether a solution is correct or helpful).Leveraging this insight, we find that we can filter up to 55\% of generated tokens during majority voting by discarding trajectories likely to end up in an "incorrect" state, which leads to significant cost savings.We additionally find that these representations can perform fine-grained control by conditioning on desired future goal-states.For example, we show that steering a Llama 3 model towards helpful generations with our approach improves helpfulness by $9.6$\% over a supervised-fine-tuning trained baseline.Similarly, steering the model towards complex generations improves complexity by $21.6$\% over the baseline.Overall, we find that training RMs in this contrastive, goal-conditioned fashion significantly improves performance and enables model steerability.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-216" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-216', event_id='93575', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2800</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93575">Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought</a></strong></h5>


                        <p class="text-muted">
                            Qiguang Chen &middot; Libo Qin &middot; Jiaqi Wang &middot; Jingxuan Zhou &middot; Wanxiang Che
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Chain-of-Thought (CoT) reasoning has emerged as a promising approach for enhancing the performance of large language models (LLMs) on complex reasoning tasks. Recently, a series of studies attempt to explain the mechanisms underlying CoT, aiming to deepen the understanding of its efficacy. Nevertheless, the existing research faces two major challenges: (1) a lack of quantitative metrics to assess CoT capabilities and (2) a dearth of guidance on optimizing CoT performance. Motivated by this, in this work, we introduce a novel reasoning boundary framework (RBF) to address these challenges. To solve the lack of quantification, we first define a reasoning boundary (RB) to quantify the upper-bound of CoT and establish a combination law for RB, enabling a practical quantitative approach applicable to various real-world CoT tasks. To address the lack of optimization, we propose three categories of RBs. We further optimize these categories with combination laws focused on RB promotion and reasoning path optimization for CoT improvement. Through extensive experiments on 27 models and 5 tasks, the study validates the existence and rationality of the proposed framework. Furthermore, it explains the effectiveness of 10 CoT strategies and guides optimization from two perspectives. We hope this work can provide a comprehensive understanding of the boundaries and optimization strategies for reasoning in LLMs. Our code and data are available at https://github.com/LightChen233/reasoning-boundary.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-217" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-217', event_id='93967', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2801</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93967">AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback</a></strong></h5>


                        <p class="text-muted">
                            Jian Guan &middot; Wei Wu &middot; zujie wen &middot; Peng Xu &middot; Hongning Wang &middot; Minlie Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM)that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets, enabling AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism. The code and data are publicly available athttps://github.com/JianGuanTHU/AMOR.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-218" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-218', event_id='94906', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2802</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94906">Learning Better Representations From Less Data For Propositional Satisfiability</a></strong></h5>


                        <p class="text-muted">
                            Mohamed Ghanem &middot; Frederik Schmitt &middot; Julian Siber &middot; Bernd Finkbeiner
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Training neural networks on NP-complete problems typically demands very large amounts of training data and often needs to be coupled with computationally expensive symbolic verifiers to ensure output correctness. In this paper, we present NeuRes, a neuro-symbolic approach to address both challenges for propositional satisfiability, being the quintessential NP-complete problem. By combining certificate-driven training and expert iteration, our model learns better representations than models trained for classification only, with a much higher data efficiency -- requiring orders of magnitude less training data. NeuRes employs propositional resolution as a proof system to generate proofs of unsatisfiability and to accelerate the process of finding satisfying truth assignments, exploring both possibilities in parallel. To realize this, we propose an attention-based architecture that autoregressively selects pairs of clauses from a dynamic formula embedding to derive new clauses. Furthermore, we employ expert iteration whereby model-generated proofs progressively replace longer teacher proofs as the new ground truth. This enables our model to reduce a dataset of proofs generated by an advanced solver by $\sim$$32$% after training on it with no extra guidance. This shows that NeuRes is not limited by the optimality of the teacher algorithm owing to its self-improving workflow. We show that our model achieves far better performance than NeuroSAT in terms of both correctly classified and proven instances.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-219" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-219', event_id='93452', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2803</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93452">Improving Deep Learning Optimization through Constrained Parameter Regularization</a></strong></h5>


                        <p class="text-muted">
                            JÃ¶rg Franke &middot; Michael Hefenbrock &middot; Gregor Koehler &middot; Frank Hutter
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Regularization is a critical component in deep learning. The most commonly used approach, weight decay, applies a constant penalty coefficient uniformly across all parameters. This may be overly restrictive for some parameters, while insufficient for others. To address this, we present Constrained Parameter Regularization (CPR) as an alternative to traditional weight decay. Unlike the uniform application of a single penalty, CPR enforces an upper bound on a statistical measure, such as the L$_2$-norm, of individual parameter matrices. Consequently, learning becomes a constraint optimization problem, which we tackle using an adaptation of the augmented Lagrangian method. CPR introduces only a minor runtime overhead and only requires setting an upper bound. We propose simple yet efficient mechanisms for initializing this bound, making CPR rely on no hyperparameter or one, akin to weight decay. Our empirical studies on computer vision and language modeling tasks demonstrate CPR's effectiveness. The results show that CPR can outperform traditional weight decay and increase performance in pre-training and fine-tuning.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-220" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-220', event_id='95563', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2804</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95563">Chain of Agents: Large Language Models Collaborating on Long-Context Tasks</a></strong></h5>


                        <p class="text-muted">
                            Yusen Zhang &middot; Ruoxi Sun &middot; Yanfei Chen &middot; Tomas Pfister &middot; Rui Zhang &middot; Sercan Arik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Addressing the challenge of effectively processing long contexts has become a critical issue for Large Language Models (LLMs). Two common strategies have emerged: 1) reducing the input length, such as retrieving relevant chunks by Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit of LLMs. However, both strategies have drawbacks: input reduction has no guarantee of covering the part with needed information, while window extension struggles with focusing on the pertinent information for solving the task. To mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks. CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, followed by a manager agent who synthesizes these contributions into a coherent final output. CoA processes the entire input by interleaving reading and reasoning, and it mitigates long context focus issues by assigning each agent a short context. We perform a comprehensive evaluation of CoA on a wide range of long-context tasks in question answering, summarization, and code completion, demonstrating significant improvements by up to 10% over strong baselines of RAG, Full-Context, and multi-agent LLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-221" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-221', event_id='96089', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2805</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96089">Recursive Introspection: Teaching Language Model Agents How to Self-Improve</a></strong></h5>


                        <p class="text-muted">
                            Yuxiao Qu &middot; Tianjun Zhang &middot; Naman Garg &middot; Aviral Kumar
                        </p>

                    </div>
                    <div class="abstract">
                        <p>A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially. In this paper, we develop $\textbf{RISE:}$ $\textbf{R}$ecursive $\textbf{I}$ntro$\textbf{S}$p$\textbf{E}$ction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation and offline reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models, without disrupting one-turn abilities as a result of expressing more complex distributions.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-222" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-222', event_id='96156', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2806</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96156">Mind&#x27;s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Wenshan Wu &middot; Shaoguang Mao &middot; Yadong Zhang &middot; Yan Xia &middot; Li Dong &middot; Lei Cui &middot; Furu Wei
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as the Mind's Eye, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate mental images to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs. Please find the dataset and codes in our <a href="https://microsoft.github.io/visualization-of-thought">project page</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-223" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-223', event_id='96659', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2807</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96659">Iterative Reasoning Preference Optimization</a></strong></h5>


                        <p class="text-muted">
                            Richard Yuanzhe Pang &middot; Weizhe Yuan &middot; He He &middot; Kyunghyun Cho &middot; Sainbayar Sukhbaatar &middot; Jason Weston
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Iterative preference optimization methods have recently been shown to perform well for general instruction tuning tasks, but typically make little improvement on reasoning tasks. In this work we develop an iterative approach that optimizes the preference between competing generated Chain-of-Thought (CoT) candidates by optimizing for winning vs. losing reasoning steps. We train using a modified DPO loss with an additional negative log-likelihood term, which we find to be crucial. We show reasoning improves across repeated iterations of this scheme. While only relying on examples in the training set, our approach results in increasing accuracy on GSM8K, MATH, and ARC-Challenge for Llama-2-70B-Chat, outperforming other Llama-2-based models not relying on additionally sourced datasets. For example, we see a large improvement from 55.6% to 81.6% on GSM8K and an accuracy of 88.7% with majority voting out of 32 samples.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-224" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-224', event_id='96701', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2808</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96701">TableRAG: Million-Token Table Understanding with Language Models</a></strong></h5>


                        <p class="text-muted">
                            Si-An Chen &middot; Lesly Miculicich &middot; Julian Eisenschlos &middot; Zifeng Wang &middot; Zilong Wang &middot; Yanfei Chen &middot; YASUHISA FUJII &middot; Hsuan-Tien Lin &middot; Chen-Yu Lee &middot; Tomas Pfister
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advancements in language models (LMs) have notably enhanced their ability to reason with tabular data, primarily through program-aided mechanisms that manipulate and analyze tables.However, these methods often require the entire table as input, leading to scalability challenges due to the positional bias or context length constraints.In response to these challenges, we introduce TableRAG, a Retrieval-Augmented Generation (RAG) framework specifically designed for LM-based table understanding.TableRAG leverages query expansion combined with schema and cell retrieval to pinpoint crucial information  before providing it to the LMs.This enables more efficient data encoding and precise retrieval, significantly reducing prompt lengths and mitigating information loss.We have developed two new million-token benchmarks from the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's effectiveness at scale.Our results demonstrate that TableRAG's retrieval design achieves the highest retrieval quality, leading to the new state-of-the-art performance on large-scale table understanding.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-225" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-225', event_id='97749', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2809</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97749">TACT: Advancing Complex Aggregative Reasoning with Information Extraction Tools</a></strong></h5>


                        <p class="text-muted">
                            Avi Caciularu &middot; Alon Jacovi &middot; Eyal Ben-David &middot; Sasha Goldshtein &middot; Tal Schuster &middot; Jonathan Herzig &middot; Gal Elidan &middot; Amir Globerson
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large Language Models (LLMs) often do not perform well on queries that require the aggregation of information across texts. To better evaluate this setting and facilitate modeling efforts, we introduce TACT - Text And Calculations through Tables, a dataset crafted to evaluate LLMs' reasoning and computational abilities using complex instructions. TACT contains challenging instructions that demand stitching information scattered across one or more texts, and performing complex integration on this information to generate the answer. We construct this dataset by leveraging an existing dataset of texts and their associated tables. For each such tables, we formulate new queries, and gather their respective answers. We demonstrate that all contemporary LLMs perform poorly on this dataset, achieving an accuracy below 38%. To pinpoint the difficulties and thoroughly dissect the problem, we analyze model performance across three components: table-generation, Pandas command-generation, and execution. Unexpectedly, we discover that each component presents substantial challenges for current LLMs. These insights lead us to propose a focused modeling framework, which we refer to as <em>IE as a tool</em>. Specifically, we propose to add "tools" for each of the above steps, and implement each such tool with few-shot prompting. This approach shows an improvement over existing prompting techniques, offering a promising direction for enhancing model capabilities in these tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-226" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-226', event_id='96722', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2810</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96722">Boosting Text-to-Video Generative Model with MLLMs Feedback</a></strong></h5>


                        <p class="text-muted">
                            Xun Wu &middot; Shaohan Huang &middot; Guolong Wang &middot; Jing Xiong &middot; Furu Wei
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advancements in text-to-video generative models, such as Sora, have showcased impressive capabilities. These models have attracted significant interest for their potential applications. However, they often rely on extensive datasets of variable quality, which can result in generated videos that lack aesthetic appeal and do not accurately reflect the input text prompts. A promising approach to mitigate these issues is to leverage Reinforcement Learning from Human Feedback (RLHF), which aims to align the outputs of text-to-video generative with human preferences. However, the considerable costs associated with manual annotation have led to a scarcity of comprehensive preference datasets. In response to this challenge, our study begins by investigating the efficacy of Multimodal Large Language Models (MLLMs) generated annotations in capturing video preferences, discovering a high degree of concordance with human judgments. Building upon this finding, we utilize MLLMs to perform fine-grained video preference annotations across two dimensions, resulting in the creation of VideoPrefer, which includes 135,000 preference annotations. Utilizing this dataset, we introduce VideoRM, the first general-purpose reward model tailored for video preference in the text-to-video domain. Our comprehensive experiments confirm the effectiveness of both VideoPrefer and VideoRM, representing a significant step forward in the field.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-227" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-227', event_id='94417', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2811</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94417">Generative Forests</a></strong></h5>


                        <p class="text-muted">
                            Richard Nock &middot; Mathieu Guillame-Bert
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We focus on generative AI for a type of data that still represent one of the most prevalent form of data: tabular data. We introduce a new powerful class of forest-based models fit for such tasks and a simple training algorithm with strong convergence guarantees in a boosting model that parallels that of the original weak / strong supervised learning setting. This algorithm can be implemented by a few tweaks to the most popular induction scheme for decision tree induction (<em>i.e. supervised learning</em>) with two classes. Experiments on the quality of generated data display substantial improvements compared to the state of the art. The losses our algorithm minimize and the structure of our models make them practical for related tasks that require fast estimation of a density given a generative model and an observation (even partially specified): such tasks include missing data imputation and density estimation. Additional experiments on these tasks reveal that our models can be notably good contenders to diverse state of the art methods, relying on models as diverse as (or mixing elements of) trees, neural nets, kernels or graphical models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-228" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-228', event_id='93176', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2900</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93176">Can Language Models Learn to Skip Steps?</a></strong></h5>


                        <p class="text-muted">
                            Tengxiao Liu &middot; Qipeng Guo &middot; Xiangkun Hu &middot; Cheng Jiayang &middot; Yue Zhang &middot; Xipeng Qiu &middot; Zheng Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Trained on vast corpora of human language, language models demonstrate emergent human-like reasoning abilities. Yet they are still far from true intelligence, which opens up intriguing opportunities to explore the parallels of humans and model behaviors. In this work, we study the ability to skip steps in reasoningâa hallmark of human expertise developed through practice. Unlike humans, who may skip steps to enhance efficiency or to reduce cognitive load, models do not inherently possess such motivations to minimize reasoning steps. To address this, we introduce a controlled framework that stimulates step-skipping behavior by iteratively refining models to generate shorter and accurate reasoning paths. Empirical results indicate that models can develop the step skipping ability under our guidance. Moreover, after fine-tuning on expanded datasets that include both complete and skipped reasoning sequences, the models can not only resolve tasks with increased efficiency without sacrificing accuracy, but also exhibit comparable and even enhanced generalization capabilities in out-of-domain scenarios. Our work presents the first exploration into human-like step-skipping ability and provides fresh perspectives on how such cognitive abilities can benefit AI models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-229" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-229', event_id='97519', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2901</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97519">Can Large Language Models Analyze Graphs like Professionals? A Benchmark and Dataset</a></strong></h5>


                        <p class="text-muted">
                            Xin Li &middot; Weize Chen &middot; Qizhi Chu &middot; Haopeng Li &middot; Zhaojun Sun &middot; Ran Li &middot; Chen Qian &middot; Yiwei Wei &middot; Chuan Shi &middot; Zhiyuan Liu &middot; Maosong Sun &middot; Cheng Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The need to analyze graphs is ubiquitous across various fields, from social networks to biological research and recommendation systems. Therefore, enabling the ability of large language models (LLMs) to process graphs is an important step toward more advanced general intelligence. However, current LLM benchmarks on graph analysis require models to directly reason over the prompts describing graph topology, and are thus limited to small graphs with only a few dozens of nodes. In contrast, human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales. To this end, a question naturally arises: can LLMs analyze graphs like professionals? In this paper, we introduce GraphPro, a manually crafted benchmark containing 3 categories of graph tasks. The benchmark expects solutions based on programming instead of directly reasoning over raw inputs. Our findings reveal that the performance of current LLMs is unsatisfactory, with the best model achieving only 36% accuracy. To bridge this gap, we propose LLM4Graph datasets, which include crawled documents and auto-generated codes based on 6 widely used graph libraries. By augmenting closed-source LLMs with document retrieval and fine-tuning open-source ones on the codes, we show 11-32% absolute improvements in their accuracies. Our results underscore that the capabilities of LLMs in handling structured data are still under-explored, and show the effectiveness of LLM4Graph in enhancing LLMs' proficiency of graph analysis.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-230" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-230', event_id='96932', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2902</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96932">GFT: Graph Foundation Model with Transferable Tree Vocabulary</a></strong></h5>


                        <p class="text-muted">
                            Zehong Wang &middot; Zheyuan Zhang &middot; Nitesh Chawla &middot; Chuxu Zhang &middot; Yanfang Ye
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Inspired by the success of foundation models in applications such as ChatGPT, as graph data has been ubiquitous, one can envision the far-reaching impacts that can be brought by Graph Foundation Models (GFMs) with broader applications in the areas such as scientific research, social network analysis, drug discovery, and e-commerce. Despite the significant progress of pre-trained graph neural networks, there havenât been GFMs that can achieve desired performance on various graph-learning-related tasks. Building GFMs may rely on a vocabulary that encodes transferable patterns shared among different tasks and domains. Unlike image and text, defining such transferable patterns for graphs remains an open question. In this paper, we aim to bridge this gap by rethinking the transferable patterns on graphs as computation trees -- i.e., tree structures derived from the message-passing process. Based on this insight, we propose a cross-task, cross-domain graph foundation model named GFT, short for Graph Foundation model with transferable Tree vocabulary. By treating computation trees as tokens within the transferable vocabulary, GFT improves model generalization and reduces the risk of negative transfer. The theoretical analyses and extensive experimental studies have demonstrated the transferability of computation trees and shown the effectiveness of GFT across diverse tasks and domains in graph learning. The open source code and data are available at https://github.com/Zehong-Wang/GFT.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-231" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-231', event_id='96775', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2903</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96775">Geometry Awakening: Cross-Geometry Learning Exhibits Superiority over Individual Structures</a></strong></h5>


                        <p class="text-muted">
                            YADONG SUN &middot; Xiaofeng Cao &middot; Yu Wang &middot; Wei Ye &middot; Jingcai Guo &middot; Qing Guo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent research has underscored the efficacy of Graph Neural Networks (GNNs) in modeling diverse geometric structures within graph data. However, real-world graphs typically exhibit geometrically heterogeneous characteristics, rendering the confinement to a single geometric paradigm insufficient for capturing their intricate structural complexities. To address this limitation, we examine the performance of GNNs across various geometries through the lens of knowledge distillation (KD) and introduce a novel cross-geometric framework. This framework encodes graphs by integrating both Euclidean and hyperbolic geometries in a space-mixing fashion. Our approach employs multiple teacher models, each generating hint embeddings that encapsulate distinct geometric properties. We then implement a structure-wise knowledge transfer module that optimally leverages these embeddings within their respective geometric contexts, thereby enhancing the training efficacy of the student model. Additionally, our framework incorporates a geometric optimization network designed to bridge the distributional disparities among these embeddings. Experimental results demonstrate that our model-agnostic framework more effectively captures topological graph knowledge, resulting in superior performance of the student models when compared to traditional KD methodologies.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-232" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-232', event_id='96687', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2905</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96687">Towards Dynamic Message Passing on Graphs</a></strong></h5>


                        <p class="text-muted">
                            Junshu Sun &middot; Chenxue Yang &middot; Xiangyang Ji &middot; Qingming Huang &middot; Shuhui Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Message passing plays a vital role in graph neural networks (GNNs) for effective feature learning. However, the over-reliance on input topology diminishes the efficacy of message passing and restricts the ability of GNNs. Despite efforts to mitigate the reliance, existing study encounters message-passing bottlenecks or high computational expense problems, which invokes the demands for flexible message passing with low complexity. In this paper, we propose a novel dynamic message-passing mechanism for GNNs. It projects graph nodes and learnable pseudo nodes into a common space with measurable spatial relations between them. With nodes moving in the space, their evolving relations facilitate flexible pathway construction for a dynamic message-passing process. Associating pseudo nodes to input graphs with their measured relations, graph nodes can communicate with each other intermediately through pseudo nodes under linear complexity. We further develop a GNN model named $\mathtt{N^2}$ based on our dynamic message-passing mechanism. $\mathtt{N^2}$ employs a single recurrent layer to recursively generate the displacements of nodes and construct optimal dynamic pathways. Evaluation on eighteen benchmarks demonstrates the superior performance of $\mathtt{N^2}$ over popular GNNs. $\mathtt{N^2}$ successfully scales to large-scale benchmarks and requires significantly fewer parameters for graph classification with the shared recurrent layer.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-233" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-233', event_id='96514', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2906</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96514">Graph neural networks and non-commuting operators</a></strong></h5>


                        <p class="text-muted">
                            Mauricio Velasco &middot; Kaiying O&amp;#x27;Hare &middot; Bernardo Rychtenberg &middot; Soledad Villar
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graph neural networks (GNNs) provide state-of-the-art results in a wide variety of tasks which typically involve predicting features at the vertices of a graph. They are built from layers of graph convolutions which serve as a powerful inductive bias for describing the flow of information among the vertices. Often, more than one data modality is available. This work considers a setting in which several graphs have the same vertex set and a common vertex-level learning task. This generalizes standard GNN models to GNNs with several graph operators that do not commute. We may call this model graph-tuple neural networks (GtNN). In this work, we develop the mathematical theory to address the stability and transferability of GtNNs using properties of non-commuting non-expansive operators. We develop a limit theory of graphon-tuple neural networks and use it to prove a universal transferability theorem that guarantees that all graph-tuple neural networks are transferable on convergent graph-tuple sequences. In particular, there is no non-transferable energy under the convergence we consider here. Our theoretical results extend well-known transferability theorems for GNNs to the case of several simultaneous graphs (GtNNs) and provide a strict improvement on what is currently known even in the GNN case.We illustrate our theoretical results with simple experiments on synthetic and real-world data. To this end, we derive a training procedure that provably enforces the stability of the resulting model.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-234" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-234', event_id='96491', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2907</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96491">Energy-based Epistemic Uncertainty for Graph Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Dominik Fuchsgruber &middot; Tom WollschlÃ¤ger &middot; Stephan GÃ¼nnemann
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In domains with interdependent data, such as graphs, quantifying the epistemic uncertainty of a Graph Neural Network (GNN) is challenging as uncertainty can arise at different structural scales. Existing techniques neglect this issue or only distinguish between structure-aware and structure-agnostic uncertainty without combining them into a single measure. We propose GEBM, an energy-based model (EBM) that provides high-quality uncertainty estimates by aggregating energy at different structural levels that naturally arise from graph diffusion. In contrast to logit-based EBMs, we provably induce an integrable density in the data space by regularizing the energy function. We introduce an evidential interpretation of our EBM that significantly improves the predictive robustness of the GNN. Our framework is a simple and effective post hoc method applicable to any pre-trained GNN that is sensitive to various distribution shifts. It consistently achieves the best separation of in-distribution and out-of-distribution data on 6 out of 7 anomaly types while having the best average rank over shifts on <em>all</em> datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-235" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-235', event_id='96433', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2908</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96433">Dissecting the Failure of Invariant Learning on Graphs</a></strong></h5>


                        <p class="text-muted">
                            Qixun Wang &middot; Yifei Wang &middot; Yisen Wang &middot; Xianghua Ying
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Enhancing node-level Out-Of-Distribution (OOD) generalization on graphs remains a crucial area. In this paper, we develop a Structural Causal Model (SCM) to theoretically dissect the performance of two prominent invariant learning methods--Invariant Risk Minimization (IRM) and Variance-Risk Extrapolation (VREx)--in node-level OOD settings. Our analysis reveals a critical limitation: these methods may struggle to identify invariant features due to the complexities introduced by the message-passing mechanism, which can obscure causal features within a range of neighboring samples. To address this, we propose Cross-environment Intra-class Alignment (CIA), which explicitly eliminates spurious features by aligning representations within the same class, bypassing the need for explicit knowledge of underlying causal patterns. To adapt CIA to node-level OOD scenarios where environment labels are hard to obtain, we further propose CIA-LRA (Localized Reweighting Alignment) that leverages the distribution of neighboring labels to selectively align node representations, effectively distinguishing and preserving invariant features while removing spurious ones, all without relying on environment labels. We theoretically prove CIA-LRA's effectiveness by deriving an OOD generalization error bound based on PAC-Bayesian analysis. Experiments on graph OOD benchmarks validate the superiority of CIA and CIA-LRA, marking a significant advancement in node-level OOD generalization.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-236" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-236', event_id='96325', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2909</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96325">Enhancing Chess Reinforcement Learning with Graph Representation</a></strong></h5>


                        <p class="text-muted">
                            Tomas Rigaux &middot; Hisashi Kashima
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Mastering games is a hard task, as games can be extremely complex, and still fundamentally different in structure from one another. While the AlphaZero algorithm has demonstrated an impressive ability to learn the rules and strategy of a large variety of games, ranging from Go and Chess, to Atari games, its reliance on extensive computational resources and rigid Convolutional Neural Network (CNN) architecture limits its adaptability and scalability. A model trained to play on a $19\times 19$ Go board cannot be used to play on a smaller $13\times 13$ board, despite the similarity between the two Go variants.In this paper, we focus on Chess, and explore using a more generic Graph-based Representation of a game state, rather than a grid-based one, to introduce a more general architecture based on Graph Neural Networks (GNN). We also expand the classical Graph Attention Network (GAT) layer to incorporate edge-features, to naturally provide a generic policy output format.Our experiments, performed on smaller networks than the initial AlphaZero paper, show that this new architecture outperforms previous architectures with a similar number of parameters, being able to increase playing strength an order of magnitude faster. We also show that the model, when trained on a smaller $5\times 5$ variant of chess, is able to be quickly fine-tuned to play on regular $8\times 8$ chess, suggesting that this approach yields promising generalization abilities.Our code is available at https://github.com/akulen/AlphaGateau.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-237" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-237', event_id='95967', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2910</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95967">Replay-and-Forget-Free Graph Class-Incremental Learning: A Task Profiling and Prompting Approach</a></strong></h5>


                        <p class="text-muted">
                            Chaoxi Niu &middot; Guansong Pang &middot; Ling Chen &middot; Bing Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Class-incremental learning (CIL) aims to continually learn a sequence of tasks, with each task consisting of a set of unique classes. Graph CIL (GCIL) follows the same setting but needs to deal with graph tasks (e.g., node classification in a graph). The key characteristic of CIL lies in the absence of task identifiers (IDs) during inference, which causes a significant challenge in separating classes from different tasks (i.e., inter-task class separation). Being able to accurately predict the task IDs can help address this issue, but it is a challenging problem. In this paper, we show theoretically that accurate task ID prediction on graph data can be achieved by a Laplacian smoothing-based graph task profiling approach, in which each graph task is modeled by a task prototype based on Laplacian smoothing over the graph. It guarantees that the task prototypes of the same graph task are nearly the same with a large smoothing step, while those of different tasks are distinct due to differences in graph structure and node attributes. Further, to avoid the catastrophic forgetting of the knowledge learned in previous graph tasks, we propose a novel graph prompting approach for GCIL which learns a small discriminative graph prompt for each task, essentially resulting in a separate classification model for each task. The prompt learning requires the training of a single graph neural network (GNN) only once on the first task, and no data replay is required thereafter, thereby obtaining a GCIL model being both replay-free and forget-free. Extensive experiments on four GCIL benchmarks show that i) our task prototype-based method can achieve 100% task ID prediction accuracy on all four datasets, ii) our GCIL model significantly outperforms state-of-the-art competing methods by at least 18% in average CIL accuracy, and iii) our model is fully free of forgetting on the four datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-238" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-238', event_id='95879', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#2911</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95879">What Is Missing For Graph Homophily? Disentangling Graph Homophily For Graph Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Yilun Zheng &middot; Sitao Luan &middot; Lihui Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Graph homophily refers to the phenomenon that connected nodes tend to share similar characteristics. Understanding this concept and its related metrics is crucial for designing effective Graph Neural Networks (GNNs). The most widely used homophily metrics, such as edge or node homophily, quantify such "similarity" as label consistency across the graph topology. These metrics are believed to be able to reflect the performance of GNNs, especially on node-level tasks. However, many recent studies have empirically demonstrated that the performance of GNNs does not always align with homophily metrics, and how homophily influences GNNs still remains unclear and controversial. Then, a crucial question arises: What is missing in our current understanding of homophily? To figure out the missing part, in this paper, we disentangle the graph homophily into three aspects: label, structural, and feature homophily, which are derived from the three basic elements of graph data. We argue that the synergy of the three homophily can provide a more comprehensive understanding of GNN performance. Our new proposed structural and feature homophily consider the neighborhood consistency and feature dependencies among nodes, addressing the previously overlooked structural and feature aspects in graph homophily. To investigate their synergy, we propose a Contextual Stochastic Block Model with three types of Homophily (CSBM-3H), where the topology and feature generation are controlled by the three metrics. Based on the theoretical analysis of CSBM-3H, we derive a new composite metric, named Tri-Hom, that considers all three aspects and overcomes the limitations of conventional homophily metrics. The theoretical conclusions and the effectiveness of Tri-Hom have been verified through synthetic experiments on CSBM-3H. In addition, we conduct experiments on $31$ real-world benchmark datasets and calculate the correlations between homophily metrics and model performance. Tri-Hom has significantly higher correlation values than $17$ existing metrics that only focus on a single homophily aspect, demonstrating its superiority and the importance of homophily synergy. Our code is available at https://github.com/zylMozart/Disentangle_GraphHom.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-239" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-239', event_id='93483', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3000</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93483">GRANOLA: Adaptive Normalization for Graph Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Moshe Eliasof &middot; Beatrice Bevilacqua &middot; Carola-Bibiane SchÃ¶nlieb &middot; Haggai Maron
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite the widespread adoption of Graph Neural Networks (GNNs), these models often incorporate off-the-shelf normalization layers like BatchNorm or InstanceNorm, which were not originally designed for GNNs. Consequently, these normalization layers may not effectively capture the unique characteristics of graph-structured data, potentially even weakening the expressive power of the overall architecture. While existing graph-specific normalization layers have been proposed, they often struggle to offer substantial and consistent benefits. In this paper, we propose GRANOLA, a novel graph-adaptive normalization layer. Unlike existing normalization layers, GRANOLA normalizes node features by adapting to the specific characteristics of the graph, particularly by generating expressive representations of its nodes, obtained by leveraging the propagation of Random Node Features (RNF) in the graph. We provide theoretical results that support our design choices as well as an extensive empirical evaluation demonstrating the superior performance of GRANOLA over existing normalization techniques. Furthermore, GRANOLA emerges as the top-performing method among all baselines in the same time complexity class of Message Passing Neural Networks (MPNNs).</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-240" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-240', event_id='93568', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3001</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93568">Learning on Large Graphs using Intersecting Communities</a></strong></h5>


                        <p class="text-muted">
                            Ben Finkelshtein &middot; Ismail Ceylan &middot; Michael Bronstein &middot; Ron Levie
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Message Passing Neural Networks (MPNNs) are a staple of graph machine learning. MPNNs iteratively update each nodeâs representation in an input graph by aggregating messages from the nodeâs neighbors, which necessitates a memory complexity of the order of the <strong>number of graph edges</strong>. This complexity might quickly become  prohibitive for large graphs provided they are not very sparse.  In this paper, we propose a novel approach to alleviate this problem by  approximating the input graph as an  intersecting community graph (ICG) -- a combination of intersecting cliques. The key insight is that the number of communities required to approximate a graph  <strong>does not depend on the graph size</strong>. We develop a new constructive version of the Weak Graph Regularity Lemma to efficiently construct an approximating ICG for any input graph. We then devise an efficient graph learning algorithm operating directly on ICG in linear memory and time with respect to the <strong>number of nodes</strong> (rather than edges).  This offers a new and fundamentally different pipeline for learning on very large non-sparse graphs, whose applicability is demonstrated empirically on node classification tasks and spatio-temporal data processing.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-241" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-241', event_id='93622', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3002</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93622">Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level</a></strong></h5>


                        <p class="text-muted">
                            Runlin Lei &middot; Yuwei Hu &middot; Yuchen Ren &middot; Zhewei Wei
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graph Neural Networks (GNNs) excel across various applications but remain vulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs), which inject malicious nodes into the original graph and pose realistic threats.Text-attributed graphs (TAGs), where nodes are associated with textual features, are crucial due to their prevalence in real-world applications and are commonly used to evaluate these vulnerabilities.However, existing research only focuses on embedding-level GIAs, which inject node embeddings rather than actual textual content, limiting their applicability and simplifying detection.In this paper, we pioneer the exploration of GIAs at the text level, presenting three novel attack designs that inject textual content into the graph.Through theoretical and empirical analysis, we demonstrate that text interpretability, a factor previously overlooked at the embedding level, plays a crucial role in attack strength. Among the designs we investigate, the Word-frequency-based Text-level GIA (WTGIA) is particularly notable for its balance between performance and interpretability. Despite the success of WTGIA, we discover that defenders can easily enhance their defenses with customized text embedding methods or large language model (LLM)--based predictors. These insights underscore the necessity for further research into the potential and practical significance of text-level GIAs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-242" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-242', event_id='93779', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3003</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93779">Deep Graph Mating</a></strong></h5>


                        <p class="text-muted">
                            Yongcheng Jing &middot; Seok-Hee Hong &middot; Dacheng Tao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we introduce the first learning-free model reuse task within the non-Euclidean domain, termed as Deep Graph Mating (Grama). We strive to create a child Graph Neural Network (GNN) that integrates knowledge from pre-trained parent models without requiring re-training, fine-tuning, or annotated labels. To this end, we begin by investigating the permutation invariance property of GNNs, which leads us to develop two vanilla approaches for Grama: Vanilla Parameter Interpolation (VPI) and Vanilla Alignment Prior to Interpolation (VAPI), both employing topology-independent interpolation in the parameter space. However, neither approach has achieved the anticipated results. Through theoretical analysis of VPI and VAPI, we identify critical challenges unique to Grama, including increased sensitivity to parameter misalignment and further the inherent topology-dependent complexities. Motivated by these findings, we propose the Dual-Message Coordination and Calibration (DuMCC) methodology, comprising the Parent Message Coordination (PMC) scheme to optimise the permutation matrices for parameter interpolation by coordinating aggregated messages, and the Child Message Calibration (CMC) scheme to mitigate over-smoothing identified in PMC by calibrating the message statistics within child GNNs. Experiments across diverse domains, including node and graph property prediction, 3D object recognition, and large-scale semantic parsing, demonstrate that the proposed DuMCC effectively enables training-free knowledge transfer, yielding results on par with those of pre-trained models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-243" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-243', event_id='93791', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3004</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93791">Unifying Generation and Prediction on Graphs with Latent Graph Diffusion</a></strong></h5>


                        <p class="text-muted">
                            Cai Zhou &middot; Xiyuan Wang &middot; Muhan Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) using one formulation. We first formulate prediction tasks including regression and classification into a generic (conditional) generation framework, which enables diffusion models to perform deterministic tasks with provable guarantees. We then propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder and decoder, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Leveraging LGD and the ``all tasks as generation'' formulation, our framework is capable of solving graph tasks of various levels and types. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results across a wide range of generation and regression tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-244" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-244', event_id='93910', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3005</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93910">Distributed-Order Fractional Graph Operating Network</a></strong></h5>


                        <p class="text-muted">
                            Kai Zhao &middot; Xuhao Li &middot; Qiyu Kang &middot; Feng Ji &middot; Qinxu Ding &middot; Yanan Zhao &middot; Wenfei Liang &middot; Wee Peng Tay
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce the Distributed-order fRActional Graph Operating Network (DRAGON), a novel continuous Graph Neural Network (GNN) framework that incorporates distributed-order fractional calculus. Unlike traditional continuous GNNs that utilize integer-order or single fractional-order differential equations, DRAGON uses a learnable probability distribution over a range of real numbers for the derivative orders. By allowing a flexible and learnable superposition of multiple derivative orders, our framework captures complex graph feature updating dynamics beyond the reach of conventional models.We provide a comprehensive interpretation of our framework's capability to capture intricate dynamics through the lens of a non-Markovian graph random walk with node feature updating driven by an anomalous diffusion process over the graph. Furthermore, to highlight the versatility of the DRAGON framework, we conduct empirical evaluations across a range of graph learning tasks. The results consistently demonstrate superior performance when compared to traditional continuous GNN models. The implementation code is available at \url{https://github.com/zknus/NeurIPS-2024-DRAGON}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-245" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-245', event_id='94176', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3006</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94176">HC-GAE: The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning</a></strong></h5>


                        <p class="text-muted">
                            Lu Bai &middot; Zhuo Xu &middot; Lixin Cui &middot; Ming Li &middot; Yue Wang &middot; Edwin Hancock
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graph Auto-Encoders (GAEs) are powerful tools for graph representation learning. In this paper, we develop a novel Hierarchical Cluster-based GAE (HC-GAE), that can learn effective structural characteristics for graph data analysis. To this end, during the encoding process, we commence by utilizing the hard node assignment to decompose a sample graph into a family of separated subgraphs. We compress each subgraph into a coarsened node, transforming the original graph into a coarsened graph. On the other hand, during the decoding process, we adopt the soft node assignment to reconstruct the original graph structure by expanding the coarsened nodes. By hierarchically performing the above compressing procedure during the decoding process as well as the expanding procedure during the decoding process, the proposed HC-GAE can effectively extract bidirectionally hierarchical structural features of the original sample graph. Furthermore, we re-design the loss function that can integrate the information from either the encoder or the decoder. Since the associated graph convolution operation of the proposed HC-GAE is restricted in each individual separated subgraph and cannot propagate the node information between different subgraphs, the proposed HC-GAE can significantly reduce the over-smoothing problem arising in the classical convolution-based GAEs. The proposed HC-GAE can generate effective representations for either node classification or graph classification, and the experiments demonstrate the effectiveness on real-world datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-246" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-246', event_id='94755', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3007</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94755">Pure Message Passing Can Estimate Common Neighbor for Link Prediction</a></strong></h5>


                        <p class="text-muted">
                            Kaiwen Dong &middot; Zhichun Guo &middot; Nitesh Chawla
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Message Passing Neural Networks (MPNNs) have emerged as the {\em de facto} standard in graph representation learning. However, when it comes to link prediction, they are not always superior to simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. We conduct experiments on benchmark datasets from various domains, where our method consistently outperforms the baseline methods, establishing new state-of-the-arts.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-247" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-247', event_id='94823', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3008</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94823">Boosting Graph Pooling with Persistent Homology</a></strong></h5>


                        <p class="text-muted">
                            Chaolong Ying &middot; Xinjian Zhao &middot; Tianshu Yu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-248" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-248', event_id='95069', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3009</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95069">Deep Equilibrium Algorithmic Reasoning</a></strong></h5>


                        <p class="text-muted">
                            Dobrik Georgiev &middot; Joseph Wilson &middot; Davide Buffelli &middot; Pietro LiÃ³
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Neural Algorithmic Reasoning (NAR) research has demonstrated that graph neural networks (GNNs) could learn to execute classical algorithms. However, most previous approaches have always used a recurrent architecture, where each iteration of the GNN matches an iteration of the algorithm. In this paper we study neurally solving algorithms from a different perspective: since the algorithmâs solution is often an equilibrium, it is possible to find the solution directly by solving an equilibrium equation. Our approach requires no information on the ground-truth number of steps of the algorithm, both during train and test time. Furthermore, the proposed method improves the performance of GNNs on executing algorithms and is a step towards speeding up existing NAR models. Our empirical evidence, leveraging algorithms from the CLRS-30 benchmark, validates that one can train a network to solve algorithmic problems by directly finding the equilibrium. We discuss the practical implementation of such models and propose regularisations to improve the performance of these equilibrium reasoners.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-249" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-249', event_id='95681', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3010</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95681">Even Sparser Graph Transformers</a></strong></h5>


                        <p class="text-muted">
                            Hamed Shirzad &middot; Honghao Lin &middot; Balaji Venkatachalam &middot; Ameya Velingker &middot; David Woodruff &middot; Danica J. Sutherland
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graph Transformers excel in long-range dependency modeling, but generally require quadratic memory complexity in the number of nodes in an input graph, and hence have trouble scaling to large graphs. Sparse attention variants such as Exphormer can help, but may require high-degree augmentations to the input graph for good performance, and do not attempt to sparsify an already-dense input graph. As the learned attention mechanisms tend to use few of these edges, however, such high-degree connections may be unnecessary. We show (empirically and with theoretical backing) that attention scores on graphs are usually quite consistent across network widths, and use this observation to propose a two-stage procedure, which we call Spexphormer: first, train a narrow network on the full augmented graph. Next, use only the active connections to train a wider network on a much sparser graph. We establish theoretical conditions when a narrow network's attention scores can match those of a wide network, and show that Spexphormer achieves good performance with drastically reduced memory requirements on various graph datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-250" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-250', event_id='95712', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3011</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95712">A Foundation Model for Zero-shot Logical Query Reasoning</a></strong></h5>


                        <p class="text-muted">
                            Michael Galkin &middot; Jincheng Zhou &middot; Bruno Ribeiro &middot; Jian Tang &middot; Zhaocheng Zhu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Complex logical query answering (CLQA) in knowledge graphs (KGs) goes beyond simple KG completion and aims at answering compositional queries comprised of multiple projections and logical operations. Existing CLQA methods that learn parameters bound to certain entity or relation vocabularies can only be applied to the graph they are trained on which requires substantial training time before being deployed on a new graph. Here we present UltraQuery, the first foundation model for inductive reasoning that can zero-shot answer logical queries on any KG. The core idea of UltraQuery is to derive both projections and logical operations as vocabulary-independent functions which generalize to new entities and relations in any KG.With the projection operation initialized from a pre-trained inductive KG completion model, UltraQuery can solve CLQA on any KG after finetuning on a single dataset. Experimenting on 23 datasets, UltraQuery in the zero-shot inference mode shows competitive or better query answering performance than best available baselines and sets a new state of the art on 15 of them.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-251" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-251', event_id='93445', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3100</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93445">Graph Coarsening with Message-Passing Guarantees</a></strong></h5>


                        <p class="text-muted">
                            Antonin Joly &middot; Nicolas Keriven
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Graph coarsening aims to reduce the size of a large graph while preserving some of its key properties, which has been used in many applications to reduce computational load and memory footprint. For instance, in graph machine learning, training Graph Neural Networks (GNNs) on coarsened graphs leads to drastic savings in time and memory. However, GNNs rely on the Message-Passing (MP) paradigm, and classical spectral preservation guarantees for graph coarsening do not directly lead to theoretical guarantees when performing naive message-passing on the coarsened graph.In this work, we propose a new message-passing operation specific to coarsened graphs, which exhibit theoretical guarantees on the preservation of the propagated signal. Interestingly, and in a sharp departure from previous proposals, this operation on coarsened graphs is oriented, even when the original graph is undirected. We conduct node classification tasks on synthetic and real data and observe improved results compared to performing naive message-passing on the coarsened graph.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-252" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-252', event_id='93204', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3101</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93204">Supra-Laplacian Encoding for Transformer on Dynamic Graphs</a></strong></h5>


                        <p class="text-muted">
                            Yannis Karmim &middot; Marc Lafon &middot; RaphaÃ«l Fournier-Sniehotta &middot; Nicolas THOME
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Fully connected Graph Transformers (GT) have rapidly become prominent in the static graph community as an alternative to Message-Passing models, which suffer from a lack of expressivity, oversquashing, and under-reaching.However, in a dynamic context, by interconnecting all nodes at multiple snapshots with self-attention,GT loose both structural and temporal information. In this work, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs (SLATE), a new spatio-temporal encoding to leverage the GT architecture while keeping spatio-temporal information.Specifically, we transform Discrete Time Dynamic Graphs into multi-layer graphs and take advantage of the spectral properties of their associated supra-Laplacian matrix.Our second contribution explicitly model nodes' pairwise relationships with a cross-attention mechanism, providing an accurate edge representation for dynamic link prediction.SLATE outperforms numerous state-of-the-art methods based on Message-Passing Graph Neural Networks combined with recurrent models (e.g, LSTM), and Dynamic Graph Transformers,on~9 datasets. Code is open-source and available at this link https://github.com/ykrmm/SLATE.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-253" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-253', event_id='93041', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3102</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93041">Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters</a></strong></h5>


                        <p class="text-muted">
                            Ya-Wei Eileen Lin &middot; Ronen Talmon &middot; Ron Levie
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Equivariant machine learning is an approach for designing deep learning models that respect the symmetries of the problem, with the aim of reducing model complexity and improving generalization. In this paper, we focus on an extension of shift equivariance, which is the basis of convolution networks on images, to general graphs. Unlike images, graphs do not have a natural notion of domain translation. Therefore, we consider the graph functional shifts as the symmetry group: the unitary operators that commute with the graph shift operator. Notably, such symmetries operate in the signal space rather than directly in the spatial space.We remark that each linear filter layer of a standard spectral graph neural network (GNN) commutes with graph functional shifts, but the activation function breaks this symmetry. Instead, we propose nonlinear spectral filters (NLSFs) that are fully equivariant to graph functional shifts and show that they have universal approximation properties. The proposed NLSFs are based on a new form of spectral domain that is transferable between graphs. We demonstrate the superior performance of NLSFs over existing spectral GNNs in node and graph classification benchmarks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-254" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-254', event_id='97544', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3103</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97544">VideoGUI: A Benchmark for GUI Automation from Instructional Videos</a></strong></h5>


                        <p class="text-muted">
                            Kevin Qinghong Lin &middot; Linjie Li &middot; Difei Gao &middot; Qinchen WU &middot; Mingyi Yan &middot; Zhengyuan Yang &middot; Lijuan Wang &middot; Mike Zheng Shou
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Graphical User Interface (GUI) automation holds significant promise for enhancing human productivity by assisting with computer tasks. Existing task formulations primarily focus on simple tasks that can be specified by a single, language-only instruction, such as \``Insert a new slide.'' However, the derived methods often struggle with complex, visually-intensive software tasks in the real world, such as ``recreating a specific animation effect shown in a video.'' The challenges include visual perception, lengthy procedural planning, and executing multiple actions. Recognizing that humans frequently rely on instructional videos to master complex skills, we introduce \textbf{\our}, a novel multi-modal benchmark designed to evaluate GUI assistants across multiple dimensions of advanced GUI tasks. Sourced from high-quality web instructional videos, \our focuses on advanced tasks involving professional and novel software (\eg Adobe Photoshop or Stable Diffusion WebUI) and complex activities (\eg video editing). Moreover, \our evaluates GUI assistants through a \textit{hierarchical} process, allowing for identification of the specific levels at which they may fail: \textbf{($i$) high-level planning:} reconstruct procedural subtasks from visual conditions without language descriptions; \textbf{($ii$) middle-level planning:} generate sequences of precise action narrations based on visual state (\ie screenshot) and goals; % transitions \textbf{($iii$) atomic action execution:} perform specific actions such as accurately clicking designated elements. For each level, we design evaluation metrics across individual dimensions to provide clear signals, such as individual performance in clicking, dragging, typing, and scrolling for atomic action execution. We evaluate representative large multimodal models on \our, revealing each model's capabilities on these different levels. We observe that the current best model GPT-4o, while proficient at planning from textual queries, still struggles with reversal planning from visual previews and execute certain actions such as dragging. These gaps show the direction for developing stronger models or agent systems for GUI automation from instructional videos.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-255" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-255', event_id='96440', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3104</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96440">Utilizing Human Behavior Modeling to Manipulate Explanations in AI-Assisted Decision Making: The Good, the Bad, and the Scary</a></strong></h5>


                        <p class="text-muted">
                            Zhuoyan Li &middot; Ming Yin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advances in AI models have increased the integration of AI-based decision aids into the human decision making process. To fully unlock the potential of AI-assisted decision making, researchers have computationally modeled how humans incorporate AI recommendations into their final decisions, and utilized these models to improve human-AI team performance. Meanwhile, due to the ``black-box'' nature of AI models, providing AI explanations to human decision makers to help them rely on AI recommendations more appropriately has become a common practice.  In this paper, we explore whether we can quantitatively model how humans integrate both AI recommendations and explanations into their decision process, and whether this quantitative understanding of human behavior from the learned model can be utilized to manipulate AI explanations, thereby nudging individuals towards making targeted decisions. Our extensive human experiments across various tasks demonstrate that  human behavior can be easily influenced by these manipulated explanations towards targeted outcomes, regardless of the intent being adversarial or benign. Furthermore, individuals often fail to detect any anomalies in these  explanations, despite their decisions being affected by them.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-256" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-256', event_id='96903', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3105</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96903">Confidence Regulation Neurons in Language Models</a></strong></h5>


                        <p class="text-muted">
                            Alessandro Stolfo &middot; Ben Wu &middot; Wes Gurnee &middot; Yonatan Belinkov &middot; Xingyi Song &middot; Mrinmaya Sachan &middot; Neel Nanda
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite their widespread use, the mechanisms by which large language models (LLMs) represent and regulate uncertainty in next-token predictions remain largely unexplored. This study investigates two critical components believed to influence this uncertainty: the recently discovered entropy neurons and a new set of components that we term token frequency neurons. Entropy neurons are characterized by an unusually high weight norm and influence the final layer normalization (LayerNorm) scale to effectively scale down the logits. Our work shows that entropy neurons operate by writing onto an \textit{unembedding null space}, allowing them to impact the residual stream norm with minimal direct effect on the logits themselves. We observe the presence of entropy neurons across a range of models, up to 7 billion parameters. On the other hand, token frequency neurons, which we discover and describe here for the first time, boost or suppress each tokenâs logit proportionally to its log frequency, thereby shifting the output distribution towards or away from the unigram distribution. Finally, we present a detailed case study where entropy neurons actively manage confidence: the setting of induction, i.e. detecting and continuing repeated subsequences.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-257" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-257', event_id='96781', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3106</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96781">Compact Proofs of Model Performance via Mechanistic Interpretability</a></strong></h5>


                        <p class="text-muted">
                            Jason Gross &middot; Rajashree Agrawal &middot; Thomas Kwa &middot; Euan Ong &middot; Chun Hei Yip &middot; Alex Gibson &middot; Soufiane Noubir &middot; Lawrence Chan
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We propose using mechanistic interpretability -- techniques for reverse engineering model weights into human-interpretable algorithms -- to derive and compactly prove formal guarantees on model performance.We prototype this approach by formally proving accuracy lower bounds for a small transformer trained on Max-of-$K$, validating proof transferability across 151 random seeds and four values of $K$.We create 102 different computer-assisted proof strategies and assess their length and tightness of bound on each of our models.Using quantitative metrics, we find that shorter proofs seem to require and provide more mechanistic understanding.Moreover, we find that more faithful mechanistic understanding leads to tighter performance bounds.We confirm these connections by qualitatively examining a subset of our proofs.Finally, we identify compounding structureless errors as a key challenge for using mechanistic interpretability to generate compact proofs on model performance.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-258" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-258', event_id='95204', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3107</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95204">CoSy: Evaluating Textual Explanations of Neurons</a></strong></h5>


                        <p class="text-muted">
                            Laura Kopf &middot; Philine L Bommer &middot; Anna HedstrÃ¶m &middot; Sebastian Lapuschkin &middot; Marina HÃ¶hne &middot; Kirill Bykov
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>A crucial aspect of understanding the complex nature of Deep Neural Networks (DNNs) is the ability to explain learned concepts within their latent representations. While methods exist to connect neurons to human-understandable textual descriptions, evaluating the quality of these explanations is challenging due to the lack of a unified quantitative approach. We introduce CoSy (Concept Synthesis), a novel, architecture-agnostic framework for evaluating textual explanations of latent neurons. Given textual explanations, our proposed framework uses a generative model conditioned on textual input to create data points representing the explanations. By comparing the neuron's response to these generated data points and control data points, we can estimate the quality of the explanation. We validate our framework through sanity checks and benchmark various neuron description methods for Computer Vision tasks, revealing significant differences in quality.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-259" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-259', event_id='95051', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3108</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95051">B-cosification: Transforming Deep Neural Networks to be Inherently Interpretable</a></strong></h5>


                        <p class="text-muted">
                            Shreyash Arya &middot; Sukrut Rao &middot; Moritz BÃ¶hle &middot; Bernt Schiele
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>B-cos Networks have been shown to be effective for obtaining highly human interpretable explanations of model decisions by architecturally enforcing stronger alignment between inputs and weight. B-cos variants of convolutional networks (CNNs) and vision transformers (ViTs), which primarily replace linear layers with B-cos transformations, perform competitively to their respective standard variants while also yielding explanations that are faithful by design. However, it has so far been necessary to train these models from scratch, which is increasingly infeasible in the era of large, pre-trained foundation models. In this work, inspired by the architectural similarities in standard DNNs and B-cos networks, we propose âB-cosificationâ, a novel approach to transform existing pre-trained models to become inherently interpretable. We perform a thorough study of design choices to perform this conversion, both for convolutional neural networks and vision transformers. We find that B-cosification can yield models that are on par with B-cos models trained from scratch in terms of interpretability, while often outperforming them in terms of classification performance at a fraction of the training cost. Subsequently, we apply B-cosification to a pretrained CLIP model, and show that, even with limited data and compute cost, we obtain a B-cosified version that is highly interpretable and competitive on zero shot performance across a variety of datasets. We release ourcode and pre-trained model weights at https://github.com/shrebox/B-cosification.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-260" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-260', event_id='93736', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3109</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93736">Explanations that reveal all through the deï¬nition of encoding</a></strong></h5>


                        <p class="text-muted">
                            Aahlad Manas Puli &middot; Nhi Nguyen &middot; Rajesh Ranganath
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Feature attributions attempt to highlight what inputs drive predictive power. Good attributions or explanations are thus those that produce inputs that retain this predictive power; accordingly, evaluations of explanations score their quality of prediction. However, evaluations produce scores better than what appears possible from the values in the explanation for a class of explanations, called encoding explanations. Probing for encoding remains a challenge because there is no general characterization of what gives the extra predictive power. We develop a deï¬nition of encoding that identiï¬es this extra predictive power via conditional dependence and show that the deï¬nition ï¬ts existing examples of encoding. This deï¬nition implies, in contrast to encoding explanations, that non-encoding explanations contain all the informative inputs used to produce the explanation, giving them a âwhat you see is what you getâ property, which makes them transparent and simple to use. Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank non-encoding explanations above encoding ones, and develop STRIPE-X which ranks them correctly. After empirically demonstrating the theoretical insights, we use STRIPE-X to uncover encoding in LLM-generated explanations for predicting the sentiment in movie reviews.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-261" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-261', event_id='93550', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3110</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93550">Causal Dependence Plots</a></strong></h5>


                        <p class="text-muted">
                            Joshua Loftus &middot; Lucius Bynum &middot; Sakina Hansen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>To use artificial intelligence and machine learning models wisely we must understand how they interact with the world, including how they depend causally on data inputs. In this work we develop Causal Dependence Plots (CDPs) to visualize how a model's predicted outcome depends on changes in a given predictor <em>along with consequent causal changes in other predictor variables</em>. Crucially, this differs from standard methods based on independence or holding other predictors constant, such as regression coefficients or Partial Dependence Plots (PDPs). Our explanatory framework generalizes PDPs, including them as a special case, as well as a variety of other interpretive plots that show, for example, the total, direct, and indirect effects of causal mediation. We demonstrate with simulations and real data experiments how CDPs can be combined in a modular way with methods for causal learning or sensitivity analysis. Since people often think causally about input-output dependence, CDPs can be powerful tools in the xAI or interpretable machine learning toolkit and contribute to applications like scientific machine learning and algorithmic fairness.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-262" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-262', event_id='93413', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3111</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93413">FFAM: Feature Factorization Activation Map for Explanation of 3D Detectors</a></strong></h5>


                        <p class="text-muted">
                            Shuai Liu &middot; Boyang Li &middot; Zhiyu Fang &middot; Mingyue Cui &middot; Kai Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>LiDAR-based 3D object detection has made impressive progress recently, yet most existing models are black-box, lacking interpretability. Previous explanation approaches primarily focus on analyzing image-based models and are not readily applicable to LiDAR-based 3D detectors. In this paper, we propose a feature factorization activation map (FFAM) to generate high-quality visual explanations for 3D detectors. FFAM employs non-negative matrix factorization to generate concept activation maps and subsequently aggregates these maps to obtain a global visual explanation. To achieve object-specific visual explanations, we refine the global visual explanation using the feature gradient of a target object. Additionally, we introduce a voxel upsampling strategy to align the scale between the activation map and input point cloud. We qualitatively and quantitatively analyze FFAM with multiple detectors on several datasets. Experimental results validate the high-quality visual explanations produced by FFAM. The code is available at \url{https://anonymous.4open.science/r/FFAM-B9AF}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-263" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-263', event_id='93529', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3200</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93529">Spectral Editing of Activations for Large Language Model Alignment</a></strong></h5>


                        <p class="text-muted">
                            Yifu QIU &middot; Zheng Zhao &middot; Yftah Ziser &middot; Anna Korhonen &middot; Edoardo Maria Ponti &middot; Shay Cohen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) often exhibit undesirable behaviours, such as generating untruthful or biased content. Editing their internal representations has been shown to be effective in mitigating such behaviours on top of the existing alignment methods. We propose a novel inference-time editing method, namely spectral editing of activations (SEA), to project the input representations into directions with maximal covariance with the positive demonstrations (e.g., truthful) while minimising covariance with the negative demonstrations (e.g., hallucinated). We also extend our method to non-linear editing using feature functions. We run extensive experiments on benchmarks concerning truthfulness and bias with six open-source LLMs of different sizes and model families. The results demonstrate the superiority of SEA in effectiveness, generalisation to similar tasks, as well as computation and data efficiency. We also show that SEA editing only has a limited negative impact on other model capabilities.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-264" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-264', event_id='97844', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3201</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97844">WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia</a></strong></h5>


                        <p class="text-muted">
                            Yufang Hou &middot; Alessandra Pascale &middot; Javier Carnerero-Cano &middot; Tigran Tchrakian &middot; Radu Marinescu &middot; Elizabeth Daly &middot; Inkit Padhi &middot; Prasanna Sattigeri
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Retrieval-augmented generation (RAG) has emerged as a promising solution to mitigate the limitations of large language models (LLMs), such as hallucinations and outdated information. However, it remains unclear how LLMs handle knowledge conflicts arising from different augmented retrieved passages, especially when these passages originate from the same source and have equal trustworthiness. In this work, we conduct a comprehensive evaluation of LLM-generated answers to questions that have varying answers based on contradictory passages from Wikipedia, a dataset widely regarded as a high-quality pre-training resource for most LLMs. Specifically, we introduce WikiContradict, a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts. We benchmark a diverse range of both closed and open-source LLMs under different QA scenarios, including RAG with a single  passage, and RAG with 2 contradictory passages. Through rigorous human evaluations on a subset of WikiContradict instances involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and limitations of these models. For instance, when provided with two passages containing contradictory facts, all models struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts requiring reasoning. Since human evaluation is costly, wealso introduce an automated model that estimates LLM performance using a strong open-source language model, achieving an F-score of 0.8. Using this automated metric, we evaluate more than 1,500 answers from seven LLMs across all WikiContradict instances.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-265" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-265', event_id='95473', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3202</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95473">IQA-EVAL: Automatic Evaluation of Human-Model Interactive Question Answering</a></strong></h5>


                        <p class="text-muted">
                            Ruosen Li &middot; Ruochen Li &middot; Barry Wang &middot; Xinya Du
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>To evaluate Large Language Models (LLMs) for question answering (QA), traditional methods typically focus on directly assessing the immediate responses generated by the models based on the given question and context. In the common use case of humans seeking AI assistantâs help in finding information, these non-interactive evaluations do not account for the dynamic nature of human-model conversations, and interaction-aware evaluations have shown that accurate models are not necessarily preferred by humans Lee et al. Recent works in human-computer interaction (HCI) have employed human evaluators to conduct interactions and evaluations, but they are often prohibitively expensive and time-consuming to scale. In this work, we introduce an automated evaluation framework IQA-EVAL to Interactive Question Answering Evaluations, more specifically, we introduce LLM-based Evaluation Agent (LEA) that can: (1) simulate human behaviors to generate interactions with IQA models; (2) automatically evaluate the generated interactions. Moreover, we propose assigning personas to LEAs to better simulate groups of real human evaluators. We show that: (1) our evaluation framework with GPT-4 (or Claude) as the backbone model achieves a high correlation with human evaluations on the IQA task; (2) assigning personas to LEA to better represent the crowd further significantly improves correlations. Finally, we use our automated metric to evaluate five recent LLMs with over 1000 questions from complex and ambiguous question answering tasks, which would cost $5k if evaluated by humans.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-266" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-266', event_id='93420', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3203</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93420">Reranking Laws for Language Generation: A Communication-Theoretic Perspective</a></strong></h5>


                        <p class="text-muted">
                            AntÃ³nio Farinhas &middot; Xiaocheng Li &middot; AndrÃ© Martins
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>To ensure large language models (LLMs) are used safely, one must reduce their propensity to hallucinate or to generate unacceptable answers. A simple and often used strategy is to first let the LLM generate multiple hypotheses and then employ a reranker to choose the best one. In this paper, we draw a parallel between this strategy and the use of redundancy to decrease the error rate in noisy communication channels. We conceptualize the generator as a sender transmitting multiple descriptions of a message through parallel noisy channels. The receiver decodes the message by ranking the (potentially corrupted) descriptions and selecting the one found to be most reliable. We provide conditions under which this protocol is asymptotically error-free (i.e., yields an acceptable answer almost surely) even in scenarios where the reranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the channel distributions are statistically dependent. We use our framework to obtain reranking laws which we validate empirically on two real-world tasks using LLMs: text-to-code generation with DeepSeek-Coder 7B and machine translation of medical data with TowerInstruct 13B.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-267" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-267', event_id='94229', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3204</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94229">The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More</a></strong></h5>


                        <p class="text-muted">
                            Ouail Kitouni &middot; Niklas S Nolte &middot; Adina Williams &middot; Michael Rabbat &middot; Diane Bouchacourt &middot; Mark Ibrahim
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Today's best language models still struggle with "hallucinations", factually incorrect generations, which impede their ability to reliably retrieve information seen during training. The <em>reversal curse</em>, where models cannot recall information when probed in a different order than was encountered during training, exemplifies limitations in information retrieval. To better understand these limitations, we reframe the reversal curse as a <em>factorization curse</em> --- a failure of models to learn the same joint distribution under different factorizations.We more closely simulate finetuning workflows which train pretrained models on specialized knowledge by introducing<em>WikiReversal</em>, a realistic testbed based on Wikipedia knowledge graphs. Through a series of controlled experiments with increasing levels of realism, including non-reciprocal relations, we find that reliable information retrieval is an inherent failure of the next-token prediction objective used in popular large language models. Moreover, we demonstrate reliable information retrieval cannot be solved with scale, reversed tokens, or even naive bidirectional-attention training. Consequently, various approaches to finetuning on specialized data would necessarily provide mixed results on downstream tasks, unless the model has already seen the right sequence of tokens. Across five tasks of varying levels of complexity, our results uncover a promising path forward: factorization-agnostic objectives can significantly mitigate the reversal curse and hint at improved knowledge storage and planning capabilities.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-268" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-268', event_id='94375', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3205</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94375">How do Large Language Models Handle Multilingualism?</a></strong></h5>


                        <p class="text-muted">
                            Yiran Zhao &middot; Wenxuan Zhang &middot; Guizhen Chen &middot; Kenji Kawaguchi &middot; Lidong Bing
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Large language models (LLMs) have demonstrated impressive capabilities across diverse languages. This study explores how LLMs handle multilingualism. Based on observed language ratio shifts among layers and the relationships between network structures and certain capabilities, we hypothesize the LLM's multilingual workflow ($\texttt{MWork}$): LLMs initially understand the query, converting multilingual inputs into English for task-solving. In the intermediate layers, they employ English for thinking and incorporate multilingual knowledge with self-attention and feed-forward structures, respectively. In the final layers, LLMs generate responses aligned with the original language of the query. To verify $\texttt{MWork}$, we introduce Parallel Language-specific Neuron Detection ($\texttt{PLND}$) to identify activated neurons for inputs in different languages without any labeled data. Using $\texttt{PLND}$, we validate $\texttt{MWork}$ through extensive experiments involving the deactivation of language-specific neurons across various layers and structures. Moreover, $\texttt{MWork}$ allows fine-tuning of language-specific neurons with a small dataset, enhancing multilingual abilities in a specific language without compromising others. This approach results in an average improvement of $3.6\%$ for high-resource languages and $2.3\%$ for low-resource languages across all tasks with just $400$ documents.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-269" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-269', event_id='94666', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3206</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94666">PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression</a></strong></h5>


                        <p class="text-muted">
                            Vladimir Malinovskii &middot; Denis Mazur &middot; Ivan Ilin &middot; Denis Kuznedelev &middot; Konstantin Burlachenko &middot; Kai Yi &middot; Dan Alistarh &middot; Peter Richtarik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>There has been significant interest in "extreme" compression of large language models (LLMs), i.e. to 1-2 bits per parameter, which allows such models to be executed efficiently on resource-constrained devices.  Existing work focused on improved one-shot quantization techniques and weight representations; yet, purely post-training  approaches are reaching diminishing returns in terms of the accuracy-vs-bit-width trade-off. State-of-the-art quantization methods such as QuIP# and AQLM include fine-tuning (part of) the compressed parameters over a limited amount of calibration data; however, such fine-tuning techniques over compressed weights often make exclusive use of straight-through estimators (STE), whose performance is not well-understood in this setting. In this work, we question the use of STE for extreme LLM compression, showing that it can be sub-optimal, and perform a systematic study of quantization-aware fine-tuning strategies for LLMs.We propose PV-Tuning - a representation-agnostic framework that generalizes and improves upon existing fine-tuning strategies, and provides convergence guarantees in restricted cases.On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms prior techniques for highly-performant models such as Llama and Mistral. Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama-2 family models at  2 bits per parameter.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-270" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-270', event_id='95366', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3207</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95366">HLM-Cite: Hybrid Language Model Workflow for Text-based Scientific Citation Prediction</a></strong></h5>


                        <p class="text-muted">
                            Qianyue Hao &middot; Jingyang Fan &middot; Fengli Xu &middot; Jian Yuan &middot; Yong Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Citation networks are critical infrastructures of modern science, serving as intricate webs of past literature and enabling researchers to navigate the knowledge production system. To mine information hiding in the link space of such networks, predicting which previous papers (candidates) will a new paper (query) cite is a critical problem that has long been studied. However, an important gap remains unaddressed: the roles of a paper's citations vary significantly, ranging from foundational knowledge basis to superficial contexts. Distinguishing these roles requires a deeper understanding of the logical relationships among papers, beyond simple edges in citation networks. The emergence of large language models (LLMs) with textual reasoning capabilities offers new possibilities for discerning these relationships, but there are two major challenges. First, in practice, a new paper may select its citations from gigantic existing papers, where the combined texts far exceed the context length of LLMs. Second, logical relationships between papers are often implicit, and directly prompting an LLM to predict citations may lead to results based primarily on surface-level textual similarities, rather than the deeper logical reasoning required. In this paper, we introduce the novel concept of core citation, which identifies the critical references that go beyond superficial mentions. Thereby, we elevate the citation prediction task from a simple binary classification to a more nuanced problem: distinguishing core citations from both superficial citations and non-citations. To address this, we propose $\textbf{HLM-Cite}$, a $\textbf{H}$ybrid $\textbf{L}$anguage $\textbf{M}$odel workflow for citation prediction, which combines embedding and generative LMs. We design a curriculum finetune procedure to adapt a pretrained text embedding model to coarsely retrieve high-likelihood core citations from vast candidate sets and then design an LLM agentic workflow to rank the retrieved papers through one-shot reasoning, revealing the implicit relationships among papers. With the two-stage pipeline, we can scale the candidate sets to 100K papers, vastly exceeding the size handled by existing methods. We evaluate HLM-Cite on a dataset across 19 scientific fields, demonstrating a 17.6\% performance improvement comparing SOTA methods. Our code is open-source at https://github.com/tsinghua-fib-lab/H-LM for reproducibility.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-271" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-271', event_id='96672', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3208</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96672">LLM Evaluators Recognize and Favor Their Own Generations</a></strong></h5>


                        <p class="text-muted">
                            Arjun Panickssery &middot; Samuel Bowman &middot; Shi Feng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self-preference, where an LLM evaluator scores its own outputs higher than othersâ while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence? In this paper, we investigate if self-recognition capability contributes to self-preference. We discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans. By finetuning LLMs, we discover a linear correlation between self-recognition capability and the strength of self-preference bias; using controlled experiments, we show that the causal explanation resists straightforward confounders. We discuss how self-recognition can interfere with unbiased evaluations and AI safety more generally.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-272" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-272', event_id='93042', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3209</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93042">ChronoEpilogi: Scalable Time Series Selection with Multiple Solutions</a></strong></h5>


                        <p class="text-muted">
                            Etienne Vareille &middot; Michele Linardi &middot; Ioannis Tsamardinos &middot; Vassilis Christophides
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We consider the problem of selecting all the minimal-size subsets of multivariate time-series (TS) variables whose past leads to an optimal predictive model for the future (forecasting) of a given target variable (multiple feature selection problem for times-series). Identifying these subsets leads to gaining insights, domain intuition,and a better understanding of the data-generating mechanism; it is often the first step in causal modeling. While identifying a single solution to the feature selection problem suffices for forecasting purposes, identifying all such minimal-size, optimally predictive subsets is necessary for knowledge discovery and important to avoid misleading a practitioner. We develop the theory of multiple feature selection for time-series data, propose the ChronoEpilogi algorithm, and prove its soundness and completeness under two mild, broad, non-parametric distributional assumptions, namely Compositionality of the distribution and Interchangeability of time-series variables in solutions. Experiments on synthetic and real datasets demonstrate the scalability of ChronoEpilogi to hundreds of TS variables and its efficacy in identifying multiple solutions. In the real datasets, ChronoEpilogi is shown to reduce the number of TS variables by 96% (on average) by conserving or even improving forecasting performance. Furthermore, it is on par with GroupLasso performance, with the added benefit of providing multiple solutions.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-273" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-273', event_id='93224', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3210</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93224">Analysing the Generalisation and Reliability of Steering Vectors</a></strong></h5>


                        <p class="text-muted">
                            Daniel Tan &middot; David Chanin &middot; Aengus Lynch &middot; Brooks Paige &middot; Dimitrios Kanoulas &middot; AdriÃ  Garriga-Alonso &middot; Robert Kirk
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Steering vectors (SVs) are a new approach to efficiently adjust language model behaviour at inference time by intervening on intermediate model activations. They have shown promise in terms of improving both capabilities and model alignment. However, the reliability and generalisation properties of this approach are unknown. In this work, we rigorously investigate these properties, and show that steering vectors have substantial limitations both in- and out-of-distribution. In-distribution, steerability is highly variable across different inputs. Depending on the concept, spurious biases can substantially contribute to how effective steering is for each input, presenting a challenge for the widespread use of steering vectors. Out-of-distribution, while steering vectors often generalise well, for several concepts they are brittle to reasonable changes in the prompt, resulting in them failing to generalise well. Overall, our findings show that while steering can work well in the right circumstances, there remain many technical difficulties of applying steering vectors to guide models' behaviour at scale.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-274" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-274', event_id='93372', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3211</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93372">Interpretable Generalized Additive Models for Datasets with Missing Values</a></strong></h5>


                        <p class="text-muted">
                            Hayden McTavish &middot; Jon Donnelly &middot; Margo Seltzer &middot; Cynthia Rudin
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Many important datasets contain samples that are missing one or more feature values. Maintaining the interpretability of machine learning models in the presence of such missing data is challenging. Singly or multiply imputing missing values complicates the modelâs mapping from features to labels. On the other hand, reasoning on indicator variables that represent missingness introduces a potentially large number of additional terms, sacrificing sparsity. We solve these problems with M-GAM, a sparse, generalized, additive modeling approach that incorporates missingness indicators and their interaction terms while maintaining sparsity through $\ell_0$ regularization. We show that M-GAM provides similar or superior accuracy to prior methods while significantly improving sparsity relative to either imputation or naÃ¯ve inclusion of indicator variables.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-275" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-275', event_id='96758', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3300</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96758">Geometric-Averaged Preference Optimization for Soft Preference Labels</a></strong></h5>


                        <p class="text-muted">
                            Hiroki Furuta &middot; Kuang-Huei Lee &middot; Shixiang (Shane) Gu &middot; Yutaka Matsuo &middot; Aleksandra Faust &middot; Heiga Zen &middot; Izzeddin Gur
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Many algorithms for aligning LLMs with human preferences assume that human preferences are binary and deterministic.However, human preferences can vary across individuals, and therefore should be represented distributionally.In this work, we introduce the distributional soft preference labels and improve Direct Preference Optimization (DPO) with a weighted geometric average of the LLM output likelihood in the loss function.This approach adjusts the scale of learning loss based on the soft labels such that the loss would approach zero when the responses are closer to equally preferred.This simple modification can be easily applied to any DPO-based methods and mitigate over-optimization and objective mismatch, which prior works suffer from.Our experiments simulate the soft preference labels with AI feedback from LLMs and demonstrate that geometric averaging consistently improves performance on standard benchmarks for alignment research. In particular, we observe more preferable responses than binary labels and significant improvements where modestly-confident labels are in the majority.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-276" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-276', event_id='96169', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3301</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96169">Unchosen Experts Can Contribute Too: Unleashing MoE Modelsâ Power by Self-Contrast</a></strong></h5>


                        <p class="text-muted">
                            Chufan Shi &middot; Cheng Yang &middot; Xinyu Zhu &middot; Jiahao Wang &middot; Taiqiang Wu &middot; Siheng Li &middot; Deng Cai &middot; Yujiu Yang &middot; Yu Meng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Mixture-of-Experts (MoE) has emerged as a prominent architecture for scaling model size while maintaining computational efficiency. In MoE, each token in the input sequence activates a different subset of experts determined by a routing mechanism. However, the unchosen experts in MoE models do not contribute to the output, potentially leading to underutilization of the model's capacity.In this work, we first conduct exploratory studies to demonstrate that increasing the number of activated experts does not necessarily improve and can even degrade the output quality. Then, we show that output distributions from an MoE model using different routing strategies substantially differ, indicating that different experts do not always act synergistically. Motivated by these findings, we propose <strong>S</strong>elf-<strong>C</strong>ontrast <strong>M</strong>ixture-<strong>o</strong>f-<strong>E</strong>xperts (SCMoE), a training-free strategy that utilizes unchosen experts in a self-contrast manner during inference. In SCMoE, the next-token probabilities are determined by contrasting the outputs from strong and weak activation using the same MoE model.Our method is conceptually simple and computationally lightweight, as it incurs minimal latency compared to greedy decoding. Experiments on several benchmarks (GSM8K, StrategyQA, MBPP and HumanEval) demonstrate that SCMoE can consistently enhance Mixtral 8x7Bâs reasoning capability across various domains. For example, it improves the accuracy on GSM8K from 61.79 to 66.94. Moreover, combining SCMoE with self-consistency yields additional gains, increasing major@20 accuracy from 75.59 to 78.31.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-277" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-277', event_id='95663', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3302</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95663">Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass</a></strong></h5>


                        <p class="text-muted">
                            Ethan Shen &middot; Alan Fan &middot; Sarah Pratt &middot; Jae Sung Park &middot; Matthew Wallingford &middot; Sham Kakade &middot; Ari Holtzman &middot; Ranjay Krishna &middot; Ali Farhadi &middot; Aditya Kusupati
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Many applications today provide users with multiple auto-complete drafts as they type, including GitHub's code completion, Gmail's smart compose, and Apple's messaging auto-suggestions. Under the hood, language models support this by running an autoregressive inference pass to provide a draft. Consequently, providing $k$ drafts to the user requires running an expensive language model $k$ times. To alleviate the computation cost of running $k$ inference passes, we propose Superposed Decoding, a new decoding algorithm that generates $k$ drafts at the computation cost of one autoregressive inference pass. We achieve this by feeding a superposition of the most recent token embeddings from the $k$ drafts as input to the next decoding step of the language model. At every inference step we combine the $k$ drafts with the top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations. Our experiments show that $k$ drafts from Superposed Decoding are at least as coherent and factual as Nucleus Sampling and Greedy Decoding respectively, while being at least $2.44\times$ faster for $k\ge3$. In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can also be combined with other decoding strategies, resulting in universal coverage gains when scaling inference time compute. Code and more examples open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-278" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-278', event_id='95123', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3303</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95123">AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data</a></strong></h5>


                        <p class="text-muted">
                            Zifan Song &middot; Yudong Wang &middot; Wenwei Zhang &middot; Kuikun Liu &middot; Chengqi Lyu &middot; Demin Song &middot; Qipeng Guo &middot; Hang Yan &middot; Dahua Lin &middot; Kai Chen &middot; Cairong Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance. However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained Code LLMs. In this paper, we present AlchemistCoder, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed AlchemistPrompts, to harmonize different data sources and instruction-response pairs. Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review. Extensive experiments demonstrate that AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence. Source code and models are available at https://github.com/InternLM/AlchemistCoder.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-279" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-279', event_id='94244', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3304</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94244">Discovering Preference Optimization Algorithms with and for Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Chris Lu &middot; Samuel Holt &middot; Claudio Fanconi &middot; Alex Chan &middot; Jakob Foerster &middot; Mihaela van der Schaar &middot; Robert Lange
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Offline preference optimization is a key method for enhancing and controlling the quality of Large Language Model (LLM) outputs.Typically, preference optimization is approached as an offline supervised learning task using manually crafted convex loss functions. While these methods are based on theoretical insights, they are inherently constrained by human creativity, so the large search space of possible loss functions remains under-explored. We address this by performing LLM-driven <em>objective discovery</em> to automatically discover new state-of-the-art preference optimization algorithms without (expert) human intervention.  Specifically, we iteratively prompt an LLM to propose and implement new preference optimization loss functions based on previously evaluated performance metrics. This process leads to the discovery of previously unknown and performant preference optimization algorithms. The best performing of these we call <em>Discovered Preference Optimization</em> (DiscoPOP), a novel algorithm that adaptively blends logistic and exponential losses. Experiments demonstrate the state-of-the-art performance of DiscoPOP and its successful transfer to held-out tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-280" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-280', event_id='94149', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3305</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94149">Panacea: Pareto Alignment via Preference Adaptation for LLMs</a></strong></h5>


                        <p class="text-muted">
                            Yifan Zhong &middot; Chengdong Ma &middot; Xiaoyuan Zhang &middot; Ziran Yang &middot; Haojun Chen &middot; Qingfu Zhang &middot; Siyuan Qi &middot; Yaodong Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Current methods for large language model alignment typically use scalar human preference labels. However, this convention tends to oversimplify the multi-dimensional and heterogeneous nature of human preferences, leading to reduced expressivity and even misalignment. This paper presents Panacea, an innovative approach that reframes alignment as a multi-dimensional preference optimization problem. Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning. A major challenge here is using a low-dimensional preference vector to guide the model's behavior, despite it being governed by an overwhelmingly large number of parameters. To address this, Panacea is designed to use singular value decomposition (SVD)-based low-rank adaptation, which allows the preference vector to be simply injected online as singular values. Theoretically, we prove that Panacea recovers the entire Pareto front with common loss aggregation methods under mild conditions. Moreover, our experiments demonstrate, for the first time, the feasibility of aligning a single LLM to represent an exponentially vast spectrum of human preferences through various optimization methods. Our work marks a step forward in effectively and efficiently aligning models to diverse and intricate human preferences in a controllable and Pareto-optimal manner.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-281" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-281', event_id='96840', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3306</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96840">Delving into the Reversal Curse: How Far Can Large Language Models Generalize?</a></strong></h5>


                        <p class="text-muted">
                            Zhengkai Lin &middot; Zhihang Fu &middot; Kai Liu &middot; Liang Xie &middot; Binbin Lin &middot; Wenxiao Wang &middot; Deng Cai &middot; Yue Wu &middot; Jieping Ye
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While large language models (LLMs) showcase unprecedented capabilities, they also exhibit certain inherent limitations when facing seemingly trivial tasks. A prime example is the recently debated "reversal curse", which surfaces when models, having been trained on the fact "A is B", struggle to generalize this knowledge to infer that "B is A".In this paper, we examine the manifestation of the reversal curse across various tasks and delve into both the generalization abilities and the problem-solving mechanisms of LLMs. This investigation leads to a series of significant insights:(1) LLMs are able to generalize to "B is A" when both A and B are presented in the context as in the case of a multiple-choice question.(2) This generalization ability is highly correlated to the structure of the fact "A is B" in the training documents. For example, this generalization only applies to biographies structured in "[Name] is [Description]" but not to "[Description] is [Name]".(3) We propose and verify the hypothesis that LLMs possess an inherent bias in fact recalling during knowledge application, which explains and underscores the importance of the document structure to successful learning.(4) The negative impact of this bias on the downstream performance of LLMs can hardly be mitigated through training alone.Based on these intriguing findings, our work not only presents a novel perspective for interpreting LLMs' generalization abilities from their intrinsic working mechanism but also provides new insights for the development of more effective learning methods for LLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-282" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-282', event_id='95886', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3307</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95886">CorDA: Context-Oriented Decomposition Adaptation of Large Language Models for Task-Aware Parameter-Efficient Fine-tuning</a></strong></h5>


                        <p class="text-muted">
                            Yibo Yang &middot; Xiaojie Li &middot; Zhongzhu Zhou &middot; Shuaiwen Song &middot; Jianlong Wu &middot; Liqiang Nie &middot; Bernard Ghanem
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Current parameter-efficient fine-tuning (PEFT) methods build adapters widely agnostic of the context of downstream task to learn, or the context of important knowledge to maintain. As a result, there is often a performance gap compared to full-parameter fine-tuning, and meanwhile the fine-tuned model suffers from catastrophic forgetting of the pre-trained world knowledge. In this paper, we propose **CorDA**, a Context-oriented Decomposition Adaptation method that builds learnable **task-aware adapters** from weight decomposition oriented by the context of downstream task or the world knowledge to maintain. Concretely, we collect a few data samples, and perform singular value decomposition for each linear layer of a pre-trained LLM multiplied by the covariance matrix of the input activation using these samples. The inverse of the covariance matrix is multiplied with the decomposed components to reconstruct the original weights. By doing so, the context of the representative samples is captured through deciding the factorizing orientation. Our method enables two options, the **knowledge-preserved adaptation** and the **instruction-previewed adaptation**. For the former, we use question-answering samples to obtain the covariance matrices, and use the decomposed components with the smallest $r$ singular values to initialize a learnable adapter, with the others frozen such that the world knowledge is better preserved. For the latter, we use the instruction data from the fine-tuning task, such as math or coding, to orientate the decomposition and train the largest $r$ components that most correspond to the task to learn. We conduct extensive experiments on Math, Code, and Instruction Following tasks. Our knowledge-preserved adaptation not only achieves better performance than LoRA on fine-tuning tasks, but also mitigates the forgetting of world knowledge. Our instruction-previewed adaptation is able to further enhance the fine-tuning performance to be comparable with full fine-tuning, surpassing the state-of-the-art PEFT methods such as LoRA, DoRA, and PiSSA.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-283" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-283', event_id='95060', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3308</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95060">Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in Language Models</a></strong></h5>


                        <p class="text-muted">
                            Minki Kang &middot; Sung Ju Hwang &middot; Gibbeum Lee &middot; Jaewoong Cho
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>As Large Language Models (LLMs) are increasingly deployed in specialized domains with continuously evolving knowledge, the need for timely and precise knowledge injection has become essential. Fine-tuning with paraphrased data is a common approach to enhance knowledge injection, yet it faces two significant challenges: high computational costs due to repetitive external model usage and limited sample diversity. To this end, we introduce LaPael, a latent-level paraphrasing method that applies input-dependent noise to early LLM layers.This approach enables diverse and semantically consistent augmentations directly within the model. Furthermore, it eliminates the recurring costs of paraphrase generation for each knowledge update. Our extensive experiments on question-answering benchmarks demonstrate that LaPael improves knowledge injection over standard fine-tuning and existing noise-based approaches. Additionally, combining LaPael with data-level paraphrasing further enhances performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-284" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-284', event_id='94366', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3309</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94366">Knowledge Graph Completion by Intermediate Variables Regularization</a></strong></h5>


                        <p class="text-muted">
                            Changyi Xiao &middot; Yixin Cao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Knowledge graph completion (KGC) can be framed as a 3-order binary tensor completion task. Tensor decomposition-based (TDB) models have demonstrated strong performance in KGC. In this paper, we provide a summary of existing TDB models and derive a general form for them, serving as a foundation for further exploration of TDB models. Despite the expressiveness of TDB models, they are prone to overfitting. Existing regularization methods merely minimize the norms of embeddings to regularize the model, leading to suboptimal performance. Therefore, we propose a novel regularization method for TDB models that addresses this limitation. The regularization is applicable to most TDB models and ensures tractable computation. Our method minimizes the norms of intermediate variables involved in the different ways of computing the predicted tensor. To support our regularization method, we provide a theoretical analysis that proves its effect in promoting low trace norm of the predicted tensor to reduce overfitting. Finally, we conduct experiments to verify the effectiveness of our regularization technique as well as the reliability of our theoretical analysis. The code is available at https://github.com/changyi7231/IVR.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-285" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-285', event_id='94043', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3310</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94043">HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Bernal Jimenez Gutierrez &middot; Yiheng Shu &middot; Yu Gu &middot; Michihiro Yasunaga &middot; Yu Su
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering (QA) and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-20 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-286" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-286', event_id='93977', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3311</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93977">Agent Planning with World Knowledge Model</a></strong></h5>


                        <p class="text-muted">
                            Shuofei Qiao &middot; Runnan Fang &middot; Ningyu Zhang &middot; Yuqi Zhu &middot; Xiang Chen &middot; Shumin Deng &middot; Yong Jiang &middot; Pengjun Xie &middot; Fei Huang &middot; Huajun Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the "real" physical world. Imitating humans' mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three real-world simulated datasets with Mistral-7B, Gemma-7B, and Llama-3-8B demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent's understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-287" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-287', event_id='97658', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3400</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97658">ClashEval: Quantifying the tug-of-war between an LLMâs internal prior and external evidence</a></strong></h5>


                        <p class="text-muted">
                            Kevin Wu &middot; Eric Wu &middot; James Zou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Retrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model's initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors.We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over 60\% of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs -- namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect. Our dataset, called ClashEval, and evaluations are open-sourced to allow for future benchmarking on top-performing models at https://github.com/kevinwu23/StanfordClashEval.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-288" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-288', event_id='92940', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3401</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92940">Entrywise error bounds for low-rank approximations of kernel matrices</a></strong></h5>


                        <p class="text-muted">
                            Alexander Modell
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we derive <em>entrywise</em> error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition). While this approximation is well-known to be optimal with respect to the spectral and Frobenius norm error, little is known about the statistical behaviour of individual entries. Our error bounds fill this gap. A key technical innovation is a delocalisation result for the eigenvectors of the kernel matrix corresponding to small eigenvalues, which takes inspiration from the field of Random Matrix Theory. Finally, we validate our theory with an empirical study of a collection of synthetic and real-world datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-289" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-289', event_id='93740', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3402</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93740">Reciprocal Learning</a></strong></h5>


                        <p class="text-muted">
                            Julian Rodemann &middot; Christoph Jansen &middot; Georg Schollmeyer
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We demonstrate that numerous machine learning algorithms are specific instances of one single paradigm: reciprocal learning. These instances range from active learning over multi-armed bandits to self-training. We show that all these algorithms not only learn parameters from data but also vice versa: They iteratively alter training data in a way that depends on the current model fit. We introduce reciprocal learning as a generalization of these algorithms using the language of decision theory. This allows us to study under what conditions they converge. The key is to guarantee that reciprocal learning contracts such that the Banach fixed-point theorem applies. In this way, we find that reciprocal learning converges at linear rates to an approximately optimal model under some assumptions on the loss function, if their predictions are probabilistic and the sample adaption is both non-greedy and either randomized or regularized. We interpret these findings and provide corollaries that relate them to active learning, self-training, and bandits.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-290" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-290', event_id='94157', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3403</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94157">Exponential Quantum Communication Advantage in Distributed Inference and Learning</a></strong></h5>


                        <p class="text-muted">
                            Dar Gilboa &middot; Hagay Michaeli &middot; Daniel Soudry &middot; Jarrod McClean
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Training and inference with large machine learning models that far exceed the memory capacity of individual devices necessitates the design of distributed architectures, forcing one to contend with communication constraints.  We present a framework for distributed computation over a quantum network in which data is encoded into specialized quantum states. We prove that for models within this framework, inference and training using gradient descent can be performed with exponentially less communication compared to their classical analogs, and with relatively modest overhead relative to standard gradient-based methods. We show that certain graph neural networks are particularly amenable to implementation within this framework, and moreover present empirical evidence that they perform well on standard benchmarks.To our knowledge, this is the first example of exponential quantum advantage for a generic class of machine learning problems that hold regardless of the data encoding cost. Moreover, we show that models in this class can encode highly nonlinear features of their inputs, and their expressivity increases exponentially with model depth.We also delineate the space of models for which exponential communication advantages hold by showing that they cannot hold for linear classification. Communication of quantum states that potentially limit the amount of information that can be extracted from them about the data and model parameters may also lead to improved privacy guarantees for distributed computation. Taken as a whole, these findings form a promising foundation for distributed machine learning over quantum networks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-291" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-291', event_id='94434', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3404</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94434">Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods</a></strong></h5>


                        <p class="text-muted">
                            Felix Dangel
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite their simple intuition, convolutions are more tedious to analyze than dense layers, which complicates the transfer of theoretical and algorithmic ideas to convolutions. We simplify convolutions by viewing them as tensor networks (TNs) that allow reasoning about the underlying tensor multiplications by drawing diagrams, manipulating them to perform function transformations like differentiation, and efficiently evaluating them with <code>einsum</code>. To demonstrate their simplicity and expressiveness, we derive diagrams of various autodiff operations and popular curvature approximations with full hyper-parameter support, batching, channel groups, and generalization to any convolution dimension. Further, we provide convolution-specific transformations based on the connectivity pattern which allow to simplify diagrams before evaluation. Finally, we probe performance. Our TN implementation accelerates a recently-proposed KFAC variant up to 4.5 x while removing the standard implementation's memory overhead, and enables new hardware-efficient tensor dropout for approximate backpropagation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-292" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-292', event_id='94485', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3405</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94485">Identifying Equivalent Training Dynamics</a></strong></h5>


                        <p class="text-muted">
                            William Redman &middot; Juan Bello-Rivas &middot; Maria Fonoberova &middot; Ryan Mohr &middot; Yannis Kevrekidis &middot; Igor Mezic
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Study of the nonlinear evolution deep neural network (DNN) parameters undergo during training has uncovered regimes of distinct dynamical behavior. While a detailed understanding of these phenomena has the potential to advance improvements in training efficiency and robustness, the lack of methods for identifying when DNN models have equivalent dynamics limits the insight that can be gained from prior work. Topological conjugacy, a notion from dynamical systems theory, provides a precise definition of dynamical equivalence, offering a possible route to address this need. However, topological conjugacies have historically been challenging to compute. By leveraging advances in Koopman operator theory, we develop a framework for identifying conjugate and non-conjugate training dynamics. To validate our approach, we demonstrate that comparing Koopman eigenvalues can correctly identify a known equivalence between online mirror descent and online gradient descent. We then utilize our approach to: (a) identify non-conjugate training dynamics between shallow and wide fully connected neural networks; (b) characterize the early phase of training dynamics in convolutional neural networks; (c) uncover non-conjugate training dynamics in Transformers that do and do not undergo grokking. Our results, across a range of DNN architectures, illustrate the flexibility of our framework and highlight its potential for shedding new light on training dynamics.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-293" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-293', event_id='94945', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3406</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94945">AGILE: A Novel Reinforcement Learning Framework of LLM Agents</a></strong></h5>


                        <p class="text-muted">
                            Feng Peiyuan &middot; Yichen He &middot; Guanhua Huang &middot; Yuan Lin &middot; Hanchong Zhang &middot; Yuchen Zhang &middot; Hang Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments)  designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/AGILE.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-294" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-294', event_id='96418', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3407</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96418">QTIP: Quantization with Trellises and Incoherence Processing</a></strong></h5>


                        <p class="text-muted">
                            Albert Tseng &middot; Qingyao Sun &middot; David Hou &middot; Christopher De Sa
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing weights to low-precision datatypes.Since LLM inference is usually memory-bound, PTQ methods can improve inference throughput.Recent state-of-the-art PTQ approaches use vector quantization (VQ) to quantize multiple weights at once, which improves information utilization through better shaping.However, VQ requires a codebook with size exponential in the dimension.This limits current VQ-based PTQ works to low VQ dimensions ($\le 8$) that in turn limit quantization quality.Here, we introduce QTIP, which instead uses trellis coded quantization (TCQ) to achieve ultra-high-dimensional quantization. TCQ uses a stateful decoder that separates the codebook size from the bitrate and effective dimension. QTIP introduces a spectrum of lookup-only to computed lookup-free trellis codes designed for a hardware-efficient "bitshift" trellis structure; these codes achieve state-of-the-art results in both quantization quality and inference speed.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-295" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-295', event_id='96738', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3408</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96738">Handling Learnwares from Heterogeneous Feature Spaces with Explicit Label Exploitation</a></strong></h5>


                        <p class="text-muted">
                            Peng Tan &middot; Hai-Tian Liu &middot; Zhi-Hao Tan &middot; Zhi-Hua Zhou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The learnware paradigm aims to help users leverage numerous existing high-performing models instead of starting from scratch, where a learnware consists of a well-trained model and the specification describing its capability. Numerous learnwares are accommodated by a learnware dock system. When users solve tasks with the system, models that fully match the task feature space are often rare or even unavailable. However, models with heterogeneous feature space can still be helpful. This paper finds that label information, particularly model outputs, is helpful yet previously less exploited in the accommodation of heterogeneous learnwares. We extend the specification to better leverage model pseudo-labels and subsequently enrich the unified embedding space for better specification evolvement. With label information, the learnware identification can also be improved by additionally comparing conditional distributions. Experiments demonstrate that, even without a model explicitly tailored to user tasks, the system can effectively handle tasks by leveraging models from diverse feature spaces.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-296" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-296', event_id='96371', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3409</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96371">An Autoencoder-Like Nonnegative Matrix Co-Factorization for Improved Student Cognitive Modeling</a></strong></h5>


                        <p class="text-muted">
                            Shenbao Yu &middot; Yinghui Pan &middot; Yifeng Zeng &middot; Prashant Doshi &middot; Guoquan Liu &middot; Kim-Leng Poh &middot; Mingwei Lin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Student cognitive modeling (SCM) is a fundamental task in intelligent education, with applications ranging from personalized learning to educational resource allocation. By exploiting students' response logs, SCM aims to predict their exercise performance as well as estimate knowledge proficiency in a subject. Data mining approaches such as matrix factorization can obtain high accuracy in predicting student performance on exercises, but the knowledge proficiency is unknown or poorly estimated. The situation is further exacerbated if only sparse interactions exist between exercises and students (or knowledge concepts). To solve this dilemma, we root monotonicity (a fundamental psychometric theory on educational assessments) in a co-factorization framework and present an autoencoder-like nonnegative matrix co-factorization (AE-NMCF), which improves the accuracy of estimating the student's knowledge proficiency via an encoder-decoder learning pipeline. The resulting estimation problem is nonconvex with nonnegative constraints. We introduce a projected gradient method based on block coordinate descent with Lipschitz constants and guarantee the method's theoretical convergence. Experiments on several real-world data sets demonstrate the efficacy of our approach in terms of both performance prediction accuracy and knowledge estimation ability, when compared with existing student cognitive models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-297" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-297', event_id='94173', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3410</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94173">SLIM: Style-Linguistics Mismatch Model for Generalized Audio Deepfake Detection</a></strong></h5>


                        <p class="text-muted">
                            Yi Zhu &middot; Surya Koppisetti &middot; Trang Tran &middot; Gaurav Bharaj
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Audio deepfake detection (ADD) is crucial to combat the misuse of speech synthesized by generative AI models. Existing ADD models suffer from generalization issues to unseen attacks, with a large performance discrepancy between in-domain and out-of-domain data. Moreover, the black-box nature of existing models limits their use in real-world scenarios, where explanations are required for model decisions. To alleviate these issues, we introduce a new ADD model that explicitly uses the Style-LInguistics Mismatch (SLIM) in fake speech to separate them from real speech. SLIM first employs self-supervised pretraining on only real samples to learn the style-linguistics dependency in the real class. The learned features are then used in complement with standard pretrained acoustic features (e.g., Wav2vec) to learn a classifier on the real and fake classes. When the feature encoders are frozen, SLIM outperforms benchmark methods on out-of-domain datasets while achieving competitive results on in-domain data. The features learned by SLIM allow us to quantify the (mis)match between style and linguistic content in a sample, hence facilitating an explanation of the model decision.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-298" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-298', event_id='93450', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3411</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93450">KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge</a></strong></h5>


                        <p class="text-muted">
                            Pengcheng Jiang &middot; Lang Cao &middot; Cao (Danica) Xiao &middot; Parminder Bhatia &middot; Jimeng Sun &middot; Jiawei Han
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery. While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters. By incorporating this hierarchical knowledge along with textual information during the fine-tuning process, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on the benchmark datasets FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over state-of-the-art pre-trained language model-based methods, achieving improvements of 14.4\%, 13.5\%, and 11.9\% in the Hits@10 metric for the link prediction task, respectively. Furthermore, KG-FIT yields substantial performance gains of 12.6\%, 6.7\%, and 17.7\% compared to the structure-based base models upon which it is built. These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-299" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-299', event_id='95845', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3500</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95845">Learning Representations for Hierarchies with Minimal Support</a></strong></h5>


                        <p class="text-muted">
                            Benjamin Rozonoyer &middot; Michael Boratko &middot; Dhruvesh Patel &middot; Wenlong Zhao &middot; Shib Dasgupta &middot; Hung Le &middot; Andrew McCallum
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>When training node embedding models to represent large directed graphs (digraphs), it is impossible to observe all entries of the adjacency matrix during training. As a consequence most methods employ sampling. For very large digraphs, however, this means many (most) entries may be unobserved during training. In general, observing every entry would be necessary to uniquely identify a graph, however if we know the graph has a certain property some entries can be omitted - for example, only half the entries would be required for a symmetric graph. In this work, we develop a novel framework to identify a subset of entries required to uniquely distinguish a graph among all transitively-closed DAGs. We give an explicit algorithm to compute the provably minimal set of entries, and demonstrate empirically that one can train node embedding models with greater efficiency and performance, provided the energy function has an appropriate inductive bias. We achieve robust performance on synthetic hierarchies and a larger real-world taxonomy, observing improved convergence rates in a resource-constrained setting while reducing the set of training examples by as much as 99%.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-300" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-300', event_id='94595', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3501</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94595">Graph-based Unsupervised Disentangled Representation Learning via Multimodal Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Baao Xie &middot; Qiuyu Chen &middot; Yunnan Wang &middot; Zequn Zhang &middot; Xin Jin &middot; Wenjun Zeng
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Disentangled representation learning (DRL) aims to identify and decompose underlying factors behind observations, thus facilitating data perception and generation. However, current DRL approaches often rely on the unrealistic assumption that semantic factors are statistically independent. In reality, these factors may exhibit correlations, which off-the-shelf solutions have yet to properly address. To tackle this challenge, we introduce a bidirectional weighted graph-based framework, to learn factorized attributes and their interrelations within complex data. Specifically, we propose a $\beta$-VAE based module to extract factors as the initial nodes of the graph, and leverage the multimodal large language model (MLLM) to discover and rank latent correlations, thereby updating the weighted edges. By integrating these complementary modules, our model successfully achieves fine-grained, practical and unsupervised disentanglement. Experiments demonstrate our method's superior performance in disentanglement and reconstruction. Furthermore, the model inherits enhanced interpretability and generalizability from MLLMs.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-301" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-301', event_id='93618', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3502</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93618">Implicit Regularization Paths of Weighted Neural Representations</a></strong></h5>


                        <p class="text-muted">
                            Jin-Hong Du &middot; Pratik Patil
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We study the implicit regularization effects induced by (observation) weighting of pretrained features.For weight and feature matrices of bounded operator norms that are infinitesimally free with respect to (normalized) trace functionals, we derive equivalence paths connecting different weighting matrices and ridge regularization levels.Specifically, we show that ridge estimators trained on weighted features along the same path are asymptotically equivalent when evaluated against test vectors of bounded norms.These paths can be interpreted as matching the effective degrees of freedom of ridge estimators fitted with weighted features.For the special case of subsampling without replacement, our results apply to independently sampled random features and kernel features and confirm recent conjectures (Conjectures 7 and 8) of the authors on the existence of such paths in Patil and Du (2023).We also present an additive risk decomposition for ensembles of weighted estimators and show that the risks are equivalent along the paths when the ensemble size goes to infinity.As a practical consequence of the path equivalences, we develop an efficient cross-validation method for tuning and apply it to subsampled pretrained representations across several models (e.g., ResNet-50) and datasets (e.g., CIFAR-100).</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-302" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-302', event_id='97434', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3503</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97434">V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark</a></strong></h5>


                        <p class="text-muted">
                            Yi Xin &middot; Siqi Luo &middot; Xuyang Liu &middot; Haodi Zhou &middot; Xinyu Cheng &middot; Christina Lee &middot; Junlong Du &middot; Yuntao Du. &middot; Haozhe Wang &middot; MingCai Chen &middot; Ting Liu &middot; Guimin Hu &middot; Zhongwei Wan &middot; rongchao zhang &middot; Aoxue Li &middot; Mingyang Yi &middot; Xiaohong Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Parameter-efficient transfer learning (PETL) methods show promise in adapting a pre-trained model to various downstream tasks while training only a few parameters. In the computer vision (CV) domain, numerous PETL algorithms have been proposed, but their direct employment or comparison remains inconvenient. To address this challenge, we construct a Unified Visual PETL Benchmark (V-PETL Bench) for the CV domain by selecting 30 diverse, challenging, and comprehensive datasets from image recognition, video action recognition, and dense prediction tasks. On these datasets, we systematically evaluate 25 dominant PETL algorithms and open-source a modular and extensible codebase for fair evaluation of these algorithms. V-PETL Bench runs on NVIDIA A800 GPUs and requires approximately 310 GPU days. We release all the checkpoints and training logs, making it more efficient and friendly to researchers. Additionally, V-PETL Bench will be continuously updated for new PETL algorithms and CV tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-303" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-303', event_id='96052', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3504</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96052">Meta-Learning Universal Priors Using Non-Injective Change of Variables</a></strong></h5>


                        <p class="text-muted">
                            Yilang Zhang &middot; Alireza Sadeghi &middot; Georgios Giannakis
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Meta-learning empowers data-hungry deep neural networks to rapidly learn from merely a few samples, which is especially appealing to tasks with small datasets. Critical in this context is the <em>prior knowledge</em> accumulated from related tasks. Existing meta-learning approaches typically rely on preselected priors, such as a Gaussian probability density function (pdf). The limited expressiveness of such priors however, hinders the enhanced performance of the trained model when dealing with tasks having exceedingly scarce data. Targeting improved expressiveness, this contribution introduces a <em>data-driven</em> prior that optimally fits the provided tasks using a novel non-injective change-of-variable (NCoV) model. Unlike preselected prior pdfs with fixed shapes, the advocated NCoV model can effectively approximate a considerably wide range of pdfs. Moreover, compared to conventional change-of-variable models, the introduced NCoV exhibits augmented expressiveness for pdf modeling, especially in high-dimensional spaces. Theoretical analysis underscores the appealing universal approximation capacity of the NCoV model. Numerical experiments conducted on three few-shot learning datasets validate the superiority of data-driven priors over the prespecified ones, showcasing its pronounced effectiveness when dealing with extremely limited data resources.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-304" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-304', event_id='95971', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3505</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95971">On Affine Homotopy between Language Encoders</a></strong></h5>


                        <p class="text-muted">
                            Robin Chan &middot; Reda Boumasmoud &middot; Anej Svete &middot; Yuxin Ren &middot; Qipeng Guo &middot; Zhijing Jin &middot; Shauli Ravfogel &middot; Mrinmaya Sachan &middot; Bernhard SchÃ¶lkopf &middot; Mennatallah El-Assady &middot; Ryan Cotterell
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Pre-trained language encoders---functions that represent text as vectors---are an integral component of many NLP tasks.    We tackle a natural question in language encoder analysis: What does it mean for two encoders to be similar?    We contend that a faithful measure of similarity needs to be \emph{intrinsic}, that is, task-independent, yet still be informative of \emph{extrinsic} similarity---the performance on downstream tasks.    It is common to consider two encoders similar if they are \emph{homotopic}, i.e., if they can be aligned through some transformation.    In this spirit, we study the properties of \emph{affine} alignment of language encoders and its implications on extrinsic similarity.    We find that while affine alignment is fundamentally an asymmetric notion of similarity, it is still informative of extrinsic similarity.    We confirm this on datasets of natural language representations.    Beyond providing useful bounds on extrinsic similarity, affine intrinsic similarity also allows us to begin uncovering the structure of the space of pre-trained encoders by defining an order over them.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-305" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-305', event_id='95922', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3506</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95922">Knowledge Composition using Task Vectors with Learned Anisotropic Scaling</a></strong></h5>


                        <p class="text-muted">
                            Frederic Z. Zhang &middot; Paul Albert &middot; Cristian Rodriguez-Opazo &middot; Anton van den Hengel &middot; Ehsan Abbasnejad
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Pre-trained models produce strong generic representations that can be adapted via fine-tuning on specialised datasets. The learned weight difference relative to the pre-trained model, known as a task vector, characterises the direction and stride of fine-tuning that enables the model to capture these specialised representations. The significance of task vectors is such that simple arithmetic operations on them can be used to combine diverse representations from different domains. This paper builds on these properties of task vectors and aims to answer (1) whether components of task vectors, particularly parameter blocks, exhibit similar characteristics, and (2) how such blocks can be used to enhance knowledge composition and transfer. To this end, we introduce aTLAS, an algorithm that linearly combines parameter blocks with different learned coefficients, resulting in anisotropic scaling at the task vector level. We show that such linear combinations explicitly exploit the low intrinsic dimensionality of pre-trained models, with only a few coefficients being the learnable parameters. Furthermore, composition of parameter blocks enables modular learning that effectively leverages the already learned representations, thereby reducing the dependency on large amounts of data. We demonstrate the effectiveness of our method in task arithmetic, few-shot recognition and test-time adaptation, with supervised or unsupervised objectives. In particular, we show that (1) learned anisotropic scaling allows task vectors to be more disentangled, causing less interference in composition; (2) task vector composition excels with scarce or no labelled data and is less prone to domain shift, thus leading to better generalisability; (3) mixing the most informative parameter blocks across different task vectors prior to training can reduce the memory footprint and improve the flexibility of knowledge transfer. Moreover, we show the potential of aTLAS as a parameter-efficient fine-tuning method, particularly with less data, and demonstrate that it can be easily scaled up for higher performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-306" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-306', event_id='95319', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3507</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95319">Transfer Learning for Latent Variable Network Models</a></strong></h5>


                        <p class="text-muted">
                            Akhil Jalan &middot; Arya Mazumdar &middot; Soumendu Sundar Mukherjee &middot; Purnamrita Sarkar
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We study transfer learning for estimation in latent variable network models. In our setting, the conditional edge probability matrices given the latent variables are represented by $P$ for the source and $Q$ for the target. We wish to estimate $Q$ given two kinds of data: (1) edge data from a subgraph induced by an $o(1)$ fraction of the nodes of $Q$, and (2) edge data from all of $P$. If the source $P$ has no relation to the target $Q$, the estimation error must be $\Omega(1)$. However, we show that if the latent variables are shared, then vanishing error is possible. We give an efficient algorithm that utilizes the ordering of a suitably defined graph distance. Our algorithm achieves $o(1)$ error and does not assume a parametric form on the source or target networks. Next, for the specific case of Stochastic Block Models we prove a minimax lower bound and show that a simple algorithm achieves this rate. Finally, we empirically demonstrate our algorithm's use on real-world and simulated graph transfer problems.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-307" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-307', event_id='94505', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3508</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94505">Disentangling and mitigating the impact of task similarity for continual learning</a></strong></h5>


                        <p class="text-muted">
                            Naoki Hiratani
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Continual learning of partially similar tasks poses a challenge for artificial neural networks, as task similarity presents both an opportunity for knowledge transfer and a risk of interference and catastrophic forgetting.However, it remains unclear how task similarity in input features and readout patterns influences knowledge transfer and forgetting, as well as how they interact with common algorithms for continual learning.Here, we develop a linear teacher-student model with latent structure and show analytically that high input feature similarity coupled with low readout similarity is catastrophic for both knowledge transfer and retention. Conversely, the opposite scenario is relatively benign. Our analysis further reveals that task-dependent activity gating improves knowledge retention at the expense of transfer, while task-dependent plasticity gating does not affect either retention or transfer performance at the over-parameterized limit. In contrast, weight regularization based on the Fisher information metric significantly improves retention, regardless of task similarity, without compromising transfer performance. Nevertheless, its diagonal approximation and regularization in the Euclidean space are much less robust against task similarity. We demonstrate consistent results in a permuted MNIST task with latent variables. Overall, this work provides insights into when continual learning is difficult and how to mitigate it.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-308" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-308', event_id='93505', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3509</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93505">A Closer Look at the CLS Token for Cross-Domain Few-Shot Learning</a></strong></h5>


                        <p class="text-muted">
                            Yixiong Zou &middot; Shuai Yi &middot; Yuhua Li &middot; Ruixuan Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Vision Transformer (ViT) has shown great power in learning from large-scale datasets. However, collecting sufficient data for expert knowledge is always difficult. To handle this problem, Cross-Domain Few-Shot Learning (CDFSL) has been proposed to transfer the source-domain knowledge learned from sufficient data to target domains where only scarce data is available. In this paper, we find an intriguing phenomenon neglected by previous works for the CDFSL task based on ViT: leaving the CLS token to random initialization, instead of loading source-domain trained parameters, could consistently improve target-domain performance. We then delve into this phenomenon for an interpretation. We find <strong>the CLS token naturally absorbs domain information</strong> due to the inherent structure of the ViT, which is represented as the low-frequency component in the Fourier frequency space of images. Based on this phenomenon and interpretation, we further propose a method for the CDFSL task to decouple the domain information in the CLS token during the source-domain training, and adapt the CLS token on the target domain for efficient few-shot learning. Extensive experiments on four benchmarks validate our rationale and state-of-the-art performance. Our codes are available at https://github.com/Zoilsen/CLS<em>Token</em>CDFSL.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-309" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-309', event_id='92938', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3510</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92938">TPR: Topology-Preserving Reservoirs for Generalized Zero-Shot Learning</a></strong></h5>


                        <p class="text-muted">
                            Hui Chen &middot; Yanbin Liu &middot; Yongqiang Ma &middot; Nanning Zheng &middot; Xin Yu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Pre-trained vision-language models (VLMs) such as CLIP have shown excellent performance for zero-shot classification. Based on CLIP, recent methods design various learnable prompts to evaluate the zero-shot generalization capability on a base-to-novel setting. This setting assumes test samples are already divided into either base or novel classes, limiting its application to realistic scenarios. In this paper, we focus on a more challenging and practical setting: generalized zero-shot learning (GZSL), i.e., testing with no information about the base/novel division. To address this challenging zero-shot problem, we introduce two unique designs that enable us to classify an image without the need of knowing whether it comes from seen or unseen classes. Firstly, most existing methods only adopt a single latent space to align visual and linguistic features, which has a limited ability to represent complex visual-linguistic patterns, especially for fine-grained tasks. Instead, we propose a dual-space feature alignment module that effectively augments the latent space with a novel attribute space induced by a well-devised attribute reservoir. In particular, the attribute reservoir consists of a static vocabulary and learnable tokens complementing each other for flexible control over feature granularity. Secondly, finetuning CLIP models (e.g., prompt learning) on seen base classes usually sacrifices the model's original generalization capability on unseen novel classes. To mitigate this issue, we present a new topology-preserving objective that can enforce feature topology structures of the combined base and novel classes to resemble the topology of CLIP. In this manner, our model will inherit the generalization ability of CLIP through maintaining the pairwise class angles in the attribute space. Extensive experiments on twelve object recognition datasets demonstrate that our model, termed Topology-Preserving Reservoir (TPR), outperforms strong baselines including both prompt learning and conventional generative-based zero-shot methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-310" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-310', event_id='98320', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3511</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/98320">Topological Hidden Markov Models</a></strong></h5>


                        <p class="text-muted">
                            Adam B Kashlak &middot; Prachi Loliencar &middot; Giseon Heo
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The Hidden Markov Model is a classic modelling tool with a wide swath of applications.  Its inception considered observations restricted to a finite alphabet, but it was quickly extended to multivariate continuous distributions.  In this article, we further extend the Hidden Markov Model from mixtures of normal distributions in $d$-dimensional Euclidean space to general Gaussian measure mixtures in locally convex topological spaces, and hence, we christen this method the Topological Hidden Markov Model. The main innovation is the use of the Onsager-Machlup functional as a proxy for the probability density function in infinite dimensional spaces. This allows for choice of a Cameron-Martin space suitable for a given application. We demonstrate the versatility of this methodology by applying it to simulated diffusion processes such as Brownian and fractional Brownian sample paths as well as the Ornstein-Uhlenbeck process. Our methodology is applied to the identification of sleep states from overnight polysomnography time series data with the aim of diagnosing Obstructive Sleep Apnea in pediatric patients.  It is also applied to a series of annual cumulative snowfall curves from 1940 to 1990 in the city of Edmonton, Alberta.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-311" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-311', event_id='95304', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3600</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95304">Hierarchical Visual Feature Aggregation for OCR-Free Document Understanding</a></strong></h5>


                        <p class="text-muted">
                            JaeYoo Park &middot; Jin Young Choi &middot; Jeonghyung Park &middot; Bohyung Han
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present a novel OCR-free document understanding framework based on pretrained Multimodal Large Language Models (MLLMs). Our approach employs multi-scale visual features to effectively handle various font sizes within document images.To address the increasing costs of considering the multi-scale visual inputs for MLLMs, we propose the Hierarchical Visual Feature Aggregation (HVFA) module, designed to reduce the number of input tokens to LLMs. Leveraging a feature pyramid with cross-attentive pooling, our approach effectively manages the trade-off between information loss and efficiency without being affected by varying document image sizes.Furthermore, we introduce a novel instruction tuning task, which facilitates the model's text-reading capability by learning to predict the relative positions of input text, eventually minimizing the risk of truncated text caused by the limited capacity of LLMs.Comprehensive experiments validate the effectiveness of our approach, demonstrating superior performance in various document understanding tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-312" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-312', event_id='95415', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3601</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95415">Test-Time Dynamic Image Fusion</a></strong></h5>


                        <p class="text-muted">
                            Bing Cao &middot; Yinan Xia &middot; Yi Ding &middot; Changqing Zhang &middot; Qinghua Hu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The inherent challenge of image fusion lies in capturing the correlation of multi-source images and comprehensively integrating effective information from different sources. Most existing techniques fail to perform dynamic image fusion while notably lacking theoretical guarantees, leading to potential deployment risks in this field. Is it possible to conduct dynamic image fusion with a clear theoretical justification? In this paper, we give our solution from a generalization perspective. We proceed to reveal the generalized form of image fusion and derive a new test-time dynamic image fusion paradigm. It provably reduces the upper bound of generalization error. Specifically, we decompose the fused image into multiple components corresponding to its source data. The decomposed components represent the effective information from the source data, thus the gap between them reflects the \textit{Relative Dominability} (RD) of the uni-source data in constructing the fusion image. Theoretically, we prove that the key to reducing generalization error hinges on the negative correlation between the RD-based fusion weight and the uni-source reconstruction loss. Intuitively, RD dynamically highlights the dominant regions of each source and can be naturally converted to the corresponding fusion weight, achieving robust results. Extensive experiments and discussions with in-depth analysis on multiple benchmarks confirm our findings and superiority. Our code is available at https://github.com/Yinan-Xia/TTD.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-313" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-313', event_id='97560', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3602</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97560">WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences</a></strong></h5>


                        <p class="text-muted">
                            Yujie Lu &middot; Dongfu Jiang &middot; Wenhu Chen &middot; William Yang Wang &middot; Yejin Choi &middot; Bill Yuchen Lin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WildVision-Arena (WV-Arena), an online platform that collects human preferences to evaluate VLMs. We curated WV-Bench by selecting 500 high-quality samples from 8,000 user submissions in WV-Arena. WV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet, achieving a Spearman correlation of 0.94 with the WV-Arena Elo. This significantly outperforms other benchmarks like MMVet, MMMU, and MMStar.Our comprehensive analysis of 20K real-world interactions reveals important insights into the failure cases of top-performing VLMs. For example, we find that although GPT-4V surpasses many other models like Reka-Flash, Opus, and Yi-VL-Plus in simple visual recognition and reasoning tasks, it still faces challenges with subtle contextual cues, spatial reasoning, visual imagination, and expert domain knowledge. Additionally, current VLMs exhibit issues with hallucinations and safety when intentionally provoked. We are releasing our chat and feedback data to further advance research in the field of VLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-314" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-314', event_id='97677', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3603</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97677">VHELM: A Holistic Evaluation of Vision Language Models</a></strong></h5>


                        <p class="text-muted">
                            Tony Lee &middot; Haoqin Tu &middot; Chi Heem Wong &middot; Wenhao Zheng &middot; Yiyang Zhou &middot; Yifan Mai &middot; Josselin Roberts &middot; Michihiro Yasunaga &middot; Huaxiu Yao &middot; Cihang Xie &middot; Percy Liang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other equally critical aspects such as fairness, unbiasedness, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various benchmark datasets and maps the scenarios to one or more of the 8 aspects: unbiasedness, fairness, knowledge, multilinguality, reasoning, robustness, toxicity mitigation, and perception. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons between models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 18 VLMs on 19 datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the observation that no single model excels across all aspects (as of the time of writing). For transparency, we release the raw model generations and complete results on our website at https://crfm.stanford.edu/helm/vhelm/v2.0.0. VHELM is meant to be a living benchmark, and we hope to continue adding new scenarios and models over time.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-315" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-315', event_id='97769', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3604</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97769">MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens</a></strong></h5>


                        <p class="text-muted">
                            Anas Awadalla &middot; Le Xue &middot; Oscar Lo &middot; Manli Shu &middot; Hannah Lee &middot; Etash Guha &middot; Sheng Shen &middot; Mohamed Awadalla &middot; Silvio Savarese &middot; Caiming Xiong &middot; Ran Xu &middot; Yejin Choi &middot; Ludwig Schmidt
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multimodal interleaved datasets featuring free-form interleaved sequences of images and text are crucial for training frontier large multimodal models (LMMs). Despite the rapid progression of open-source LMMs, there remains a pronounced scarcity of large-scale, diverse open-source multimodal interleaved datasets.In response, we introduce MINT-1T, the most extensive and diverse open-source Multimodal INTerleaved dataset to date. MINT-1T comprises one trillion text tokens and three billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. As scaling multimodal interleaved datasets requires substantial engineering effort, sharing the data curation process and releasing the dataset greatly benefits the community. Our experiments show that LMMs trained on MINT-1T rival the performance of models trained on the previous leading dataset, OBELICS.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-316" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-316', event_id='93416', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3605</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93416">OwMatch: Conditional Self-Labeling with Consistency for Open-world Semi-Supervised Learning</a></strong></h5>


                        <p class="text-muted">
                            Shengjie Niu &middot; Lifan Lin &middot; Jian Huang &middot; Chao Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Semi-supervised learning (SSL) offers a robust framework for harnessing the potential of unannotated data. Traditionally, SSL mandates that all classes possess labeled instances. However, the emergence of open-world SSL (OwSSL) introduces a more practical challenge, wherein unlabeled data may encompass samples from unseen classes. This scenario leads to misclassification of unseen classes as known ones, consequently undermining classification accuracy. To overcome this challenge, this study revisits two methodologies from self-supervised and semi-supervised learning, self-labeling and consistency, tailoring them to address the OwSSL problem. Specifically, we propose an effective framework called <em>OwMatch</em>, combining conditional self-labeling and open-world hierarchical thresholding.  Theoretically, we analyze the estimation of class distribution on unlabeled data through rigorous statistical analysis, thus demonstrating that OwMatch can ensure the unbiasedness of the label assignment estimator with reliability.  Comprehensive empirical analyses demonstrate that our method yields substantial performance enhancements across both known and unknown classes in comparison to previous studies. Code is available at <a href="https://github.com/niusj03/OwMatch">https://github.com/niusj03/OwMatch</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-317" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-317', event_id='93486', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3606</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93486">Protected Test-Time Adaptation via Online Entropy Matching: A Betting Approach</a></strong></h5>


                        <p class="text-muted">
                            Yarin Bar &middot; Shalev Shaer &middot; Yaniv Romano
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present a novel approach for test-time adaptation via online self-training, consisting of two components. First, we introduce a statistical framework that detects distribution shifts in the classifier's entropy values obtained on a stream of unlabeled samples. Second, we devise an online adaptation mechanism that utilizes the evidence of distribution shifts captured by the detection tool to dynamically update the classifier's parameters. The resulting adaptation process drives the distribution of test entropy values obtained from the self-trained classifier to match those of the source domain, building invariance to distribution shifts. This approach departs from the conventional self-training method, which focuses on minimizing the classifier's entropy. Our approach combines concepts in betting martingales and online learning to form a detection tool capable of quickly reacting to distribution shifts. We then reveal a tight relation between our adaptation scheme and optimal transport, which forms the basis of our novel self-supervised loss. Experimental results demonstrate that our approach improves test-time accuracy under distribution shifts while maintaining accuracy and calibration in their absence, outperforming leading entropy minimization methods across various scenarios.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-318" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-318', event_id='94661', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3607</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94661">The tree autoencoder model, with application to hierarchical data visualization</a></strong></h5>


                        <p class="text-muted">
                            Miguel A. Carreira-Perpinan &middot; Kuat Gazizov
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose a new model for dimensionality reduction, the PCA tree, which works like a regular autoencoder, having explicit projection and reconstruction mappings. The projection is effected by a sparse oblique tree, having hard, hyperplane splits using few features and linear leaves. The reconstruction mapping is a set of local linear mappings. Thus, rather than producing a global map as in t-SNE and other methods, which often leads to distortions, it produces a hierarchical set of local PCAs. The use of a sparse oblique tree and PCA makes the overall model interpretable and very fast to project or reconstruct new points. Joint optimization of all the parameters in the tree is a nonconvex nondifferentiable problem. We propose an algorithm that is guaranteed to decrease the error monotonically and which scales to large datasets without any approximation. In experiments, we show PCA trees are able to identify a wealth of low-dimensional and cluster structure in image and document datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-319" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-319', event_id='95632', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3608</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95632">Why the Metric Backbone Preserves Community Structure</a></strong></h5>


                        <p class="text-muted">
                            Maximilien Dreveton &middot; Charbel Chucri &middot; Matthias Grossglauser &middot; Patrick Thiran
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The metric backbone of a weighted graph is the union of all-pairs shortest paths. It is obtained by removing all edges $(u,v)$ that are not the shortest path between $u$ and $v$. In networks with well-separated communities, the metric backbone tends to preserve many inter-community edges, because these edges serve as bridges connecting two communities, but tends to delete many intra-community edges because the communities are dense. This suggests that the metric backbone would dilute or destroy the community structure of the network. However, this is not borne out by prior empirical work, which instead showed that the metric backbone of real networks preserves the community structure of the original network well. In this work, we analyze the metric backbone of a broad class of weighted random graphs with communities, and we formally prove the robustness of the community structure with respect to the deletion of all the edges that are not in the metric backbone. An empirical comparison of several graph sparsification techniques confirms our theoretical finding and shows that the metric backbone is an efficient sparsifier in the presence of communities.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-320" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-320', event_id='95899', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3609</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95899">Bridging Gaps: Federated Multi-View Clustering in Heterogeneous Hybrid Views</a></strong></h5>


                        <p class="text-muted">
                            Xinyue Chen &middot; Yazhou Ren &middot; Jie Xu &middot; Fangfei Lin &middot; Xiaorong Pu &middot; Yang Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recently, federated multi-view clustering (FedMVC) has emerged to explore cluster structures in multi-view data distributed on multiple clients. Many existing approaches tend to assume that clients are isomorphic and all of them belong to either single-view clients or multi-view clients. While these methods have succeeded, they may encounter challenges in practical FedMVC scenarios involving heterogeneous hybrid views, where a mixture of single-view and multi-view clients exhibit varying degrees of heterogeneity. In this paper, we propose a novel FedMVC framework, which concurrently addresses two challenges associated with heterogeneous hybrid views, i.e., client gap and view gap. To address the client gap, we design a local-synergistic contrastive learning approach that helps single-view clients and multi-view clients achieve consistency for mitigating heterogeneity among all clients. To address the view gap, we develop a global-specific weighting aggregation method, which encourages global models to learn complementary features from hybrid views. The interplay between local-synergistic contrastive learning and global-specific weighting aggregation mutually enhances the exploration of the data cluster structures distributed on multiple clients. Theoretical analysis and extensive experiments demonstrate that our method can handle the heterogeneous hybrid views in FedMVC and outperforms state-of-the-art methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-321" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-321', event_id='96369', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3610</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96369">MambaAD: Exploring State Space Models for Multi-class Unsupervised Anomaly Detection</a></strong></h5>


                        <p class="text-muted">
                            Haoyang He &middot; Yuhu Bai &middot; Jiangning Zhang &middot; Qingdong He &middot; Hongxu Chen &middot; Zhenye Gan &middot; Chengjie Wang &middot; Xiangtai Li &middot; Guanzhong Tian &middot; Lei Xie
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advancements in anomaly detection have seen the efficacy of CNN- and transformer-based approaches. However, CNNs struggle with long-range dependencies, while transformers are burdened by quadratic computational complexity. Mamba-based models, with their superior long-range modeling and linear efficiency, have garnered substantial attention. This study pioneers the application of Mamba to multi-class unsupervised anomaly detection, presenting MambaAD, which consists of a pre-trained encoder and a Mamba decoder featuring (Locality-Enhanced State Space) LSS modules at multi-scales. The proposed LSS module, integrating parallel cascaded (Hybrid State Space) HSS blocks and multi-kernel convolutions operations, effectively captures both long-range and local information. The HSS block, utilizing (Hybrid Scanning) HS encoders, encodes feature maps into five scanning methods and eight directions, thereby strengthening global connections through the (State Space Model) SSM. The use of Hilbert scanning and eight directions significantly improves feature sequence modeling. Comprehensive experiments on six diverse anomaly detection datasets and seven metrics demonstrate state-of-the-art performance, substantiating the method's effectiveness. The code and models are available at https://lewandofskee.github.io/projects/MambaAD.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-322" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-322', event_id='98305', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3611</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/98305">Unsupervised Anomaly Detection Algorithms on Real-world Data: How Many Do We Need?</a></strong></h5>


                        <p class="text-muted">
                            Roel Bouman &middot; Zaharah Bukhsh &middot; Tom Heskes
                        </p>

                    </div>
                    <div class="abstract">
                        <p>In this study we evaluate 33 unsupervised anomaly detection algorithms on 52 real-world multivariate tabular data sets, performing the largest comparison of unsupervised anomaly detection algorithms to date. On this collection of data sets, the EIF (Extended Isolation Forest) algorithm significantly outperforms the most other algorithms. Visualizing and then clustering the relative performance of the considered algorithms on all data sets, we identify two clear clusters: one with "local'' data sets, and another with "global'' data sets. "Local'' anomalies occupy a region with low density when compared to nearby samples, while "global'' occupy an overall low density region in the feature space. On the local data sets the $k$NN ($k$-nearest neighbor) algorithm comes out on top. On the global data sets, the EIF (extended isolation forest) algorithm performs the best. Also taking into consideration the algorithms' computational complexity, a toolbox with these two unsupervised anomaly detection algorithms suffices for finding anomalies in this representative collection of multivariate data sets. By providing access to code and data sets, our study can be easily reproduced and extended with more algorithms and/or data sets.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-323" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-323', event_id='95075', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3700</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95075">Unveiling Encoder-Free Vision-Language Models</a></strong></h5>


                        <p class="text-muted">
                            Haiwen Diao &middot; Yufeng Cui &middot; Xiaotong Li &middot; Yueze Wang &middot; Huchuan Lu &middot; Xinlong Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Existing vision-language models (VLMs) mostly rely on vision encoders to extract visual features followed by large language models (LLMs) for visual-language tasks. However, the vision encoders set a strong inductive bias in abstracting visual representation, e.g., resolution, aspect ratio, and semantic priors, which could impede the flexibility and efficiency of the VLMs. Training pure VLMs that accept the seamless vision and language inputs, i.e., without vision encoders, remains challenging and rarely explored. Empirical observations reveal that direct training without encoders results in slow convergence and large performance gaps. In this work, we bridge the gap between encoder-based and encoder-free models, and present a simple yet effective training recipe towards pure VLMs. Specifically, we unveil the key aspects of training encoder-free VLMs efficiently via thorough experiments: (1) Bridging vision-language representation inside one unified decoder; (2) Enhancing visual recognition capability via extra supervision. With these strategies, we launch EVE, an encoder-free vision-language model that can be trained and forwarded efficiently. Notably, solely utilizing 35M publicly accessible data, EVE can impressively rival the encoder-based VLMs of similar capacities across multiple vision-language benchmarks. It significantly outperforms the counterpart Fuyu-8B with mysterious training procedures and undisclosed training data. We believe that EVE provides a transparent and efficient route for developing pure decoder-only architecture across modalities.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-324" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-324', event_id='94861', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3701</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94861">Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning</a></strong></h5>


                        <p class="text-muted">
                            Brandon Huang &middot; Chancharik Mitra &middot; Leonid Karlinsky &middot; Assaf Arbelle &middot; Trevor Darrell &middot; Roei Herzig
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The recent success of interleaved Large Multimodal Models (LMMs) in few-shot learning suggests that in-context learning (ICL) with many examples can be promising for learning new tasks. However, this many-shot multimodal ICL setting has one crucial problem: it is fundamentally limited by the model's context length set at pretraining. The problem is especially prominent in the multimodal domain, which processes both text and images, requiring additional tokens. This motivates the need for a multimodal method to compress many shots into fewer tokens without finetuning. In this work, we enable LMMs to perform multimodal, many-shot in-context learning by leveraging Multimodal Task Vectors (MTV)---compact implicit representations of in-context examples compressed in the model's attention heads. Specifically, we first demonstrate the existence of such MTV in LMMs and then leverage these extracted MTV to enable many-shot in-context learning for various vision-and-language tasks. Our experiments suggest that MTV can scale in performance with the number of compressed shots and generalize to similar out-of-domain tasks without additional context length for inference. Code: https://github.com/Brandon3964/MultiModal-Task-Vector</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-325" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-325', event_id='94738', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3702</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94738">MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Leyang Shen &middot; Gongwei Chen &middot; Rui Shao &middot; Weili Guan &middot; Liqiang Nie
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multimodal large language models (MLLMs) have demonstrated impressive capabilities across various vision-language tasks. However, a generalist MLLM typically underperforms compared with a specialist MLLM on most VL tasks, which can be attributed to task interference. In this paper, we propose a mixture of multimodal experts (MoME) to mitigate task interference and obtain a generalist MLLM. Our MoME is composed of two key components, a mixture of vision experts (MoVE) and a mixture of language experts (MoLE). MoVE can adaptively modulate the features transformed from various vision encoders, and has a strong compatibility in transformation architecture. MoLE incorporates sparsely gated experts into LLMs to achieve painless improvements with roughly unchanged inference costs. In response to task interference, our MoME specializes in both vision and language modality to adapt to task discrepancies. Extensive experiments show that MoME significantly improves the performance of generalist MLLMs across various VL tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-326" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-326', event_id='94460', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3703</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94460">Advancing Cross-domain Discriminability in Continual Learning of Vision-Language Models</a></strong></h5>


                        <p class="text-muted">
                            Yicheng Xu &middot; Yuxin Chen &middot; Jiahao Nie &middot; Yusong Wang &middot; HUIPING ZHUANG &middot; Manabu Okumura
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Continual learning (CL) with Vision-Language Models (VLMs) has overcome the constraints of traditional CL, which only focuses on previously encountered classes. During the CL of VLMs, we need not only to prevent the catastrophic forgetting on incrementally learned knowledge but also to preserve the zero-shot ability of VLMs. However, existing methods require additional reference datasets to maintain such zero-shot ability and rely on domain-identity hints to classify images across different domains. In this study, we propose Regression-based Analytic Incremental Learning (RAIL), which utilizes a recursive ridge regression-based adapter to learn from a sequence of domains in a non-forgetting manner and decouple the cross-domain correlations by projecting features to a higher-dimensional space. Cooperating with a training-free fusion module, RAIL absolutely preserves the VLM's zero-shot ability on unseen domains without any reference data.Additionally, we introduce Cross-domain Task-Agnostic Incremental Learning (X-TAIL) setting. In this setting, a CL learner is required to incrementally learn from multiple domains and classify test images from both seen and unseen domains without any domain-identity hint.We theoretically prove RAIL's absolute memorization on incrementally learned domains. Experiment results affirm RAIL's state-of-the-art performance in both X-TAIL and existing Multi-domain Task-Incremental Learning settings. The code is released at https://github.com/linghan1997/Regression-based-Analytic-Incremental-Learning.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-327" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-327', event_id='94237', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3704</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94237">Are We on the Right Way for Evaluating Large Vision-Language Models?</a></strong></h5>


                        <p class="text-muted">
                            Lin Chen &middot; Jinsong Li &middot; Xiaoyi Dong &middot; Pan Zhang &middot; Yuhang Zang &middot; Zehui Chen &middot; Haodong Duan &middot; Jiaqi Wang &middot; Yu Qiao &middot; Dahua Lin &middot; Feng Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves 42.7% on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks near 24% on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM backbone with 17.9%. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-328" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-328', event_id='93873', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3705</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93873">Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration</a></strong></h5>


                        <p class="text-muted">
                            Kaihang Pan &middot; Zhaoyu Fan &middot; Juncheng Li &middot; Qifan Yu &middot; Hao Fei &middot; Siliang Tang &middot; Richang Hong &middot; Hanwang Zhang &middot; QIANRU SUN
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The swift advancement in Multimodal LLMs (MLLMs) also presents significant challenges for effective knowledge editing. Current methods, including intrinsic knowledge editing and external knowledge resorting, each possess strengths and weaknesses, struggling to balance the desired properties of reliability, generality, and locality when applied to MLLMs. In this paper, we propose \textbf{UniKE}, a novel multimodal editing method that establishes a unified perspective and paradigm for intrinsic knowledge editing and external knowledge resorting. Both types of knowledge are conceptualized as vectorized key-value memories, with the corresponding editing processes resembling the assimilation and accommodation phases of human cognition, conducted at the same semantic levels.  Within such a unified framework, we further promote knowledge collaboration by disentangling the knowledge representations into the semantic and truthfulness spaces. Extensive experiments validate the effectiveness of our method, which ensures that the post-edit MLLM simultaneously maintains excellent reliability, generality, and locality. The code for UniKE is available at https://github.com/beepkh/UniKE.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-329" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-329', event_id='93737', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3706</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93737">Yo&#x27;LLaVA: Your Personalized Language and Vision Assistant</a></strong></h5>


                        <p class="text-muted">
                            Thao Nguyen &middot; Haotian Liu &middot; Yuheng Li &middot; Mu Cai &middot; Utkarsh Ojha &middot; Yong Jae Lee
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large Multimodal Models (LMMs) have shown remarkable capabilities across a variety of tasks (e.g., image captioning, visual question answering).While broad, their knowledge remains generic (e.g., recognizing a dog), and they are unable to handle personalized subjects (e.g., recognizing a user's pet dog).Human reasoning, in contrast, typically operates within the context of specific subjects in our surroundings. For example, one might ask, "What should I buy for <em>my dog</em>'s birthday?"; as opposed to a generic inquiry about "What should I buy for <em>a dog</em>'s birthday?".Similarly, when looking at a friend's image, the interest lies in seeing their activities (e.g., "<em>my friend</em> is holding a cat"), rather than merely observing generic human actions (e.g., "<em>a man</em> is holding a cat").In this paper, we introduce the novel task of personalizing LMMs, so that they can have conversations about a specific subject. We propose Yo'LLaVA, which learns to embed a personalized subject into a set of latent tokens given a handful of example images of the subject.  Our qualitative and quantitative analyses reveal that Yo'LLaVA can learn the concept more efficiently using fewer tokens and more effectively encode the visual attributes compared to strong prompting baselines (e.g., LLaVA).</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-330" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-330', event_id='93527', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3707</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93527">Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching</a></strong></h5>


                        <p class="text-muted">
                            Yongqi Wang &middot; Wenxiang Guo &middot; Rongjie Huang &middot; Jiawei Huang &middot; Zehan Wang &middot; Fuming You &middot; Ruiqi Li &middot; Zhou Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22\%, and 6.2\% improvement in inception score over the strong diffusion-based baseline. Audio samples and code are available at http://frieren-v2a.github.io.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-331" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-331', event_id='93501', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3708</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93501">Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs</a></strong></h5>


                        <p class="text-muted">
                            Yuxuan Qiao &middot; Haodong Duan &middot; Xinyu Fang &middot; Junming Yang &middot; Lin Chen &middot; Songyang Zhang &middot; Jiaqi Wang &middot; Dahua Lin &middot; Kai Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Vision Language Models (VLMs) demonstrate remarkable proficiency in addressing a wide array of visual questions, which requires strong perception and reasoning faculties. Assessing these two competencies independently is crucial for model refinement, despite the inherent difficulty due to the intertwined nature of seeing and reasoning in existing VLMs. To tackle this issue, we present Prism, an innovative framework designed to disentangle the perception and reasoning processes involved in visual question solving. Prism comprises two distinct stages: a perception stage that utilizes a VLM to extract and articulate visual information in textual form, and a reasoning stage that formulates responses based on the extracted visual information using a Large Language Model (LLM). This modular design enables the systematic comparison and assessment of both proprietary and open-source VLM for their perception and reasoning strengths. Our analytical framework provides several valuable insights, underscoring Prism's potential as a cost-effective solution for vision-language tasks.By combining a streamlined VLM focused on perception with a powerful LLM tailored for reasoning, Prism achieves superior results in general vision-language tasks while substantially cutting down on training and operational expenses. Quantitative evaluations show that Prism, when configured with a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on par with VLMs $10 \times$ larger on the rigorous multimodal benchmark MMStar.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-332" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-332', event_id='93496', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3709</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93496">4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</a></strong></h5>


                        <p class="text-muted">
                            Roman Bachmann &middot; Oguzhan Fatih Kar &middot; David Mizrahi &middot; Ali Garjani &middot; Mingfei Gao &middot; David Griffiths &middot; Jiaming Hu &middot; Afshin Dehghan &middot; Amir Zamir
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Current multimodal and multitask foundation models, like 4M or UnifiedIO, show promising results. However, their out-of-the-box abilities to accept diverse inputs and perform diverse tasks are limited by the (usually small) number of modalities and tasks they are trained on. In this paper, we develop a single any-to-any model trained on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora. This includes training on images and text along with several semantic and geometric modalities, feature maps from recent state of the art models like DINOv2 and ImageBind, pseudo labels of specialist models like SAM and 4DHumans, and a range of new modalities that allow for novel ways to interact with the model and steer the generation, for example, image metadata or color palettes.A crucial step in this process is performing discrete tokenization on various modalities, whether they are image-like, neural network feature maps, vectors, structured data like instance segmentation or human poses, or data that can be represented as text.    Through this, we show the possibility of training one model to solve at least 3x more tasks/modalities than existing models and doing so without a loss in performance. In addition, this enables more fine-grained and controllable multimodal generation capabilities and allows studying the distillation of models trained on diverse data and objectives into one unified model.We scale the training to a three billion parameter and different datasets. The multimodal models and training code are open sourced at https://4m.epfl.ch/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-333" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-333', event_id='93307', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3710</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93307">Unveiling the Tapestry of Consistency in Large Vision-Language Models</a></strong></h5>


                        <p class="text-muted">
                            Yuan Zhang &middot; Fei xiao &middot; Tao Huang &middot; Chun-Kai Fan &middot; Hongyuan Dong &middot; Jiawen Li &middot; Jiacong Wang &middot; Kuan Cheng &middot; Shanghang Zhang &middot; Haoyuan Guo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large vision-language models (LVLMs) have recently achieved rapid progress, exhibiting great perception and reasoning abilities concerning visual information. However, when faced with prompts in different sizes of solution spaces, LVLMs fail to always give consistent answers regarding the same knowledge point. This inconsistency of answers between different solution spaces is prevalent in LVLMs and erodes trust. To this end, we provide a multi-modal benchmark ConBench, to intuitively analyze how LVLMs perform when the solution space of a prompt revolves around a knowledge point. Based on the ConBench tool, we are the first to reveal the tapestry and get the following findings: (1) In the discriminate realm, the larger the solution space of the prompt, the lower the accuracy of the answers. (2) Establish the relationship between the discriminative and generative realms: the accuracy of the discriminative question type exhibits a strong positive correlation with its Consistency with the caption. (3) Compared to open-source models, closed-source models exhibit a pronounced bias advantage in terms of Consistency. Eventually, we ameliorate the consistency of LVLMs by trigger-based diagnostic refinement, indirectly improving the performance of their caption. We hope this paper will accelerate the research community in better evaluating their models and encourage future advancements in the consistency domain.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-334" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-334', event_id='93210', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3711</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93210">LOVA3: Learning to Visual Question Answering, Asking and Assessment</a></strong></h5>


                        <p class="text-muted">
                            Henry Hengyuan Zhao &middot; Pan Zhou &middot; Difei Gao &middot; Zechen Bai &middot; Mike Zheng Shou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Question answering, asking, and assessment are three innate human traits crucial for understanding the world and acquiring knowledge. By enhancing these capabilities, humans can more effectively utilize data, leading to better comprehension and learning outcomes. However, current Multimodal Large Language Models (MLLMs) primarily focus on question answering, often neglecting the full potential of questioning and assessment skills. In this study, we introduce LOVA3, an innovative framework named ``Learning tO Visual Question Answering, Asking and Assessment,'' designed to equip MLLMs with these additional capabilities. Our approach involves the creation of two supplementary training tasks GenQA and EvalQA, aiming at fostering the skills of asking and assessing questions in the context of images. To develop the questioning ability, we compile a comprehensive set of multimodal foundational tasks. For assessment, we introduce a new benchmark called EvalQABench, comprising 64,000 training samples (split evenly between positive and negative samples) and 5,000 testing samples. We posit that enhancing MLLMs with the capabilities to answer, ask, and assess questions will enhance their multimodal comprehension, ultimately improving overall performance. To validate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate them on a range of multimodal datasets and benchmarks. Our results demonstrate consistent performance gains, underscoring the critical role of these additional tasks in fostering comprehensive intelligence in MLLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-335" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-335', event_id='93693', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3800</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93693">Towards a &quot;Universal Translator&quot; for Neural Dynamics at Single-Cell, Single-Spike Resolution</a></strong></h5>


                        <p class="text-muted">
                            Yizi Zhang &middot; Yanchen Wang &middot; Donato JimÃ©nez-BenetÃ³ &middot; Zixuan Wang &middot; Mehdi Azabou &middot; Blake Richards &middot; Renee Tung &middot; Olivier Winter &middot; Brain Laboratory International &middot; Eva Dyer &middot; Liam Paninski &middot; Cole Hurwitz
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Neuroscience research has made immense progress over the last decade, but our understanding of the brain remains fragmented and piecemeal: the dream of probing an arbitrary brain region and automatically reading out the information encoded in its neural activity remains out of reach. In this work, we build towards a first foundation model for neural spiking data that can solve a diverse set of tasks across multiple brain areas. We introduce a novel self-supervised modeling approach for population activity in which the model alternates between masking out and reconstructing neural activity across different time steps, neurons, and brain regions. To evaluate our approach, we design unsupervised and supervised prediction tasks using the International Brain Laboratory repeated site dataset, which is comprised of Neuropixels recordings targeting the same brain locations across 48 animals and experimental sessions. The prediction tasks include single-neuron and region-level activity prediction, forward prediction, and behavior decoding. We demonstrate that our multi-task-masking (MtM) approach significantly improves the performance of current state-of-the-art population models and enables multi-task learning. We also show that by training on multiple animals, we can improve the generalization ability of the model to unseen animals, paving the way for a foundation model of the brain at single-cell, single-spike resolution.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-336" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-336', event_id='93720', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3801</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93720">Crafting Interpretable Embeddings for Language Neuroscience by Asking LLMs Questions</a></strong></h5>


                        <p class="text-muted">
                            Vinamra Benara &middot; Chandan Singh &middot; John Morris &middot; Richard Antonello &middot; Ion Stoica &middot; Alexander Huth &middot; Jianfeng Gao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks. However, their opaqueness and proliferation into scientific domains such as neuroscience have created a growing need for interpretability. Here, we ask whether we can obtain interpretable embeddings through LLM prompting. We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of underlying questions rather than learning model weights.We use QA-Emb to flexibly generate interpretable models for predicting fMRI voxel responses to language stimuli. QA-Emb significantly outperforms an established interpretable baseline, and does so while requiring very few questions. This paves the way towards building flexible feature spaces that can concretize and evaluate our understanding of semantic brain representations. We additionally find that QA-Emb can be effectively approximated with an efficient model, and we explore broader applications in simple NLP tasks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-337" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-337', event_id='93768', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3802</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93768">Exact Gradients for Stochastic Spiking Neural Networks Driven by Rough Signals</a></strong></h5>


                        <p class="text-muted">
                            Christian Holberg &middot; Cristopher Salvi
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We introduce a mathematically rigorous framework based on rough path theory to model stochastic spiking neural networks (SSNNs) as stochastic differential equations with event discontinuities (Event SDEs) and driven by cÃ dlÃ g rough paths. Our formalism is general enough to allow for potential jumps to be present both in the solution trajectories as well as in the driving noise. We then identify a set of sufficient conditions ensuring the existence of pathwise gradients of solution trajectories and event times with respect to the network's parameters and show how these gradients satisfy a recursive relation. Furthermore, we introduce a general-purpose loss function defined by means of a new class of signature kernels indexed on cÃ dlÃ g rough paths and use it to train SSNNs as generative models. We provide an end-to-end autodifferentiable solver for Event SDEs  and make its implementation available as part of the $\texttt{diffrax}$ library. Our framework is, to our knowledge, the first enabling gradient-based training of SSNNs with noise affecting both the spike timing and the network's dynamics.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-338" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-338', event_id='96558', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3803</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96558">Efficient Large Multi-modal Models via Visual Context Compression</a></strong></h5>


                        <p class="text-muted">
                            Jieneng Chen &middot; Luoxin Ye &middot; Ju He &middot; Zhaoyang Wang &middot; Daniel Khashabi &middot; Alan Yuille
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While significant advancements have been made in compressed representations for text embeddings in large language models (LLMs), the compression of visual tokens in multi-modal LLMs (MLLMs) has remained a largely overlooked area. In this work, we present the study on the analysis of redundancy concerning visual tokens and efficient training within these models. Our initial experimentsshow that eliminating up to 70% of visual tokens at the testing stage by simply average pooling only leads to a minimal 3% reduction in visual question answering accuracy on the GQA benchmark, indicating significant redundancy in visual context. Addressing this, we introduce Visual Context Compressor, which reduces the number of visual tokens to enhance training and inference efficiency without sacrificing performance. To minimize information loss caused by the compression on visual tokens while maintaining training efficiency, we develop LLaVolta as a light and staged training scheme that incorporates stage-wise visual context compression to progressively compress the visual tokens from heavily to lightly compression during training, yielding no loss of information when testing. Extensive experiments demonstrate that our approach enhances the performance of MLLMs in both image-language and video-language understanding, while also significantly cutting training costs and improving inference efficiency.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-339" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-339', event_id='93940', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3804</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93940">Medformer: A Multi-Granularity Patching Transformer for Medical Time-Series Classification</a></strong></h5>


                        <p class="text-muted">
                            Yihe Wang &middot; Nan Huang &middot; Taida Li &middot; Yujun Yan &middot; Xiang Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Medical time series (MedTS) data, such as Electroencephalography (EEG) and Electrocardiography (ECG), play a crucial role in healthcare, such as diagnosing brain and heart diseases. Existing methods for MedTS classification primarily rely on handcrafted biomarkers extraction and CNN-based models, with limited exploration of transformer-based models. In this paper, we introduce Medformer, a multi-granularity patching transformer tailored specifically for MedTS classification. Our method incorporates three novel mechanisms to leverage the unique characteristics of MedTS: cross-channel patching to leverage inter-channel correlations, multi-granularity embedding for capturing features at different scales, and two-stage (intra- and inter-granularity) multi-granularity self-attention for learning features and correlations within and among granularities. We conduct extensive experiments on five public datasets under both subject-dependent and challenging subject-independent setups. Results demonstrate Medformer's superiority over 10 baselines, achieving top averaged ranking across five datasets on all six evaluation metrics. These findings underscore the significant impact of our method on healthcare applications, such as diagnosing Myocardial Infarction, Alzheimer's, and Parkinson's disease. We release the source code at https://github.com/DL4mHealth/Medformer.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-340" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-340', event_id='94178', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3805</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94178">Back to the Continuous Attractor</a></strong></h5>


                        <p class="text-muted">
                            Ãbel SÃ¡godi &middot; Guillermo MartÃ­n-SÃ¡nchez &middot; Piotr Sokol &middot; Memming Park
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Continuous attractors offer a unique class of solutions for storing continuous-valued variables in recurrent system states for indefinitely long time intervals.Unfortunately, continuous attractors suffer from severe structural instability in general---they are destroyed by most infinitesimal changes of the dynamical law that defines them.This fragility limits their utility especially in biological systems as their recurrent dynamics are subject to constant perturbations.We observe that the bifurcations from continuous attractors in theoretical neuroscience models display various structurally stable forms.Although their asymptotic behaviors to maintain memory are categorically distinct, their finite-time behaviors are similar.We build on the persistent manifold theory to explain the commonalities between bifurcations from and approximations of continuous attractors.Fast-slow decomposition analysis uncovers the existence of a persistent slow manifold that survives the seemingly destructive bifurcation, relating the flow within the manifold to the size of the perturbation. Moreover, this allows the bounding of the memory error of these approximations of continuous attractors.Finally, we train recurrent neural networks on analog memory tasks to support the appearance of these systems as solutions and their generalization capabilities.Therefore, we conclude that continuous attractors are functionally robust and remain useful as a universal analogy for understanding analog memory.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-341" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-341', event_id='94198', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3806</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94198">Learning predictable and robust neural representations by straightening image sequences</a></strong></h5>


                        <p class="text-muted">
                            Julie Xueyan Niu &middot; Cristina Savin &middot; Eero Simoncelli
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Prediction is a fundamental capability of all living organisms, and has been proposed as an objective for learning sensory representations.  Recent work demonstrates that in primate visual systems, prediction is facilitated by neural representations that follow straighter temporal trajectories than their initial photoreceptor encoding, which allows for prediction by linear extrapolation. Inspired by these experimental findings, we develop a self-supervised learning (SSL) objective that explicitly quantifies and promotes straightening. We demonstrate the power of this objective in training deep feedforward neural networks on smoothly-rendered synthetic image sequences that mimic commonly-occurring properties of natural videos. The learned model contains neural embeddings that are predictive, but also factorize the geometric, photometric, and semantic attributes of objects. The representations also prove more robust to noise and adversarial attacks compared to previous SSL methods that optimize for invariance to random augmentations. Moreover, these beneficial properties can be transferred to other training procedures by using the straightening objective as a regularizer, suggesting a broader utility for straightening as a principle for robust unsupervised learning.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-342" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-342', event_id='94903', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3807</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94903">The Bayesian sampling in a canonical recurrent circuit with a diversity of inhibitory interneurons</a></strong></h5>


                        <p class="text-muted">
                            Eryn Sale &middot; Wenhao Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Accumulating evidence suggests stochastic cortical circuits can perform sampling-based Bayesian inference to compute the latent stimulus posterior. Canonical cortical circuits consist of excitatory (E) neurons and types of inhibitory (I) interneurons. Nevertheless, nearly no sampling neural circuit models consider the diversity of interneurons, and thus how interneurons contribute to sampling remains poorly understood. To provide theoretical insight, we build a nonlinear canonical circuit model consisting of recurrently connected E neurons and two types of I neurons including Parvalbumin (PV) and Somatostatin (SOM) neurons. The E neurons are modeled as a canonical ring (attractor) model, receiving global inhibition from PV neurons, and locally tuning-dependent inhibition from SOM neurons.We theoretically analyze the nonlinear circuit dynamics and analytically identify the Bayesian sampling algorithm performed by the circuit dynamics. We found a reduced circuit with only E and PV neurons performs Langevin sampling, and the inclusion of SOM neurons with tuning-dependent inhibition speeds up the sampling via upgrading the Langevin into Hamiltonian sampling. Moreover, the Hamiltonian framework requires SOM neurons to receive no direct feedforward connections, consistent with neuroanatomy. Our work provides overarching connections between nonlinear circuits with various types of interneurons and sampling algorithms, deepening our understanding of circuit implementation of Bayesian inference.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-343" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-343', event_id='95362', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3808</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95362">MonkeySee: Space-time-resolved reconstructions of natural images from macaque multi-unit activity</a></strong></h5>


                        <p class="text-muted">
                            Lynn Le &middot; Paolo Papale &middot; Katja Seeliger &middot; Antonio Lozano &middot; Thirza Dado &middot; Feng Wang &middot; Pieter Roelfsema &middot; Marcel A. J. van Gerven &middot; YaÄmur GÃ¼Ã§lÃ¼tÃ¼rk &middot; Umut GÃ¼Ã§lÃ¼
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this paper, we reconstruct naturalistic images directly from macaque brain signals using a convolutional neural network (CNN) based decoder. We investigate the ability of this CNN-based decoding technique to differentiate among neuronal populations from areas V1, V4, and IT, revealing distinct readout characteristics for each. This research marks a progression from low-level to high-level brain signals, thereby enriching the existing framework for utilizing CNN-based decoders to decode brain activity. Our results demonstrate high-precision reconstructions of naturalistic images, highlighting the efficiency of CNN-based decoders in advancing our knowledge of how the brain's representations translate into pixels. Additionally, we present a novel space-time-resolved decoding technique, demonstrating how temporal resolution in decoding can advance our understanding of neural representations. Moreover, we introduce a learned receptive field layer that sheds light on the CNN-based model's data processing during training, enhancing understanding of its structure and interpretive capacity.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-344" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-344', event_id='96225', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3810</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96225">$\textit{NeuroPath}$: A Neural Pathway Transformer for Joining the Dots of Human Connectomes</a></strong></h5>


                        <p class="text-muted">
                            Ziquan Wei &middot; Tingting Dan &middot; Jiaqi Ding &middot; Guorong Wu
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Although modern imaging technologies allow us to study connectivity between two distinct brain regions $\textit{in-vivo}$, an in-depth understanding of how anatomical structure supports brain function and how spontaneous functional fluctuations emerge remarkable cognition is still elusive. Meanwhile, tremendous efforts have been made in the realm of machine learning to establish the nonlinear mapping between neuroimaging data and phenotypic traits. However, the absence of neuroscience insight in the current approaches poses significant challenges in understanding cognitive behavior from transient neural activities. To address this challenge, we put the spotlight on the coupling mechanism of structural connectivity (SC) and functional connectivity (FC) by formulating such network neuroscience question into an expressive graph representation learning problem for high-order topology. Specifically, we introduce the concept of $\textit{topological detour}$ to characterize how a ubiquitous instance of FC (direct link) is supported by neural pathways (detour) physically wired by SC, which forms a cyclic loop interacted by brain structure and function. In the clich\'e of machine learning, the multi-hop detour pathway underlying SC-FC coupling allows us to devise a novel multi-head self-attention mechanism within Transformer to capture multi-modal feature representation from paired graphs of SC and FC. Taken together, we propose a biological-inspired deep model, coined as $\textit{NeuroPath}$, to find putative connectomic feature representations from the unprecedented amount of neuroimages, which can be plugged into various downstream applications such as task recognition and disease diagnosis. We have evaluated $\textit{NeuroPath}$ on large-scale public datasets including Human Connectome Project (HCP) and UK Biobank (UKB) under different experiment settings of supervised and zero-shot learning, where the state-of-the-art performance by our $\textit{NeuroPath}$ indicates great potential in network neuroscience.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-345" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-345', event_id='96248', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3811</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96248">Linking In-context Learning in Transformers to Human Episodic Memory</a></strong></h5>


                        <p class="text-muted">
                            Ji-An Li &middot; Corey Zhou &middot; Marcus Benna &middot; Marcelo G Mattar
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Understanding connections between artificial and biological intelligent systems can reveal fundamental principles of general intelligence. While many artificial intelligence models have a neuroscience counterpart, such connections are largely missing in Transformer models and the self-attention mechanism. Here, we examine the relationship between interacting attention heads and human episodic memory. We focus on induction heads, which contribute to in-context learning in Transformer-based large language models (LLMs). We demonstrate that induction heads are behaviorally, functionally, and mechanistically similar to the contextual maintenance and retrieval (CMR) model of human episodic memory. Our analyses of LLMs pre-trained on extensive text data show that CMR-like heads often emerge in the intermediate and late layers, qualitatively mirroring human memory biases. The ablation of CMR-like heads suggests their causal role in in-context learning. Our findings uncover a parallel between the computational mechanisms of LLMs and human memory, offering valuable insights into both research fields.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-346" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-346', event_id='93690', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3900</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93690">RTify: Aligning Deep Neural Networks with Human Behavioral Decisions</a></strong></h5>


                        <p class="text-muted">
                            Yu-Ang Cheng &middot; Ivan F Rodriguez Rodriguez &middot; Sixuan Chen &middot; Kohitij Kar &middot; Takeo Watanabe &middot; Thomas Serre
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Current neural network models of primate vision focus on replicating overall levels of behavioral accuracy, often neglecting perceptual decisions' rich, dynamic nature. Here, we introduce a novel computational framework to model the dynamics of human behavioral choices by learning to align the temporal dynamics of a recurrent neural network (RNN) to human reaction times (RTs). We describe an approximation that allows us to constrain the number of time steps an RNN takes to solve a task with human RTs. The approach is extensively evaluated against various psychophysics experiments. We also show that the approximation can be used to optimize an ``ideal-observer'' RNN model to achieve an optimal tradeoff between speed and accuracy without human data. The resulting model is found to account well for human RT data. Finally, we use the approximation to train a deep learning implementation of the popular Wong-Wang decision-making model. The model is integrated with a convolutional neural network (CNN) model of visual processing and evaluated using both artificial and natural image stimuli. Overall, we present a novel framework that helps align current vision models with human behavior, bringing us closer to an integrated model of human vision.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-347" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-347', event_id='93607', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3901</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93607">Neuro-Vision to Language: Enhancing Brain Recording-based Visual Reconstruction and Language Interaction</a></strong></h5>


                        <p class="text-muted">
                            Guobin Shen &middot; Dongcheng Zhao &middot; Xiang He &middot; Linghao Feng &middot; Yiting Dong &middot; Jihang Wang &middot; Qian Zhang &middot; Yi Zeng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Decoding non-invasive brain recordings is pivotal for advancing our understanding of human cognition but faces challenges due to individual differences and complex neural signal representations. Traditional methods often require customized models and extensive trials, lacking interpretability in visual reconstruction tasks. Our framework integrates 3D brain structures with visual semantics using a <em>Vision Transformer 3D</em>. This unified feature extractor efficiently aligns fMRI features with multiple levels of visual embeddings, eliminating the need for subject-specific models and allowing extraction from single-trial data. The extractor consolidates multi-level visual features into one network, simplifying integration with Large Language Models (LLMs). Additionally, we have enhanced the fMRI dataset with diverse fMRI-image-related textual data to support multimodal large model development. Integrating with LLMs enhances decoding capabilities, enabling tasks such as brain captioning, complex reasoning, concept localization, and visual reconstruction. Our approach demonstrates superior performance across these tasks, precisely identifying language-based concepts within brain signals, enhancing interpretability, and providing deeper insights into neural processes. These advances significantly broaden the applicability of non-invasive brain decoding in neuroscience and human-computer interaction, setting the stage for advanced brain-computer interfaces and cognitive models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-348" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-348', event_id='94396', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3902</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94396">Learning Macroscopic Dynamics from Partial Microscopic Observations</a></strong></h5>


                        <p class="text-muted">
                            Mengyi Chen &middot; Qianxiao Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Macroscopic observables of a system are of keen interest in real applications such as the design of novel materials. Current methods rely on microscopic trajectory simulations, where the forces on all microscopic coordinates need to be computed or measured. However,  this can be computationally prohibitive for realistic systems.  In this paper, we propose a method to learn macroscopic dynamics requiring only force computations on a subset of the microscopic coordinates. Our method relies on a sparsity assumption: the force on each microscopic coordinate relies only on a small number of other coordinates. The main idea of our approach is to map the training procedure on the macroscopic coordinates back to the microscopic coordinates, on which partial force computations can be used as stochastic estimation to update model parameters. We provide a theoretical justification of this under suitable conditions. We demonstrate the accuracy, force computation efficiency, and robustness of our method on learning macroscopic closure models from a variety of microscopic systems, including those modeled by partial differential equations or molecular dynamics simulations.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-349" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-349', event_id='93621', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3903</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93621">Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators</a></strong></h5>


                        <p class="text-muted">
                            Benedikt Alkin &middot; Andreas FÃ¼rst &middot; Simon Schmid &middot; Lukas Gruber &middot; Markus Holzleitner &middot; Johannes Brandstetter
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Neural operators, serving as physics surrogate models, have recently gained increased interest. With ever increasing problem complexity, the natural question arises: what is an efficient way to scale neural operators to larger and more complex simulations - most importantly by taking into account different types of simulation datasets. This is of special interest since, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. Whereas the flexibility of transformers has enabled unified architectures across domains, neural operators mostly follow a problem specific design, where GNNs are commonly used for Lagrangian simulations and grid-based models predominate Eulerian simulations. We introduce Universal Physics Transformers (UPTs), an efficient and unified learning paradigm for a wide range of spatio-temporal problems. UPTs operate without grid- or particle-based latent structures, enabling flexibility and scalability across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space representation at any point in space-time. We demonstrate diverse applicability and efficacy of UPTs in mesh-based fluid simulations, and steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-350" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-350', event_id='93270', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3904</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93270">PACE: Pacing Operator Learning to Accurate Optical Field Simulation for Complicated Photonic Devices</a></strong></h5>


                        <p class="text-muted">
                            Hanqing Zhu &middot; Wenyan Cong &middot; Guojin Chen &middot; Shupeng Ning &middot; Ray Chen &middot; Jiaqi Gu &middot; David Z. Pan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Electromagnetic field simulation is central to designing, optimizing, and validating photonic devices and circuits. However, costly computation associated with numerical simulation poses a significant bottleneck, hindering scalability and turnaround time in the photonic circuit design process.Neural operators offer a promising alternative, but existing SOTA approaches, Neurolight, struggle with predicting high-fidelity fields for real-world complicated photonic devices, with the best reported 0.38 normalized mean absolute error in Neurolight.The interplays of highly complex light-matter interaction, e.g., scattering and resonance, sensitivity to local structure details, non-uniform learning complexity for full-domain simulation, and rich frequency information, contribute to the failure of existing neural PDE solvers.In this work, we boost the prediction fidelity to an unprecedented level for simulating complex photonic devices with a novel operator design driven by the above challenges.We propose a novel cross-axis factorized PACE operator with a strong long-distance modeling capacity to connect the full-domain complex field pattern with local device structures.Inspired by human learning, we further divide and conquer the simulation task for extremely hard cases into two progressively easy tasks, with a first-stage model learning an initial solution refined by a second model.On various complicated photonic device benchmarks, we demonstrate one sole PACE model is capable of achieving 73% lower error with 50% fewer parameters compared with various recent ML for PDE solvers.The two-stage setup further advances high-fidelity simulation for even more intricate cases.In terms of runtime, PACE demonstrates 154-577x and 11.8-12x simulation speedup over numerical solver using scipy or highly-optimized pardiso solver, respectively.We open-sourced the code and <em>complicated</em> optical device dataset at <a href="https://github.com/zhuhanqing/PACE-Light">PACE-Light</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-351" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-351', event_id='95762', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3905</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95762">Probablistic Emulation of a Global Climate Model with Spherical DYffusion</a></strong></h5>


                        <p class="text-muted">
                            Salva RÃ¼hling Cachay &middot; Brian Henn &middot; Oliver Watt-Meyer &middot; Christopher S. Bretherton &middot; Rose Yu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Data-driven deep learning models are transforming global weather forecasting. It is an open question if this success can extend to climate modeling, where the complexity of the data and long inference rollouts pose significant challenges. Here, we present the first conditional generative model that produces accurate and physically consistent global climate ensemble simulations by emulating a coarse version of the United States' primary operational global forecast model, FV3GFS.Our model integrates the dynamics-informed diffusion framework (DYffusion) with the Spherical Fourier Neural Operator (SFNO) architecture, enabling stable 100-year simulations at 6-hourly timesteps while maintaining low computational overhead compared to single-step deterministic baselines.The model achieves near gold-standard performance for climate model emulation, outperforming existing approaches and demonstrating promising ensemble skill.This work represents a significant advance towards efficient, data-driven climate simulations that can enhance our understanding of the climate system and inform adaptation strategies. Code is available at <a href="https://github.com/Rose-STL-Lab/spherical-dyffusion">https://github.com/Rose-STL-Lab/spherical-dyffusion</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-352" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-352', event_id='94582', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3906</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94582">Scaling transformer neural networks for skillful and reliable medium-range weather forecasting</a></strong></h5>


                        <p class="text-muted">
                            Tung Nguyen &middot; Rohan Shah &middot; Hritik Bansal &middot; Troy Arcomano &middot; Romit Maulik &middot; Rao Kotamarthi &middot; Ian Foster &middot; Sandeep Madireddy &middot; Aditya Grover
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Weather forecasting is a fundamental problem for anticipating and mitigating the impacts of climate change. Recently, data-driven approaches for weather forecasting based on deep learning have shown great promise, achieving accuracies that are competitive with operational systems. However, those methods often employ complex, customized architectures without sufficient ablation analysis, making it difficult to understand what truly contributes to their success. Here we introduce Stormer, a simple transformer model that achieves state-of-the art performance on weather forecasting with minimal changes to the standard transformer backbone. We identify the key components of Stormer through careful empirical analyses, including weather-specific embedding, randomized dynamics forecast, and pressure-weighted loss. At the core of Stormer is a randomized forecasting objective that trains the model to forecast the weather dynamics over varying time intervals. During inference, this allows us to produce multiple forecasts for a target lead time and combine them to obtain better forecast accuracy. On WeatherBench 2, Stormer performs competitively at short to medium-range forecasts and outperforms current methods beyond 7 days, while requiring orders-of-magnitude less training data and compute. Additionally, we demonstrate Stormerâs favorable scaling properties, showing consistent improvements in forecast accuracy with increases in model size and training tokens. Code and checkpoints are available at https://github.com/tung-nd/stormer.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-353" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-353', event_id='94049', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3907</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94049">Precipitation Downscaling with Spatiotemporal Video Diffusion</a></strong></h5>


                        <p class="text-muted">
                            Prakhar Srivastava &middot; Ruihan Yang &middot; Gavin Kerrigan &middot; Gideon Dresdner &middot; Jeremy McGibbon &middot; Christopher S. Bretherton &middot; Stephan Mandt
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In climate science and meteorology, high-resolution local precipitation (rain and snowfall) predictions are limited by the computational costs of simulation-based methods. Statistical downscaling, or super-resolution, is a common workaround where a low-resolution prediction is improved using statistical approaches. Unlike traditional computer vision tasks, weather and climate applications require capturing the accurate conditional distribution of high-resolution given low-resolution patterns to assure reliable ensemble averages and unbiased estimates of extreme events, such as heavy rain. This work extends recent video diffusion models to precipitation super-resolution, employing a deterministic downscaler followed by a temporally-conditioned diffusion model to capture noise characteristics and high-frequency patterns. We test our approach on FV3GFS output, an established large-scale global atmosphere model, and compare it against six state-of-the-art baselines. Our analysis, capturing CRPS, MSE, precipitation distributions, and qualitative aspects using California and the Himalayas as examples, establishes our method as a new standard for data-driven precipitation downscaling.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-354" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-354', event_id='96233', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3908</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96233">AROMA: Preserving Spatial Structure for Latent PDE Modeling with Local Neural Fields</a></strong></h5>


                        <p class="text-muted">
                            Louis Serrano &middot; Thomas X Wang &middot; Etienne Le Naour &middot; Jean-NoÃ«l Vittaut &middot; Patrick Gallinari
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields. Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds. This versatility eliminates the need for patching and allows efficient processing of diverse geometries. The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs. By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training. AROMA's superior performance in simulating 1D and 2D equations underscores the efficacy of our approach in capturing complex dynamical behaviors.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-355" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-355', event_id='96140', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3909</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96140">Quantum Deep Equilibrium Models</a></strong></h5>


                        <p class="text-muted">
                            Philipp Schleich &middot; Marta Skreta &middot; Lasse Kristensen &middot; Rodrigo Vargas-Hernandez &middot; Alan Aspuru-Guzik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The feasibility of variational quantum algorithms, the most popular correspondent of neural networks on noisy, near-term quantum hardware, is highly impacted by the circuit depth of the involved parametrized quantum circuits (PQCs). Higher depth increases expressivity, but also results in a detrimental accumulation of errors.  Furthermore, the number of parameters involved in the PQC significantly influences the performance through the necessary number of measurements to evaluate gradients, which scales linearly with the number of parameters.   Motivated by this, we look at deep equilibrium models (DEQs), which mimic an infinite-depth, weight-tied network using a fraction of the memory by employing a root solver to find the fixed points of the network. In this work, we present Quantum Deep Equilibrium Models (QDEQs): a training paradigm that learns parameters of a quantum machine learning model given by a PQC using DEQs. To our knowledge, no work has yet explored the application of DEQs to QML models. We apply QDEQs to find the parameters of a quantum circuit in two settings: the first involves classifying MNIST-4 digits with 4 qubits; the second extends it to 10 classes of MNIST, FashionMNIST and CIFAR.  We find that QDEQ is not only  competitive with comparable existing baseline models, but also achieves higher performance than a network with 5 times more layers. This demonstrates that the QDEQ paradigm can be used to develop significantly more shallow quantum circuits for a given task, something which is essential for the utility of near-term quantum computers.    Our code is available at \url{https://github.com/martaskrt/qdeq}.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-356" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-356', event_id='95877', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3910</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95877">Physical Consistency Bridges Heterogeneous Data in Molecular Multi-Task Learning</a></strong></h5>


                        <p class="text-muted">
                            Yuxuan Ren &middot; Dihan Zheng &middot; Chang Liu &middot; Peiran Jin &middot; Yu Shi &middot; Lin Huang &middot; Jiyan He &middot; Shengjie Luo &middot; Tao Qin &middot; Tie-Yan Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In recent years, machine learning has demonstrated impressive capability in handling molecular science tasks. To support various molecular properties at scale, machine learning models are trained in the multi-task learning paradigm. Nevertheless, data of different molecular properties are often not aligned: some quantities, e.g. equilibrium structure, demand more cost to compute than others, e.g. energy, so their data are often generated by cheaper computational methods at the cost of lower accuracy, which cannot be directly overcome through multi-task learning. Moreover, it is not straightforward to leverage abundant data of other tasks to benefit a particular task. To handle such data heterogeneity challenges, we exploit the specialty of molecular tasks that there are physical laws connecting them, and design consistency training approaches that allow different tasks to exchange information directly so as to improve one another. Particularly, we demonstrate that the more accurate energy data can improve the accuracy of structure prediction. We also find that consistency training can directly leverage force and off-equilibrium structure data to improve structure prediction, demonstrating a broad capability for integrating heterogeneous data.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-357" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-357', event_id='94796', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#3911</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94796">Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics</a></strong></h5>


                        <p class="text-muted">
                            Jonas Spinner &middot; Victor Breso &middot; Pim de Haan &middot; Tilman Plehn &middot; Jesse Thaler &middot; Johann Brehmer
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Extracting scientific understanding from particle-physics experiments requires solving diverse learning problems with high precision and good data efficiency. We propose the Lorentz Geometric Algebra Transformer (L-GATr), a new multi-purpose architecture for high-energy physics. L-GATr represents high-energy data in a geometric algebra over four-dimensional space-time and is equivariant under Lorentz transformations, the symmetry group of relativistic kinematics. At the same time, the architecture is a Transformer, which makes it versatile and scalable to large systems. L-GATr is first demonstrated on regression and classification tasks from particle physics. We then construct the first Lorentz-equivariant generative model: a continuous normalizing flow based on an L-GATr network, trained with Riemannian flow matching. Across our experiments, L-GATr is on par with or outperforms strong domain-specific baselines.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-358" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-358', event_id='96620', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4000</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96620">Gaussian Process Bandits for Top-k Recommendations</a></strong></h5>


                        <p class="text-muted">
                            Mohit Yadav &middot; Cameron Musco &middot; Daniel Sheldon
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Algorithms that utilize bandit feedback to optimize top-k recommendations are vital for online marketplaces, search engines, and content platforms.  However, the combinatorial nature of this problem poses a significant challenge,  as the possible number of ordered top-k recommendations from $n$ items grows exponentially with $k$. As a result, previous work often relies on restrictive assumptions about the reward or bandit feedback models, such as assuming that the feedback discloses rewards for each recommended item rather than a single scalar feedback for the entire set of top-k recommendations. We introduce a novel contextual bandit algorithm for top-k recommendations, leveraging a Gaussian process with a Kendall kernel to model the reward function.Our algorithm requires only scalar feedback from the top-k recommendations and does not impose restrictive assumptions on the reward structure. Theoretical analysis confirms that the proposed algorithm achieves sub-linear regret in relation to the number of rounds and arms. Additionally, empirical results using a bandit simulator demonstrate that the proposed algorithm outperforms other baselines across various scenarios.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-359" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-359', event_id='93269', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4001</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93269">Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models</a></strong></h5>


                        <p class="text-muted">
                            Jinlin Lai &middot; Daniel Sheldon &middot; Justin Domke
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and often requires advanced sampling techniques like Markov chain Monte Carlo (MCMC).A common approach is to write the model in a probabilistic programming language and then sample via Hamiltonian Monte Carlo (HMC).However, there are many ways a user can transform a model that make inference more or less efficient.In particular, marginalizing some variables can greatly improve inference but is difficult for users to do manually.We develop an algorithm to easily marginalize random effects in LMMs.A naive approach introduces cubic time operations within an inference algorithm like HMC, but we reduce the running time to linear using fast linear algebra techniques.We show that marginalization is always beneficial when applicable and highlight improvements in various models, especially ones from cognitive sciences.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-360" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-360', event_id='95507', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4002</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95507">BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Yibin Wang &middot; Haizhou Shi &middot; Ligong Han &middot; Dimitris Metaxas &middot; Hao Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large Language Models (LLMs) often suffer from overconfidence during inference, particularly when adapted to downstream domain-specific tasks with limited data. Previous work addresses this issue by employing approximate Bayesian estimation after the LLMs are trained, enabling them to quantify uncertainty. However, such post-training approaches' performance is severely limited by the parameters learned during training. In this paper, we go beyond post-training Bayesianization and propose Bayesian Low-Rank Adaptation by Backpropagation (BLoB), an algorithm that continuously and jointly adjusts both the mean and covariance of LLM parameters throughout the whole fine-tuning process. Our empirical results verify the effectiveness of BLoB in terms of generalization and uncertainty estimation, when evaluated on both in-distribution and out-of-distribution data.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-361" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-361', event_id='95874', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4003</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95874">Sparse Bayesian Generative Modeling for Compressive Sensing</a></strong></h5>


                        <p class="text-muted">
                            Benedikt BÃ¶ck &middot; Sadaf Syed &middot; Wolfgang Utschick
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This work addresses the fundamental linear inverse problem in compressive sensing (CS) by introducing a new type of regularizing generative prior. Our proposed method utilizes ideas from classical dictionary-based CS and, in particular, sparse Bayesian learning (SBL), to integrate a strong regularization towards sparse solutions. At the same time, by leveraging the notion of conditional Gaussianity, it also incorporates the adaptability from generative models to training data. However, unlike most state-of-the-art generative models, it is able to learn from a few compressed and noisy data samples and requires no optimization algorithm for solving the inverse problem. Additionally, similar to Dirichlet prior networks, our model parameterizes a conjugate prior enabling its application for uncertainty quantification. We support our approach theoretically through the concept of variational inference and validate it empirically using different types of compressible signals.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-362" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-362', event_id='95977', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4004</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95977">TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks</a></strong></h5>


                        <p class="text-muted">
                            Benjamin Feuer &middot; Robin Schirrmeister &middot; Valeriia Cherepanova &middot; Chinmay Hegde &middot; Frank Hutter &middot; Micah Goldblum &middot; Niv Cohen &middot; Colin White
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs via context optimization. We introduce TuneTables, a parameter-efficient fine-tuning strategy for PFNs that compresses large datasets into a smaller learned context. We conduct extensive experiments on nineteen algorithms over 98 datasets and find that TuneTables achieves the best performance on average, outperforming boosted trees such as CatBoost, while optimizing fewer than 5\% of TabPFN's parameters. Furthermore, we show that TuneTables can be used as an interpretability tool and can even be used to mitigate biases by optimizing a fairness objective.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-363" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-363', event_id='93163', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4005</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93163">FasMe: Fast and Sample-efficient Meta Estimator for Precision Matrix Learning in Small Sample Settings</a></strong></h5>


                        <p class="text-muted">
                            Xiao Tan &middot; Yiqin Wang &middot; Yangyang Shen &middot; Dian Shen &middot; Meng Wang &middot; Peibo Duan &middot; Beilun Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Precision matrix estimation is a ubiquitous task featuring numerous applications such as rare disease diagnosis and neural connectivity exploration. However, this task becomes challenging in small sample settings, where the number of samples is significantly less than the number of dimensions, leading to unreliable estimates. Previous approaches either fail to perform well in small sample settings or suffer from inefficient estimation processes, even when incorporating meta-learning techniques.To this end, we propose a novel approach FasMe for Fast and Sample-efficient Meta Precision Matrix Learning, which first extracts meta-knowledge through a multi-task learning diagram. Then, meta-knowledge constraints are applied using a maximum determinant matrix completion algorithm for the novel task. As a result, we reduce the sample size requirements to $O(\log p/K)$ per meta-training task and $O(\log\vert \mathcal{G}\vert)$ for the meta-testing task. Moreover, the hereby proposed model only needs $O(p \log\epsilon^{-1})$ time and $O(p)$ memory for converging to an $\epsilon$-accurate solution. On multiple synthetic and biomedical datasets, FasMe is at least ten times faster than the four baselines while promoting prediction accuracy in small sample settings.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-364" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-364', event_id='93438', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4006</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93438">Interaction-Force Transport Gradient Flows</a></strong></h5>


                        <p class="text-muted">
                            Egor Gladin &middot; Pavel Dvurechenskii &middot; Alexander Mielke &middot; Jia-Jie Zhu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper presents a new gradient flow dissipation geometry over non-negative and probability measures.This is motivated by a principled construction that combines the unbalanced optimal transport and interaction forces modeled by reproducing kernels. Using a precise connection between the Hellinger geometry and the maximum mean discrepancy (MMD), we propose the interaction-force transport (IFT) gradient flows and its spherical variant via an infimal convolution of the Wasserstein and spherical MMD tensors. We then develop a particle-based optimization algorithm based on the JKO-splitting scheme of the mass-preserving spherical IFT gradient flows. Finally, we provide both theoretical global exponential convergence guarantees and improved empirical simulation results for applying the IFT gradient flows to the sampling task of MMD-minimization. Furthermore, we prove that the spherical IFT  gradient flow enjoys the best of both worlds by providing the global exponential convergence guarantee for both the MMD and KL energy.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-365" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-365', event_id='93652', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4007</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93652">Probabilistic Federated Prompt-Tuning with Non-IID and Imbalanced Data</a></strong></h5>


                        <p class="text-muted">
                            Pei-Yau Weng &middot; Minh Hoang &middot; Lam Nguyen &middot; My T. Thai &middot; Lily Weng &middot; Nghia Hoang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Fine-tuning pre-trained models is a popular approach in machine learning for solving complex tasks with moderate data. However, fine-tuning the entire pre-trained model is ineffective in federated data scenarios where local data distributions are diversely skewed. To address this, we explore integrating federated learning with a more effective prompt-tuning method, optimizing for a small set of input prefixes to reprogram the pre-trained model's behavior. Our approach transforms federated learning into a distributed set modeling task, aggregating diverse sets of prompts to globally fine-tune the pre-trained model. We benchmark various baselines based on direct adaptations of existing federated model aggregation techniques and introduce a new probabilistic prompt aggregation method that substantially outperforms these baselines. Our reported results on a variety of computer vision datasets confirm that the proposed method is most effective to combat extreme data heterogeneity in federated learning.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-366" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-366', event_id='94844', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4008</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94844">Annealed Multiple Choice Learning: Overcoming limitations of Winner-takes-all with annealing</a></strong></h5>


                        <p class="text-muted">
                            David Perera &middot; Victor Letzelter &middot; Theo Mariotte &middot; Adrien Cortes &middot; Mickael Chen &middot; Slim Essid &middot; GaÃ«l Richard
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce Annealed Multiple Choice Learning (aMCL) which combines simulated annealing with MCL. MCL is a learning framework handling ambiguous tasks by predicting a small set of plausible hypotheses. These hypotheses are trained using the Winner-takes-all (WTA) scheme, which promotes the diversity of the predictions. However, this scheme may converge toward an arbitrarily suboptimal local minimum, due to the greedy nature of WTA. We overcome this limitation using annealing, which enhances the exploration of the hypothesis space during training. We leverage insights from statistical physics and information theory to provide a detailed description of the model training trajectory. Additionally, we validate our algorithm by extensive experiments on synthetic datasets, on the standard UCI benchmark, and on speech separation.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-367" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-367', event_id='95881', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4009</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95881">Abductive Reasoning in Logical Credal Networks</a></strong></h5>


                        <p class="text-muted">
                            Radu Marinescu &middot; Junkyu Lee &middot; Debarun Bhattacharjya &middot; Fabio Cozman &middot; Alexander Gray
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Logical Credal Networks or LCNs were recently introduced as a powerful probabilistic logic framework for representing and reasoning with imprecise knowledge. Unlike many existing formalisms, LCNs have the ability to represent cycles and allow specifying marginal and conditional probability bounds on logic formulae which may be important in many realistic scenarios. Previous work on LCNs has focused exclusively on marginal inference, namely computing posterior lower and upper probability bounds on a query formula. In this paper, we explore abductive reasoning tasks such as solving MAP and Marginal MAP queries in LCNs given some evidence. We first formally define the MAP and Marginal MAP tasks for LCNs and subsequently show how to solve these tasks exactly using search-based approaches. We then propose several approximate schemes that allow us to scale MAP and Marginal MAP inference to larger problem instances. An extensive empirical evaluation demonstrates the effectiveness of our algorithms on both random LCN instances as well as LCNs derived from more realistic use-cases.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-368" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-368', event_id='93158', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4010</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93158">Space-Time Continuous PDE Forecasting using Equivariant Neural Fields</a></strong></h5>


                        <p class="text-muted">
                            David Knigge &middot; David Wessels &middot; Riccardo Valperga &middot; Samuele Papa &middot; Jan-jakob Sonke &middot; Erik Bekkers &middot; Efstratios Gavves
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Recently, Conditional Neural Fields (NeFs) have emerged as a powerful modelling paradigm for PDEs, by learning solutions as flows in the latent space of the Conditional NeF. Although benefiting from favourable properties of NeFs such as grid-agnosticity and space-time-continuous dynamics modelling, this approach limits the ability to impose known constraints of the PDE on the solutions -- such as symmetries or boundary conditions -- in favour of modelling flexibility. Instead, we propose a  space-time continuous NeF-based solving framework that - by preserving geometric information in the latent space of the Conditional NeF - preserves known symmetries of the PDE. We show that modelling solutions as flows of pointclouds over the group of interest $G$ improves generalization and data-efficiency. Furthermore, we validate that our framework readily generalizes to unseen spatial and temporal locations, as well as geometric transformations of the initial conditions - where other NeF-based PDE forecasting methods fail -, and improve over baselines in a number of challenging geometries.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-369" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-369', event_id='94031', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4011</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94031">Learning to Predict Structural Vibrations</a></strong></h5>


                        <p class="text-muted">
                            Jan van Delden &middot; Julius Schultz &middot; Christopher Blech &middot; Sabine Langer &middot; Timo LÃ¼ddecke
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In mechanical structures like airplanes, cars and houses, noise is generated and transmitted through vibrations. To take measures to reduce this noise, vibrations need to be simulated with expensive numerical computations. Deep learning surrogate models present a promising alternative to classical numerical simulations as they can be evaluated magnitudes faster, while trading-off accuracy. To quantify such trade-offs systematically and foster the development of methods, we present a benchmark on the task of predicting the vibration of harmonically excited plates. The benchmark features a total of 12,000 plate geometries with varying forms of beadings, material, boundary conditions, load position and sizes with associated numerical solutions. To address the benchmark task, we propose a new network architecture, named \modelname, which predicts vibration patterns of plate geometries given a specific excitation frequency. Applying principles from operator learning and implicit models for shape encoding, our approach effectively addresses the prediction of highly variable frequency response functions occurring in dynamic systems. To quantify the prediction quality, we introduce a set of evaluation metrics and evaluate the method on our vibrating-plates benchmark. Our method outperforms DeepONets, Fourier Neural Operators and more traditional neural network architectures and can be used for design optimization.Code, dataset and visualizations: https://github.com/ecker-lab/Learning<em>Vibrating</em>Plates</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-370" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-370', event_id='95782', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4100</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95782">Principled Bayesian Optimization in Collaboration with Human Experts</a></strong></h5>


                        <p class="text-muted">
                            Wenjie Xu &middot; Masaki Adachi &middot; Colin Jones &middot; Michael A Osborne
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Bayesian optimisation for real-world problems is often performed interactively with human experts, and integrating their domain knowledge is key to accelerate the optimisation process. We consider a setup where experts provide advice on the next query point through binary accept/reject recommendations (labels). Expertsâ labels are often costly, requiring efficient use of their efforts, and can at the same time be unreliable, requiring careful adjustment of the degree to which any expert is trusted. We introduce the first principled approach that provides two key guarantees. (1) Handover guarantee: similar to a no-regret property, we establish a sublinear bound on the cumulative number of expertsâ binary labels. Initially, multiple labels per query are needed, but the number of expert labels required asymptotically converges to zero, saving both expert effort and computation time. (2) No-harm guarantee with data-driven trust level adjustment: our adaptive trust level ensures that the convergence rate will not be worse than the one without using advice, even if the advice from experts is adversarial. Unlike existing methods that employ a user-defined function that hand-tunes the trust level adjustment, our approach enables data-driven adjustments. Real-world applications empirically demonstrate that our method not only outperforms existing baselines, but also maintains robustness despite varying labelling accuracy, in tasks of battery design with human experts.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-371" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-371', event_id='93350', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4101</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93350">Computation-Aware Gaussian Processes: Model Selection And Linear-Time Inference</a></strong></h5>


                        <p class="text-muted">
                            Jonathan Wenger &middot; Kaiwen Wu &middot; Philipp Hennig &middot; Jacob Gardner &middot; Geoff Pleiss &middot; John Cunningham
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Model selection in Gaussian processes scales prohibitively with the size of the training dataset, both in time and memory.While many approximations exist, all incur inevitable approximation error.Recent work accounts for this error in the form of computational uncertainty, which enables---at the cost of quadratic complexity---an explicit tradeoff between computation and precision.Here we extend this development to model selection, which requires significant enhancements to the existing approach, including linear-time scaling in the size of the dataset.We propose a novel training loss for hyperparameter optimization and demonstrate empirically that the resulting method can outperform SGPR, CGGP and SVGP, state-of-the-art methods for GP model selection, on medium to large-scale datasets.Our experiments show that model selection for computation-aware GPs trained on 1.8 million data points can be done within a few hours on a single GPU.As a result of this work, Gaussian processes can be trained on large-scale datasets without significantly compromising their ability to quantify uncertainty---a fundamental prerequisite for optimal decision-making.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-372" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-372', event_id='96356', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4102</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96356">Fast Channel Simulation via Error-Correcting Codes</a></strong></h5>


                        <p class="text-muted">
                            Sharang Sriramu &middot; Rochelle Barsz &middot; Elizabeth Polito &middot; Aaron Wagner
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We consider the design of practically-implementable schemes for the task of channel simulation. Existing methods do not scale with the number of simultaneous uses of the channel and are therefore unable to harness the amortization gains associated with simulating many uses of the channel at once. We show how techniques from the theory of error-correcting codes can be applied to achieve scalability and hence improved performance. As an exemplar, we focus on how polar codes can be used to efficiently simulate i.i.d. copies of a class of binary-output channels.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-373" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-373', event_id='94581', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4103</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94581">Hamiltonian Monte Carlo on ReLU Neural Networks is Inefficient</a></strong></h5>


                        <p class="text-muted">
                            Vu Dinh &middot; Lam Ho &middot; Cuong V. Nguyen
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We analyze the error rates of the Hamiltonian Monte Carlo algorithm with leapfrog integrator for Bayesian neural network inference. We show that due to the non-differentiability of activation functions in the ReLU family, leapfrog HMC for networks with these activation functions has a large local error rate of $\Omega(\epsilon)$ rather than the classical error rate of $\mathcal{O}(\epsilon^3)$. This leads to a higher rejection rate of the proposals, making the method inefficient. We then verify our theoretical findings through empirical simulations as well as experiments on a real-world dataset that highlight the inefficiency of HMC inference on ReLU-based neural networks compared to analytical networks.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-374" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-374', event_id='93258', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4104</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93258">A Neural Network Approach for Efficiently Answering Most Probable Explanation Queries in Probabilistic Models</a></strong></h5>


                        <p class="text-muted">
                            Shivvrat Arya &middot; Tahrima Rahman &middot; Vibhav Gogate
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We propose a novel neural networks based approach to efficiently answer arbitrary Most Probable Explanation (MPE) queriesâa well-known NP-hard taskâin large probabilistic models such as Bayesian and Markov networks, probabilistic circuits, and neural auto-regressive models. By arbitrary MPE queries, we mean that there is no predefined partition of variables into evidence and non-evidence variables. The key idea is to distill all MPE queries over a given probabilistic model into a neural network and then use the latter for answering queries, eliminating the need for time-consuming inference algorithms that operate directly on the probabilistic model. We improve upon this idea by incorporating inference-time optimization with self-supervised loss to iteratively improve the solutions and employ a teacher-student framework that provides a better initial network, which in turn, helps reduce the number of inference-time optimization steps. The teacher network utilizes a self-supervised loss function optimized for getting the exact MPE solution, while the student network learns from the teacher's near-optimal outputs through supervised loss. We demonstrate the efficacy and scalability of our approach on various datasets and a broad class of probabilistic models, showcasing its practical effectiveness.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-375" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-375', event_id='93819', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4105</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93819">VISA: Variational Inference with Sequential Sample-Average Approximations</a></strong></h5>


                        <p class="text-muted">
                            Heiko Zimmermann &middot; Christian Andersson Naesseth &middot; Jan-Willem van de Meent
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We present variational inference with sequential sample-average approximations (VISA), a method for approximate inference in computationally intensive models, such as those based on numerical simulations. VISA extends importance-weighted forward-KL variational inference by employing a sequence of sample-average approximations, which are considered valid inside a trust region. This makes it possible to reuse model evaluations across multiple gradient steps, thereby reducing computational cost. We perform experiments on high-dimensional Gaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate that VISA can achieve comparable approximation accuracy to standard importance-weighted forward-KL variational inference with computational savings of a factor two or more for conservatively chosen learning rates.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-376" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-376', event_id='97803', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4106</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97803">Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions</a></strong></h5>


                        <p class="text-muted">
                            Minghan Li &middot; Heng Li &middot; Zhi-Qi Cheng &middot; Yifei Dong &middot; Yuxuan Zhou &middot; Jun-Yan He &middot; Qi Dai &middot; Teruko Mitamura &middot; Alexander Hauptmann
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Vision-and-Language Navigation (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce Human-Aware Vision-and-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN's unique challenges, underscores the need for further research to enhance HA-VLN agents' real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and Sim2Real transfer, paving the way for more realistic and applicable VLN systems in human-populated environments.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-377" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-377', event_id='97680', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4107</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97680">SurgicAI: A Fine-grained Platform for Data Collection and Benchmarking in Surgical Policy Learning</a></strong></h5>


                        <p class="text-muted">
                            Jin Wu &middot; Haoying Zhou &middot; Peter Kazanzides &middot; Adnan Munawar &middot; Anqi Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite advancements in robotic-assisted surgery, automating complex tasks like suturing remain challenging due to the need for adaptability and precision. Learning-based approaches, particularly reinforcement learning (RL) and imitation learning (IL), require realistic simulation environments for efficient data collection. However, current platforms often include only relatively simple, non-dexterous manipulations and lack the flexibility required for effective learning and generalization. We introduce SurgicAI, a novel platform for development and benchmarking addressing these challenges by providing the flexibility to accommodate both modular subtasks and more importantly task decomposition in RL-based surgical robotics. Compatible with the da Vinci Surgical System, SurgicAI offers a standardized pipeline for collecting and utilizing expert demonstrations. It supports deployment of multiple RL and IL approaches, and the training of both singular and compositional subtasks in suturing scenarios, featuring high dexterity and modularization. Meanwhile, SurgicAI sets clear metrics and benchmarks for the assessment of learned policies. We implemented and evaluated multiple RL and IL algorithms on SurgicAI. Our detailed benchmark analysis underscores SurgicAI's potential to advance policy learning in surgical robotics. Details: https://github.com/surgical-robotics-ai/SurgicAI</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-378" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-378', event_id='96791', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4108</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96791">RadarOcc: Robust 3D Occupancy Prediction with 4D Imaging Radar</a></strong></h5>


                        <p class="text-muted">
                            Fangqiang Ding &middot; Xiangyu Wen &middot; Yunzhou Zhu &middot; Yiming Li &middot; Chris Xiaoxuan Lu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>3D occupancy-based perception pipeline has significantly advanced autonomous driving by capturing detailed scene descriptions and demonstrating strong generalizability across various object categories and shapes. Current methods predominantly rely on LiDAR or camera inputs for 3D occupancy prediction. These methods are susceptible to adverse weather conditions, limiting the all-weather deployment of self-driving cars. To improve perception robustness, we leverage the recent advances in automotive radars and introduce a novel approach that utilizes 4D imaging radar sensors for 3D occupancy prediction. Our method, RadarOcc, circumvents the limitations of sparse radar point clouds by directly processing the 4D radar tensor, thus preserving essential scene details. RadarOcc innovatively addresses the challenges associated with the voluminous and noisy 4D radar data by employing Doppler bins descriptors, sidelobe-aware spatial sparsification, and range-wise self-attention mechanisms. To minimize the interpolation errors associated with direct coordinate transformations, we also devise a spherical-based feature encoding followed by spherical-to-Cartesian feature aggregation. We benchmark various baseline methods based on distinct modalities on the public K-Radar dataset. The results demonstrate RadarOcc's state-of-the-art performance in radar-based 3D occupancy prediction and promising results even when compared with LiDAR- or camera-based methods. Additionally, we present qualitative evidence of the superior performance of 4D radar in adverse weather conditions and explore the impact of key pipeline components through ablation studies.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-379" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-379', event_id='96591', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4109</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96591">AED: Adaptable Error Detection for Few-shot Imitation Policy</a></strong></h5>


                        <p class="text-muted">
                            Jia-Fong Yeh &middot; Kuo-Han Hung &middot; Pang-Chi Lo &middot; Chi Ming Chung &middot; Tsung-Han Wu &middot; Hung-Ting Su &middot; Yi-Ting Chen &middot; Winston Hsu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce a new task called Adaptable Error Detection (AED), which aims to identify behavior errors in few-shot imitation (FSI) policies based on visual observations in novel environments. The potential to cause serious damage to surrounding areas limits the application of FSI policies in real-world scenarios. Thus, a robust system is necessary to notify operators when FSI policies are inconsistent with the intent of demonstrations. This task introduces three challenges: (1) detecting behavior errors in novel environments, (2) identifying behavior errors that occur without revealing notable changes, and (3) lacking complete temporal information of the rollout due to the necessity of online detection. However, the existing benchmarks cannot support the development of AED because their tasks do not present all these challenges. To this end, we develop a cross-domain AED benchmark, consisting of 322 base and 153 novel environments. Additionally, we propose Pattern Observer (PrObe) to address these challenges. PrObe is equipped with a powerful pattern extractor and guided by novel learning objectives to parse discernible patterns in the policy feature representations of normal or error states. Through our comprehensive evaluation, PrObe demonstrates superior capability to detect errors arising from a wide range of FSI policies, consistently surpassing strong baselines. Moreover, we conduct detailed ablations and a pilot study on error correction to validate the effectiveness of the proposed architecture design and the practicality of the AED task, respectively. The AED project page can be found at https://aed-neurips.github.io/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-380" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-380', event_id='96537', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4110</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96537">Active Perception for Grasp Detection via Neural Graspness Field</a></strong></h5>


                        <p class="text-muted">
                            Haoxiang Ma &middot; Modi Shi &middot; Boyang Gao &middot; Di Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper tackles the challenge of active perception for robotic grasp detection in cluttered environments. Incomplete 3D geometry information can negatively affect the performance of learning-based grasp detection methods, and scanning the scene from multiple views introduces significant time costs. To achieve reliable grasping performance with efficient camera movement, we propose an active grasp detection framework based on the Neural Graspness Field (NGF), which models the scene incrementally and facilitates next-best-view planning. Constructed in real-time as the camera moves, the NGF effectively models the grasp distribution in 3D space by rendering graspness predictions from each view. For next-best-view planning, we aim to reduce the uncertainty of the NGF through a graspness inconsistency-guided policy, selecting views based on discrepancies between NGF outputs and a pre-trained graspness network. Additionally, we present a neural graspness sampling method that decodes graspness values from the NGF to improve grasp pose detection results. Extensive experiments on the GraspNet-1Billion benchmark demonstrate significant performance improvements compared to previous works. Real-world experiments show that our method achieves a superior trade-off between grasping performance and time costs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-381" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-381', event_id='95690', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4111</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95690">RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation</a></strong></h5>


                        <p class="text-muted">
                            Jiaming Liu &middot; Mengzhen Liu &middot; Zhenyu Wang &middot; Pengju An &middot; Xiaoqi Li &middot; Kaichen Zhou &middot; Senqiao Yang &middot; Renrui Zhang &middot; Yandong Guo &middot; Shanghang Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>A fundamental objective in robot manipulation is to enable models to comprehend visual scenes and execute actions. Although existing Vision-Language-Action (VLA) models for robots can handle a range of basic tasks, they still face challenges in two areas: (1) insufficient reasoning ability to tackle complex tasks, and (2) high computational costs for VLA model fine-tuning and inference. The recently proposed state space model (SSM) known as Mamba demonstrates promising capabilities in non-trivial sequence modeling with linear inference complexity. Inspired by this, we introduce RoboMamba, an end-to-end robotic VLA model that leverages Mamba to deliver both robotic reasoning and action capabilities, while maintaining efficient fine-tuning and inference. Specifically, we first integrate the vision encoder with Mamba, aligning visual tokens with language embedding through co-training, empowering our model with visual common sense and robotic-related reasoning. To further equip RoboMamba with SE(3) pose prediction abilities, we explore an efficient fine-tuning strategy with a simple policy head. We find that once RoboMamba possesses sufficient reasoning capability, it can acquire manipulation skills with minimal fine-tuning parameters (0.1\% of the model) and time. In experiments, RoboMamba demonstrates outstanding reasoning capabilities on general and robotic evaluation benchmarks. Meanwhile, our model showcases impressive pose prediction results in both simulation and real-world experiments, achieving inference speeds 3 times faster than existing VLA models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-382" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-382', event_id='95167', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4200</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95167">DDN: Dual-domain Dynamic Normalization for Non-stationary Time Series Forecasting</a></strong></h5>


                        <p class="text-muted">
                            Tao Dai &middot; Beiliang Wu &middot; Peiyuan Liu &middot; Naiqi Li &middot; Xue Yuerong &middot; Shu-Tao Xia &middot; Zexuan Zhu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deep neural networks (DNNs) have recently achieved remarkable advancements in time series forecasting (TSF) due to their powerful ability of sequence dependence modeling. To date, existing DNN-based TSF methods still suffer from unreliable predictions for real-world data due to its non-stationarity characteristics, i.e., data distribution varies quickly over time. To mitigate this issue, several normalization methods (e.g., SAN) have recently been specifically designed by normalization in a fixed period/window in the time domain. However, these methods still struggle to capture distribution variations, due to the complex time patterns of time series in the time domain. Based on the fact that wavelet transform can decompose time series into a linear combination of different frequencies, which exhibits distribution variations with time-varying periods, we propose a novel Dual-domain Dynamic Normalization (DDN) to dynamically capture distribution variations in both time and frequency domains. Specifically, our DDN tries to eliminate the non-stationarity of time series via both frequency and time domain normalization in a sliding window way. Besides, our DDN can serve as a plug-in-play module, and thus can be easily incorporated into other forecasting models. Extensive experiments on public benchmark datasets under different forecasting models demonstrate the superiority of our DDN over other normalization methods. Code will be made available following the review process.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-383" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-383', event_id='95175', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4201</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95175">Ada-MSHyper: Adaptive Multi-Scale Hypergraph Transformer for Time Series Forecasting</a></strong></h5>


                        <p class="text-muted">
                            Zongjiang Shang &middot; Ling Chen &middot; Binqing Wu &middot; Dongliang Cui
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Although transformer-based methods have achieved great success in multi-scale temporal pattern interaction modeling, two key challenges limit their further development: (1) Individual time points contain less semantic information, and leveraging attention to model pair-wise interactions may cause the information utilization bottleneck. (2) Multiple inherent temporal variations (e.g., rising, falling, and fluctuating) entangled in temporal patterns. To this end, we propose Adaptive Multi-Scale Hypergraph Transformer (Ada-MSHyper) for time series forecasting. Specifically, an adaptive hypergraph learning module is designed to provide foundations for modeling group-wise interactions, then a multi-scale interaction module is introduced to promote more comprehensive pattern interactions at different scales. In addition, a node and hyperedge constraint mechanism is introduced to cluster nodes with similar semantic information and differentiate the temporal variations within each scales. Extensive experiments on 11 real-world datasets demonstrate that Ada-MSHyper achieves state-of-the-art performance, reducing prediction errors by an average of 4.56%, 10.38%, and 4.97% in MSE for long-range, short-range, and ultra-long-range time series forecasting, respectively.  Code is available at https://github.com/shangzongjiang/Ada-MSHyper.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-384" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-384', event_id='96299', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4202</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96299">Knowledge-Empowered Dynamic Graph Network for Irregularly Sampled Medical Time Series</a></strong></h5>


                        <p class="text-muted">
                            Yicheng Luo &middot; Zhen Liu &middot; Linghao Wang &middot; Binquan Wu &middot; Junhao Zheng &middot; Qianli Ma
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Irregularly Sampled Medical Time Series (ISMTS) are commonly found in the healthcare domain, where different variables exhibit unique temporal patterns while interrelated. However, many existing methods fail to efficiently consider the differences and correlations among medical variables together, leading to inadequate capture of fine-grained features at the variable level in ISMTS. We propose Knowledge-Empowered Dynamic Graph Network (KEDGN), a graph neural network empowered by variables' textual medical knowledge, aiming to model variable-specific temporal dependencies and inter-variable dependencies in ISMTS. Specifically, we leverage a pre-trained language model to extract semantic representations for each variable from their textual descriptions of medical properties, forming an overall semantic view among variables from a medical perspective. Based on this, we allocate variable-specific parameter spaces to capture variable-specific temporal patterns and generate a complete variable graph to measure medical correlations among variables. Additionally, we employ a density-aware mechanism to dynamically adjust the variable graph at different timestamps, adapting to the time-varying correlations among variables in ISMTS. The variable-specific parameter spaces and dynamic graphs are injected into the graph convolutional recurrent network to capture intra-variable and inter-variable dependencies in ISMTS together. Experiment results on four healthcare datasets demonstrate that KEDGN significantly outperforms existing methods.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-385" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-385', event_id='96868', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4203</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96868">Causal Deciphering and Inpainting in Spatio-Temporal Dynamics via Diffusion Model</a></strong></h5>


                        <p class="text-muted">
                            Yifan Duan &middot; Jian Zhao &middot; pengcheng &middot; Junyuan Mao &middot; Hao Wu &middot; Jingyu Xu &middot; shilong wang &middot; Caoyuan Ma &middot; Kai Wang &middot; Kun Wang &middot; Xuelong Li
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Spatio-temporal (ST) prediction has garnered a De facto attention in earth sciences, such as meteorological prediction, human mobility perception. However, the scarcity of data coupled with the high expenses involved in sensor deployment results in notable data imbalances. Furthermore, models that are excessively customized and devoid of causal connections further undermine the generalizability and  interpretability. To this end, we establish a causal framework for ST predictions, termed CaPaint, which targets to identify causal regions in data and endow model with causal reasoning ability in a two-stage process. Going beyond this process, we utilize the back-door adjustment to specifically address the sub-regions identified as non-causal in the upstream phase. Specifically, we employ a novel image inpainting technique. By using a fine-tuned unconditional Diffusion Probabilistic Model (DDPM) as the generative prior, we in-fill the masks defined as environmental parts, offering the possibility of reliable extrapolation for potential data distributions. CaPaint overcomes the high complexity dilemma of optimal ST causal discovery models by reducing the data generation complexity from exponential to quasi-linear levels. Extensive experiments conducted on five real-world ST benchmarks demonstrate that integrating the CaPaint concept allows models to achieve improvements ranging from 4.3% to 77.3%. Moreover, compared to traditional mainstream ST augmenters, CaPaint underscores the potential of diffusion models in ST enhancement, offering a novel paradigm for this field. Our project is available at https://anonymous.4open.science/r/12345-DFCC.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-386" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-386', event_id='93520', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4204</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93520">Apathetic or Empathetic? Evaluating LLMs&#x27; Emotional Alignments with Humans</a></strong></h5>


                        <p class="text-muted">
                            Jen-Tse Huang &middot; Man Ho LAM &middot; Eric John Li &middot; Shujie Ren &middot; Wenxuan Wang &middot; Wenxiang Jiao &middot; Zhaopeng Tu &middot; Michael R Lyu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Evaluating Large Language Modelsâ (LLMs) anthropomorphic capabilities has become increasingly important in contemporary discourse. Utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of LLMs, i.e., how their feelings change when presented with specific situations. After a careful and comprehensive survey, we collect a dataset containing over 400 situations that have proven effective in eliciting the eight emotions central to our study. Categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide. With the human evaluation results as references, our evaluation includes seven LLMs, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as GPT-4, Mixtral-8x22B, and LLaMA-3.1. We find that, despite several misalignments, LLMs can generally respond appropriately to certain situations. Nevertheless, they fall short in alignment with the emotional behaviors of human beings and cannot establish connections between similar situations. Our collected dataset of situations, the human evaluation results, and the code of our testing framework, i.e., EmotionBench, are publicly available at https://github.com/CUHK-ARISE/EmotionBench.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-387" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-387', event_id='93232', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4205</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93232">Vision-Language Navigation with Energy-Based Policy</a></strong></h5>


                        <p class="text-muted">
                            Rui Liu &middot; Wenguan Wang &middot; Yi Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Vision-language navigation (VLN) requires an agent to execute actions following human instructions. Existing VLN models are optimized through expert demonstrations by supervised behavioural cloning or incorporating manual reward engineering. While straightforward, these efforts overlook the accumulation of errors in the Markov decision process, and struggle to match the distribution of the expert policy. Going beyond this, we propose an Energy-based Navigation Policy (ENP) to model the joint state-action distribution using an energy-based model. At each step, low energy values correspond to the state-action pairs that the expert is most likely to perform, and vice versa. Theoretically, the optimization objective is equivalent to minimizing the forward divergence between the occupancy measure of the expert and ours. Consequently, ENP learns to globally align with the expert policy by maximizing the likelihood of the actions and modeling the dynamics of the navigation states in a collaborative manner. With a variety of VLN architectures, ENP achieves promising performances on R2R, REVERIE, RxR, and R2R-CE, unleashing the power of existing VLN models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-388" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-388', event_id='93282', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4206</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93282">BAKU: An Efficient Transformer for Multi-Task Policy Learning</a></strong></h5>


                        <p class="text-muted">
                            Siddhant Haldar &middot; Zhuoran Peng &middot; Lerrel Pinto
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Training generalist agents capable of solving diverse tasks is challenging, often requiring large datasets of expert demonstrations. This is particularly problematic in robotics, where each data point requires physical execution of actions in the real world. Thus, there is a pressing need for architectures that can effectively leverage the available training data. In this work, we present BAKU, a simple transformer architecture that enables efficient learning of multi-task robot policies. BAKU builds upon recent advancements in offline imitation learning and meticulously combines observation trunks, action chunking, multi-sensory observations, and action heads to substantially improve upon prior work. Our experiments on 129 simulated tasks across LIBERO, Meta-World suite, and the Deepmind Control suite exhibit an overall 18% absolute improvement over RT-1 and MT-ACT, with a 36% improvement on the harder LIBERO benchmark. On 30 real-world manipulation tasks, given an average of just 17 demonstrations per task, BAKU achieves a 91% success rate. Videos of the robot are best viewed at baku-robot.github.io.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-389" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-389', event_id='93318', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4207</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93318">Prediction with Action: Visual Policy Learning via Joint Denoising Process</a></strong></h5>


                        <p class="text-muted">
                            Yanjiang Guo &middot; Yucheng Hu &middot; Jianke Zhang &middot; Yen-Jen Wang &middot; Xiaoyu Chen &middot; Chaochao Lu &middot; Jianyu Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion models have demonstrated remarkable capabilities in image generation tasks, including image editing and video creation, representing a good understanding of the physical world. On the other line, diffusion models have also shown promise in robotic control tasks by denoising actions, known as diffusion policy. Although the diffusion generative model and diffusion policy exhibit distinct capabilitiesâimage prediction and robotic action, respectivelyâthey technically follow similar denoising process. In robotic tasks, the ability to predict future images and generate actions is highly correlated since they share the same underlying dynamics of the physical world. Building on this insight, we introduce \textbf{PAD}, a novel visual policy learning framework that unifies image \textbf{P}rediction and robot \textbf{A}ction within a joint \textbf{D}enoising process.  Specifically, PAD utilizes Diffusion Transformers (DiT) to seamlessly integrate images and robot states, enabling the simultaneous prediction of future images and robot actions. Additionally, PAD supports co-training on both robotic demonstrations and large-scale video datasets and can be easily extended to other robotic modalities, such as depth images. PAD outperforms previous methods, achieving a significant 38.9\% relative improvement on the full Metaworld benchmark, by utilizing a single text-conditioned visual policy within a data-efficient imitation learning setting. Furthermore, PAD demonstrates superior generalization to unseen tasks in real-world robot manipulation settings with 28.0\% success rate increase compared to the strongest baseline. Videos of PAD can be found at https://sites.google.com/view/pad-paper</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-390" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-390', event_id='93535', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4208</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93535">HumanVLA: Towards Vision-Language Directed Object Rearrangement by Physical Humanoid</a></strong></h5>


                        <p class="text-muted">
                            Xinyu Xu &middot; Yizheng Zhang &middot; Yong-Lu Li &middot; Lei Han &middot; Cewu Lu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Physical Human-Scene Interaction (HSI) plays a crucial role in numerous applications.     However, existing HSI techniques are limited to specific object dynamics and privileged information, which prevents the development of more comprehensive applications.    To address this limitation, we introduce HumanVLA for general object rearrangement directed by practical vision and language.     A teacher-student framework is utilized to develop HumanVLA.    A state-based teacher policy is trained first using goal-conditioned reinforcement learning and adversarial motion prior.    Then, it is distilled into a vision-language-action model via behavior cloning.    We propose several key insights to facilitate the large-scale learning process.    To support general object rearrangement by physical humanoid, we introduce a novel Human-in-the-Room dataset encompassing various rearrangement tasks.    Through extensive experiments and analysis, we demonstrate the effectiveness of our approach.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-391" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-391', event_id='95294', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4210</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95294">Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers</a></strong></h5>


                        <p class="text-muted">
                            Lirui Wang &middot; Xinlei Chen &middot; Jialiang Zhao &middot; Kaiming He
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through heterogeneous pre-training on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20% on unseen tasks in multiple simulator benchmarks and real-world settings. See the project website (liruiw.github.io/hpt) for code and videos.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-392" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-392', event_id='95471', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4211</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95471">MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-Object Demand-driven Navigation</a></strong></h5>


                        <p class="text-muted">
                            Hongcheng Wang &middot; Peiqi Liu &middot; Wenzhe Cai &middot; Mingdong Wu &middot; Zhengyu Qian &middot; Hao Dong
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The process of satisfying daily demands is a fundamental aspect of humans' daily lives. With the advancement of embodied AI, robots are increasingly capable of satisfying human demands. Demand-driven navigation (DDN) is a task in which an agent must locate an object to satisfy a specified demand instruction, such as "I am thirsty." The previous study typically assumes that each demand instruction requires only one object to be fulfilled and does not consider individual preferences. However, the realistic human demand may involve multiple objects. In this paper, we introduce the Multi-object Demand-driven Navigation (MO-DDN) benchmark, which addresses these nuanced aspects, including multi-object search and personal preferences, thus making the MO-DDN task more reflective of real-life scenarios compared to DDN. Building upon previous work, we employ the concept of ``attribute'' to tackle this new task. However, instead of solely relying on attribute features in an end-to-end manner like DDN, we propose a modular method that involves constructing a coarse-to-fine attribute-based exploration agent (C2FAgent). Our experimental results illustrate that this coarse-to-fine exploration strategy capitalizes on the advantages of attributes at various decision-making levels, resulting in superior performance compared to baseline methods. Code and video can be found at https://sites.google.com/view/moddn.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-393" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-393', event_id='94982', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4300</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94982">Frequency-aware Generative Models for Multivariate Time Series Imputation</a></strong></h5>


                        <p class="text-muted">
                            XINYU YANG &middot; Yu Sun &middot; Yuan xiaojie &middot; Xinyang Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Missing data in multivariate time series are common issues that can affect the analysis and downstream applications.Although multivariate time series data generally consist of the trend, seasonal and residual terms, existing works mainly focus on optimizing the modeling for the first two items. However, we find that the residual term is more crucial for getting accurate fillings, since it is more related to the diverse changes of data and the biggest component of imputation errors.Therefore, in this study, we introduce frequency-domain information and design Frequency-aware Generative Models for Multivariate Time Series Imputation (FGTI). Specifically, FGTI employs a high-frequency filter to boost the residual term imputation, supplemented by a dominant-frequency filter for the trend and seasonal imputation. Cross-domain representation learning module then fuses frequency-domain insights with deep representations.Experiments over various datasets with real-world missing values show that FGTI achieves superiority in both data imputation and downstream applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-394" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-394', event_id='94642', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4301</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94642">SDformer: Similarity-driven Discrete Transformer For Time Series Generation</a></strong></h5>


                        <p class="text-muted">
                            Zhicheng Chen &middot; FENG SHIBO &middot; Zhong Zhang &middot; Xi Xiao &middot; Xingyu Gao &middot; Peilin Zhao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The superior generation capabilities of Denoised Diffusion Probabilistic Models (DDPMs) have been effectively showcased across a multitude of domains. Recently, the application of DDPMs has extended to time series generation tasks, where they have significantly outperformed other deep generative models, often by a substantial margin. However, we have discovered two main challenges with these methods: 1) the inference time is excessively long; 2) there is potential for improvement in the quality of the generated time series. In this paper, we propose a method based on discrete token modeling technique called Similarity-driven Discrete Transformer (SDformer). Specifically, SDformer utilizes a similarity-driven vector quantization method for learning high-quality discrete token representations of time series, followed by a discrete Transformer for data distribution modeling at the token level. Comprehensive experiments show that our method significantly outperforms competing approaches in terms of the generated time series quality while also ensuring a short inference time. Furthermore, without requiring retraining, SDformer can be directly applied to predictive tasks and still achieve commendable results.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-395" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-395', event_id='97561', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4302</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97561">Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models</a></strong></h5>


                        <p class="text-muted">
                            Nitzan Bitton Guetta &middot; Aviv Slobodkin &middot; Aviya Maimon &middot; Eliya Habba &middot; Royi Rassin &middot; Yonatan Bitton &middot; Idan Szpektor &middot; Amir Globerson &middot; Yuval Elovici
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Imagine observing someone scratching their arm; to understand why, additional context would be necessary. However, spotting a mosquito nearby would immediately offer a likely explanation for the personâs discomfort, thereby alleviating the need for further information. This example illustrates how subtle visual cues can challenge our cognitive skills and demonstrates the complexity of interpreting visual scenarios. To study these skills, we present Visual Riddles, a benchmark aimed to test vision and language models on visual riddles requiring commonsense and world knowledge. The benchmark comprises 400 visual riddles, each featuring a unique image created by a variety of text-to-image models, question, ground-truth answer, textual hint, and attribution. Human evaluation reveals that existing models lag significantly behind human performance, which is at 82% accuracy, with Gemini-Pro-1.5 leading with 40% accuracy. Our benchmark comes with automatic evaluation tasks to make assessment scalable. These findings underscore the potential of Visual Riddles as a valuable resource for enhancing vision and language modelsâ capabilities in interpreting complex visual scenarios. Data, code, and leaderboard are available at https://visual-riddles.github.io/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-396" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-396', event_id='93717', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4303</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93717">Task-oriented Time Series Imputation Evaluation via Generalized Representers</a></strong></h5>


                        <p class="text-muted">
                            Zhixian Wang &middot; Linxiao Yang &middot; Liang Sun &middot; Qingsong Wen &middot; Yi Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Time series analysis is widely used in many fields such as power energy, economics, and transportation, including different tasks such as forecasting, anomaly detection, classification, etc. Missing values are widely observed in these tasks, and often leading to unpredictable negative effects on existing methods, hindering their further application. In response to this situation, existing time series imputation methods mainly focus on restoring sequences based on their data characteristics, while ignoring the performance of the restored sequences in downstream tasks. Considering different requirements of downstream tasks (e.g., forecasting), this paper proposes an efficient downstream task-oriented time series imputation evaluation approach. By combining time series imputation with neural network models used for downstream tasks, the gain of different imputation strategies on downstream tasks is estimated without retraining, and the most favorable imputation value for downstream tasks is given by combining different imputation strategies according to the estimated gain.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-397" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-397', event_id='93709', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4304</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93709">UniTS: A Unified Multi-Task Time Series Model</a></strong></h5>


                        <p class="text-muted">
                            Shanghua Gao &middot; Teddy Koker &middot; Owen Queen &middot; Tom Hartvigsen &middot; Theodoros Tsiligkaridis &middot; Marinka Zitnik
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Although pre-trained transformers and reprogrammed text-based LLMs have shown strong performance on time series tasks, the best-performing architectures vary widely across tasks, with most models narrowly focused on specific areas, such as time series forecasting. Unifying predictive and generative time series tasks within a single model remains challenging. We introduce UniTS, a unified multi-task time series model that utilizes task tokenization to integrate predictive and generative tasks into a single framework. UniTS employs a modified transformer block to capture universal time series representations, enabling transferability from a heterogeneous, multi-domain pre-training datasetâcharacterized by diverse dynamic patterns, sampling rates, and temporal scalesâto a wide range of downstream datasets with varied task specifications and data domains. Tested on 38 datasets across human activity sensors, healthcare, engineering, and finance, UniTS achieves superior performance compared to 12 forecasting models, 20 classification models, 18 anomaly detection models, and 16 imputation models, including adapted text-based LLMs. UniTS also demonstrates strong few-shot and prompt capabilities when applied to new domains and tasks. In single-task settings, UniTS outperforms competitive task-specialized time series models. Code and datasets are available at https://github.com/mims-harvard/UniTS.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-398" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-398', event_id='93205', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4305</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93205">Large Pre-trained time series models for cross-domain Time series analysis tasks</a></strong></h5>


                        <p class="text-muted">
                            Harshavardhan Prabhakar Kamarthi &middot; B. Aditya Prakash
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large pre-trained models have been vital in recent advancements in domains like language and vision, making model training for individual downstream tasks more efficient and provide superior performance. However, tackling time-series analysis tasks usually involves designing and training a separate model from scratch leveraging training data and domain expertise specific to the task. We tackle a significant challenge for pre-training a foundational time-series model from multi-domain time-series datasets: extracting semantically useful tokenized inputs to the model across heterogeneous time-series from different domains. We propose Large Pre-trained Time-series Models (LPTM) that introduces a novel method of adaptive segmentation that automatically identifies optimal dataset-specific segmentation strategy during pre-training. This enables LPTM to perform similar to or better than domain-specific state-of-art model when fine-tuned to different downstream time-series analysis tasks and under zero-shot settings. LPTM achieves superior forecasting and time-series classification results taking up to 40% less data and 50% less training time compared to state-of-art baselines.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-399" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-399', event_id='97606', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4306</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97606">MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models</a></strong></h5>


                        <p class="text-muted">
                            Tessa Han &middot; Aounon Kumar &middot; Chirag Agarwal &middot; Himabindu Lakkaraju
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health, patient safety, and human rights. However, there is little to no understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. To address this gap, we first define the notion of medical safety in LLMs based on the Principles of Medical Ethics set forth by the American Medical Association. We then leverage this understanding to introduce MedSafetyBench, the first benchmark dataset specifically designed to measure the medical safety of LLMs. We demonstrate the utility of MedSafetyBench by using it to evaluate and improve the medical safety of LLMs. Our results show that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using MedSafetyBench improves their medical safety. By introducing this new benchmark dataset, our work enables a systematic study of the state of medical safety in LLMs and motivates future work in this area, thereby mitigating the safety risks of LLMs in medicine.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-400" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-400', event_id='97587', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4307</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97587">The Art of Saying No: Contextual Noncompliance in Language Models</a></strong></h5>


                        <p class="text-muted">
                            Faeze Brahman &middot; Sachin Kumar &middot; Vidhisha Balachandran &middot; Pradeep Dasigi &middot; Valentina Pyatkin &middot; Abhilasha Ravichander &middot; Sarah Wiegreffe &middot; Nouha Dziri &middot; Khyathi Chandu &middot; Jack Hessel &middot; Yulia Tsvetkov &middot; Noah Smith &middot; Yejin Choi &middot; Hannaneh Hajishirzi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of ``unsafe'' queries, we posit that the scope of noncompliance should be broadened. We introduce  a comprehensive taxonomy of contextual noncompliance describing when and how models should <em>not</em> comply with user requests. Our taxonomy spans a wide range of categories including <em>incomplete</em>, <em>unsupported</em>, <em>indeterminate</em>, and <em>humanizing</em> requests (in addition to <em>unsafe</em> requests). To test noncompliance capabilities of language models, we use this taxonomy to develop a new evaluation suite of 1000 noncompliance prompts. We find that most existing models show significantly high compliance rates in certain previously understudied categories with models like GPT-4 incorrectly complying with as many as 30\% of requests.To address these gaps, we explore different training strategies using a synthetically-generated training set of requests and expected noncompliant responses. Our experiments demonstrate that while direct finetuning of instruction-tuned models can lead to both over-refusal and a decline in general capabilities, using parameter efficient methods like low rank adapters helps to strike a good balance between appropriate noncompliance and other capabilities.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-401" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-401', event_id='97431', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4308</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97431">Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs</a></strong></h5>


                        <p class="text-muted">
                            Zhao Xu &middot; Fan LIU &middot; Hao Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs. Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks.  However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. To address these issues,  we evaluate the impact of various attack settings on LLM performance and provide a baseline benchmark for jailbreak attacks, encouraging the adoption of a standardized evaluation framework. Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives. We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 320 experiments with about 50,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-402" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-402', event_id='96916', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4309</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96916">DeNetDM: Debiasing by Network Depth Modulation</a></strong></h5>


                        <p class="text-muted">
                            Silpa Vadakkeeveetil Sreelatha &middot; Adarsh Kappiyath &middot; ABHRA CHAUDHURI &middot; Anjan Dutta
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Neural networks trained on biased datasets tend to inadvertently learn spurious correlations, hindering generalization. We formally prove that (1) samples that exhibit spurious correlations lie on a lower rank manifold relative to the ones that do not; and (2) the depth of a network acts as an implicit regularizer on the rank of the attribute subspace that is encoded in its representations. Leveraging these insights, we present DeNetDM, a novel debiasing method that uses network depth modulation as a way of developing robustness to spurious correlations. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Our method requires no bias annotations or explicit data augmentation while performing on par with approaches that require either or both. We demonstrate that DeNetDM outperforms existing debiasing techniques on both synthetic and real-world datasets by 5\%. The project page is available at https://vssilpa.github.io/denetdm/.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-403" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-403', event_id='96865', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4310</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96865">BackdoorAlign: Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment</a></strong></h5>


                        <p class="text-muted">
                            Jiongxiao Wang &middot; Jiazhao LI &middot; Yiquan Li &middot; Xiangyu Qi &middot; Junjie Hu &middot; Sharon Li &middot; Patrick McDaniel &middot; Muhao Chen &middot; Bo Li &middot; Chaowei Xiao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Despite the general capabilities of Large Language Models (LLMs) like GPT-4, these models still request fine-tuning or adaptation with customized data when meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack) under the setting of Language-Model-as-a-Service (LMaaS), where the model's safety has been significantly compromised by fine-tuning on users' uploaded examples that contain just a few harmful examples. Though potential defenses have been proposed that the service providers of LMaaS can integrate safety examples into the fine-tuning dataset to reduce safety issues, such approaches require incorporating a substantial amount of data, making it inefficient. To effectively defend against the FJAttack with limited safety examples under LMaaS, we propose the Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In particular, service providers will construct prefixed safety examples with a secret prompt, acting as a "backdoor trigger". By integrating prefixed safety examples into the fine-tuning dataset, the subsequent fine-tuning process effectively acts as the "backdoor attack", establishing a strong correlation between the secret prompt and safety generations. Consequently, safe responses are ensured once service providers prepend this secret prompt ahead of any user input during inference. Our comprehensive experiments demonstrate that through the Backdoor Enhanced Safety Alignment with adding as few as 11 prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar safety performance as the original aligned models without harming the benign performance. Furthermore, we also present the effectiveness of our method in a more practical setting where the fine-tuning data consists of both FJAttack examples and the fine-tuning task data.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-404" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-404', event_id='96737', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4311</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96737">A Swiss Army Knife for Heterogeneous Federated Learning: Flexible Coupling via Trace Norm</a></strong></h5>


                        <p class="text-muted">
                            Tianchi Liao &middot; Lele Fu &middot; Jialong Chen &middot; Zhen Wang &middot; Zibin Zheng &middot; Chuan Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The heterogeneity issue in federated learning (FL) has attracted increasing attention, which is attempted to be addressed by most existing methods. Currently, due to systems and objectives heterogeneity, enabling clients to hold models of different architectures and tasks of different demands has become an important direction in FL. Most existing FL methods are based on the homogeneity assumption, namely, different clients have the same architectural models with the same tasks, which are unable to handle complex and multivariate data and tasks. To flexibly address these heterogeneity limitations, we propose a novel federated multi-task learning framework with the help of tensor trace norm, FedSAK. Specifically, it treats each client as a task and splits the local model into a feature extractor and a prediction head. Clients can flexibly choose shared structures based on heterogeneous situations and upload them to the server, which learns correlations among client models by mining model low-rank structures through tensor trace norm.Furthermore, we derive convergence and generalization bounds under non-convex settings. Evaluated on 6 real-world datasets compared to 13 advanced FL models, FedSAK demonstrates superior performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-405" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-405', event_id='95055', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4400</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95055">Conformal Prediction for Class-wise Coverage via Augmented Label Rank Calibration</a></strong></h5>


                        <p class="text-muted">
                            Yuanjie Shi &middot; Subhankar Ghosh &middot; Taha Belkhouja &middot; Jana Doppa &middot; Yan Yan
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Conformal prediction (CP) is an emerging uncertainty quantification framework that allows us to construct a prediction set to cover the true label with a pre-specified marginal or conditional probability.Although the valid coverage guarantee has been extensively studied for classification problems, CP often produces large prediction sets which may not be practically useful.This issue is exacerbated for the setting of class-conditional coverage on imbalanced classification tasks with many and/or imbalanced classes.This paper proposes the Rank Calibrated Class-conditional CP (RC3P) algorithm to reduce the prediction set sizes to achieve class-conditional coverage, where the valid coverage holds for each class.In contrast to the standard class-conditional CP (CCP) method that uniformly thresholds the class-wise conformity score for each class, the augmented label rank calibration step allows RC3P to selectively iterate this class-wise thresholding subroutine only for a subset of classes whose class-wise top-$k$ error is small.We prove that agnostic to the classifier and data distribution, RC3P achieves class-wise coverage. We also show that RC3P reduces the size of prediction sets compared to the CCP method. Comprehensive experiments on multiple real-world datasets demonstrate that RC3P achieves class-wise coverage and  $26.25\\%$ $\downarrow$ reduction in prediction set sizes on average.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-406" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-406', event_id='95078', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4401</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95078">Tree of Attacks: Jailbreaking Black-Box LLMs Automatically</a></strong></h5>


                        <p class="text-muted">
                            Anay Mehrotra &middot; Manolis Zampetakis &middot; Paul Kassianik &middot; Blaine Nelson &middot; Hyrum Anderson &middot; Yaron Singer &middot; Amin Karbasi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed <em>jailbreaks</em>. In this work, we present <em>Tree of Attacks with Pruning</em>  (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an attacker LLM to iteratively refine candidate (attack) prompts until one of the refined prompts jailbreaks the target. In addition, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks, reducing the number of queries sent to the target LLM. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the prompts. This significantly improves upon the previous state-of-the-art black-box methods for generating jailbreaks while using a smaller number of queries than them. Furthermore, TAP is also capable of jailbreaking LLMs protected by state-of-the-art <em>guardrails</em>, e.g., LlamaGuard.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-407" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-407', event_id='95089', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4402</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95089">Unlearnable 3D Point Clouds: Class-wise Transformation Is All You Need</a></strong></h5>


                        <p class="text-muted">
                            Xianlong Wang &middot; Minghui Li &middot; Wei Liu &middot; Hangtao Zhang &middot; Shengshan Hu &middot; Yechao Zhang &middot; Ziqi Zhou &middot; Hai Jin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Traditional unlearnable strategies have been proposed to prevent unauthorized users from training on the 2D image data. With more 3D point cloud data containing sensitivity information, unauthorized usage of this new type data has also become a serious concern. To address this, we propose the first integral unlearnable framework for 3D point clouds including two processes: (i) we propose an unlearnable data protection scheme, involving a class-wise setting established by a category-adaptive allocation strategy and multi-transformations assigned to samples; (ii) we propose a data restoration scheme that utilizes class-wise inverse matrix transformation, thus enabling authorized-only training for unlearnable data. This restoration process is a practical issue overlooked in most existing unlearnable literature, i.e., even authorized users struggle to gain knowledge from 3D unlearnable data. Both theoretical and empirical results (including 6 datasets, 16 models, and 2 tasks) demonstrate the effectiveness of our proposed unlearnable framework. Our code is available at https://github.com/CGCL-codes/UnlearnablePC.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-408" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-408', event_id='95147', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4403</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95147">Exploring Adversarial Robustness of Deep State Space Models</a></strong></h5>


                        <p class="text-muted">
                            Biqing Qi &middot; Yiang Luo &middot; Junqi Gao &middot; Pengfei Li &middot; Kai Tian &middot; Zhiyuan Ma &middot; Bowen Zhou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Deep State Space Models (SSMs) have proven effective in numerous task scenarios but face significant security challenges due to Adversarial Perturbations (APs) in real-world deployments. Adversarial Training (AT) is a mainstream approach to enhancing Adversarial Robustness (AR) and has been validated on various traditional DNN architectures. However, its effectiveness in improving the AR of SSMs remains unclear.While many enhancements in SSM components, such as integrating Attention mechanisms and expanding to data-dependent SSM parameterizations, have brought significant gains in Standard Training (ST) settings, their potential benefits in AT remain unexplored. To investigate this, we evaluate existing structural variants of SSMs with AT to assess their AR performance. We observe that pure SSM structures struggle to benefit from AT, whereas incorporating Attention yields a markedly better trade-off between robustness and generalization for SSMs in AT compared to other components. Nonetheless, the integration of Attention also leads to Robust Overfitting (RO) issues.To understand these phenomena, we empirically and theoretically  analyze the output error of SSMs under AP. We find that fixed-parameterized SSMs have output error bounds strictly related to their parameters, limiting their AT benefits, while input-dependent SSMs may face the problem of error explosion. Furthermore, we show that the Attention component effectively scales the output error of SSMs during training, enabling them to benefit more from AT, but at the cost of introducing RO due to its high model complexity.Inspired by this, we propose a simple and effective Adaptive Scaling (AdS) mechanism that brings AT performance close to Attention-integrated SSMs without introducing the issue of RO.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-409" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-409', event_id='95329', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4404</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95329">Are Uncertainty Quantification Capabilities of Evidential Deep Learning a Mirage?</a></strong></h5>


                        <p class="text-muted">
                            Maohao Shen &middot; Jongha (Jon) Ryu &middot; Soumya Ghosh &middot; Yuheng Bu &middot; Prasanna Sattigeri &middot; Subhro Das &middot; Gregory Wornell
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper questions the effectiveness of a modern predictive uncertainty quantification approach, called <em>evidential deep learning</em> (EDL), in which a single neural network model is trained to learn a meta distribution over the predictive distribution by minimizing a specific objective function. Despite their perceived strong empirical performance on downstream tasks, a line of recent studies by Bengs et al. identify limitations of the existing methods to conclude their learned epistemic uncertainties are unreliable, e.g., in that they are non-vanishing even with infinite data. Building on and sharpening such analysis, we 1) provide a sharper understanding of the asymptotic behavior of a wide class of EDL methods by unifying various objective functions; 2) reveal that the EDL methods can be better interpreted as an out-of-distribution detection algorithm based on energy-based-models; and  3) conduct extensive ablation studies to better assess their empirical effectiveness with real-world datasets. Through all these analyses, we conclude that even when EDL methods are empirically effective on downstream tasks, this occurs despite their poor uncertainty quantification capabilities. Our investigation suggests that incorporating model uncertainty can help EDL methods faithfully quantify uncertainties and further improve performance on representative downstream tasks, albeit at the cost of additional computational complexity.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-410" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-410', event_id='95360', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4405</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95360">Amortized Eigendecomposition for Neural Networks</a></strong></h5>


                        <p class="text-muted">
                            Tianbo Li &middot; Zekun Shi &middot; Jiaxi Zhao &middot; Min Lin
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Performing eigendecomposition during neural network training is essential for tasks such as dimensionality reduction, network compression, image denoising, and graph learning. However, eigendecomposition is computationally expensive as it is orders of magnitude slower than other neural network operations. To address this challenge, we propose a novel approach called "amortized eigendecomposition" that relaxes the exact eigendecomposition by introducing an additional loss term called eigen loss. Our approach offers significant speed improvements by replacing the computationally expensive eigendecomposition with a more affordable QR decomposition at each iteration. Theoretical analysis guarantees that the desired eigenpair is attained as optima of the eigen loss. Empirical studies on nuclear norm regularization, latent-space principal component analysis, and graphs adversarial learning demonstrate significant improvements in training efficiency while producing nearly identical outcomes to conventional approaches. This novel methodology promises to integrate eigendecomposition efficiently into neural network training, overcoming existing computational challenges and unlocking new potential for advanced deep learning applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-411" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-411', event_id='95435', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4406</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95435">Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification and Object Detection</a></strong></h5>


                        <p class="text-muted">
                            Dongsu Song &middot; Daehwa Ko &middot; Jay Hoon Jung
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>It is well known that query-based attacks tend to have relatively higher successrates in adversarial black-box attacks. While research on black-box attacks is activelybeing conducted, relatively few studies have focused on pixel attacks thattarget only a limited number of pixels. In image classification, query-based pixelattacks often rely on patches, which heavily depend on randomness and neglectthe fact that scattered pixels are more suitable for adversarial attacks. Moreover, tothe best of our knowledge, query-based pixel attacks have not been explored in thefield of object detection. To address these issues, we propose a novel pixel-basedblack-box attack called Remember and Forget Pixel Attack using ReinforcementLearning(RFPAR), consisting of two main components: the Remember and Forgetprocesses. RFPAR mitigates randomness and avoids patch dependency byleveraging rewards generated through a one-step RL algorithm to perturb pixels.RFPAR effectively creates perturbed images that minimize the confidence scoreswhile adhering to limited pixel constraints. Furthermore, we advance our proposedattack beyond image classification to object detection, where RFPAR reducesthe confidence scores of detected objects to avoid detection. Experimentson the ImageNet-1K dataset for classification show that RFPAR outperformedstate-of-the-art query-based pixel attacks. For object detection, using the MSCOCOdataset with YOLOv8 and DDQ, RFPAR demonstrates comparable mAPreduction to state-of-the-art query-based attack while requiring fewer query. Furtherexperiments on the Argoverse dataset using YOLOv8 confirm that RFPAReffectively removed objects on a larger scale dataset. Our code is available athttps://github.com/KAU-QuantumAILab/RFPAR.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-412" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-412', event_id='95814', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4407</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95814">BiScope: AI-generated Text Detection by Checking Memorization of Preceding Tokens</a></strong></h5>


                        <p class="text-muted">
                            Hanxi Guo &middot; Siyuan Cheng &middot; Xiaolong Jin &middot; Zhuo Zhang &middot; Kaiyuan Zhang &middot; Guanhong Tao &middot; Guangyu Shen &middot; Xiangyu Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Detecting text generated by Large Language Models (LLMs) is a pressing need in order to identify and prevent misuse of these powerful models in a wide range of applications, which have highly undesirable consequences such as misinformation and academic dishonesty. Given a piece of subject text, many existing detection methods work by measuring the difficulty of LLM predicting the next token in the text from their prefix. In this paper, we make a critical observation that how well the current tokenâs output logits memorizes the closely preceding input tokens also provides strong evidence. Therefore, we propose a novel bi-directional calculation method that measures the cross-entropy losses between an output logits and the ground-truth token (forward) and between the output logits and the immediately preceding input token (backward). A classifier is trained to make the final prediction based on the statistics of these losses. We evaluate our system, named BISCOPE, on texts generated by five latest commercial LLMs across five heterogeneous datasets, including both natural language and code. BISCOPE demonstrates superior detection accuracy and robustness compared to six existing baseline methods, exceeding the state-of-the-art non-commercial methodsâ detection accuracy by over 0.30 F1 score, achieving over 0.95 detection F1 score on average. It also outperforms the best commercial tool GPTZero that is based on a commercial LLM trained with an enormous volume of data. Code is available at https://github.com/MarkGHX/BiScope.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-413" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-413', event_id='95831', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4408</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95831">Noisy Label Learning with Instance-Dependent Outliers: Identifiability via Crowd Wisdom</a></strong></h5>


                        <p class="text-muted">
                            Tri Nguyen &middot; Shahana Ibrahim &middot; Xiao Fu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The generation of label noise is often modeled as a process involving a probability transition matrix (also interpreted as the <em>annotator confusion matrix</em>) imposed onto the label distribution. Under this model, learning the ``ground-truth classifier''---i.e., the classifier that can be learned if no noise was present---and the confusion matrix boils down to a model identification problem. Prior works along this line demonstrated appealing empirical performance, yet identifiability of the model was mostly established by assuming an instance-invariant confusion matrix. Having an (occasionally) instance-dependent confusion matrix across data samples is apparently more realistic, but inevitably introduces outliers to the model. Our interest lies in confusion matrix-based noisy label learning with such outliers taken into consideration. We begin with pointing out that under the model of interest, using labels produced by only one annotator is fundamentally insufficient to detect the outliers or identify the ground-truth classifier. Then, we prove that by employing a crowdsourcing strategy involving multiple annotators, a carefully designed loss function can establish the desired model identifiability under reasonable conditions. Our development builds upon a link between the noisy label model and a column-corrupted matrix factorization mode---based on which we show that crowdsourced annotations distinguish nominal data and instance-dependent outliers using a low-dimensional subspace. Experiments show that our learning scheme substantially improves outlier detection and the classifier's testing accuracy.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-414" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-414', event_id='95859', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4409</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95859">ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users</a></strong></h5>


                        <p class="text-muted">
                            Guanlin Li &middot; Kangjie Chen &middot; Shudong Zhang &middot; Jie Zhang &middot; Tianwei Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large-scale pre-trained generative models are taking the world by storm, due to their abilities in generating creative content. Meanwhile, safeguards for these generative models are developed, to protect users' rights and safety, most of which are designed for large language models. Existing methods primarily focus on jailbreak and adversarial attacks, which mainly evaluate the model's safety under malicious prompts. Recent work found that manually crafted safe prompts can unintentionally trigger unsafe generations. To further systematically evaluate the safety risks of text-to-image models, we propose a novel Automatic Red-Teaming framework, ART. Our method leverages both vision language model and large language model to establish a connection between unsafe generations and their prompts, thereby more efficiently identifying the model's vulnerabilities. With our comprehensive experiments, we reveal the toxicity of the popular open-source text-to-image models. The experiments also validate the effectiveness, adaptability, and great diversity of ART. Additionally, we introduce three large-scale red-teaming datasets for studying the safety risks associated with text-to-image models. Datasets and models can be found in https://github.com/GuanlinLee/ART.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-415" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-415', event_id='96383', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4410</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96383">Large Language Model Unlearning</a></strong></h5>


                        <p class="text-muted">
                            Yuanshun Yao &middot; Xiaojun Xu &middot; Yang Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in the standard alignment process. (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-416" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-416', event_id='96641', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4411</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96641">Improving Robustness of 3D Point Cloud Recognition from a Fourier Perspective</a></strong></h5>


                        <p class="text-muted">
                            Yibo Miao &middot; Yinpeng Dong &middot; Jinlai Zhang &middot; Lijia Yu &middot; Xiao Yang &middot; Xiao-Shan Gao
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Although 3D point cloud recognition has achieved substantial progress on standard benchmarks, the typical models are vulnerable to point cloud corruptions, leading to security threats in real-world applications. To improve the corruption robustness, various data augmentation methods have been studied, but they are mainly limited to the spatial domain. As the point cloud has low information density and significant spatial redundancy, it is challenging to analyze the effects of corruptions. In this paper, we focus on the frequency domain to observe the underlying structure of point clouds and their corruptions. Through graph Fourier transform (GFT), we observe a correlation between the corruption robustness of point cloud recognition models and their sensitivity to different frequency bands, which is measured by the GFT spectrum of the modelâs Jacobian matrix. To reduce the sensitivity and improve the corruption robustness, we propose Frequency Adversarial Training (FAT) that adopts frequency-domain adversarial examples as data augmentation to train robust point cloud recognition models against corruptions. Theoretically, we provide a guarantee of FAT on its out-of-distribution generalization performance. Empirically, we conduct extensive experiments with various network architectures to validate the effectiveness of FAT, which achieves the new state-of-the-art results.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-417" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-417', event_id='95020', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4500</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95020">DAT: Improving Adversarial Robustness via Generative Amplitude Mix-up in Frequency Domain</a></strong></h5>


                        <p class="text-muted">
                            Fengpeng Li &middot; Kemou Li &middot; Haiwei Wu &middot; Jinyu Tian &middot; Jiantao Zhou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>To protect deep neural networks (DNNs) from adversarial attacks, adversarial training (AT) is developed by incorporating adversarial examples (AEs) into model training. Recent studies show that adversarial attacks disproportionately impact the patterns within the phase of the sample's frequency spectrum---typically containing crucial semantic information---more than those in the amplitude, resulting in the model's erroneous categorization of AEs. We find that, by mixing the amplitude of training samples' frequency spectrum with those of distractor images for AT, the model can be guided to focus on phase patterns unaffected by adversarial perturbations. As a result, the model's robustness can be improved. Unfortunately, it is still challenging to select appropriate distractor images, which should mix the amplitude without affecting the phase patterns. To this end, in this paper, we propose an optimized <strong>Adversarial Amplitude Generator (AAG)</strong> to achieve a better tradeoff between improving the model's robustness and retaining phase patterns. Based on this generator, together with an efficient AE production procedure, we design a new <strong>Dual Adversarial Training (DAT)</strong> strategy. Experiments on various datasets show that our proposed DAT leads to significantly improved robustness against diverse adversarial attacks. The source code is available at https://github.com/Feng-peng-Li/DAT.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-418" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-418', event_id='94752', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4501</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94752">What If the Input is Expanded in OOD Detection?</a></strong></h5>


                        <p class="text-muted">
                            Boxuan Zhang &middot; Jianing Zhu &middot; Zengmao Wang &middot; Tongliang Liu &middot; Bo Du &middot; Bo Han
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Out-of-distribution (OOD) detection aims to identify OOD inputs from unknown classes, which is important for the reliable deployment of machine learning models in the open world. Various scoring functions are proposed to distinguish it from in-distribution (ID) data. However, existing methods generally focus on excavating the discriminative information from a single input, which implicitly limits its representation dimension. In this work, we introduce a novel perspective, i.e., employing different common corruptions on the input space, to expand that. We reveal an interesting phenomenon termed <em>confidence mutation</em>, where the confidence of OOD data can decrease significantly under the corruptions, while the ID data shows a higher confidence expectation considering the resistance of semantic features. Based on that, we formalize a new scoring method, namely, <em>Confidence aVerage</em> (CoVer), which can capture the dynamic differences by simply averaging the scores obtained from different corrupted inputs and the original ones, making the OOD and ID distributions more separable in detection tasks. Extensive experiments and analyses have been conducted to understand and verify the effectiveness of CoVer.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-419" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-419', event_id='94664', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4502</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94664">United We Stand, Divided We Fall: Fingerprinting Deep Neural Networks via Adversarial Trajectories</a></strong></h5>


                        <p class="text-muted">
                            Tianlong Xu &middot; Chen Wang &middot; Gaoyang Liu &middot; Yang Yang &middot; Kai Peng &middot; Wei Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In recent years, deep neural networks (DNNs) have witnessed extensive applications, and protecting their intellectual property (IP) is thus crucial. As a non-invasive way for model IP protection, model fingerprinting has become popular. However, existing single-point based fingerprinting methods are highly sensitive to the changes in the decision boundary, and may suffer from the misjudgment of the resemblance of sparse fingerprinting, yielding high false positives of innocent models. In this paper, we propose ADV-TRA, a more robust fingerprinting scheme that utilizes adversarial trajectories to verify the ownership of DNN models. Benefited from the intrinsic progressively adversarial level, the trajectory is capable of tolerating greater degree of alteration in decision boundaries. We further design novel schemes to generate a surface trajectory that involves a series of fixed-length trajectories with dynamically adjusted step sizes. Such a design enables a more unique and reliable fingerprinting with relatively low querying costs. Experiments on three datasets against four types of removal attacks show that ADV-TRA exhibits superior performance in distinguishing between infringing and innocent models, outperforming the state-of-the-art comparisons.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-420" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-420', event_id='94553', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4503</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94553">Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control</a></strong></h5>


                        <p class="text-muted">
                            Yuxin Xiao &middot; Wan Chaoqun &middot; Yonggang Zhang &middot; Wenxiao Wang &middot; Binbin Lin &middot; Xiaofei He &middot; Xu Shen &middot; Jieping Ye
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>As the development and application of Large Language Models (LLMs) continue to advance rapidly, enhancing their trustworthiness and aligning them with human preferences has become a critical area of research. Traditional methods rely heavily on extensive data for Reinforcement Learning from Human Feedback (RLHF), but representation engineering offers a new, training-free approach. This technique leverages semantic features to control the representation of LLM's intermediate hidden states, enabling the model to meet specific requirements such as increased honesty or heightened safety awareness. However, a significant challenge arises when attempting to fulfill multiple requirements simultaneously. It proves difficult to encode various semantic contents, like honesty and safety, into a singular semantic feature, restricting its practicality.In this work, we address this challenge through Sparse Activation Control. By delving into the intrinsic mechanisms of LLMs, we manage to identify and pinpoint modules that are closely related to specific tasks within the model, i.e. attention heads. These heads display sparse characteristics that allow for near-independent control over different tasks. Our experiments, conducted on the open-source Llama series models, have yielded encouraging results. The models were able to align with human preferences on issues of safety, factualness, and bias concurrently.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-421" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-421', event_id='94486', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4504</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94486">Bridging Multicalibration and Out-of-distribution Generalization Beyond Covariate Shift</a></strong></h5>


                        <p class="text-muted">
                            Jiayun Wu &middot; Jiashuo Liu &middot; Peng Cui &middot; Steven Wu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We establish a new model-agnostic optimization framework for out-of-distribution generalization via multicalibration, a criterion that ensures a predictor is calibrated across a family of overlapping groups. Multicalibration is shown to be associated with robustness of statistical inference under covariate shift. We further establish a link between multicalibration and robustness for prediction tasks both under and beyond covariate shift. We accomplish this by extending multicalibration to incorporate grouping functions that consider covariates and labels jointly. This leads to an equivalence of the extended multicalibration and invariance, an objective for robust learning in existence of concept shift. We show a linear structure of the grouping function class spanned by density ratios, resulting in a unifying framework for robust learning by designing specific grouping functions. We propose MC-Pseudolabel, a post-processing algorithm to achieve both extended multicalibration and out-of-distribution generalization. The algorithm, with lightweight hyperparameters and optimization through a series of supervised regression steps, achieves superior performance on real-world datasets with distribution shift.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-422" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-422', event_id='94477', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4505</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94477">Fast yet Safe: Early-Exiting with Risk Control</a></strong></h5>


                        <p class="text-muted">
                            Metod Jazbec &middot; Alexander Timans &middot; Tin HadÅ¾i VeljkoviÄ &middot; Kaspar Sakmann &middot; Dan Zhang &middot; Christian Andersson Naesseth &middot; Eric Nalisnick
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Scaling machine learning models significantly improves their performance.  However, such gains come at the cost of inference being slow and resource-intensive.  Early-exit neural networks (EENNs) offer a promising solution: they accelerate inference by allowing intermediate layers to exit and produce a prediction early.  Yet a fundamental issue with EENNs is how to determine when to exit without severely degrading performance.  In other words, when is it 'safe' for an EENN to go 'fast'?  To address this issue, we investigate how to adapt frameworks of risk control to EENNs. Risk control offers a distribution-free, post-hoc solution that tunes the EENN's exiting mechanism so that exits only occur when the output is of sufficient quality.  We empirically validate our insights on a range of vision and language tasks, demonstrating that risk control can produce substantial computational savings, all the while preserving user-specified performance goals.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-423" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-423', event_id='94219', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4506</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94219">TARP-VP: Towards Evaluation of Transferred  Adversarial Robustness and Privacy on Label  Mapping Visual Prompting Models</a></strong></h5>


                        <p class="text-muted">
                            Zhen Chen &middot; Yi Zhang &middot; Fu Wang &middot; Xingyu Zhao &middot; Xiaowei Huang &middot; Wenjie Ruan
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Adversarial robustness and privacy of deep learning (DL) models are two widely studied topics in AI security. Adversarial training (AT) is an effective approach to improve the robustness of DL models against adversarial attacks. However, while models with AT demonstrate enhanced robustness, they become more susceptible to membership inference attacks (MIAs), thus increasing the risk of privacy leakage. This indicates a negative trade-off between adversarial robustness and privacy in general deep learning models. Visual prompting is a novel model reprogramming (MR) technique used for fine-tuning pre-trained models, achieving good performance in vision tasks, especially when combined with the label mapping technique. However, the performance of label-mapping-based visual prompting (LM-VP) under adversarial attacks and MIAs lacks evaluation. In this work, we regard the MR of LM-VP as a unified entity, referred to as the LM-VP model, and take a step toward jointly evaluating the adversarial robustness and privacy of LM-VP models. Experimental results show that the choice of pre-trained models significantly affects the white-box adversarial robustness of LM-VP, and standard AT even substantially degrades its performance. In contrast, transfer AT-trained LM-VP achieves a good trade-off between transferred adversarial robustness and privacy, a finding that has been consistently validated across various pre-trained models. Code is available at https://github.com/TrustAI/TARP-VP.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-424" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-424', event_id='94059', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4507</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94059">Embedding Trajectory for Out-of-Distribution Detection in Mathematical Reasoning</a></strong></h5>


                        <p class="text-muted">
                            Yiming Wang &middot; Pei Zhang &middot; Baosong Yang &middot; Derek Wong &middot; Zhuosheng Zhang &middot; Rui Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Real-world data deviating from the independent and identically distributed (\textit{i.i.d.}) assumption of in-distribution training data poses security threats to deep networks, thus advancing out-of-distribution (OOD) detection algorithms. Detection methods in generative language models (GLMs) mainly focus on uncertainty estimation and embedding distance measurement, with the latter proven to be most effective in traditional linguistic tasks like summarization and translation. However, another complex generative scenario mathematical reasoning poses significant challenges to embedding-based methods due to its high-density feature of output spaces, but this feature causes larger discrepancies in the embedding shift trajectory between different samples in latent spaces. Hence, we propose a trajectory-based method TV score, which uses trajectory volatility for OOD detection in mathematical reasoning. Experiments show that our method outperforms all traditional algorithms on GLMs under mathematical reasoning scenarios and can be extended to more applications with high-density features in output spaces, such as multiple-choice questions.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-425" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-425', event_id='93490', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4508</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93490">Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense</a></strong></h5>


                        <p class="text-muted">
                            Rui Min &middot; Zeyu Qin &middot; Nevin L. Zhang &middot; Li Shen &middot; Minhao Cheng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Backdoor attacks pose a significant threat to Deep Neural Networks (DNNs) as they allow attackers to manipulate model predictions with backdoor triggers. To address these security vulnerabilities, various backdoor purification methods have been proposed to purify compromised models. Typically, these purified models exhibit low Attack Success Rates (ASR), rendering them resistant to backdoored inputs. However, \textit{Does achieving a low ASR through current safety purification methods truly eliminate learned backdoor features from the pretraining phase?} In this paper, we provide an affirmative answer to this question by thoroughly investigating the \textit{Post-Purification Robustness} of current backdoor purification methods. We find that current safety purification methods are vulnerable to the rapid re-learning of backdoor behavior, even when further fine-tuning of purified models is performed using a very small number of poisoned samples. Based on this, we further propose the practical Query-based Reactivation Attack (QRA) which could effectively reactivate the backdoor by merely querying purified models. We find the failure to achieve satisfactory post-purification robustness stems from the insufficient deviation of purified models from the backdoored model along the backdoor-connected path. To improve the post-purification robustness, we propose a straightforward tuning defense, Path-Aware Minimization (PAM), which promotes deviation along backdoor-connected paths with extra model updates. Extensive experiments demonstrate that PAM significantly improves post-purification robustness while maintaining a good clean accuracy and low ASR. Our work provides a new perspective on understanding the effectiveness of backdoor safety tuning and highlights the importance of faithfully assessing the model's safety.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-426" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-426', event_id='93290', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4509</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93290">Protecting Your LLMs with Information Bottleneck</a></strong></h5>


                        <p class="text-muted">
                            Zichuan Liu &middot; Zefan Wang &middot; Linjie Xu &middot; Jinyu Wang &middot; Lei Song &middot; Tianchun Wang &middot; Chunlin Chen &middot; Wei Cheng &middot; Jiang Bian
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content.Despite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts.To address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions.The IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor, preserving only essential information for the target LLMs to respond with the expected answer.Moreover, we further consider a situation where the gradient is not visible to be compatible with any LLM.Our empirical evaluations show that IBProtector outperforms current defense methods in mitigating jailbreak attempts, without overly affecting response quality or inference speed. Its effectiveness and adaptability across various attack methods and target LLMs underscore the potential of IBProtector as a novel, transferable defense that bolsters the security of LLMs without requiring modifications to the underlying models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-427" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-427', event_id='93223', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4510</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93223">Learning from Uncertain Data: From Possible Worlds to Possible Models</a></strong></h5>


                        <p class="text-muted">
                            Jiongli Zhu &middot; Su Feng &middot; Boris Glavic &middot; Babak Salimi
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>We introduce an efficient method for learning linear models from uncertain data, where uncertainty is represented as a set of possible variations in the data, leading to predictive multiplicity. Our approach leverages abstract interpretation and zonotopes, a type of convex polytope, to compactly represent these dataset variations, enabling the symbolic execution of gradient descent on all possible worlds simultaneously. We develop techniques to ensure that this process converges to a fixed point and derive closed-form solutions for this fixed point. Our method provides sound over-approximations of all possible optimal models and viable prediction ranges. We demonstrate the effectiveness of our approach through theoretical and empirical analysis, highlighting its potential to reason about model and prediction uncertainty due to data quality issues in training data.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-428" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-428', event_id='93192', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4511</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93192">Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature</a></strong></h5>


                        <p class="text-muted">
                            Tong Zhou &middot; Xuandong Zhao &middot; Xiaolin Xu &middot; Shaolei Ren
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially misattributing blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy. Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-429" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-429', event_id='96862', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4600</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96862">Multilingual Diversity Improves Vision-Language Representations</a></strong></h5>


                        <p class="text-muted">
                            Thao Nguyen &middot; Matthew Wallingford &middot; Sebastin Santy &middot; Wei-Chiu Ma &middot; Sewoong Oh &middot; Ludwig Schmidt &middot; Pang Wei Koh &middot; Ranjay Krishna
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Massive web-crawled image-text datasets lay the foundation for recent progress in multimodal learning. These datasets are designed with the goal of training a model to do well on standard computer vision benchmarks, many of which, however, have been shown to be English-centric (e.g., ImageNet). Consequently, existing data curation techniques gravitate towards using predominantly English image-text pairs and discard many potentially useful non-English samples. Our work questions this practice. Multilingual data is inherently enriching not only because it provides a gateway to learn about culturally salient concepts, but also because it depicts common concepts differently from monolingual data. We thus conduct a systematic study to explore the performance benefits of using more samples of non-English origins with respect to English vision tasks. By translating all multilingual image-text pairs from a raw web crawl to English and re-filtering them, we increase the prevalence of (translated) multilingual data in the resulting training set. Pre-training on this dataset outperforms using English-only or English-dominated datasets on ImageNet, ImageNet distribution shifts, image-English-text retrieval and on average across 38 tasks from the DataComp benchmark. On a geographically diverse task like GeoDE, we also observe improvements across all regions, with the biggest gain coming from Africa. In addition, we quantitatively show that English and non-English data are significantly different in both image and (translated) text space. We hope that our findings motivate future work to be more intentional about including multicultural and multilingual data, not just when non-English or geographically diverse tasks are involved, but to enhance model capabilities at large.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-430" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-430', event_id='96931', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Oral Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4601</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96931">Not All Tokens Are What You Need for Pretraining</a></strong></h5>


                        <p class="text-muted">
                            Zhenghao Lin &middot; Zhibin Gou &middot; Yeyun Gong &middot; Xiao Liu &middot; yelong shen &middot; Ruochen Xu &middot; Chen Lin &middot; Yujiu Yang &middot; Jian Jiao &middot; Nan Duan &middot; Weizhu Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that ''Not all tokens in a corpus are equally important for language model training''. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring training tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both data efficiency and performance of the language model pre-training.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-431" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-431', event_id='96955', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4602</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96955">MemoryFormer : Minimize Transformer Computation by Removing Fully-Connected Layers</a></strong></h5>


                        <p class="text-muted">
                            Ning Ding &middot; Yehui Tang &middot; Haochen Qin &middot; Zhenli Zhou &middot; Chao Xu &middot; Lin Li &middot; Kai Han &middot; Liao Heng &middot; Yunhe Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In order to reduce the computational complexity of large language models, great efforts have been made to to improve the efficiency of transformer models such as linear attention and flash-attention. However, the model size and corresponding computational complexity are constantly scaled up in pursuit of higher performance. In this work, we present MemoryFormer, a novel transformer architecture which significantly reduces the computational complexity (FLOPs) from a new perspective.  We eliminate nearly all the computations of the transformer model except for the necessary computation required by the multi-head attention operation. This is made possible by utilizing an alternative method for feature transformation to replace the linear projection of fully-connected layers. Specifically, we first construct a group of in-memory lookup tables that store a large amount of discrete vectors to replace the weight matrix used in linear projection. We then use a hash algorithm to retrieve a correlated subset of vectors dynamically based on the input embedding. The retrieved vectors combined together will form the output embedding, which provides an estimation of the result of matrix multiplication operation in a fully-connected layer. Compared to conducting matrix multiplication, retrieving data blocks from memory is a much cheaper operation which requires little computations. We train MemoryFormer from scratch and conduct extensive experiments on various benchmarks to demonstrate the effectiveness of the proposed model.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-432" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-432', event_id='97468', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4603</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97468">OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</a></strong></h5>


                        <p class="text-muted">
                            Tianbao Xie &middot; Danyang Zhang &middot; Jixuan Chen &middot; Xiaochuan Li &middot; Siheng Zhao &middot; Ruisheng Cao &middot; Jing Hua Toh &middot; Zhoujun Cheng &middot; Dongchan Shin &middot; Fangyu Lei &middot; Yitao Liu &middot; Yiheng Xu &middot; Shuyan Zhou &middot; Silvio Savarese &middot; Caiming Xiong &middot; Victor Zhong &middot; Tao Yu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at <a href="https://os-world.github.io/">this https URL</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-433" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-433', event_id='97588', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4604</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97588">LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch</a></strong></h5>


                        <p class="text-muted">
                            Xiaoyuan Zhang &middot; Liang ZHAO &middot; Yingying Yu &middot; Xi Lin &middot; Yifan Chen &middot; Han Zhao &middot; Qingfu Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Multiobjective optimization problems (MOPs) are prevalent in machine learning, with applications in multi-task learning, learning under fairness or robustness constraints, etc. Instead of reducing multiple objective functions into a scalar objective, MOPs aim to optimize for the so-called Pareto optimality or Pareto set learning, which involves optimizing more than one objective function simultaneously, over models with millions of parameters. Existing benchmark libraries for MOPs mainly focus on evolutionary algorithms, most of which are zeroth-order methods that do not utilize higher-order information from multiple objectives and cannot scale to large-scale models with millions of parameters. In light of the above gap, this paper introduces \algoname, the first multiobjective optimization library that supports state-of-the-art gradient-based methods, provides a fair benchmark, and is open-sourced for the community.\footnote{\algoname~is available at \url{https://github.com/xzhang2523/libmoon} and can be installed via ``\texttt{pip install libmoon}''.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-434" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-434', event_id='97663', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4605</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/97663">FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Tong Wu &middot; Yinghao Xu &middot; Ryan Po &middot; Mengchen Zhang &middot; Guandao Yang &middot; Jiaqi Wang &middot; Ziwei Liu &middot; Dahua Lin &middot; Gordon Wetzstein
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advances in text-to-image generation have enabled the creation of high-quality images with diverse applications. However, accurately describing desired visual attributes can be challenging, especially for non-experts in art and photography. An intuitive solution involves adopting favorable attributes from source images. Current methods attempt to distill identity and style from source images. However, "style" is a broad concept that includes texture, color, and artistic elements, but does not cover other important attributes like lighting and dynamics. Additionally, a simplified "style" adaptation prevents combining multiple attributes from different sources into one generated image. In this work, we formulate a more effective approach to decompose the aesthetics of a picture into specific visual attributes, letting users apply characteristics like lighting, texture, and dynamics from different images. To achieve this goal, we constructed the first fine-grained visual attributes dataset (FiVA) to the best of our knowledge. This FiVA dataset features a well-organized taxonomy for visual attributes and includes 700K high-quality generated images with visual attribute annotations. Leveraging this dataset, we propose a fine-grained visual attributes adaptation framework (FiVA-Adapter) , which decouples and adapts visual attributes from one or more source images into a generated one. This approach enhances user-friendly customization, allowing users to selectively apply desired attributes to create images that meet their unique preferences and specific content requirements.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-435" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-435', event_id='98302', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4606</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/98302">Distributed Sparse Regression via Penalization</a></strong></h5>


                        <p class="text-muted">
                            Yao Ji &middot; Gesualdo Scutari &middot; Ying Sun &middot; Harsha Honnappa
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We study sparse linear regression over a network of agents, modeled as an undirected graph (with no centralized node). The estimation problem is formulated as the minimization of the sum of the local LASSO loss functions plus a quadratic penalty of the consensus constraintâthe latter being instrumental to obtain distributed solution methods. While penalty-based consensus methods have been extensively studied in the optimization literature, their statistical and computational guarantees in the high dimensional setting remain unclear. This work provides an answer to this open problem. Our contribution is two-fold. First, we establish statistical consistency of the estimator: under a suitable choice of the penalty parameter, the optimal solution of the penalized problem achieves near optimal minimax rate $O(s \log d/N)$ in $\ell_2$-loss, where $s$ is the sparsity value, $d$ is the ambient dimension, and $N$ is the total sample size in the networkâthis matches centralized sample rates. Second, we show that the proximal-gradient algorithm applied to the penalized problem, which naturally leads to distributed implementations, converges linearly up to a tolerance of the order of the centralized statistical error---the rate scales as $O(d)$, revealing an unavoidable speed-accuracy dilemma. Numerical results demonstrate the tightness of the derived sample rate and convergence rate scalings.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-436" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-436', event_id='93888', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4607</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93888">Adaptive Passive-Aggressive Framework for Online Regression with Side Information</a></strong></h5>


                        <p class="text-muted">
                            Runhao Shi &middot; Jiaxi Ying &middot; Daniel Palomar
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The Passive-Aggressive (PA) method is widely used in online regression problems for handling large-scale streaming data, typically updating model parameters in a passive-aggressive manner based on whether the error exceeds a predefined threshold. However, this approach struggles with determining optimal thresholds and adapting to complex scenarios with side information, where tracking accuracy is not the sole metric in the regression model. To address these challenges, we introduce a novel adaptive framework that allows finer adjustments to the weight vector in PA using side information. This framework adaptively selects the threshold parameter in PA, theoretically ensuring convergence to the optimal setting. Additionally, we present an efficient implementation of our algorithm that significantly reduces computational complexity. Numerical experiments show that our model achieves outstanding performance associated with the side information while maintaining low tracking error, demonstrating marked improvements over traditional PA methods across various scenarios.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-437" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-437', event_id='92933', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4608</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92933">RashomonGB: Analyzing the Rashomon Effect and Mitigating Predictive Multiplicity in Gradient Boosting</a></strong></h5>


                        <p class="text-muted">
                            Hsiang Hsu &middot; Ivan Brugere &middot; Shubham Sharma &middot; Freddy Lecue &middot; Richard Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The Rashomon effect is a mixed blessing in responsible machine learning. It enhances the prospects of finding models that perform well in accuracy while adhering to ethical standards, such as fairness or interpretability. Conversely, it poses a risk to the credibility of machine decisions through predictive multiplicity. While recent studies have explored the Rashomon effect across various machine learning algorithms, its impact on gradient boosting---an algorithm widely applied to tabular datasets---remains unclear. This paper addresses this gap by systematically analyzing the Rashomon effect and predictive multiplicity in gradient boosting algorithms. We provide rigorous theoretical derivations to examine the Rashomon effect in the context of gradient boosting and offer an information-theoretic characterization of the Rashomon set. Additionally, we introduce a novel inference technique called RashomonGB to efficiently inspect the Rashomon effect in practice. On more than 20 datasets, our empirical results show that RashomonGB outperforms existing baselines in terms of improving the estimation of predictive multiplicity metrics and model selection with group fairness constraints. Lastly, we propose a framework to mitigate predictive multiplicity in gradient boosting and empirically demonstrate its effectiveness.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-438" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-438', event_id='93018', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4609</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93018">Aligning Large Language Models with Representation Editing: A Control Perspective</a></strong></h5>


                        <p class="text-muted">
                            Lingkai Kong &middot; Haorui Wang &middot; Wenhao Mu &middot; Yuanqi Du &middot; Yuchen Zhuang &middot; Yifei Zhou &middot; Yue Song &middot; Rongzhi Zhang &middot; Kai Wang &middot; Chao Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Aligning large language models (LLMs) with human objectives is crucial for real-world applications. However, fine-tuning LLMs for alignment often suffers from unstable training and requires substantial computing resources. Test-time alignment techniques, such as prompting and guided decoding, do not modify the underlying model, and their performance remains dependent on the original model's capabilities. To address these challenges, we propose aligning LLMs through representation editing. The core of our method is to view a pre-trained autoregressive LLM as a discrete-time stochastic dynamical system. To achieve alignment for specific objectives, we introduce external control signals into the state space of this language dynamical system. We train a value function directly on the hidden states according to the Bellman equation, enabling gradient-based optimization to obtain the optimal control signals at test time. Our experiments demonstrate that our method outperforms existing test-time alignment techniques while requiring significantly fewer resources compared to fine-tuning methods. Our code is available at <a href="https://github.com/Lingkai-Kong/RE-Control">https://github.com/Lingkai-Kong/RE-Control</a>.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-439" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-439', event_id='93069', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4610</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93069">FedGMark: Certifiably Robust Watermarking for Federated Graph Learning</a></strong></h5>


                        <p class="text-muted">
                            Yuxin Yang &middot; Qiang Li &middot; Yuan Hong &middot; Binghui Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Federated graph learning (FedGL) is an emerging learning paradigm to collaboratively train graph data from various clients. However, during the development and deployment of FedGL models, they are susceptible to illegal copying and model theft. Backdoor-based watermarking is a well-known method for mitigating these attacks, as it offers ownership verification to the model owner. We take the first step to protect the ownership of FedGL models via backdoor-based watermarking. Existing techniques have challenges in achieving the goal: 1) they either cannot be directly applied or yield unsatisfactory performance; 2) they are vulnerable to watermark removal attacks; and 3) they lack of formal guarantees. To address all the challenges, we propose FedGMark, the first certified robust backdoor-based watermarking for FedGL. FedGMark leverages the unique graph structure and client information in FedGL to learn customized and diverse watermarks. It also designs a novel GL architecture that facilitates defending against both the empirical and theoretically worst-case watermark removal attacks. Extensive experiments validate the promising empirical and provable watermarking performance of FedGMark. Source code is available at: https://github.com/Yuxin104/FedGMark.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-440" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-440', event_id='93172', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4611</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93172">Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection</a></strong></h5>


                        <p class="text-muted">
                            Geng Yu &middot; Jianing Zhu &middot; Jiangchao Yao &middot; Bo Han
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Out-of-distribution (OOD) detection is crucial for deploying reliable machine learning models in open-world applications. Recent advances in CLIP-based OOD detection have shown promising results via regularizing prompt tuning with OOD features extracted from ID data. However, the irrelevant context mined from ID data can be spurious due to the inaccurate foreground-background decomposition, thus limiting the OOD detection performance. In this work, we propose a novel framework, namely, \textit{Self-Calibrated Tuning (SCT)}, to mitigate this problem for effective OOD detection with only the given few-shot ID data. Specifically, SCT introduces modulating factors respectively on the two components of the original learning objective. It adaptively directs the optimization process between the two tasks during training on data with different prediction uncertainty to calibrate the influence of OOD regularization, which is compatible with many prompt tuning based OOD detection methods. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed SCT. The code is publicly available at: https://github.com/tmlr-group/SCT.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-441" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-441', event_id='96616', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4700</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96616">T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback</a></strong></h5>


                        <p class="text-muted">
                            Jiachen Li &middot; Weixi Feng &middot; Tsu-Jui Fu &middot; Xinyi Wang &middot; S Basu &middot; Wenhu Chen &middot; William Yang Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion-based text-to-video (T2V) models have achieved significant success but continue to be hampered by the slow sampling speed of their iterative sampling processes. To address the challenge, consistency models have been proposed to facilitate fast inference, albeit at the cost of sample quality. In this work, we aim to break the quality bottleneck of a video consistency model (VCM) to achieve <strong>both fast and high-quality video generation</strong>. We introduce T2V-Turbo, which integrates feedback from a mixture of differentiable reward models into the consistency distillation (CD) process of a pre-trained T2V model. Notably, we directly optimize rewards associated with single-step generations that arise naturally from computing the CD loss, effectively bypassing the memory constraints imposed by backpropagating gradients through an iterative sampling process. Remarkably, the 4-step generations from our T2V-Turbo achieve the highest total score on VBench, even surpassing Gen-2 and Pika. We further conduct human evaluations to corroborate the results, validating that the 4-step generations from our T2V-Turbo are preferred over the 50-step DDIM samples from their teacher models, representing more than a tenfold acceleration while improving video generation quality.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-442" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-442', event_id='96588', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4701</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96588">Transfer Q-star : Principled Decoding for LLM Alignment</a></strong></h5>


                        <p class="text-muted">
                            Souradip Chakraborty &middot; Soumya Suvra Ghosal &middot; Ming Yin &middot; Dinesh Manocha &middot; Mengdi Wang &middot; Amrit Singh Bedi &middot; Furong Huang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Aligning foundation models is essential for their safe and trustworthy deployment. However, traditional fine-tuning methods are computationally intensive and require updating billions of model parameters. A promising alternative, alignment via decoding, adjusts the response distribution directly without model updates to maximize a target reward $r$, thus providing a lightweight and adaptable framework for alignment. However, principled decoding methods rely on oracle access to an optimal Q-function ($Q^*$), which is often unavailable in practice. Hence, prior SoTA methods either approximate this $Q^*$ using $Q^{\pi_{\text{sft}}}$ (derived from the reference $\texttt{SFT}$ model) or rely on short-term rewards, resulting in sub-optimal decoding performance. In this work, we propose $\texttt{Transfer Q}^*$, which implicitly estimates the optimal value function for a target reward $r$ through a baseline model $\rho_{\texttt{BL}}$  aligned with a baseline reward $r_{\texttt{BL}}$ (which can be different from the target reward $r$). Theoretical analyses of $\texttt{Transfer Q}^*$ provide a rigorous characterization of its optimality, deriving an upper bound on the sub-optimality gap and identifying a hyperparameter to control the deviation from the pre-trained reference $\texttt{SFT}$ model based on user needs. Our approach significantly reduces the sub-optimality gap observed in prior SoTA methods and demonstrates superior empirical performance across key metrics such as coherence, diversity, and quality in extensive tests on several synthetic and real datasets.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-443" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-443', event_id='96357', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4702</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96357">Efficient Adversarial Training in LLMs with Continuous Attacks</a></strong></h5>


                        <p class="text-muted">
                            Sophie Xhonneux &middot; Alessandro Sordoni &middot; Stephan GÃ¼nnemann &middot; Gauthier Gidel &middot; Leo Schwinn
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on five models from different families (Gemma, Phi3, Mistral, Zephyr, Llama2) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-444" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-444', event_id='96340', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4703</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96340">GO4Align: Group Optimization for Multi-Task Alignment</a></strong></h5>


                        <p class="text-muted">
                            Jiayi Shen &middot; Qi Wang &middot; Zehao Xiao &middot; Nanne van Noord &middot; Marcel Worring
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>This paper proposes <strong>GO4Align</strong>, a multi-task optimization approach that tackles task imbalance by explicitly aligning the optimization across tasks. To achieve this, we design an adaptive group risk minimization strategy, comprising two techniques in implementation: (i) dynamical group assignment, which clusters similar tasks based on task interactions; (ii) risk-guided group indicators, which exploit consistent task correlations with risk information from previous iterations. Comprehensive experimental results on diverse benchmarks demonstrate our method's performance superiority with even lower computational costs.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-445" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-445', event_id='96219', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4704</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96219">SLowcalSGD : Slow Query Points Improve Local-SGD for Stochastic Convex Optimization</a></strong></h5>


                        <p class="text-muted">
                            Tehila Dahan &middot; Kfir Y. Levy
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We consider  distributed learning scenarios where $M$ machines interact with a parameter server along several communication rounds in order to minimize a joint objective function. Focusing on the heterogeneous case, where different machines may draw samples from different data-distributions, we design the first local update method that provably benefits over the two most prominent distributed baselines: namely Minibatch-SGD and Local-SGD. Key to our approach is a slow querying technique  that we customize to the distributed setting, which in turn enables a better mitigation of the bias caused by local updates.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-446" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-446', event_id='96151', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4705</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96151">Neuro-Symbolic Data Generation for Math Reasoning</a></strong></h5>


                        <p class="text-muted">
                            Zenan Li &middot; Zhi Zhou &middot; Yuan Yao &middot; Xian Zhang &middot; Yu-Feng Li &middot; Chun Cao &middot; Fan Yang &middot; Xiaoxing Ma
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>A critical question about Large Language Models (LLMs) is whether their apparent deficiency in mathematical reasoning is inherent, or merely a result of insufficient exposure to high-quality mathematical data. To explore this, we developed an automated method for generating high-quality, supervised mathematical datasets. The method carefully mutates existing math problems, ensuring both diversity and validity of the newly generated problems. This is achieved by a neuro-symbolic data generation framework combining the intuitive informalization strengths of LLMs, and the precise symbolic reasoning of math solvers along with projected Markov chain Monte Carlo sampling in the highly-irregular symbolic space.Empirical experiments demonstrate the high quality of data generated by the proposed method, and that the LLMs, specifically LLaMA-2 and Mistral, when realigned with the generated data, surpass their state-of-the-art counterparts.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-447" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-447', event_id='96074', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4706</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96074">Make Continual Learning Stronger via C-Flat</a></strong></h5>


                        <p class="text-muted">
                            Ang Bian &middot; Wei Li &middot; Hangjie Yuan &middot; yu chengrong &middot; Mang Wang &middot; Zixiang Zhao &middot; Aojun Lu &middot; Pengliang Ji &middot; Tao Feng
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>How to balance the learning âsensitivity-stabilityâ upon new task training and memory preserving is critical in CL to resolve catastrophic forgetting. Improving model generalization ability within each learning phase is one solution to help CL learning overcome the gap in the joint knowledge space. Zeroth-order loss landscape sharpness-aware minimization is a strong training regime improving model generalization in transfer learning compared with optimizer like SGD. It has also been introduced into CL to improve memory representation or learning efficiency. However, zeroth-order sharpness alone could favors sharper over flatter minima in certain scenarios, leading to a rather sensitive minima rather than a global optima. To further enhance learning stability, we propose a Continual Flatness (C-Flat) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer and flat minima based CL approaches is presented in this paper, showing that our method can boost CL performance in almost all cases. Code is available at https://github.com/WanNaa/C-Flat.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-448" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-448', event_id='95963', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4707</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95963">Edit Distance Robust Watermarks via Indexing Pseudorandom Codes</a></strong></h5>


                        <p class="text-muted">
                            Noah Golowich &middot; Ankur Moitra
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Motivated by the problem of detecting AI-generated text, we consider the problem of watermarking the output of language models with provable guarantees. We aim for watermarks which satisfy: (a) undetectability, a cryptographic notion introduced by Christ, Gunn, &amp; Zamir (2023) which stipulates that it is computationally hard to distinguish watermarked language model outputs from the model's actual output distribution; and (b) robustness to channels which introduce a constant fraction of adversarial insertions, substitutions, and deletions to the watermarked text. Earlier schemes could only handle stochastic substitutions and deletions, and thus we are aiming for a more natural and appealing robustness guarantee that holds with respect to edit distance.  Our main result is a watermarking scheme which achieves both (a) and (b) when the alphabet size for the language model is allowed to grow as a polynomial in the security parameter. To derive such a scheme, we follow an approach introduced by Christ &amp; Gunn (2024), which proceeds via first constructing pseudorandom codes satisfying undetectability and robustness properties analogous to those above; our codes have the additional benefit of relying on weaker computational assumptions than used in previous work.   Then we show that there is a generic transformation from such codes over large alphabets to watermarking schemes for arbitrary language models.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-449" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-449', event_id='95501', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4708</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95501">A Modular Conditional Diffusion Framework for Image Reconstruction</a></strong></h5>


                        <p class="text-muted">
                            Magauiya Zhussip &middot; Iaroslav Koshelev &middot; Stamatios Lefkimmiatis
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Diffusion Probabilistic Models (DPMs) have been recently utilized to deal with various blind image restoration (IR) tasks, where they have demonstrated outstanding performance in terms of perceptual quality. However, the task-specific nature of existing solutions and the excessive computational costs related to their training, make such models impractical and challenging to use for different IR tasks than those that were initially trained for. This hinders their wider adoption especially by those who lack access to powerful computational resources and vast amounts of training data. In this work we aim to address the above issues and enable the successful adoption of DPMs in practical IR-related applications. Towards this goal, we propose a modular diffusion probabilistic IR framework (DP-IR), which allows us to combine the performance benefits of existing pre-trained state-of-the-art IR networks and generative DPMs, while it requires only the additional training of a small module (0.7M params) related to the particular IR task of interest. Moreover, the architecture of our proposed framework allows us to employ a sampling strategy that leads to at least four times reduction of neural function evaluations without any performance loss, while it can also be combined with existing acceleration techniques (e.g. DDIM). We evaluate our model on four benchmarks for the tasks of burst JDD-SR, dynamic scene deblurring, and super-resolution. Our method outperforms existing approaches in terms of perceptual quality while retaining a competitive performance in relation to fidelity metrics.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-450" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-450', event_id='95488', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4709</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95488">Learning in Markov Games with Adaptive Adversaries: Policy Regret, Fundamental Barriers, and Efficient Algorithms</a></strong></h5>


                        <p class="text-muted">
                            Thanh Nguyen-Tang &middot; Raman Arora
                        </p>

                    </div>
                    <div class="abstract">
                        <p>We study learning in a dynamically evolving environment modeled as a  Markov game between a learner and a strategic opponent that can adapt to the learner's strategies. While most existing works in Markov games focus on external regret as the  learning objective, external regret becomes inadequate when the adversaries are adaptive. In this work, we focus on \emph{policy regret} -- a counterfactual notion that aims to compete with the return that would have been attained if the learner had followed the best fixed sequence of policy, in hindsight. We show that if the opponent has unbounded memory or if it is non-stationary, then sample-efficient learning is not possible. For memory-bounded and stationary, we show that learning is still statistically hard if the set of feasible strategies for the learner is exponentially large. To guarantee learnability, we introduce a new notion of \emph{consistent} adaptive adversaries, wherein, the adversary responds similarly to similar strategies of the learner. We provide algorithms that achieve $\sqrt{T}$ policy regret against memory-bounded, stationary, and consistent adversaries.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-451" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-451', event_id='95483', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4710</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95483">Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning</a></strong></h5>


                        <p class="text-muted">
                            Wuyang Chen &middot; Jialin Song &middot; Pu Ren &middot; Shashank Subramanian &middot; Dmitriy Morozov &middot; Michael Mahoney
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insights for solving scientific problems based on partial differential equations (PDEs).  However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining for PDE operator learning. To reduce the need for training data with heavy simulation costs, we mine unlabeled PDE data without simulated solutions,and we pretrain neural operators with physics-inspired reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging a similarity-based method that learns in-context examples, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PDEs demonstrate that our method is highly data-efficient, more generalizable, and even outperforms conventional vision-pretrained models. We provide our code at https://github.com/delta-lab-ai/data<em>efficient</em>nopt.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-452" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-452', event_id='95449', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4711</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95449">VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation</a></strong></h5>


                        <p class="text-muted">
                            Shiwei Wu &middot; Joya Chen &middot; Kevin Qinghong Lin &middot; Qimeng Wang &middot; Yan Gao &middot; Qianli Xu &middot; Tong Xu &middot; Yao Hu &middot; Enhong Chen &middot; Mike Zheng Shou
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is that while increasing the number of vision tokens generally enhances visual understanding, it also significantly raises memory and computational costs, especially in long-term, dense video frame streaming scenarios. Although learnable approaches like Q-Former and Perceiver Resampler have been developed to reduce the vision token burden, they overlook the context causally modeled by LLMs (i.e., key-value cache), potentially leading to missed visual cues when addressing user queries. In this paper, we introduce a novel approach to reduce vision compute by leveraging redundant vision tokens ``skipping layers'' rather than decreasing the number of vision tokens. Our method, VideoLLM-MoD, is inspired by mixture-of-depths LLMs and addresses the challenge of numerous vision tokens in long-term or streaming video. Specifically, for certain transformer layer, we learn to skip the computation for a high proportion (e.g., 80\%) of vision tokens, passing them directly to the next layer. This approach significantly enhances model efficiency, achieving approximately 42% time and 30% memory savings for the entire training. Moreover, our method reduces the computation in the context and avoid decreasing the vision tokens, thus preserving or even improving performance compared to the vanilla model. We conduct extensive experiments to demonstrate the effectiveness of VideoLLM-MoD, showing its state-of-the-art results on multiple benchmarks, including narration, forecasting, and summarization tasks in COIN, Ego4D, and Ego-Exo4D datasets. The code and checkpoints will be made available at github.com/showlab/VideoLLM-online.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-453" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-453', event_id='94381', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4800</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94381">Unrolled denoising networks provably learn to perform optimal Bayesian inference</a></strong></h5>


                        <p class="text-muted">
                            Aayush Karan &middot; Kulin Shah &middot; Sitan Chen &middot; Yonina Eldar
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Much of Bayesian inference centers around the design of estimators for inverse problems which are optimal assuming the data comes from a known prior. But what do these optimality guarantees mean if the prior is unknown? In recent years, algorithm unrolling has emerged as deep learning's answer to this age-old question: design a neural network whose layers can in principle simulate iterations of inference algorithms and train on data generated by the unknown prior.  Despite its empirical success, however, it has remained unclear whether this method can provably recover the performance of its optimal, prior-aware counterparts.In this work, we prove the first rigorous learning guarantees for neural networks based on unrolling approximate message passing (AMP). For compressed sensing, we prove that when trained on data drawn from a product prior, the layers of the network approximately converge to the same denoisers used in Bayes AMP. We also provide extensive numerical experiments for compressed sensing and rank-one matrix estimation demonstrating the advantages of our unrolled architecture --- in addition to being able to obliviously adapt to general priors, it exhibits improvements over Bayes AMP in more general settings of low dimensions, non-Gaussian designs, and non-product priors.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-454" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-454', event_id='94526', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4801</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94526">BAN: Detecting Backdoors Activated by Neuron Noise</a></strong></h5>


                        <p class="text-muted">
                            Xiaoyun Xu &middot; Zhuoran Liu &middot; Stefanos Koffas &middot; Shujian Yu &middot; Stjepan Picek
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Backdoor attacks on deep learning represent a recent threat that has gained significant attention in the research community. Backdoor defenses are mainly based on backdoor inversion, which has been shown to be generic, model-agnostic, and applicable to practical threat scenarios. State-of-the-art backdoor inversion recovers a mask in the feature space to locate prominent backdoor features, where benign and backdoor features can be disentangled.  However, it suffers from high computational overhead, and we also find that it overly relies on prominent backdoor features that are highly distinguishable from benign features. To tackle these shortcomings, this paper improves backdoor feature inversion for backdoor detection by incorporating extra neuron activation information. In particular, we adversarially increase the loss of backdoored models with respect to weights to activate the backdoor effect, based on which we can easily differentiate backdoored and clean models. Experimental results demonstrate our defense, BAN, is 1.37$\times$ (on CIFAR-10) and 5.11$\times$ (on ImageNet200) more efficient with an average 9.99\% higher detect success rate than the state-of-the-art defense BTI DBF. Our code and trained models are publicly available at https://github.com/xiaoyunxxy/ban.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-455" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-455', event_id='94679', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4802</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94679">Graph-based Uncertainty Metrics for Long-form Language Model Generations</a></strong></h5>


                        <p class="text-muted">
                            Mingjian Jiang &middot; Yangjun Ruan &middot; Prasanna Sattigeri &middot; Salim Roukos &middot; Tatsunori Hashimoto
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent advancements in Large Language Models (LLMs) have significantly improved text generation capabilities, but these systems are still known to hallucinate, and granular uncertainty estimation for long-form LLM generations remains challenging. In this work, we propose Graph Uncertainty -- which represents the relationship between LLM generations and claims within them as a bipartite graph and estimates the claim-level uncertainty with a family of graph centrality metrics. Under this view, existing uncertainty estimation methods based on the concept of self-consistency can be viewed as using degree centrality as an uncertainty measure, and we show that more sophisticated alternatives such as closeness centrality provide consistent gains at claim-level uncertainty estimation.Moreover, we present uncertainty-aware decoding techniques that leverage both the graph structure and uncertainty estimates to improve the factuality of LLM generations by preserving only the most reliable claims. Compared to existing methods, our graph-based uncertainty metrics lead to an average of 6.8% relative gains on AUPRC across various long-form generation settings, and our end-to-end system provides consistent 2-4% gains in factuality over existing decoding techniques while significantly improving the informativeness of generated responses.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-456" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-456', event_id='94790', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4803</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94790">Why Transformers Need Adam: A Hessian Perspective</a></strong></h5>


                        <p class="text-muted">
                            Yushun Zhang &middot; Congliang Chen &middot; Tian Ding &middot; Ziniu Li &middot; Ruoyu Sun &middot; Zhiquan Luo
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>SGD performs worse than Adam by a significant margin on Transformers, but the reason remains unclear. In this work, we provide an explanation through the lens of Hessian: (i) Transformers are "heterogeneous'': the Hessian spectrum across parameter blocks vary dramatically, a phenomenon we call "block heterogeneity"; (ii) Heterogeneity hampers SGD: SGD performs worse than Adam on problems with block heterogeneity.  To validate (i) and (ii), we check various Transformers, CNNs, MLPs, and quadratic problems, and find that SGD can perform on par with Adam on problems without block heterogeneity, but performs worse than Adam when the heterogeneity exists. Our initial theoretical analysis indicates that SGD performs worse because it applies one single learning rate to all blocks, which cannot handle the heterogeneity among blocks. This limitation could be ameliorated if we use coordinate-wise learning rates, as designed in Adam.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-457" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-457', event_id='94801', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Spotlight Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4804</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94801">Generalized Protein Pocket Generation with Prior-Informed Flow Matching</a></strong></h5>


                        <p class="text-muted">
                            ZAIXI ZHANG &middot; Marinka Zitnik &middot; Qi Liu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Designing ligand-binding proteins, such as enzymes and biosensors, is essential in bioengineering and protein biology. One critical step in this process involves designing protein pockets, the protein interface binding with the ligand. Current approaches to pocket generation often suffer from time-intensive physical computations or template-based methods, as well as compromised generation quality due to the overlooking of domain knowledge. To tackle these challenges, we propose PocketFlow, a generative model that incorporates protein-ligand interaction priors based on flow matching. During training, PocketFlow learns to model key types of protein-ligand interactions, such as hydrogen bonds. In the sampling, PocketFlow leverages multi-granularity guidance (overall binding affinity and interaction geometry constraints) to facilitate generating high-affinity and valid pockets. Extensive experiments show that PocketFlow outperforms baselines on multiple benchmarks, e.g., achieving an average improvement of 1.29 in Vina Score and 0.05 in scRMSD. Moreover, modeling interactions make PocketFlow a generalized generative model across multiple ligand modalities, including small molecules, peptides, and RNA.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-458" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-458', event_id='94856', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4805</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94856">MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning</a></strong></h5>


                        <p class="text-muted">
                            Stella Li &middot; Vidhisha Balachandran &middot; Shangbin Feng &middot; Jonathan Ilgen &middot; Emma Pierson &middot; Pang Wei Koh &middot; Yulia Tsvetkov
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Users typically engage with LLMs interactively, yet most existing benchmarks evaluate them in a static, single-turn format, posing reliability concerns in interactive scenarios. We identify a key obstacle towards reliability: LLMs are trained to answer any question, even with incomplete context or insufficient knowledge. In this paper, we propose to change the static paradigm to an interactive one, develop systems that proactively ask questions to gather more information and respond reliably, and introduce an benchmarkâMEDIQâto evaluate question-asking ability in LLMs. MEDIQ simulates clinical interactions consisting of a Patient System and an adaptive Expert System; with potentially incomplete initial information, the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details via follow-up questions. We provide a pipeline to convert single-turn medical benchmarks into an interactive format. Our results show that directly prompting state-of-the-art LLMs to ask questions degrades performance, indicating that adapting LLMs to proactive information-seeking settings is nontrivial. We experiment with abstention strategies to better estimate model confidence and decide when to ask questions, improving diagnostic accuracy by 22.3%; however, performance still lags compared to an (unrealistic in practice) upper bound with complete information upfront. Further analyses show improved interactive performance with filtering irrelevant contexts and reformatting conversations. Overall, we introduce a novel problem towards LLM reliability, an interactive MEDIQ benchmark and a novel question-asking system, and highlight directions to extend LLMsâ information-seeking abilities in critical domains.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-459" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-459', event_id='94882', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4806</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94882">$\textit{Bifr\&quot;ost}$: 3D-Aware Image Compositing with Language Instructions</a></strong></h5>


                        <p class="text-muted">
                            Lingxiao Li &middot; Kaixiong Gong &middot; Wei-Hong Li &middot; xili dai &middot; Tao Chen &middot; Xiaojun Yuan &middot; Xiangyu Yue
                        </p>

                    </div>
                    <div class="abstract">
                        <p>This paper introduces $\textit{BifrÃ¶st}$, a novel 3D-aware framework that is built upon diffusion models to perform instruction-based image composition. Previous methods concentrate on image compositing at the 2D level, which fall short in handling complex spatial relationships ($\textit{e.g.}$, occlusion). $\textit{BifrÃ¶st}$ addresses these issues by training MLLM as a 2.5D location predictor and integrating depth maps as an extra condition during the generation process to bridge the gap between 2D and 3D, which enhances spatial comprehension and supports sophisticated spatial interactions. Our method begins by fine-tuning MLLM with a custom counterfactual dataset to predict 2.5D object locations in complex backgrounds from language instructions. Then, the image-compositing model is uniquely designed to process multiple types of input features, enabling it to perform high-fidelity image compositions that consider occlusion, depth blur, and image harmonization. Extensive qualitative and quantitative evaluations demonstrate that $\textit{BifrÃ¶st}$ significantly outperforms existing methods, providing a robust solution for generating realistically composited images in scenarios demanding intricate spatial understanding. This work not only pushes the boundaries of generative image compositing but also reduces reliance on expensive annotated datasets by effectively utilizing existing resources in innovative ways.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-460" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-460', event_id='95037', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4807</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95037">Open-Vocabulary Object Detection via Language Hierarchy</a></strong></h5>


                        <p class="text-muted">
                            Jiaxing Huang &middot; Jingyi Zhang &middot; Kai Jiang &middot; Shijian Lu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Recent studies on generalizable object detection have attracted increasing attention with additional weak supervision from large-scale datasets with image-level labels.However, weakly-supervised detection learning often suffers from image-to-box label mismatch, i.e., image-levellabels do not convey precise object information.We design Language Hierarchical Self-training (LHST) that introduces language hierarchy into weakly-supervised detector training for learning more generalizable detectors.LHST expands the image-level labels with language hierarchy and enables co-regularization between the expanded labels and self-training. Specifically, the expanded labels regularize self-training by providing richer supervision and mitigating the image-to-box label mismatch, while self-training allows assessing and selecting the expanded labels according to the predicted reliability.In addition, we design language hierarchical prompt generation that introduces language hierarchy into prompt generation which helps bridge the vocabulary gaps between training and testing.Extensive experiments show that the proposed techniques achieve superior generalization performance consistently across 14 widely studied object detection datasets.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-461" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-461', event_id='95038', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4808</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95038">Confident Natural Policy Gradient for Local Planning in  $q_\pi$-realizable Constrained MDPs</a></strong></h5>


                        <p class="text-muted">
                            Tian Tian &middot; Lin Yang &middot; Csaba Szepesvari
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The constrained Markov decision process (CMDP) framework emerges as an important reinforcement learning approach for imposing safety or other critical objectives while maximizing cumulative reward. However, the current understanding of how to learn efficiently in a CMDP environment with a potentially infinite number of states remains under investigation, particularly when function approximation is applied to the value functions. In this paper, we address the learning problem given linear function approximation with $q_{\pi}$-realizability, where the value functions of all policies are linearly representable with a known feature map, a setting known to be more general and challenging than other linear settings. Utilizing a local-access model, we propose a novel primal-dual algorithm that, after $\tilde{O}(\text{poly}(d) \epsilon^{-3})$ iterations, outputs with high probability a policy that strictly satisfies the constraints while nearly optimizing the value with respect to a reward function. Here, $d$ is the feature dimension and $\epsilon > 0$ is a given  error. The algorithm relies on a carefully crafted off-policy evaluation procedure to evaluate the policy using historical data, which informs policy updates through policy gradients and conserves samples. To our knowledge, this is the first result achieving polynomial sample complexity for CMDP in the $q_{\pi}$-realizable setting.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-462" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-462', event_id='95062', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4809</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95062">Universal Rates for Active Learning</a></strong></h5>


                        <p class="text-muted">
                            Steve Hanneke &middot; Amin Karbasi &middot; Shay Moran &middot; Grigoris Velegkas
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In this work we study the problem of actively learning binary classifiers  from a given concept class, i.e., learning by utilizing unlabeled data   and submitting targeted queries about their labels to a domain expert.  We evaluate the quality of our solutions by considering the learning curves  they induce, i.e., the rate of decrease  of the misclassification probability as the number of label queries  increases. The majority of the literature on active learning has   focused on obtaining uniform guarantees on the error rate which are  only able to explain the upper envelope of the learning curves over families  of different data-generating distributions. We diverge from this line of  work and we focus on the distribution-dependent framework of universal  learning whose goal is to obtain guarantees that hold for any fixed distribution,  but do not apply uniformly over all the distributions. We provide a   complete characterization of the optimal learning rates that are achievable  by algorithms that have to specify the number of unlabeled examples they  use ahead of their execution. Moreover, we identify combinatorial complexity  measures that give rise to each case of our tetrachotomic characterization.  This resolves an open question that was posed by Balcan et al. (2010).  As a byproduct of our main result,  we develop an active learning algorithm for partial concept classes  that achieves exponential learning rates in the uniform setting.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-463" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-463', event_id='95118', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4810</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95118">Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization</a></strong></h5>


                        <p class="text-muted">
                            Beitao Chen &middot; Xinyu Lyu &middot; Lianli Gao &middot; Hengtao Shen &middot; Jingkuan Song
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Although Large Visual Language Models (LVLMs) have demonstrated exceptional abilities in understanding multimodal data, they invariably suffer from hallucinations, leading to a disconnection between the generated text and the corresponding images. Almost all  current visual contrastive decoding methods attempt to mitigate these hallucinations by introducing visual uncertainty information that appropriately widens the contrastive logits gap between hallucinatory and targeted ones.    However, due to uncontrollable nature of the global visual uncertainty, they struggle to precisely induce the hallucinatory tokens, which severely limits their effectiveness in mitigating hallucinations and may even lead to the generation of undesired hallucinations.    To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding. Building on this insight, we introduce a novel optimization strategy named Hallucination-Induced Optimization (HIO). This strategy seeks to amplify the contrast between hallucinatory and targeted tokens relying on a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model), thereby facilitating efficient contrast decoding to alleviate hallucinations in LVLMs.    Extensive experimental research demonstrates that our HIO strategy can effectively reduce hallucinations in LVLMs, outperforming state-of-the-art methods across various benchmarks.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-464" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-464', event_id='95359', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4811</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/95359">Geometric Trajectory Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Jiaqi Han &middot; Minkai Xu &middot; Aaron Lou &middot; Haotian Ye &middot; Stefano Ermon
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Generative models have shown great promise in generating 3D geometric systems, which is a fundamental problem in many natural science domains such as molecule and protein design. However, existing approaches only operate on static structures, neglecting the fact that physical systems are always dynamic in nature. In this work, we propose geometric trajectory diffusion models (GeoTDM), the first diffusion model for modeling the temporal distribution of 3D geometric trajectories. Modeling such distribution is challenging as it requires capturing both the complex spatial interactions with physical symmetries and temporal correspondence encapsulated in the dynamics. We theoretically justify that diffusion models with equivariant temporal kernels can lead to density with desired symmetry, and  develop a novel transition kernel leveraging SE(3)-equivariant spatial convolution and temporal attention. Furthermore, to induce an expressive trajectory distribution for conditional generation, we introduce a generalized learnable geometric prior into the forward diffusion process to enhance temporal conditioning. We conduct extensive experiments on both unconditional and conditional generation in various scenarios, including physical simulation, molecular dynamics, and pedestrian motion. Empirical results on a wide suite of metrics demonstrate that GeoTDM can generate realistic geometric trajectories with significantly higher quality.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-465" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-465', event_id='94148', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4900</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/94148">Fine-Grained Dynamic Framework for Bias-Variance Joint Optimization on Data Missing Not at Random</a></strong></h5>


                        <p class="text-muted">
                            Mingming Ha &middot; Taoxuewen &middot; Wenfang Lin &middot; QIONGXU MA &middot; Wujiang Xu &middot; Linxun Chen
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>In most practical applications such as recommendation systems, display advertising, and so forth, the collected data often contains missing values and those missing values are generally missing-not-at-random, which deteriorates the prediction performance of models. Some existing estimators and regularizers attempt to achieve unbiased estimation to improve the predictive performance. However, variances and generalization bound of these methods are generally unbounded when the propensity scores tend to zero, compromising their stability and robustness. In this paper, we first theoretically reveal that limitations of regularization techniques. Besides, we further illustrate that, for more general estimators, unbiasedness will inevitably lead to unbounded variance. These general laws inspire us that the estimator designs is not merely about eliminating bias, reducing variance, or simply achieve a bias-variance trade-off. Instead, it involves a quantitative joint optimization of bias and variance. Then, we develop a systematic fine-grained dynamic learning framework to jointly optimize bias and variance, which adaptively selects an appropriate estimator for each user-item pair according to the predefined objective function. With this operation, the generalization bounds and variances of models are reduced and bounded with theoretical guarantees. Extensive experiments are conducted to verify the theoretical results and the effectiveness of the proposed dynamic learning framework.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-466" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-466', event_id='93987', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4901</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93987">Customizing Language Models with Instance-wise LoRA for Sequential Recommendation</a></strong></h5>


                        <p class="text-muted">
                            Xiaoyu Kong &middot; Jiancan Wu &middot; An Zhang &middot; Leheng Sheng &middot; Hui Lin &middot; Xiang Wang &middot; Xiangnan He
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Sequential recommendation systems predict the next interaction item based on users' past interactions, aligning recommendations with individual preferences. Leveraging the strengths of Large Language Models (LLMs) in knowledge comprehension and reasoning, recent approaches  are eager to apply LLMs to sequential recommendation. A common  paradigm is converting user behavior sequences into instruction data, and fine-tuning the LLM with parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA). However, the uniform application of LoRA across diverse user behaviors is insufficient to capture individual variability, resulting in negative transfer between disparate sequences.To address these challenges, we propose Instance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation task as a form of multi-task learning, integrating LoRA with the Mixture of Experts (MoE) framework. This approach encourages different experts to capture various aspects of user behavior. Additionally, we introduce a sequence representation guided gate function that generates customized expert participation weights for each user sequence, which allows dynamic parameter adjustment for instance-wise recommendations. In sequential recommendation, iLoRA achieves an average relative improvement of 11.4\% over basic LoRA in the hit ratio metric, with less than a 1\% relative increase in trainable parameters.Extensive experiments on three benchmark datasets demonstrate the effectiveness of iLoRA, highlighting its superior performance compared to existing methods in mitigating negative transfer and improving recommendation accuracy.Our data and code are available at https://github.com/AkaliKong/iLoRA.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-467" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-467', event_id='93946', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4902</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93946">Revisiting Score Propagation in Graph Out-of-Distribution Detection</a></strong></h5>


                        <p class="text-muted">
                            Longfei Ma &middot; Yiyou Sun &middot; Kaize Ding &middot; Zemin Liu &middot; Fei Wu
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The field of graph learning has been substantially advanced by the development of deep learning models, in particular graph neural networks. However, one salient yet largely under-explored challenge is detecting Out-of-Distribution (OOD) nodes on graphs. Prevailing OOD detection techniques developed in other domains like computer vision, do not cater to the interconnected nature of graphs. This work aims to fill this gap by exploring the potential of a simple yet effective method -- OOD score propagation, which propagates OOD scores among neighboring nodes along the graph structure. This post hoc solution can be easily integrated with existing OOD scoring functions, showcasing its excellent flexibility and effectiveness in most scenarios. However, the conditions under which score propagation proves beneficial remain not fully elucidated. Our study meticulously derives these conditions and, inspired by this discovery, introduces an innovative edge augmentation strategy with theoretical guarantee. Empirical evaluations affirm the superiority of our proposed method, outperforming strong OOD detection baselines in various scenarios and settings.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-468" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-468', event_id='93803', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4903</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93803">Human-Object Interaction Detection Collaborated with Large Relation-driven Diffusion Models</a></strong></h5>


                        <p class="text-muted">
                            Liulei Li &middot; Wenguan Wang &middot; Yi Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Prevalent human-object interaction (HOI) detection approaches typically leverage large-scale visual-linguistic models to help recognize events involving humans and objects. Though promising, models trained via contrastive learning on text-image pairs often neglect mid/low-level visual cues and struggle at compositional reasoning. In response, we introduce DIFFUSIONHOI, a new HOI detector shedding light on text-to-image diffusion models. Unlike the aforementioned models, diffusion models excel in discerning mid/low-level visual concepts as generative models, and possess strong compositionality to handle novel concepts expressed in text inputs. Considering diffusion models usually emphasize instance objects, we first devise an inversion-based strategy to learn the expression of relation patterns between humans and objects in embedding space. These learned relation embeddings then serve as textual prompts, to steer diffusion models generate images that depict specific interactions, and extract HOI-relevant cues from images without heavy finetuning. Benefited from above, DIFFUSIONHOI achieves SOTA performance on three datasets under both regular and zero-shot setups.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-469" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-469', event_id='93677', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4904</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93677">One for All: Multi-Domain Joint Training for Point Cloud Based 3D Object Detection</a></strong></h5>


                        <p class="text-muted">
                            Zhenyu Wang &middot; Ya-Li Li &middot; Hengshuang Zhao &middot; Shengjin Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The current trend in computer vision is to utilize one universal model to address all various tasks. Achieving such a universal model inevitably requires incorporating multi-domain data for joint training to learn across multiple problem scenarios. In point cloud based 3D object detection, however, such multi-domain joint training is highly challenging, because large domain gaps among point clouds from different datasets lead to the severe domain-interference problem. In this paper, we propose OneDet3D, a universal one-for-all model that addresses 3D detection across different domains, including diverse indoor and outdoor scenes, within the same framework and only one set of parameters. We propose the domain-aware partitioning in scatter and context, guided by a routing mechanism, to address the data interference issue, and further incorporate the text modality for a language-guided classification to unify the multi-dataset label spaces and mitigate the category interference issue. The fully sparse structure and anchor-free head further accommodate point clouds with significant scale disparities. Extensive experiments demonstrate the strong universal ability of OneDet3D to utilize only one trained model for addressing almost all 3D object detection tasks (Fig. 1). We will open-source the code for future research and applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-470" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-470', event_id='93624', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4905</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93624">Identify Then Recommend: Towards Unsupervised Group Recommendation</a></strong></h5>


                        <p class="text-muted">
                            Yue Liu &middot; Shihao Zhu &middot; Tianyuan Yang &middot; Jian Ma &middot; Wenliang Zhong
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Group Recommendation (GR), which aims to recommend items to groups of users, has become a promising and practical direction for recommendation systems. This paper points out two issues of the state-of-the-art GR models. (1) The pre-defined and fixed number of user groups is inadequate for real-time industrial recommendation systems, where the group distribution can shift dynamically. (2) The training schema of existing GR methods is supervised, necessitating expensive user-group and group-item labels, leading to significant annotation costs. To this end, we present a novel unsupervised group recommendation framework named $\underline{\text{I}}$dentify $\underline{\text{T}}$hen $\underline{\text{R}}$ecommend ($\underline{\text{ITR}}$), where it first identifies the user groups in an unsupervised manner even without the pre-defined number of groups, and then two pre-text tasks are designed to conduct self-supervised group recommendation. Concretely, at the group identification stage, we first estimate the adaptive density of each user point, where areas with higher densities are more likely to be recognized as group centers. Then, a heuristic merge-and-split strategy is designed to discover the user groups and decision boundaries. Subsequently, at the self-supervised learning stage, the pull-and-repulsion pre-text task is proposed to optimize the user-group distribution. Besides, the pseudo group recommendation pre-text task is designed to assist the recommendations. Extensive experiments demonstrate the superiority and effectiveness of ITR on both user recommendation (e.g., 22.22\% NDCG@5 $\uparrow$) and group recommendation (e.g., 22.95\% NDCG@5 $\uparrow$). Furthermore, we deploy ITR on the industrial recommender and achieve promising results.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-471" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-471', event_id='93371', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4906</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93371">Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models</a></strong></h5>


                        <p class="text-muted">
                            Yeming Wen &middot; Swarat Chaudhuri
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Presenting users with diverse responses from foundation models is crucial for enhancing user experience and accommodating varying preferences. However, generating multiple high-quality and diverse responses without sacrificing accuracy remains a challenge, especially when using greedy sampling. In this work, we propose a novel framework, Synthesize-Partition-Adapt (SPA), that leverages the abundant synthetic data available in many domains to elicit diverse responses from foundation models.By leveraging signal provided by data attribution methods such as influence functions, SPA partitions data into subsets, each targeting unique aspects of the data, and trains multiple model adaptations optimized for these subsets.Experimental results demonstrate the effectiveness of our approach in diversifying foundation model responses while maintaining high quality, showcased through the HumanEval and MBPP tasks in the code generation domain and several tasks in the natural language understanding domain, highlighting its potential to enrich user experience across various applications.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-472" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-472', event_id='93156', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4907</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93156">Neural Collapse Inspired Feature Alignment for Out-of-Distribution Generalization</a></strong></h5>


                        <p class="text-muted">
                            Zhikang Chen &middot; Min Zhang &middot; Sen Cui &middot; Haoxuan Li &middot; Gang Niu &middot; Mingming Gong &middot; Changshui Zhang &middot; Kun Zhang
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>The spurious correlation between the background features of the image and its label arises due to that the samples labeled with the same class in the training set often co-occurs with a specific background, which will cause the encoder to extract non-semantic features for classification, resulting in poor out-of-distribution generalization performance. Although many studies have been proposed to address this challenge, the semantic and spurious features are still difficult to accurately decouple from the original image and fail to achieve high performance with deep learning models. This paper proposes a novel perspective inspired by neural collapse to solve the spurious correlation problem through the alternate execution of environment partitioning and learning semantic masks. Specifically, we propose to assign an environment to each sample by learning a local model for each environment and using maximum likelihood probability. At the same time, we require that the learned semantic mask neurally collapses to the same simplex equiangular tight frame (ETF) in each environment after being applied to the original input. We conduct extensive experiments on four datasets, and the results demonstrate that our method significantly improves out-of-distribution performance.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-473" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-473', event_id='93095', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4908</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93095">Multi-modal Transfer Learning between Biological Foundation Models</a></strong></h5>


                        <p class="text-muted">
                            Juan Jose Garau-Luis &middot; Patrick Bordes &middot; Liam Gonzalez &middot; MaÅ¡a Roller &middot; Bernardo de Almeida &middot; Christopher Blum &middot; Lorenz Hexemer &middot; Stefan Laurent &middot; Maren Lang &middot; Thomas PIERROT &middot; Guillaume Richard
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Biological sequences encode fundamental instructions for the building blocks of life, in the form of DNA, RNA, and proteins. Modeling these sequences is key to understand disease mechanisms and is an active research area in computational biology. Recently, Large Language Models have shown great promise in solving certain biological tasks but current approaches are limited to a single sequence modality (DNA, RNA, or protein). Key problems in genomics intrinsically involve multiple modalities, but it remains unclear how to adapt general-purpose sequence models to those cases. In this work we propose a multi-modal model that connects DNA, RNA, and proteins by leveraging information from different pre-trained modality-specific encoders. We demonstrate its capabilities by applying it to the largely unsolved problem of predicting how multiple \rna transcript isoforms originate from the same gene (i.e. same DNA sequence) and map to different transcription expression levels across various human tissues. We show that our model, dubbed IsoFormer, is able to accurately predict differential transcript expression, outperforming existing methods and leveraging the use of multiple modalities. Our framework also achieves efficient transfer knowledge from the encoders pre-training as well as in between modalities. We open-source our model, paving the way for new multi-modal gene expression approaches.</p>
</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-474" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-474', event_id='93092', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4909</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/93092">Communication-Efficient Federated Group Distributionally Robust Optimization</a></strong></h5>


                        <p class="text-muted">
                            Zhishuai Guo &middot; Tianbao Yang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>Federated learning faces challenges due to the heterogeneity in data volumes and distributions at different clients, which can compromise model generalization ability to various distributions. Existing approaches to address this issue based on group distributionally robust optimization (GDRO) often lead to high communication and sample complexity.To this end, this work introduces algorithms tailored for communication-efficient Federated Group Distributionally Robust Optimization (FGDRO). Our contributions are threefold: Firstly, we introduce the FGDRO-CVaR algorithm, which optimizes the average top-K losses while reducing communication complexity to $O(1/\epsilon^4)$, where $\epsilon$ denotes the desired precision level. Secondly, our FGDRO-KL algorithm is crafted to optimize KL regularized FGDRO, cutting communication complexity to $O(1/\epsilon^3)$. Lastly, we propose FGDRO-KL-Adam to utilize Adam-type local updates in FGDRO-KL, which not only maintains a communication cost of $O(1/\epsilon^3)$ but also shows potential to surpass SGD-type local steps in practical applications.The effectiveness of our algorithms has been demonstrated on a variety of real-world tasks, including natural language processing and computer vision.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-475" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-475', event_id='92944', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4910</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/92944">Bridging Geometric States via Geometric Diffusion Bridge</a></strong></h5>


                        <p class="text-muted">
                            Shengjie Luo &middot; Yixian Xu &middot; Di He &middot; Shuxin Zheng &middot; Tie-Yan Liu &middot; Liwei Wang
                        </p>

                    </div>
                    <div class="abstract">
                        <p>The accurate prediction of geometric state evolution in complex systems is critical for advancing scientific domains such as quantum chemistry and material modeling. Traditional experimental and computational methods face challenges in terms of environmental constraints and computational demands, while current deep learning approaches still fall short in terms of precision and generality. In this work, we introduce the Geometric Diffusion Bridge (GDB), a novel generative modeling framework that accurately bridges initial and target geometric states. GDB leverages a probabilistic approach to evolve geometric state distributions, employing an equivariant diffusion bridge derived by a modified version of Doob's $h$-transform for connecting geometric states. This tailored diffusion process is anchored by initial and target geometric states as fixed endpoints and governed by equivariant transition kernels. Moreover, trajectory data can be seamlessly leveraged in our GDB framework by using a chain of equivariant diffusion bridges, providing a more detailed and accurate characterization of evolution dynamics. Theoretically, we conduct a thorough examination to confirm our framework's ability to preserve joint distributions of geometric states and capability to completely model the underlying dynamics inducing trajectory distributions with negligible error. Experimental evaluations across various real-world scenarios show that GDB surpasses existing state-of-the-art approaches, opening up a new pathway for accurately bridging geometric states and tackling crucial scientific challenges with improved accuracy and applicability.</p>
                    </div>

                </div>
            
                <div class="track-schedule-card">
                    <div None  >
                        <div class="btn-spacer float-end track-pad" style="position:relative; top:-3px">
                            
                            <div class="track-child-bookmark">

    <span id="bookmark-number-476" class="bump20 bookmark-cell fa-lg" title="Add / Remove Bookmark"
          onclick="toggle_bookmark(bookmark_id='bookmark-number-476', event_id='96599', bookmark_event_number='1',
                  alt_bookmark_element_id='');">
        
            
                <i class="fa-bookmark fa-solid solid-bookmark" style="display:none;"></i>
                <i class="fa-bookmark fa-regular regular-bookmark" ></i>
            
        
    </span>


</div>
                            <br>
                            
                                <div class="float-end small">
                                    Poster
                                </div>
                            
                            
                                <div class="small" title="Poster Position">#4911</div>
                            
                        </div>
                        

                        <h5><strong><a href="/virtual/2024/poster/96599">Grammar-Aligned Decoding</a></strong></h5>


                        <p class="text-muted">
                            Kanghee Park &middot; Jiayu Wang &middot; Taylor Berg-Kirkpatrick &middot; Nadia Polikarpova &middot; Loris D&#x27;Antoni
                        </p>

                    </div>
                    <div class="abstract">
                        <p><p>Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.</p>
</p>
                    </div>

                </div>
            
        </div>



    




    
        </div>
    

</main>
<!--END BLOCK CONTENT-->


<!--Footer for the edit button-->

    <div id="editFooter" class="noprint" style="width:70px;">

        <button class="editFooterButton btn btn-outline-primary" title="Enable editing of content where possible"
                id="editpage"
                onclick="start_edit();"><i class="far fa-edit"></i></button>
        <button class="editFooterButton btn btn-outline-primary" title="Save edited content and reload the page"
                id="noeditpage"
                onclick="stop_edit();" style="display:none"><i class="fas fa-save"></i>
        </button>
    </div>


<script>

    $(function () {
        if ($(".editable").length == 0) {
            $("#editFooter").hide();
        }
    })
</script>

<script src="/static/core/js/fastclick.min.js" ></script>

<!--We don't know if there are editable tags on the page until after the django template engine has rendered the page. So,
test in javascript for "editable" tags and if present, load the ckeditor engine dynamically. -->

<script>
    if (document.getElementsByClassName('editable').length > 0) {
        var script = document.createElement("script");
        script.type = "text/javascript";
        script.src = "/static/core/ckeditor/4.18/ckeditor.js";    // use this for linked script
        script.text = "alert('voila!');"               // use this for inline script
        document.body.appendChild(script);
    }

</script>


<script>
    function fetchContent() {
        $(".editable").each(function (index) {
            debugger
            var myself = this;
            var docvID = this.getAttribute('id').replace("id_", "");
            var blurbtext = this.getAttribute("blurbtext");
            $.ajax({
                url: "/Admin/RetrieveDocumentVersion",
                type: "POST",
                data: {
                    docvID: docvID,
                    blurbtext: blurbtext,
                    csrfmiddlewaretoken: csrftoken,
                },
                success: function (data, textStatus, jqXHR) {
                    myself.setAttribute("contenteditable", "true");
                    myself.innerHTML = data;
                    CKEDITOR.inline(myself.id);
                },
            });
        })
    }

    $("#nopageedit").hide();

    function start_edit() {

        $(".editable").addClass("warning-ring");

        //At the beginning of an edit, we need to replace the content of the
        //editable div with it's databased content in order to preserve the
        //template tags. We want the tag, not the rendered tag.

        /* You must remove any countdown.js timers on the page before replacing the page with it's
        document version otherwise, Javascript will throw an exception.  */


        $("[class$='-countdown']").parent().remove();
        fetchContent();
        $(".editable").attr("onblur", "ckeditorsave(this)");
        window.status.bold();
        window.status = "Click outside the editable area to save. Changes are LIVE!! Refresh page to discard changes.";
        $("#editpage").hide();
        $("#noeditpage").show();
    }


    function stop_edit() {
        ckeditorsave();
        $("#noeditpage").hide();
        $("#editpage").show();
        window.location.reload();
    }

    function ckeditorsave(event) {
        for (var name in CKEDITOR.instances) {
            if (CKEDITOR.instances[name].checkDirty()) {
                editor = CKEDITOR.instances[name];
                saveEditable(editor);
            }
        }
    }

    function saveEditable(editor) {
        var content = editor.getData();
        var contentId = editor.name;
        var pageId = window.location.pathname;
        var originalContent = "N/A";
        var documentversion = editor.container.getAttribute("id").replace("id_", "");
        var blurbtext = editor.container.getAttribute("blurbtext");
        if (contentId.match(/-aloha$/gi)) {
            contentId = contentId.replace(/-aloha/gi, '');
        }  /*I'm not sure what this does but it seems like it would matter*/
        var request = jQuery.ajax({
            url: "/Admin/SaveDocument",
            type: "POST",
            async: false,
            data: {
                content: content,
                originalContent: originalContent,
                contentId: contentId,
                pageId: pageId,
                documentversion: documentversion,
                blurbtext: blurbtext,
                csrfmiddlewaretoken: csrftoken
            },
            success: function (data) {
                if (data['message']) {
                    alert(data['message']);
                }
            },
            error: function (xqXHR, textStatus) {
                window.status = textStatus;
                debugger;
            }

        });

    };


</script>

<script>
    jQuery(document).ajaxSend(function (event, xhr, settings) {
        function getCookie(name) {
            var cookieValue = null;
            if (document.cookie && document.cookie != '') {
                var cookies = document.cookie.split(';');
                for (var i = 0; i < cookies.length; i++) {
                    var cookie = jQuery.trim(cookies[i]);
                    // Does this cookie string begin with the name we want?
                    if (cookie.substring(0, name.length + 1) == (name + '=')) {
                        cookieValue = decodeURIComponent(cookie.substring(name.length + 1));
                        break;
                    }
                }
            }
            return cookieValue;
        }

        function sameOrigin(url) {
            // url could be relative or scheme relative or absolute
            var host = document.location.host; // host + port
            var protocol = document.location.protocol;
            var sr_origin = '//' + host;
            var origin = protocol + sr_origin;
            // Allow absolute or scheme relative URLs to same origin
            return (url == origin || url.slice(0, origin.length + 1) == origin + '/') ||
                (url == sr_origin || url.slice(0, sr_origin.length + 1) == sr_origin + '/') ||
                // or any other URL that isn't scheme relative or absolute i.e relative.
                !(/^(\/\/|http:|https:).*/.test(url));
        }

        function safeMethod(method) {
            return (/^(GET|HEAD|OPTIONS|TRACE)$/.test(method));
        }

        if (!safeMethod(settings.type) && sameOrigin(settings.url)) {
            xhr.setRequestHeader("X-CSRFToken", getCookie('csrftoken'));
        }
    });
</script>





<div id="successful-page-load" class='hidden'>Successful Page Load</div>





    
        <link href="/static/conf_gdpr/css/conf_gdpr.css" rel="stylesheet">
        <div id="cookie-bar" style="z-index: 8">
            <table class="gdpr-statement">
                <col>
                <col style="width:120px">
                <tr>
                    <td style="padding:5px">
                        NeurIPS uses cookies for essential functions only. We do not sell your personal
                        information.
                        <a href="/public/PrivacyPolicy">Our Privacy Policy &raquo;&nbsp;</a>
                    </td>
                    <td>
                        <button float-end class="btn btn-light btn-sm btn btn-outline-dark" onClick="accept_cookies();">Accept
                            Cookies
                        </button>
                    </td>
                </tr>
            </table>
        </div>

        <script>
            function accept_cookies() {

                $.ajax({
                    method: "POST",
                    url: "/conf_gdpr/accept",
                    data: {
                        csrfmiddlewaretoken: csrftoken,
                    },
                }).done(function (data) {
                    console.log(data);
                    $("#cookie-bar").fadeOut();
                }).fail(function (jqXHR, textStatus) {
                    alert(textStatus);
                });
            }
        </script>

    







<br>
<div class="noprint">
    <footer id="bootstrap-footer" class="text-center text-lg-start bg-light text-muted noprint">

        <div class="text-center p-1 border-top border-dark">
        </div>
        <!-- Section: Links  -->
        <section class="pt-1">
            <div class="container text-center text-md-start mt-3">
                <!-- Grid row -->
                <div class="row mt-3">
                    <!-- Grid column -->
                    <div class="col-md-3 col-lg-3 col-xl-3 mx-auto mb-3">
                        <!-- Content -->
                        <h6 class="text-uppercase fw-bold mb-4">
                            <img src="/static/core/img/NeurIPS-logo.svg" alt="NeurIPS logo" height='30'>
                        </h6>
                        <p>
                            The NeurIPS Logo above may be used on presentations. Right-click and choose
                            download. It is a vector graphic and may be used at any scale.
                        </p>

                    </div>


                    <!-- Grid column -->
                    <div class="col-md-5 col-lg-4 col-xl-3 mx-auto mb-4" style="max-width: 300px;">
                        <!-- Links -->
                        <h6 class="text-uppercase fw-bold mb-4 text-center">
                            Useful links
                        </h6>
                        <div>
             <ul>
	<li><a href="/Conferences/2024/Press">Press</a></li>
	<li><a href="/Exhibitors/exhibitorinfo">Exhibitor Information</a></li>
</ul>

            </div>
                    </div>
                    <!-- Grid column -->

                    <!-- Grid column -->
                    <div class="col-md-4 col-lg-3 col-xl-3 mx-auto mb-md-0 mb-4">
                        <!-- Links -->
                        <h6 class="text-uppercase fw-bold mb-4">Contact</h6>
                        
                            <p>
                                <i class="fas fa-home me-3"></i> 1269 Law St, San Diego CA 92109
                            </p>
                        
                        <p>
                            <i class="fas fa-envelope me-3"></i> <a href="/Help/Contact">Email</a>
                        </p>
                        
                        


                    </div>
                    <!-- Grid column -->
                </div>
                <!-- Grid row -->
            </div>
        </section>
        <!-- Section: Links  -->

        <!-- Copyright -->
        <div class="text-center p-4" style="background-color: rgba(0, 0, 0, 0.05);">
            <div>
             <p><a href="https://proceedings.neurips.cc">NeurIPS Proceedings</a></p>

            </div>
        </div>
        <!-- Copyright -->
    </footer>
</div>
<!-- Footer -->


<!-- Footer -->

</body>
</html>
